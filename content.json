{"meta":{"title":"靡不有初 鲜克有终","subtitle":"","description":"","author":"黄志远","url":"http://huangzhiyuan.github.io","root":"/"},"pages":[{"title":"about","date":"2022-01-31T06:05:17.000Z","updated":"2022-01-31T06:17:47.654Z","comments":true,"path":"about/index.html","permalink":"http://huangzhiyuan.github.io/about/index.html","excerpt":"","text":"愿漂泊的人都有酒喝 愿孤独的人都会唱歌。 愿相爱的人都有未来，愿等待的人都有回答。 愿孤单的人不必永远逞强，愿逞强的人身边永远都有个肩膀。 愿肩膀可以接住你的欢喜忧伤，愿有情人永生执手相望。 愿你如阳光，明媚不忧伤；愿你如月光，明亮不清冷。 愿你历经磨难 半生归来仍是少年。"},{"title":"books","date":"2022-01-31T06:05:07.000Z","updated":"2022-01-31T06:05:07.843Z","comments":true,"path":"books/index.html","permalink":"http://huangzhiyuan.github.io/books/index.html","excerpt":"","text":""},{"title":"archives","date":"2022-01-31T06:04:00.000Z","updated":"2022-01-31T06:04:00.392Z","comments":true,"path":"archives/index.html","permalink":"http://huangzhiyuan.github.io/archives/index.html","excerpt":"","text":""},{"title":"分类","date":"2022-01-31T05:57:32.799Z","updated":"2022-01-30T08:20:40.886Z","comments":false,"path":"categories/index.html","permalink":"http://huangzhiyuan.github.io/categories/index.html","excerpt":"","text":""},{"title":"links","date":"2022-01-31T06:05:13.000Z","updated":"2022-01-31T06:05:13.118Z","comments":true,"path":"links/index.html","permalink":"http://huangzhiyuan.github.io/links/index.html","excerpt":"","text":""},{"title":"tags","date":"2022-01-31T06:03:15.000Z","updated":"2022-01-31T06:03:16.005Z","comments":true,"path":"tags/index.html","permalink":"http://huangzhiyuan.github.io/tags/index.html","excerpt":"","text":""},{"title":"repository","date":"2022-01-31T06:04:41.000Z","updated":"2022-01-31T06:04:41.818Z","comments":true,"path":"repository/index.html","permalink":"http://huangzhiyuan.github.io/repository/index.html","excerpt":"","text":""}],"posts":[{"title":"Linux性能优化实战之IO性能篇","slug":"Linux-perf-optimize-actual-combat-IO","date":"2021-05-24T16:53:03.000Z","updated":"2021-05-25T14:10:12.451Z","comments":true,"path":"2021/05/25/Linux-perf-optimize-actual-combat-IO/","link":"","permalink":"http://huangzhiyuan.github.io/2021/05/25/Linux-perf-optimize-actual-combat-IO/","excerpt":"实际上，性能优化一直都是大多数软件工程师头上的紧箍咒，甚至许多工作多年的资深工程师，有时也无法准确地分析出线上的很多性能问题。 性能分析问题为什么这么难呢？性能优化是个系统工具，常常牵一发而动全身。它涉及了从程序设计、算法分析、编程语言，再到系统、存储、网络等各种底层基础设施的方方面面。每一个组件都可能出现问题，甚至多个组件同时出现问题。毫无疑问，性能优化是软件系统中最有挑战的工作指引，也是最考验体现你综合能力的工作之一。该篇blog记录总结了倪朋飞老师在极客时间开设的[《Linux性能优化实战》][1]课程笔记。","text":"实际上，性能优化一直都是大多数软件工程师头上的紧箍咒，甚至许多工作多年的资深工程师，有时也无法准确地分析出线上的很多性能问题。 性能分析问题为什么这么难呢？性能优化是个系统工具，常常牵一发而动全身。它涉及了从程序设计、算法分析、编程语言，再到系统、存储、网络等各种底层基础设施的方方面面。每一个组件都可能出现问题，甚至多个组件同时出现问题。毫无疑问，性能优化是软件系统中最有挑战的工作指引，也是最考验体现你综合能力的工作之一。该篇blog记录总结了倪朋飞老师在极客时间开设的[《Linux性能优化实战》][1]课程笔记。","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://huangzhiyuan.github.io/tags/Linux/"}]},{"title":"Linux性能优化实战之CPU性能篇","slug":"Linux-perf-optimize-actual-combat-CPU","date":"2021-05-24T16:53:03.000Z","updated":"2021-05-28T05:26:13.649Z","comments":true,"path":"2021/05/25/Linux-perf-optimize-actual-combat-CPU/","link":"","permalink":"http://huangzhiyuan.github.io/2021/05/25/Linux-perf-optimize-actual-combat-CPU/","excerpt":"实际上，性能优化一直都是大多数软件工程师头上的紧箍咒，甚至许多工作多年的资深工程师，有时也无法准确地分析出线上的很多性能问题。 性能分析问题为什么这么难呢？性能优化是个系统工具，常常牵一发而动全身。它涉及了从程序设计、算法分析、编程语言，再到系统、存储、网络等各种底层基础设施的方方面面。每一个组件都可能出现问题，甚至多个组件同时出现问题。毫无疑问，性能优化是软件系统中最有挑战的工作指引，也是最考验体现你综合能力的工作之一。该篇blog记录总结了倪朋飞老师在极客时间开设的《Linux性能优化实战》课程笔记。","text":"实际上，性能优化一直都是大多数软件工程师头上的紧箍咒，甚至许多工作多年的资深工程师，有时也无法准确地分析出线上的很多性能问题。 性能分析问题为什么这么难呢？性能优化是个系统工具，常常牵一发而动全身。它涉及了从程序设计、算法分析、编程语言，再到系统、存储、网络等各种底层基础设施的方方面面。每一个组件都可能出现问题，甚至多个组件同时出现问题。毫无疑问，性能优化是软件系统中最有挑战的工作指引，也是最考验体现你综合能力的工作之一。该篇blog记录总结了倪朋飞老师在极客时间开设的《Linux性能优化实战》课程笔记。 如何理解“平均负载” 我们常用top或者uptime命令，来了解系统的负载情况。比如： 12zhiyuanhuang@ZhiyuandeMacBook-Pro blog % uptime22:06 up 2:40, 2 users, load averages: 2.05 2.39 2.51 分别表示当前时间、系统运行时间、正在登陆用户数。最后三个数字则分别表示过去1分钟、5分钟、15分钟的平均负载。但是平均负载究竟代表了什么？简单来说，平均负载是指单位时间内，系统处于可运行状态和不可中断转态的平均进程数，也就是平均活跃进程数，它和CPU使用率并没有直接关系。所谓可运行转态的进程，是指正在使用CPU或者正在等待CPU的进程，也即是我们用ps命令看到的R状态（running或Runnable）的的进程。不可中断的进程是指正处于内核状态关键流程中的进程，并且这些进程是不可打断的。比如最常见的是等待硬件设备的IO响应，也就是我们在ps命令中看到的D状态（Uninterruptible Sleep，也称为Disk Sleep）的进程。 比如，当一个进程向磁盘读写数据时，为了保证数据的一致性，在得到磁盘回复前，它是不能被其他进程或者中断打断的，这个时候的进程就处于不可中断的转态。否则会出现磁盘数据与进程数据不一致的情况。所以，不可中断状态实际上是系统对进程和硬件设备的一种保护机制。 既然平均的是活跃进程数，那么最理想的情况就是每个CPU上面都刚好运行着一个进程，这个利用率最高。比如平均负载为2时， 在只有2个CPU的系统上，所有的CPU都刚好被完全占用。 在4个CPU的系统上，意味着有50%的空闲。 在只有1个CPU的系统中，有一半的进程竞争不到CPU。 平均负载多少最合适 要评判平均负载，首先你要知道系统有几个CPU，这个可以通过top命令或者从文件/proc/cpuinfo获取，比如： 1$ grep &quot;model name&quot; /proc/cpuinfo | wc -l 从例子中可以看出，平均负载有三个数值，到底该参考哪一个呢？实际上都要看，三个不同时间间隔的平均值，其实给我们提供了，分析系统负载趋势的数据来源，让我们更加全面更立体了解目前的负载状况。一般来说，当平均负载高于CPU数量70%的时候就应该分析排查负载高的问题。一旦负载过高，就可能导致进程响应变慢，进而影响服务的正常功能。 平均负载与CPU使用率 现实中，经常会把平均负载和CPU使用率混淆。可能会困惑，既然平均负载代表的是活跃进程数，那平均负载高了，不就意味着CPU使用率高吗？回到平均负载的含义上来，平均负载是指在单位时间内，处于可运行状态和不可中断状态的进程数。所以，它不仅包括了正在使用的CPU的进程，还包括等待CPU和等待IO的进程。而CPU使用率，是单位时间内繁忙情况的统计，和平均负载并不一定完全对应。比如 CPU密集型进程，使用大量CPU会导致平均进程负载升高，此时两者是一致的； IO密集型进程，等待IO也会导致平均负载升高，但CPU使用率不一定很高； 大量等待CPU的进程调度也会导致平均负载升高，此时的CPU使用率也会比较高。 案例分析 工具iostat、mpstat、pidstat。其中stress是一个Linux系统压力的测试工具，这里用作异常进程模拟平均负载升高的场景。而sysstat包括了常用的Linux性能工具，用来监控和分析系统的性能。mpstat是一个常用的多核CPU性能分析工具，用来实时查看每个CPU的性能指标，以及所有CPU的平均指标。pidstat是一个常用的进程性能分析工具，用来实时查看进程的CPU、内存、IO以及上下文切换等性能指标。每种场景都需要打开3个中端。 场景一： CPU密集型场景终端1运行stress命令，模拟一个CPU使用率100%的场景： 1$ stress --cpu 1 --timeout 600 终端2运行uptime查看平均负载的变化情况： 123# -d 参数表示高亮显示变化的区域$ watch -d uptime..., load average: 1.00, 0.75, 0.39 终端3运行mpstat查看CPU使用率变化： 1234567# -P ALL 表示监控所有CPU，后面数字5表示间隔5秒后输出一组数据$ mpstat -P ALL 5Linux 4.15.0 (ubuntu) 09/22/18 _x86_64_ (2 CPU)13:30:06 CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle13:30:11 all 50.05 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 49.9513:30:11 0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0013:30:11 1 100.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 从终端二中可以看到，1 分钟的平均负载会慢慢增加到 1.00，而从终端三中还可以看到，正好有一个 CPU 的使用率为 100%，但它的 iowait 只有 0。这说明，平均负载的升高正是由于 CPU 使用率为 100%。到底是哪个进程导致了 CPU 使用率为 100% 呢？你可以使用 pidstat 来查询： 1234# 间隔5秒后输出一组数据$ pidstat -u 5 113:37:07 UID PID %usr %system %guest %wait %CPU CPU Command13:37:12 0 2962 100.00 0.00 0.00 0.00 100.00 1 stress 从这里可以明显看到，stress 进程的 CPU 使用率为 100%。 场景二： IO密集型场景首先还是运行 stress 命令，但这次模拟 I/O 压力，即不停地执行 sync： 1$ stress -i 1 --timeout 600 在第二个终端运行 uptime 查看平均负载的变化情况： 12$ watch -d uptime..., load average: 1.06, 0.58, 0.37 第三个终端运行 mpstat 查看 CPU 使用率的变化情况： 1234567# 显示所有CPU的指标，并在间隔5秒输出一组数据$ mpstat -P ALL 5 1Linux 4.15.0 (ubuntu) 09/22/18 _x86_64_ (2 CPU)13:41:28 CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle13:41:33 all 0.21 0.00 12.07 32.67 0.00 0.21 0.00 0.00 0.00 54.8413:41:33 0 0.43 0.00 23.87 67.53 0.00 0.43 0.00 0.00 0.00 7.7413:41:33 1 0.00 0.00 0.81 0.20 0.00 0.00 0.00 0.00 0.00 98.99 从这里可以看到，1 分钟的平均负载会慢慢增加到 1.06，其中一个 CPU 的系统 CPU 使用率升高到了 23.87，而 iowait 高达 67.53%。这说明，平均负载的升高是由于 iowait 的升高。那么到底是哪个进程，导致 iowait 这么高呢？我们还是用 pidstat 来查询： 12345678# 间隔5秒后输出一组数据，-u表示CPU指标$ pidstat -u 5 1Linux 4.15.0 (ubuntu) 09/22/18 _x86_64_ (2 CPU)13:42:08 UID PID %usr %system %guest %wait %CPU CPU Command13:42:13 0 104 0.00 3.39 0.00 0.00 3.39 1 kworker/1:1H13:42:13 0 109 0.00 0.40 0.00 0.00 0.40 0 kworker/0:1H13:42:13 0 2997 2.00 35.53 0.00 3.99 37.52 1 stress13:42:13 0 3057 0.00 0.40 0.00 0.00 0.40 0 pidstat 可以发现，还是 stress 进程导致的。 场景三： 大量进程的场景当系统中运行进程超出 CPU 运行能力时，就会出现等待 CPU 的进程。我们还是使用 stress，但这次模拟的是 8 个进程： 1$ stress -c 8 --timeout 600 由于系统只有 2 个 CPU，明显比 8 个进程要少得多，因而，系统的 CPU 处于严重过载状态，平均负载高达 7.97： 12$ uptime..., load average: 7.97, 5.93, 3.02 接着再运行 pidstat 来看一下进程的情况： 123456789101112# 间隔5秒后输出一组数据$ pidstat -u 5 114:23:25 UID PID %usr %system %guest %wait %CPU CPU Command14:23:30 0 3190 25.00 0.00 0.00 74.80 25.00 0 stress14:23:30 0 3191 25.00 0.00 0.00 75.20 25.00 0 stress14:23:30 0 3192 25.00 0.00 0.00 74.80 25.00 1 stress14:23:30 0 3193 25.00 0.00 0.00 75.00 25.00 1 stress14:23:30 0 3194 24.80 0.00 0.00 74.60 24.80 0 stress14:23:30 0 3195 24.80 0.00 0.00 75.00 24.80 0 stress14:23:30 0 3196 24.80 0.00 0.00 74.60 24.80 1 stress14:23:30 0 3197 24.80 0.00 0.00 74.80 24.80 1 stress14:23:30 0 3200 0.00 0.20 0.00 0.20 0.20 0 pidstat 可以看出，8 个进程在争抢 2 个 CPU，每个进程等待 CPU 的时间（也就是代码块中的 %wait 列）高达 75%。这些超出 CPU 计算能力的进程，最终导致 CPU 过载。 经常说的CPU上下文切换是什么意思 进程在竞争CPU的时候并没有真正运行，为什么还会导致系统的负载升高？原因就是CPU上下文切换。 Linux是一个多任务操作系统，它支持远大于CPU数量的任务同时运行。这些任务实际上并不是真的在同时运行，而是因为在很短的时间内，将CPU轮流分配给它们，造成多任务同时运行的错觉。在每个任务开启钱，CPU都需要知道任务从哪里加载，从哪里开始运行。即需要系统事先帮助设置好CPU寄存器和程序计数器。 CPU寄存器，是CPU内资的容量小、但速度极快的内存。而程序计数器是用来存储CPU正在执行的指令位置、或者即将执行的下一条指令位置。它们都是CPU在运行任何任务钱，必须来来的环境，因此被叫做CPU上下文。 上下文切换就是把前一个任务的CPU上下文（CPU寄存器和程序计数器）保存起来，然后然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务。而这些保存下来的上下文，会存储在系统内核中，并在任务重新调度执行时再次加载进来。这样就能保证任务原来的状态不受影响，让任务看起来还是连续运行。 有人说，CPU 上下文切换无非就是更新了 CPU 寄存器的值嘛，但这些寄存器，本身就是为了快速运行任务而设计的，为什么会影响系统的 CPU 性能呢？根据任务的不同，CPU 的上下文切换就可以分为几个不同的场景，也就是进程上下文切换、线程上下文切换以及中断上下文切换。 进程上下文切换 Linux 按照特权等级，把进程的运行空间分为内核空间和用户空间，分别对应着下图中， CPU 特权等级的 Ring 0 和 Ring 3。 内核空间（Ring 0）具有最高权限，可以直接访问所有资源；用户空间（Ring 3）只能访问受限资源，不能直接访问内存等硬件设备，必须通过系统调用陷入到内核中，才能访问这些特权资源。 换个角度看，也就是说，进程既可以在用户空间运行，又可以在内核空间中运行。进程在用户空间运行时，被称为进程的用户态，而陷入内核空间的时候，被称为进程的内核态。从用户态到内核态的转变，需要通过系统调用来完成。比如，当我们查看文件内容时，就需要多次系统调用来完成：首先调用 open() 打开文件，然后调用 read() 读取文件内容，并调用 write() 将内容写到标准输出，最后再调用 close() 关闭文件。 系统调用的过程有没有发生 CPU 上下文的切换呢？答案自然是肯定的。CPU寄存器里原来用户态的指令位置，需要先保存起来。接着，为了执行内核态代码，CPU寄存器需要更新为内核态指令的新位置。最后才是跳转到内核态运行内核任务。而系统调用结束后，CPU 寄存器需要恢复原来保存的用户态，然后再切换到用户空间，继续运行进程。所以，一次系统调用的过程，其实是发生了两次 CPU 上下文切换。 需要注意的是，系统调动过程中，并不会设计到虚拟内存等进程用户态的资源，也不会切换进程。 进程上下文切换，是指从一个进程切换到另一个进程。而系统调用过程中一直是同一个进程在运行。 所以，系统调用过程通常称为特权模式切换，而不是上下文切换。实际上，系统调用过程中，CPU的上下文切换还是无法避免的。那么，进程上下文切换和系统调用有什么区别呢？ 首先，进程是由内核来管理和调度，进程的切换只能发生在内核态。所以进程的上下文切换不仅包括了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的状态。因此，进程的上下文切换就比系统调用时多了一步：在保存当前进程的内核状态和 CPU 寄存器之前，需要先把该进程的虚拟内存、栈等保存下来；而加载了下一进程的内核态后，还需要刷新进程的虚拟内存和用户栈。如下图所示，保存上下文和恢复上下文的过程并不是“免费”的，需要内核在 CPU 上运行才能完成。 根据 Tsuna 的测试报告，每次上下文切换都需要几十纳秒到数微秒的 CPU 时间。这个时间还是相当可观的，特别是在进程上下文切换次数较多的情况下，很容易导致 CPU 将大量时间耗费在寄存器、内核栈以及虚拟内存等资源的保存和恢复上，进而大大缩短了真正运行进程的时间。这也正是上一节中我们所讲的，导致平均负载升高的一个重要因素。 另外，我们知道， Linux 通过 TLB（Translation Lookaside Buffer）来管理虚拟内存到物理内存的映射关系。当虚拟内存更新后，TLB 也需要刷新，内存的访问也会随之变慢。特别是在多处理器系统上，缓存是被多个处理器共享的，刷新缓存不仅会影响当前处理器的进程，还会影响共享缓存的其他处理器的进程。 显然，进程切换时才需要切换上下文，换句话说，只有在进程调度的时候，才需要切换上下文。Linux 为每个 CPU 都维护了一个就绪队列，将活跃进程（即正在运行和正在等待 CPU 的进程）按照优先级和等待 CPU 的时间排序，然后选择最需要 CPU 的进程，也就是优先级最高和等待 CPU 时间最长的进程来运行。那么，进程在什么时候才会被调度到 CPU 上运行呢？最容易想到的一个时机，就是进程执行完终止了，它之前使用的 CPU 会释放出来，这个时候再从就绪队列里，拿一个新的进程过来运行。其实还有很多其他场景，也会触发进程调度，在这里我给你逐个梳理下。 其一，为了保证所有进程可以得到公平调度，CPU 时间被划分为一段段的时间片，这些时间片再被轮流分配给各个进程。这样，当某个进程的时间片耗尽了，就会被系统挂起，切换到其它正在等待 CPU 的进程运行。其二，进程在系统资源不足（比如内存不足）时，要等到资源满足后才可以运行，这个时候进程也会被挂起，并由系统调度其他进程运行。其三，当进程通过睡眠函数 sleep 这样的方法将自己主动挂起时，自然也会重新调度。其四，当有优先级更高的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行。最后一个，发生硬件中断时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序。 上下文切换引起的性能问题的幕后元凶往往就是以上几个场景。 线程上下文切换线程与进程最大的区别在于，线程是调度的基本单位，而进程则是资源拥有的基本单位。说白了，所谓内核中的任务调度，实际上的调度对象是线程；而进程只是给线程提供了虚拟内存、全局变量等资源。所以，对于线程和进程，我们可以这么理解： 当进程只有一个线程时，可以认为进程就等于线程。 当进程拥有多个线程时，这些线程会共享相同的虚拟内存和全局变量等资源。这些资源在上下文切换时是不需要修改的。 另外，线程也有自己的私有数据，比如栈和寄存器等，这些在上下文切换时也是需要保存的。 这么一来，线程的上下文切换其实就可以分为两种情况：第一种， 前后两个线程属于不同进程。此时，因为资源不共享，所以切换过程就跟进程上下文切换是一样。第二种，前后两个线程属于同一个进程。此时，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据。到这里你应该也发现了，虽然同为上下文切换，但同进程内的线程切换，要比多进程间的切换消耗更少的资源，而这，也正是多线程代替多进程的一个优势。 中断上下文切换为了快速响应硬件的事件，中断处理会打断进程的正常调度和执行，转而调用中断处理程序，响应设备事件。而在打断其他进程时，就需要将进程当前的状态保存下来，这样在中断结束后，进程仍然可以从原来的状态恢复运行。 跟进程上下文不同，中断上下文切换并不涉及到进程的用户态。所以，即便中断过程打断了一个正处在用户态的进程，也不需要保存和恢复这个进程的虚拟内存、全局变量等用户态资源。中断上下文，其实只包括内核态中断服务程序执行所必需的状态，包括 CPU 寄存器、内核堆栈、硬件中断参数等。对同一个 CPU 来说，中断处理比进程拥有更高的优先级，所以中断上下文切换并不会与进程上下文切换同时发生。同样道理，由于中断会打断正常进程的调度和执行，所以大部分中断处理程序都短小精悍，以便尽可能快的执行结束。另外，跟进程上下文切换一样，中断上下文切换也需要消耗 CPU，切换次数过多也会耗费大量的 CPU，甚至严重降低系统的整体性能。 vmstatvmstat 是一个常用的系统性能分析工具，主要用来分析系统的内存使用情况，也常用来分析 CPU 上下文切换和中断的次数。用法如下： 123456# 每隔5秒输出1组数据$ vmstat 5procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 0 7005360 91564 818900 0 0 0 0 25 33 0 0 100 0 0 需要重点关注下面四列内容： **cs (context switch)**是每秒上下文切换的次数in (interrupt) 则是每秒中断的次数**r (Running or Runnable)**是就绪队列的长度，也就是正在运行和等待CPU的进程数。**b (Blocked)**则是处于不可中断睡眠状态的进程数。 vmstat 只给出了系统总体的上下文切换情况，要想查看每个进程的详细情况，就需要使用我们前面提到过的 pidstat 了。给它加上 -w 选项，你就可以查看每个进程上下文切换的情况了。 12345678# 每隔5秒输出1组数据$ pidstat -w 5Linux 4.15.0 (ubuntu) 09/23/18 _x86_64_ (2 CPU)08:18:26 UID PID cswch/s nvcswch/s Command08:18:31 0 1 0.20 0.00 systemd08:18:31 0 8 5.40 0.00 rcu_sched... 这个结果中有两列内容是我们的重点关注对象。一个是 cswch ，表示每秒自愿上下文切换（voluntary context switches）的次数，另一个则是 nvcswch ，表示每秒非自愿上下文切换（non voluntary context switches）的次数。 这两个概念你一定要牢牢记住，因为它们意味着不同的性能问题：所谓自愿上下文切换，是指进程无法获取所需资源，导致的上下文切换。比如说， I/O、内存等系统资源不足时，就会发生自愿上下文切换。而非自愿上下文切换，则是指进程由于时间片已到等原因，被系统强制调度，进而发生的上下文切换。比如说，大量进程都在争抢 CPU 时，就容易发生非自愿上下文切换。 CPU哪性能优化的几个思路 CPU 的性能指标那么多，CPU性能分析工具也是一抓一大把，如果离开专栏，换成实际的工作场景，我又该观察什么指标、选择哪个性能工具呢？我们先来回顾下，描述 CPU 的性能指标都有哪些。 首先，最容易想到的应该是 CPU 使用率，这也是实际环境中最常见的一个性能指标。CPU 使用率描述了非空闲时间占总 CPU 时间的百分比，根据 CPU 上运行任务的不同，又被分为用户 CPU、系统 CPU、等待 I/O CPU、软中断和硬中断等。 用户 CPU 使用率，包括用户态 CPU 使用率（user）和低优先级用户态 CPU 使用率（nice），表示 CPU 在用户态运行的时间百分比。用户 CPU 使用率高，通常说明有应用程序比较繁忙。系统 CPU 使用率，表示 CPU 在内核态运行的时间百分比（不包括中断）。系统 CPU 使用率高，说明内核比较繁忙。等待 I/O 的 CPU 使用率，通常也称为 iowait，表示等待 I/O 的时间百分比。iowait 高，通常说明系统与硬件设备的 I/O 交互时间比较长。软中断和硬中断的CPU使用率，分别表示内核调用软中断处理程序、硬中断处理程序的时间百分比。它们的使用率高，通常说明系统发生了大量的中断。除了上面这些，还有在虚拟化环境中会用到的窃取CPU使用率（steal）和客户CPU使用率（guest），分别表示被其他虚拟机占用的 CPU 时间百分比，和运行客户虚拟机的 CPU 时间百分比。 第二个比较容易想到的，应该是平均负载（Load Average），也就是系统的平均活跃进程数。它反应了系统的整体负载情况，主要包括三个数值，分别指过去 1 分钟、过去 5 分钟和过去15分钟的平均负载。理想情况下，平均负载等于逻辑CPU个数，这表示每个CPU都恰好被充分利用。如果平均负载大于逻辑 CPU 个数，就表示负载比较重了。 第三个，也是在专栏学习前你估计不太会注意到的，进程上下文切换，包括： 无法获取资源而导致的自愿上下文切换；被系统强制调度导致的非自愿上下文切换。 上下文切换，本身是保证 Linux 正常运行的一项核心功能。但过多的上下文切换，会将原本运行进程的CPU时间，消耗在寄存器、内核栈以及虚拟内存等数据的保存和恢复上，缩短进程真正运行的时间，成为性能瓶颈。 除了上面几种，还有一个指标，CPU 缓存的命中率。由于 CPU 发展的速度远快于内存的发展CPU的处理速度就比内存的访问速度快得多。这样，CPU在访问内存的时候，免不了要等待内存的响应。为了协调这两者巨大的性能差距，CPU 缓存（通常是多级缓存）就出现了。 就像上面这张图显示的，CPU缓存的速度介于CPU和内存之间，缓存的是热点的内存数据。根据不断增长的热点数据，这些缓存按照大小不同分为 L1、L2、L3 等三级缓存，其中 L1 和 L2 常用在单核中，L3则用在多核中。从L1到L3，三级缓存的大小依次增大，相应的，性能依次降低（当然比内存还是好得多）。而它们的命中率，衡量的是 CPU 缓存的复用情况，命中率越高，则表示性能越好。 现将CPU常用性能指标总结如下： 活学活用，把性能指标和性能工具联系起来第一个维度，从CPU的性能指标出发。也就是说，当你要查看某个性能指标时，要清楚知道哪些工具可以做到。 在实际生产环境中，我们通常都希望尽可能快地定位系统的瓶颈，然后尽可能快地优化性能，也就是要又快又准地解决性能问题。所以，为了缩小排查范围，我通常会先运行几个支持指标较多的工具，如 top、vmstat 和 pidstat 。为什么是这三个工具呢？仔细看看下面这张图，你就清楚了。 这张图里，列出了top、vmstat和pidstat分别提供的重要的CPU指标，并用虚线表示关联关系，对应出了性能分析下一步的方向。通过这张图你可以发现，这三个命令，几乎包含了所有重要的 CPU 性能指标，比如： 从 top 的输出可以得到各种 CPU 使用率以及僵尸进程和平均负载等信息。 从 vmstat 的输出可以得到上下文切换次数、中断次数、运行状态和不可中断状态的进程数。 从 pidstat 的输出可以得到进程的用户 CPU 使用率、系统 CPU 使用率、以及自愿上下文切换和非自愿上下文切换情况。","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://huangzhiyuan.github.io/tags/Linux/"}]},{"title":"Linux性能优化实战之网络性能篇","slug":"Linux-perf-optimize-actual-combat-Network","date":"2021-05-24T16:53:03.000Z","updated":"2021-05-25T12:24:25.411Z","comments":true,"path":"2021/05/25/Linux-perf-optimize-actual-combat-Network/","link":"","permalink":"http://huangzhiyuan.github.io/2021/05/25/Linux-perf-optimize-actual-combat-Network/","excerpt":"实际上，性能优化一直都是大多数软件工程师头上的紧箍咒，甚至许多工作多年的资深工程师，有时也无法准确地分析出线上的很多性能问题。 性能分析问题为什么这么难呢？性能优化是个系统工具，常常牵一发而动全身。它涉及了从程序设计、算法分析、编程语言，再到系统、存储、网络等各种底层基础设施的方方面面。每一个组件都可能出现问题，甚至多个组件同时出现问题。毫无疑问，性能优化是软件系统中最有挑战的工作指引，也是最考验体现你综合能力的工作之一。该篇blog记录总结了倪朋飞老师在极客时间开设的[《Linux性能优化实战》][1]课程笔记。","text":"实际上，性能优化一直都是大多数软件工程师头上的紧箍咒，甚至许多工作多年的资深工程师，有时也无法准确地分析出线上的很多性能问题。 性能分析问题为什么这么难呢？性能优化是个系统工具，常常牵一发而动全身。它涉及了从程序设计、算法分析、编程语言，再到系统、存储、网络等各种底层基础设施的方方面面。每一个组件都可能出现问题，甚至多个组件同时出现问题。毫无疑问，性能优化是软件系统中最有挑战的工作指引，也是最考验体现你综合能力的工作之一。该篇blog记录总结了倪朋飞老师在极客时间开设的[《Linux性能优化实战》][1]课程笔记。","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://huangzhiyuan.github.io/tags/Linux/"}]},{"title":"Linux性能优化实战之内存性能篇","slug":"Linux-perf-optimize-actual-combat-Memory","date":"2021-05-24T16:53:03.000Z","updated":"2021-06-01T13:14:14.328Z","comments":true,"path":"2021/05/25/Linux-perf-optimize-actual-combat-Memory/","link":"","permalink":"http://huangzhiyuan.github.io/2021/05/25/Linux-perf-optimize-actual-combat-Memory/","excerpt":"实际上，性能优化一直都是大多数软件工程师头上的紧箍咒，甚至许多工作多年的资深工程师，有时也无法准确地分析出线上的很多性能问题。 性能分析问题为什么这么难呢？性能优化是个系统工具，常常牵一发而动全身。它涉及了从程序设计、算法分析、编程语言，再到系统、存储、网络等各种底层基础设施的方方面面。每一个组件都可能出现问题，甚至多个组件同时出现问题。毫无疑问，性能优化是软件系统中最有挑战的工作指引，也是最考验体现你综合能力的工作之一。该篇blog记录总结了倪朋飞老师在极客时间开设的《Linux性能优化实战》课程笔记。","text":"实际上，性能优化一直都是大多数软件工程师头上的紧箍咒，甚至许多工作多年的资深工程师，有时也无法准确地分析出线上的很多性能问题。 性能分析问题为什么这么难呢？性能优化是个系统工具，常常牵一发而动全身。它涉及了从程序设计、算法分析、编程语言，再到系统、存储、网络等各种底层基础设施的方方面面。每一个组件都可能出现问题，甚至多个组件同时出现问题。毫无疑问，性能优化是软件系统中最有挑战的工作指引，也是最考验体现你综合能力的工作之一。该篇blog记录总结了倪朋飞老师在极客时间开设的《Linux性能优化实战》课程笔记。 Linux内存是怎么工作的 同 CPU 管理一样，内存管理也是操作系统最核心的功能之一。内存主要用来存储系统和应用程序的指令、数据、缓存等。 内存映射 我们通常所说的内存容量，提到的笔记本电脑内存8GB，其实指的是物理内存。物理内存也称为主存，大多数计算机用的主存都是动态随机访问内存（DRAM）。只有内核才可以直接访问物理内存。那么，进程要访问内存时，该怎么办呢？ Linux内核给每个进程都提供了一个独立的虚拟地址空间，并且这个地址空间是连续的。这样，进程就可以很方便地访问内存，更确切地说是访问虚拟内存。 虚拟地址空间的内部又被分为内核空间和用户空间两部分，不同字长（也就是单个CPU指令可以处理数据的最大长度）的处理器，地址空间的范围也不同。比如最常见的 32 位和 64 位系统，我画了两张图来分别表示它们的虚拟地址空间，如下所示： 这里可以看出，32 位系统的内核空间占用 1G，位于最高处，剩下的 3G 是用户空间。而 64 位系统的内核空间和用户空间都是 128T，分别占据整个内存空间的最高和最低处，剩下的中间部分是未定义的。 还记得进程的用户态和内核态吗？进程在用户态时，只能访问用户空间内存；只有进入内核态后，才可以访问内核空间内存。虽然每个进程的地址空间都包含了内核空间，但这些内核空间，其实关联的都是相同的物理内存。这样，进程切换到内核态后，就可以很方便地访问内核空间内存。 既然每个进程都有一个这么大的地址空间，那么所有进程的虚拟内存加起来，自然要比实际的物理内存大得多。所以，并不是所有的虚拟内存都会分配物理内存，只有那些实际使用的虚拟内存才分配物理内存，并且分配后的物理内存，是通过内存映射来管理的。 内存映射，其实就是将虚拟内存地址映射到物理内存地址。为了完成内存映射，内核为每个进程都维护了一张页表，记录虚拟地址与物理地址的映射关系，如下图所示： 页表实际上存储在 CPU 的内存管理单元 MMU 中，这样，正常情况下，处理器就可以直接通过硬件，找出要访问的内存。而当进程访问的虚拟地址在页表中查不到时，系统会产生一个缺页异常，进入内核空间分配物理内存、更新进程页表，最后再返回用户空间，恢复进程的运行。 在 CPU 上下文切换的文章中曾经提到， TLB（Translation Lookaside Buffer，转译后备缓冲器）会影响 CPU 的内存访问性能，在这里其实就可以得到解释。 TLB其实就是MMU中页表的高速缓存。由于进程的虚拟地址空间是独立的，而TLB的访问速度又比MMU快得多，所以，通过减少进程的上下文切换，减少 TLB 的刷新次数，就可以提高 TLB 缓存的使用率，进而提高CPU的内存访问性能。不过要注意，MMU并不以字节为单位来管理内存，而是规定了一个内存映射的最小单位，也就是页，通常是 4 KB 大小。这样，每一次内存映射，都需要关联 4 KB 或者 4KB 整数倍的内存空间。 页的大小只有 4 KB ，导致的另一个问题就是，整个页表会变得非常大。比方说，仅 32 位系统就需要 100 多万个页表项（4GB/4KB），才可以实现整个地址空间的映射。为了解决页表项过多的问题，Linux 提供了两种机制，也就是多级页表和大页（HugePage）。 多级页表就是把内存分成区块来管理，将原来的映射关系改成区块索引和区块内的偏移。由于虚拟内存空间通常只用了很少一部分，那么，多级页表就只保存这些使用中的区块，这样就可以大大地减少页表的项数。 Linux 用的正是四级页表来管理内存页，如下图所示，虚拟地址被分为 5 个部分，前 4 个表项用于选择页，而最后一个索引表示页内偏移。 再看大页，顾名思义，就是比普通页更大的内存块，常见的大小有 2MB 和 1GB。大页通常用在使用大量内存的进程上，比如 Oracle、DPDK 等。通过这些机制，在页表的映射下，进程就可以通过虚拟地址来访问物理内存了。那么具体到一个 Linux 进程中，这些内存又是怎么使用的呢？ 虚拟内存空间分布 首先，我们需要进一步了解虚拟内存空间的分布情况。最上方的内核空间不用多讲，下方的用户空间内存，其实又被分成了多个不同的段。以 32 位系统为例，我画了一张图来表示它们的关系。 通过这张图你可以看到，用户空间内存，从低到高分别是五种不同的内存段。 只读段，包括代码和常量等。 数据段，包括全局变量等。 堆，包括动态分配的内存，从低地址开始向上增长。 文件映射段，包括动态库、共享内存等，从高地址开始向下增长。 栈，包括局部变量和函数调用的上下文等。栈的大小是固定的，一般是 8 MB。 在这五个内存段中，堆和文件映射段的内存是动态分配的。比如说，使用 C 标准库的 malloc() 或者 mmap() ，就可以分别在堆和文件映射段动态分配内存。其实64位系统的内存分布也类似，只不过内存空间要大得多。那么，更重要的问题来了，内存究竟是怎么分配的呢？ 内存分配和回收malloc() 是 C 标准库提供的内存分配函数，对应到系统调用上，有两种实现方式，即**brk()和mmap()**。 对小块内存（小于 128K），C标准库使用brk()来分配，也就是通过移动堆顶的位置来分配内存。这些内存释放后并不会立刻归还系统，而是被缓存起来，这样就可以重复使用。而大块内存（大于 128K），则直接使用内存映射 mmap() 来分配，也就是在文件映射段找一块空闲内存分配出去。 这两种方式，自然各有优缺点。brk() 方式的缓存，可以减少缺页异常的发生，提高内存访问效率。不过，由于这些内存没有归还系统，在内存工作繁忙时，频繁的内存分配和释放会造成内存碎片。brk() 方式的缓存，可以减少缺页异常的发生，提高内存访问效率。不过，由于这些内存没有归还系统，在内存工作繁忙时，频繁的内存分配和释放会造成内存碎片。而 mmap() 方式分配的内存，会在释放时直接归还系统，所以每次mmap都会发生缺页异常。在内存工作繁忙时，频繁的内存分配会导致大量的缺页异常，使内核的管理负担增大。这也是 malloc 只对大块内存使用 mmap 的原因。 了解这两种调用方式后，我们还需要清楚一点，那就是，当这两种调用发生后，其实并没有真正分配内存。这些内存，都只在首次访问时才分配，也就是通过缺页异常进入内核中，再由内核来分配内存。 整体来说，Linux 使用伙伴系统来管理内存分配。前面我们提到过，这些内存在 MMU 中以页为单位进行管理，伙伴系统也一样，以页为单位来管理内存，并且会通过相邻页的合并，减少内存碎片化（比如 brk 方式造成的内存碎片）。 如果遇到比页更小的对象，比如不到1K的时候，该怎么分配内存呢？实际系统运行中，确实有大量比页还小的对象，如果为它们也分配单独的页，那就太浪费内存了。 所以，在用户空间，malloc 通过 brk() 分配的内存，在释放时并不立即归还系统，而是缓存起来重复利用。在内核空间，Linux 则通过 slab 分配器来管理小内存。你可以把 slab 看成构建在伙伴系统上的一个缓存，主要作用就是分配并释放内核中的小对象。对内存来说，如果只分配而不释放，就会造成内存泄漏，甚至会耗尽系统内存。所以，在应用程序用完内存后，还需要调用 free() 或 unmap() ，来释放这些不用的内存。 当然，系统也不会任由某个进程用完所有内存。在发现内存紧张时，系统就会通过一系列机制来回收内存，比如下面这三种方式： 回收缓存，比如使用 LRU（Least Recently Used）算法，回收最近使用最少的内存页面； 回收不常访问的内存，把不常用的内存通过交换分区直接写到磁盘中； 杀死进程，内存紧张时系统还会通过 OOM（Out of Memory），直接杀掉占用大量内存的进程。 其中，第二种方式回收不常访问的内存时，会用到交换分区（以下简称 Swap）。Swap 其实就是把一块磁盘空间当成内存来用。它可以把进程暂时不用的数据存储到磁盘中（这个过程称为换出），当进程访问这些内存时，再从磁盘读取这些数据到内存中（这个过程称为换入）。 所以，你可以发现，Swap把系统的可用内存变大了。不过要注意，通常只在内存不足时，才会发生Swap交换。并且由于磁盘读写的速度远比内存慢，Swap 会导致严重的内存性能问题。 第三种方式提到的 OOM（Out of Memory），其实是内核的一种保护机制。它监控进程的内存使用情况，并且使用 oom_score 为每个进程的内存使用情况进行评分： 一个进程消耗的内存越大，oom_score 就越大； 一个进程运行占用的 CPU 越多，oom_score 就越小。 这样，进程的oom_score越大，代表消耗的内存越多，也就越容易被OOM杀死，从而可以更好保护系统。当然，为了实际工作的需要，管理员可以通过 /proc 文件系统，手动设置进程的 oom_adj ，从而调整进程的 oom_score。oom_adj 的范围是 [-17, 15]，数值越大，表示进程越容易被 OOM 杀死；数值越小，表示进程越不容易被 OOM 杀死，其中 -17 表示禁止 OOM。比如用下面的命令，你就可以把 sshd 进程的 oom_adj 调小为 -16，这样， sshd 进程就不容易被 OOM 杀死。 1echo -16 &gt; /proc/$(pidof sshd)/oom_adj 如何查看内存使用情况通过了解内存空间的分布，以及内存的分配和回收，我想你对内存的工作原理应该有了大概的认识。当然，系统的实际工作原理更加复杂，也会涉及其他一些机制，这里我只讲了最主要的原理。掌握了这些，你可以对内存的运作有一条主线认识，不至于脑海里只有术语名词的堆砌。 那么在了解内存的工作原理之后，我们又该怎么查看系统内存使用情况呢？其实前面CPU内容的学习中，我们也提到过一些相关工具。在这里，你第一个想到的应该是 free 工具吧。下面是一个 free 的输出示例： 12345# 注意不同版本的free输出可能会有所不同$ free total used free shared buff/cache availableMem: 8169348 263524 6875352 668 1030472 7611064Swap: 0 0 0 free 输出的是一个表格，其中的数值都默认以字节为单位。表格总共有两行六列，这两行分别是物理内存 Mem 和交换分区 Swap 的使用情况，而六列中，每列数据的含义分别为： 第一列，total 是总内存大小；第二列，used 是已使用内存的大小，包含了共享内存；第三列，free 是未使用内存的大小；第四列，shared 是共享内存的大小；第五列，buff/cache 是缓存和缓冲区的大小；最后一列，available 是新进程可用内存的大小。 这里尤其注意一下，最后一列的可用内存 available 。available不仅包含未使用内存，还包括了可回收的缓存，所以一般会比未使用内存更大。不过，并不是所有缓存都可以回收，因为有些缓存可能正在使用中。free 显示的是整个系统的内存使用情况。如果你想查看进程的内存使用情况，可以用 top 或者 ps 等工具。比如，下面是 top 的输出示例： 12345678910111213141516# 按下M切换到内存排序$ top...KiB Mem : 8169348 total, 6871440 free, 267096 used, 1030812 buff/cacheKiB Swap: 0 total, 0 free, 0 used. 7607492 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 430 root 19 -1 122360 35588 23748 S 0.0 0.4 0:32.17 systemd-journal 1075 root 20 0 771860 22744 11368 S 0.0 0.3 0:38.89 snapd 1048 root 20 0 170904 17292 9488 S 0.0 0.2 0:00.24 networkd-dispat 1 root 20 0 78020 9156 6644 S 0.0 0.1 0:22.92 systemd12376 azure 20 0 76632 7456 6420 S 0.0 0.1 0:00.01 systemd12374 root 20 0 107984 7312 6304 S 0.0 0.1 0:00.00 sshd... top 输出界面的顶端，也显示了系统整体的内存使用情况，这些数据跟free类似，我就不再重复解释。我们接着看下面的内容，跟内存相关的几列数据，比如 VIRT、RES、SHR 以及 %MEM 等。 这些数据，包含了进程最重要的几个内存使用情况，我们挨个来看。 VIRT 是进程虚拟内存的大小，只要是进程申请过的内存，即便还没有真正分配物理内存，也会计算在内。 RES 是常驻内存的大小，也就是进程实际使用的物理内存大小，但不包括 Swap 和共享内存。 SHR 是共享内存的大小，比如与其他进程共同使用的共享内存、加载的动态链接库以及程序的代码段等。 %MEM 是进程使用物理内存占系统总内存的百分比。 除了要认识这些基本信息，在查看 top 输出时，你还要注意两点。第一，虚拟内存通常并不会全部分配物理内存。从上面的输出，你可以发现每个进程的虚拟内存都比常驻内存大得多。第二，共享内存 SHR 并不一定是共享的，比方说，程序的代码段、非共享的动态链接库，也都算在 SHR 里。当然，SHR 也包括了进程间真正共享的内存。所以在计算多个进程的内存使用时，不要把所有进程的 SHR 直接相加得出结果。 怎么理解内存中的Buffer和Cache在上面free命令里面 Buffer 和 Cache 可能不太好区分。从字面上来说，Buffer是缓冲区，而Cache是缓存，两者都是数据在内存中的临时存储。那么，你知道这两种“临时存储”有什么区别吗？ free数据的来源用 man 命令查询 free 的文档，就可以找到对应指标的详细说明。比如，我们执行 man free ，就可以看到下面这个界面。 123456buffers Memory used by kernel buffers (Buffers in /proc/meminfo)cache Memory used by the page cache and slabs (Cached and SReclaimable in /proc/meminfo)buff/cache Sum of buffers and cache 从 free 的手册中，你可以看到 buffer 和 cache 的说明。 Buffers 是内核缓冲区用到的内存，对应的是 /proc/meminfo 中的 Buffers 值。 Cache 是内核页缓存和 Slab 用到的内存，对应的是 /proc/meminfo 中的 Cached 与 SReclaimable 之和。 有没有更简单、更准确的方法，来查询它们的含义呢？ proc 文件系统在前面 CPU 性能模块就曾经提到过，/proc 是 Linux 内核提供的一种特殊文件系统，是用户跟内核交互的接口。比方说，用户可以从 /proc 中查询内核的运行状态和配置选项，查询进程的运行状态、统计数据等，当然，你也可以通过 /proc 来修改内核的配置。 proc 文件系统同时也是很多性能工具的最终数据来源。比如我们刚才看到的 free ，就是通过读取/proc/meminfo，得到内存的使用情况。 继续说回/proc/meminfo，既然 Buffers、Cached、SReclaimable 这几个指标不容易理解，那我们还得继续查 proc 文件系统，获取它们的详细定义。 执行man proc，你就可以得到proc文件系统的详细文档。注意这个文档比较长，你最好搜索一下（比如搜索meminfo），以便更快定位到内存部分。 1234567891011Buffers %lu Relatively temporary storage for raw disk blocks that shouldn&#x27;t get tremendously large (20MB or so).Cached %lu In-memory cache for files read from the disk (the page cache). Doesn&#x27;t include SwapCached....SReclaimable %lu (since Linux 2.6.19) Part of Slab, that might be reclaimed, such as caches. SUnreclaim %lu (since Linux 2.6.19) Part of Slab, that cannot be reclaimed on memory pressure. 通过这个文档，我们可以看到： Buffers 是对原始磁盘块的临时存储，也就是用来缓存磁盘的数据，通常不会特别大（20MB 左右）。这样，内核就可以把分散的写集中起来，统一优化磁盘的写入，比如可以把多次小的写合并成单次大的写等等。 Cached 是从磁盘读取文件的页缓存，也就是用来缓存从文件读取的数据。这样，下次访问这些文件数据时，就可以直接从内存中快速获取，而不需要再次访问缓慢的磁盘。 SReclaimable 是 Slab 的一部分。Slab 包括两部分，其中的可回收部分，用 SReclaimable 记录；而不可回收部分，用 SUnreclaim 记录。 好了，我们终于找到了这三个指标的详细定义。到这里，你是不是长舒一口气，满意地想着，总算弄明白 Buffer 和 Cache 了。不过，知道这个定义就真的理解了吗？这里我给你提了两个问题，你先想想能不能回答出来。 第一个问题，Buffer 的文档没有提到这是磁盘读数据还是写数据的缓存，而在很多网络搜索的结果中都会提到 Buffer 只是对将要写入磁盘数据的缓存。那反过来说，它会不会也缓存从磁盘中读取的数据呢？ 第二个问题，文档中提到，Cache 是对从文件读取数据的缓存，那么它是不是也会缓存写文件的数据呢？ 接下来以实际案例来说明，首先要安装sysstat，是因为我们要用vmstat来观察buffer和cache的变化情况。虽然从/proc/meminfo里可以读到相同的结果，但是还是vmstat的结果更加直观。最后欧，为了减少缓存的影响，运行如下命令来清理系统缓存： 12# 清理文件页、目录项、Inodes等各种缓存$ echo 3 &gt; /proc/sys/vm/drop_caches 这里的 /proc/sys/vm/drop_caches ，就是通过 proc 文件系统修改内核行为的一个示例，写入 3 表示清理文件页、目录项、Inodes 等各种缓存。 场景1： 磁盘和文件写案例 先来模拟第一个场景，打开第一个终端，运行vmstat命令： 1234567# 每隔1秒输出1组数据$ vmstat 1procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----r b swpd free buff cache si so bi bo in cs us sy id wa st0 0 0 7743608 1112 92168 0 0 0 0 52 152 0 1 100 0 0 0 0 0 7743608 1112 92168 0 0 0 0 36 92 0 0 100 0 0 buff 和 cache 就是我们前面看到的 Buffers 和 Cache，单位是 KB。 bi 和 bo 则分别表示块设备读取和写入的大小，单位为块 / 秒。因为 Linux 中块的大小是 1KB，所以这个单位也就等价于 KB/s。 正常情况下，空闲系统中，你应该看到的是，这几个值在多次结果中一直保持不变。接下来，到第二个终端执行 dd 命令，通过读取随机设备，生成一个 500MB 大小的文件： 1$ dd if=/dev/urandom of=/tmp/file bs=1M count=500 然后再回到第一个终端，观察 Buffer 和 Cache 的变化情况： 1234567891011procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----r b swpd free buff cache si so bi bo in cs us sy id wa st0 0 0 7499460 1344 230484 0 0 0 0 29 145 0 0 100 0 0 1 0 0 7338088 1752 390512 0 0 488 0 39 558 0 47 53 0 0 1 0 0 7158872 1752 568800 0 0 0 4 30 376 1 50 49 0 0 1 0 0 6980308 1752 747860 0 0 0 0 24 360 0 50 50 0 0 0 0 0 6977448 1752 752072 0 0 0 0 29 138 0 0 100 0 0 0 0 0 6977440 1760 752080 0 0 0 152 42 212 0 1 99 1 0... 0 1 0 6977216 1768 752104 0 0 4 122880 33 234 0 1 51 49 0 0 1 0 6977440 1768 752108 0 0 0 10240 38 196 0 0 50 50 0 通过观察 vmstat 的输出，我们发现，在 dd 命令运行时， Cache 在不停地增长，而 Buffer 基本保持不变。再进一步观察 I/O 的情况，你会看到， 在 Cache 刚开始增长时，块设备 I/O 很少，bi 只出现了一次488 KB/s，bo则只有一次4KB。而过一段时间后，才会出现大量的块设备写，比如 bo 变成了 122880。 当 dd 命令结束后，Cache 不再增长，但块设备写还会持续一段时间，并且，多次 I/O 写的结果加起来，才是 dd 要写的 500M 的数据。 运行下面的命令。清理缓存后，向磁盘分区 /dev/sdb1 写入 2GB 的随机数据： 1234# 首先清理缓存$ echo 3 &gt; /proc/sys/vm/drop_caches# 然后运行dd命令向磁盘分区/dev/sdb1写入2G数据$ dd if=/dev/urandom of=/dev/sdb1 bs=1M count=2048 再回到终端一，观察内存和 I/O 的变化情况： 123456789procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st1 0 0 7584780 153592 97436 0 0 684 0 31 423 1 48 50 2 0 1 0 0 7418580 315384 101668 0 0 0 0 32 144 0 50 50 0 0 1 0 0 7253664 475844 106208 0 0 0 0 20 137 0 50 50 0 0 1 0 0 7093352 631800 110520 0 0 0 0 23 223 0 50 50 0 0 1 1 0 6930056 790520 114980 0 0 0 12804 23 168 0 50 42 9 0 1 0 0 6757204 949240 119396 0 0 0 183804 24 191 0 53 26 21 0 1 1 0 6591516 1107960 123840 0 0 0 77316 22 232 0 52 16 33 0 从这里你会看到，虽然同是写数据，写磁盘跟写文件的现象还是不同的。写磁盘时（也就是 bo 大于 0 时），Buffer 和 Cache 都在增长，但显然 Buffer 的增长快得多。这说明，写磁盘用到了大量的 Buffer，这跟我们在文档中查到的定义是一样的。对比两个案例，我们发现，写文件时会用到 Cache 缓存数据，而写磁盘则会用到 Buffer 来缓存数据。所以，回到刚刚的问题，虽然文档上只提到，Cache 是文件读的缓存，但实际上，Cache 也会缓存写文件时的数据。 场景2： 磁盘和文件读案例我们再反过来想，磁盘和文件读的时候，又是怎样的呢？我们回到第二个终端，运行下面的命令。清理缓存后，从文件 /tmp/file 中，读取数据写入空设备： 1234# 首先清理缓存$ echo 3 &gt; /proc/sys/vm/drop_caches# 运行dd命令读取文件数据$ dd if=/tmp/file of=/dev/null 然后，再回到终端一，观察内存和 I/O 的变化情况： 1234567procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 1 0 7724164 2380 110844 0 0 16576 0 62 360 2 2 76 21 0 0 1 0 7691544 2380 143472 0 0 32640 0 46 439 1 3 50 46 0 0 1 0 7658736 2380 176204 0 0 32640 0 54 407 1 4 50 46 0 0 1 0 7626052 2380 208908 0 0 32640 40 44 422 2 2 50 46 0 观察 vmstat 的输出，你会发现读取文件时（也就是 bi 大于 0 时），Buffer 保持不变，而 Cache 则在不停增长。这跟我们查到的定义“Cache 是对文件读的页缓存”是一致的。那么，磁盘读又是什么情况呢？我们再运行第二个案例来看看。回到第二个终端，运行下面的命令。清理缓存后，从磁盘分区 /dev/sda1 中读取数据，写入空设备： 12345# 首先清理缓存$ echo 3 &gt; /proc/sys/vm/drop_caches# 运行dd命令读取文件$ dd if=/dev/sda1 of=/dev/null bs=1M count=1024 再回到终端一，观察内存和 I/O 的变化情况： 12345678procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st0 0 0 7225880 2716 608184 0 0 0 0 48 159 0 0 100 0 0 0 1 0 7199420 28644 608228 0 0 25928 0 60 252 0 1 65 35 0 0 1 0 7167092 60900 608312 0 0 32256 0 54 269 0 1 50 49 0 0 1 0 7134416 93572 608376 0 0 32672 0 53 253 0 0 51 49 0 0 1 0 7101484 126320 608480 0 0 32748 0 80 414 0 1 50 49 0 观察 vmstat 的输出，你会发现读磁盘时（也就是 bi 大于 0 时），Buffer 和 Cache 都在增长，但显然 Buffer 的增长快很多。这说明读磁盘时，数据缓存到了 Buffer 中。 当然，我想，经过上一个场景中两个案例的分析，你自己也可以对比得出这个结论：读文件时数据会缓存到 Cache 中，而读磁盘时数据会缓存到 Buffer 中。 到这里你应该发现了，虽然文档提供了对 Buffer 和 Cache 的说明，但是仍不能覆盖到所有的细节。比如说，今天我们了解到的这两点： Buffer 既可以用作“将要写入磁盘数据的缓存”，也可以用作“从磁盘读取数据的缓存”。 Cache 既可以用作“从文件读取数据的页缓存”，也可以用作“写文件的页缓存”。这样，我们就回答了案例开始前的两个问题。 Buffer 是对磁盘数据的缓存，而 Cache 是文件数据的缓存，它们既会用在读请求中，也会用在写请求中。 如何利用系统缓存优化程序的运行效率既然 Buffer 和 Cache 对系统性能有很大影响，那我们在软件开发的过程中，能不能利用这一点，来优化 I/O 性能，提升应用程序的运行效率呢？答案是肯定的。 缓存命中率我们想利用缓存来提升程序的运行效率，应该怎么评估这个效果呢？换句话说，有没有哪个指标可以衡量缓存使用的好坏呢？我估计你已经想到了，缓存的命中率。所谓缓存命中率，是指直接通过缓存获取数据的请求次数，占所有数据请求次数的百分比。 命中率越高，表示使用缓存带来的收益越高，应用程序的性能也就越好。实际上，缓存是现在所有高并发系统必需的核心模块，主要作用就是把经常访问的数据（也就是热点数据），提前读入到内存中。这样，下次访问时就可以直接从内存读取数据，而不需要经过硬盘，从而加快应用程序的响应速度。 这些独立的缓存模块通常会提供查询接口，方便我们随时查看缓存的命中情况。不过Linux系统中并没有直接提供这些接口，所以这里我要介绍一下，cachestat 和 cachetop ，它们正是查看系统缓存命中情况的工具。 cachestat 提供了整个操作系统缓存的读写命中情况。 cachetop 提供了每个进程的缓存命中情况。 这两个工具都是 bcc 软件包的一部分，它们基于 Linux 内核的 eBPF（extended Berkeley Packet Filters）机制，来跟踪内核中管理的缓存，并输出缓存的使用和命中情况。 使用 cachestat 和 cachetop 前，我们首先要安装 bcc 软件包。比如，在 Ubuntu 系统中，你可以运行下面的命令来安装： 1234sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 4052245BD4284CDDecho &quot;deb https://repo.iovisor.org/apt/xenial xenial main&quot; | sudo tee /etc/apt/sources.list.d/iovisor.listsudo apt-get updatesudo apt-get install -y bcc-tools libbcc-examples linux-headers-$(uname -r) 操作完这些步骤，bcc 提供的所有工具就都安装到 /usr/share/bcc/tools 这个目录中了。不过这里提醒你，bcc 软件包默认不会把这些工具配置到系统的 PATH 路径中，所以你得自己手动配置： 1$ export PATH=$PATH:/usr/share/bcc/tools 配置完，你就可以运行 cachestat 和 cachetop 命令了。比如，下面就是一个 cachestat 的运行界面，它以 1 秒的时间间隔，输出了 3 组缓存统计数据： 123456$ cachestat 1 3 TOTAL MISSES HITS DIRTIES BUFFERS_MB CACHED_MB 2 0 2 1 17 279 2 0 2 1 17 279 2 0 2 1 17 279 你可以看到，cachestat 的输出其实是一个表格。每行代表一组数据，而每一列代表不同的缓存统计指标。这些指标从左到右依次表示： TOTAL ，表示总的 I/O 次数； MISSES ，表示缓存未命中的次数； HITS ，表示缓存命中的次数； DIRTIES， 表示新增到缓存中的脏页数； BUFFERS_MB 表示 Buffers 的大小，以 MB 为单位； CACHED_MB 表示 Cache 的大小，以 MB 为单位。 接下来我们再来看一个 cachetop 的运行界面： 1234$ cachetop11:58:50 Buffers MB: 258 / Cached MB: 347 / Sort: HITS / Order: ascendingPID UID CMD HITS MISSES DIRTIES READ_HIT% WRITE_HIT% 13029 root python 1 0 0 100.0% 0.0% 它的输出跟 top 类似，默认按照缓存的命中次数（HITS）排序，展示了每个进程的缓存命中情况。具体到每一个指标，这里的 HITS、MISSES 和 DIRTIES ，跟 cachestat 里的含义一样，分别代表间隔时间内的缓存命中次数、未命中次数以及新增到缓存中的脏页数。而 READ_HIT 和 WRITE_HIT ，分别表示读和写的缓存命中率。 指定文件的缓存大小除了缓存的命中率外，还有一个指标你可能也会很感兴趣，那就是指定文件在内存中的缓存大小。你可以使用pcstat这个工具，来查看文件在内存中的缓存大小以及缓存比例。pcstat 是一个基于 Go 语言开发的工具，所以安装它之前，你首先应该安装 Go 语言，你可以点击这里下载安装。安装完 Go 语言，再运行下面的命令安装 pcstat： 1234$ export GOPATH=~/go$ export PATH=~/go/bin:$PATH$ go get golang.org/x/sys/unix$ go get github.com/tobert/pcstat/pcstat 全部安装完成后，你就可以运行 pcstat 来查看文件的缓存情况了。比如，下面就是一个 pcstat 运行的示例，它展示了 /bin/ls 这个文件的缓存情况： 123456$ pcstat /bin/ls+---------+----------------+------------+-----------+---------+| Name | Size (bytes) | Pages | Cached | Percent ||---------+----------------+------------+-----------+---------|| /bin/ls | 133792 | 33 | 0 | 000.000 |+---------+----------------+------------+-----------+---------+ 这个输出中，Cached 就是 /bin/ls 在缓存中的大小，而 Percent 则是缓存的百分比。你看到它们都是 0，这说明 /bin/ls 并不在缓存中。接着，如果你执行一下 ls 命令，再运行相同的命令来查看的话，就会发现 /bin/ls 都在缓存中了： 12345678$ ls$ pcstat /bin/ls+---------+----------------+------------+-----------+---------+| Name | Size (bytes) | Pages | Cached | Percent ||---------+----------------+------------+-----------+---------|| /bin/ls | 133792 | 33 | 33 | 100.000 |+---------+----------------+------------+-----------+---------+ 知道了缓存相应的指标和查看系统缓存的方法后，接下来，我们就进入今天的正式案例。 案例第一个案例，我们先来看一下上一节提到的 dd 命令。dd 作为一个磁盘和文件的拷贝工具，经常被拿来测试磁盘或者文件系统的读写性能。不过，既然缓存会影响到性能，如果用 dd 对同一个文件进行多次读取测试，测试的结果会怎么样呢？首先，打开两个终端，连接到 Ubuntu 机器上，确保 bcc 已经安装配置成功。 使用 dd 命令生成一个临时文件，用于后面的文件读取测试： 12345# 生成一个512MB的临时文件$ dd if=/dev/sda1 of=file bs=1M count=512# 清理缓存$ echo 3 &gt; /proc/sys/vm/drop_caches 继续在第一个终端，运行 pcstat 命令，确认刚刚生成的文件不在缓存中。如果一切正常，你会看到 Cached 和 Percent 都是 0: 1234567$ pcstat file+-------+----------------+------------+-----------+---------+| Name | Size (bytes) | Pages | Cached | Percent ||-------+----------------+------------+-----------+---------|| file | 536870912 | 131072 | 0 | 000.000 |+-------+----------------+------------+-----------+---------+ 还是在第一个终端中，现在运行 cachetop 命令： 123# 每隔5秒刷新一次数据$ cachetop 5 这次是第二个终端，运行 dd 命令测试文件的读取速度： 1234$ dd if=file of=/dev/null bs=1M512+0 records in512+0 records out536870912 bytes (537 MB, 512 MiB) copied, 16.0509 s, 33.4 MB/s 从 dd 的结果可以看出，这个文件的读性能是 33.4 MB/s。由于在 dd 命令运行前我们已经清理了缓存，所以 dd 命令读取数据时，肯定要通过文件系统从磁盘中读取。不过，这是不是意味着， dd 所有的读请求都能直接发送到磁盘呢？我们再回到第一个终端， 查看 cachetop 界面的缓存命中情况 1234PID UID CMD HITS MISSES DIRTIES READ_HIT% WRITE_HIT%\\.\\.\\. 3264 root dd 37077 37330 0 49.8% 50.2% 从 cachetop 的结果可以发现，并不是所有的读都落到了磁盘上，事实上读请求的缓存命中率只有 50% 。接下来，我们继续尝试相同的测试命令。先切换到第二个终端，再次执行刚才的 dd 命令： 12345$ dd if=file of=/dev/null bs=1M512+0 records in512+0 records out536870912 bytes (537 MB, 512 MiB) copied, 0.118415 s, 4.5 GB/s 看到这次的结果，有没有点小惊讶？磁盘的读性能居然变成了 4.5 GB/s，比第一次的结果明显高了太多。为什么这次的结果这么好呢？不妨再回到第一个终端，看看 cachetop 的情况： 123410:45:22 Buffers MB: 4 / Cached MB: 719 / Sort: HITS / Order: ascendingPID UID CMD HITS MISSES DIRTIES READ_HIT% WRITE_HIT%\\.\\.\\. 32642 root dd 131637 0 0 100.0% 0.0% cachetop 也有了不小的变化。你可以发现，这次的读的缓存命中率是 100.0%，也就是说这次的 dd 命令全部命中了缓存，所以才会看到那么高的性能。然后，回到第二个终端，再次执行 pcstat 查看文件 file 的缓存情况： 1234567$ pcstat file+-------+----------------+------------+-----------+---------+| Name | Size (bytes) | Pages | Cached | Percent ||-------+----------------+------------+-----------+---------|| file | 536870912 | 131072 | 131072 | 100.000 |+-------+----------------+------------+-----------+---------+ 从 pcstat 的结果你可以发现，测试文件 file 已经被全部缓存了起来，这跟刚才观察到的缓存命中率 100% 是一致的。这两次结果说明，系统缓存对第二次 dd 操作有明显的加速效果，可以大大提高文件读取的性能。但同时也要注意，如果我们把 dd 当成测试文件系统性能的工具，由于缓存的存在，就会导致测试结果严重失真。 总结 Buffers 和 Cache 可以极大提升系统的 I/O 性能。通常，我们用缓存命中率，来衡量缓存的使用效率。命中率越高，表示缓存被利用得越充分，应用程序的性能也就越好。你可以用 cachestat 和 cachetop 这两个工具，观察系统和进程的缓存命中情况。其中， cachestat 提供了整个系统缓存的读写命中情况。 cachetop 提供了每个进程的缓存命中情况。不过要注意，Buffers 和 Cache 都是操作系统来管理的，应用程序并不能直接控制这些缓存的内容和生命周期。所以，在应用程序开发中，一般要用专门的缓存组件，来进一步提升性能。比如，程序内部可以使用堆或者栈明确声明内存空间，来存储需要缓存的数据。再或者，使用 Redis 这类外部缓存服务，优化数据的访问效率。 如何定位和处理内存泄漏为什么系统的Swap变高了如何快准狠找到系统内存问题文件系统与磁盘的区别","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://huangzhiyuan.github.io/tags/Linux/"}]},{"title":"Linux性能优化实战","slug":"Linux-perf-optimize-actual-combat","date":"2021-05-24T16:53:03.000Z","updated":"2021-05-26T14:44:47.377Z","comments":true,"path":"2021/05/25/Linux-perf-optimize-actual-combat/","link":"","permalink":"http://huangzhiyuan.github.io/2021/05/25/Linux-perf-optimize-actual-combat/","excerpt":"实际上，性能优化一直都是大多数软件工程师头上的紧箍咒，甚至许多工作多年的资深工程师，有时也无法准确地分析出线上的很多性能问题。 性能分析问题为什么这么难呢？性能优化是个系统工具，常常牵一发而动全身。它涉及了从程序设计、算法分析、编程语言，再到系统、存储、网络等各种底层基础设施的方方面面。每一个组件都可能出现问题，甚至多个组件同时出现问题。毫无疑问，性能优化是软件系统中最有挑战的工作指引，也是最考验体现你综合能力的工作之一。该篇blog记录总结了倪朋飞老师在极客时间开设的《Linux性能优化实战》课程笔记。","text":"实际上，性能优化一直都是大多数软件工程师头上的紧箍咒，甚至许多工作多年的资深工程师，有时也无法准确地分析出线上的很多性能问题。 性能分析问题为什么这么难呢？性能优化是个系统工具，常常牵一发而动全身。它涉及了从程序设计、算法分析、编程语言，再到系统、存储、网络等各种底层基础设施的方方面面。每一个组件都可能出现问题，甚至多个组件同时出现问题。毫无疑问，性能优化是软件系统中最有挑战的工作指引，也是最考验体现你综合能力的工作之一。该篇blog记录总结了倪朋飞老师在极客时间开设的《Linux性能优化实战》课程笔记。 开篇词 对于我们大多数人来说，最好的学习方式之一就是带着问题学习。这种方法远比抱着大厚本原理书籍高效，且保证不会轻易把自己积累下来的信心打垮。学习更要学会抓住重点。其实只要了解了少数几个系统组件的基本原理和协作方式，掌握基本的性能指标和工具，学会实际工作中性能优化的常用技巧，就可以准确分析和优化大多数的性能问题了。在此基础之上，再去阅读哪些经典的操作系统或者图书，更能事半功倍。 在这个专栏里面，作者以案例驱动的思路，分别讲解了Linux性能的基本指标、工具、以及相应的观测、分析和调优方法。具体来看分为5个模块。包括CPU性能、磁盘IO性能、内存性能以及网络性能。每个模块还浅入深出分为4个不同的篇章。 基础篇， 介绍Linux必备的基本原理以及对应的性能指标和性能工具。比如理解平均负载，怎么理解上下文切换，Linux内存的工作原理。 案例篇， 通过模拟案例，帮助分析高手在遇到资源瓶颈时，是如何观测、定位、分析并优化这些性能问题的。 套路篇， 理解了基础，亲身体验了模拟案例，梳理出排查问题的整体思路，也就是检查性能问题的一般步骤。 答疑篇， 系统解答出现频次较多的问题。 如何入门学习Linux性能优化性能指标的概念 其实，性能问题并没有我们想象中的那么复杂。也不需要我们了解每个组件的所有实现细节。 当看到性能指标是，首先会想到的就是“高并发”和“响应快”。它们正对应着性能优化的两个核心目标“吞吐”和“延时”，这两个指标是从应用负载的视角来考察性能，直接影响了产品终端的用户体验。跟它们对应的，是从系统资源的视角出发的指标，比如资源利用率和饱和度等。 随着应用负载的增加，系统资源的使用率也会升高，甚至到达极限。而性能问题的本质就是资源以及达到瓶颈，但请求的处理却还不够快，无法支撑更多的请求。性能分析，其实就是找出应用和系统的瓶颈，并设法去避免或者缓解它们。从而更高效的利用资源处理更多请求。常用步骤如下： 选择指标评估应用程序和系统的性能；为应用程序和系统设置性能目标；进行性能基准测试；性能分析定位瓶颈；优化系统和应用程序；性能监控和告警。 (图片来自：brendangregg.com) 这个图是Linux性能分析最重要的参考资料之一。它告诉我们，在不同的子系统中出现性能问题后，应该用什么样的工具来观测和分析。总结的思维导图如下：","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://huangzhiyuan.github.io/tags/Linux/"}]},{"title":"PyTorch中的量化总结","slug":"quantization-in-pytorch","date":"2021-01-10T12:04:42.000Z","updated":"2021-01-10T13:07:16.000Z","comments":true,"path":"2021/01/10/quantization-in-pytorch/","link":"","permalink":"http://huangzhiyuan.github.io/2021/01/10/quantization-in-pytorch/","excerpt":"背景：在深度学习中，量化指的是使用更少的bit来存储原本以浮点数存储的tensor，以及使用更少的bit来完成原本以浮点数完成的计算。这么做的好处主要有如下几点： 更少的模型体积，接近4倍的减少； 可以更快的计算，由于更少的内存访问和更快的int8计算，可以快2~4倍。一个量化后的模型，其部分或者全部的tensor操作会使用int类型来计算，而不是使用量化之前的float类型。当然，量化还需要底层硬件支持，x86 CPU（支持AVX2）、ARM CPU、Google TPU、Nvidia Volta/Turing/Ampere、Qualcomm DSP这些主流硬件都对量化提供了支持。","text":"背景：在深度学习中，量化指的是使用更少的bit来存储原本以浮点数存储的tensor，以及使用更少的bit来完成原本以浮点数完成的计算。这么做的好处主要有如下几点： 更少的模型体积，接近4倍的减少； 可以更快的计算，由于更少的内存访问和更快的int8计算，可以快2~4倍。一个量化后的模型，其部分或者全部的tensor操作会使用int类型来计算，而不是使用量化之前的float类型。当然，量化还需要底层硬件支持，x86 CPU（支持AVX2）、ARM CPU、Google TPU、Nvidia Volta/Turing/Ampere、Qualcomm DSP这些主流硬件都对量化提供了支持。 PyTorch 1.1的时候开始添加torch.qint8 dtype、torch.quantize_linear转换函数来开始对量化提供有限的实验性支持。PyTorch 1.3开始正式支持量化，在可量化的Tensor之外，PyTorch开始支持CNN中最常见的operator的量化操作，包括： Tensor上的函数: view, clone, resize, slice, add, multiply, cat, mean, max, sort, topk； 常见的模块（在torch.nn.quantized中）：Conv2d, Linear, Avgpool2d, AdaptiveAvgpool2d, MaxPool2d, AdaptiveMaxPool2d, Interpolate, Upsample； 为了量化后还维持更高准确率的合并操作（在torch.nn.intrinsic中）：ConvReLU2d, ConvBnReLU2d, ConvBn2d，LinearReLU，add_relu。 在PyTorch 1.4的时候，PyTorch添加了nn.quantized.Conv3d，与此同时，torchvision 0.5开始提供量化版本的 ResNet、ResNext、MobileNetV2、GoogleNet、InceptionV3和ShuffleNetV2。到PyTorch 1.5的时候，QNNPACK添加了对dynamic quantization的支持，也就为量化版的LSTM在手机平台上使用提供了支撑——也就是添加了对PyTorch mobile的dynamic quantization的支持；增加了量化版本的sigmoid、leaky relu、batch_norm、BatchNorm2d、 Avgpool3d、quantized_hardtanh、quantized ELU activation、quantized Upsample3d、quantized batch_norm3d、 batch_norm3d + relu operators的fused、quantized hardsigmoid。 在PyTorch 1.6的时候，添加了quantized Conv1d、quantized hardswish、quantized layernorm、quantized groupnorm、quantized instancenorm、quantized reflection_pad1d、quantized adaptive avgpool、quantized channel shuffle op、Quantized Threshold；添加ConvBn3d, ConvBnReLU3d, BNReLU2d, BNReLU3d；per-channel的量化得到增强；添加对LSTMCell、RNNCell、GRUCell的Dynamic quantization支持； 在nn.DataParallel 和 nn.DistributedDataParallel中可以使用Quantization aware training；支持CUDA上的quantized tensor。 到目前的最新版本的PyTorch 1.7，又添加了Embedding 和EmbeddingBag quantization、aten::repeat、aten::apend、tensor的stack、tensor的fill_、per channel affine quantized tensor的clone、1D batch normalization、N-Dimensional constant padding、CELU operator、FP16 quantization的支持。 PyTorch对量化的支持目前有如下三种方式： Post Training Dynamic Quantization，模型训练完毕后的动态量化； Post Training Static Quantization，模型训练完毕后的静态量化； QAT（Quantization Aware Training），模型训练中开启量化。 在开始这三部分之前，Gemfield先介绍下最基础的Tensor的量化。 Tensor的量化PyTorch为了实现量化，首先就得需要具备能够表示量化数据的Tensor，这就是从PyTorch 1.1之后引入的Quantized Tensor。 Quantized Tensor可以存储 int8/uint8/int32类型的数据，并携带有scale、zero_point这些参数。把一个标准的float Tensor转换为量化Tensor的步骤如下： 12345678910111213&gt;&gt;&gt; x = torch.rand(2,3, dtype=torch.float32)&gt;&gt;&gt; xtensor([[0.6839, 0.4741, 0.7451], [0.9301, 0.1742, 0.6835]])&gt;&gt;&gt; xq = torch.quantize_per_tensor(x, scale = 0.5, zero_point = 8, dtype=torch.quint8)tensor([[0.5000, 0.5000, 0.5000], [1.0000, 0.0000, 0.5000]], size=(2, 3), dtype=torch.quint8, quantization_scheme=torch.per_tensor_affine, scale=0.5, zero_point=8)&gt;&gt;&gt; xq.int_repr()tensor([[ 9, 9, 9], [10, 8, 9]], dtype=torch.uint8) quantize_per_tensor函数就是使用给定的scale和zp来把一个float tensor转化为quantized tensor，后文你还会遇到这个函数。通过上面这几个数的变化，你可以感受到，量化tensor，也就是xq，和fp32 tensor的关系大概就是: 1xq = round(x / scale + zero_point) scale这个缩放因子和zero_point是两个参数，建立起了fp32 tensor到量化tensor的映射关系。scale体现了映射中的比例关系，而zero_point则是零基准，也就是fp32中的零在量化tensor中的值。因为当x为零的时候，上述xq就变成了： 1xq = round(zero_point) = zero_point 现在xq已经是一个量化tensor了，我们可以把xq在反量化回来，如下所示： 12345# xq is a quantized tensor with data represented as quint8&gt;&gt;&gt; xdq = xq.dequantize()&gt;&gt;&gt; xdqtensor([[0.5000, 0.5000, 0.5000], [1.0000, 0.0000, 0.5000]]) dequantize函数就是quantize_per_tensor的反义词，把一个量化tensor转换为float tensor。也就是： 1xdq = (xq - zero_point) * scale xdq和x的值已经出现了偏差的事实告诉了我们两个道理： 量化会有精度损失； 我们这里随便选取的scale和zp太烂，选择合适的scale和zp可以有效降低精度损失。不信你把scale和zp分别换成scale = 0.0036, zero_point = 0试试。 而在PyTorch中，选择合适的scale和zp的工作就由各种observer来完成。 Tensor的量化支持两种模式：per tensor 和 per channel。Per tensor 是说一个tensor里的所有value按照同一种方式去scale和offset； per channel是对于tensor的某一个维度（通常是channel的维度）上的值按照一种方式去scale和offset，也就是一个tensor里有多种不同的scale和offset的方式（组成一个vector），如此以来，在量化的时候相比per tensor的方式会引入更少的错误。PyTorch目前支持conv2d()、conv3d()、linear()的per channel量化。 Post Training Dynamic Quantization这种量化方式经常缩略前面的两个单词从而称之为Dynamic Quantization，中文为动态量化。这是什么意思呢？你看到全称中的两个关键字了吗：Post、Dynamic： Post：也就是训练完成后再量化模型的权重参数； Dynamic：也就是网络在前向推理的时候动态的量化float32类型的输入。Dynamic Quantization使用下面的API来完成模型的量化： 1torch.quantization.quantize_dynamic(model, qconfig_spec=None, dtype=torch.qint8, mapping=None, inplace=False) quantize_dynamic这个API把一个float model转换为dynamic quantized model，也就是只有权重被量化的model，dtype参数可以取值 float16 或者 qint8。当对整个模型进行转换时，默认只对以下的op进行转换： Linear LSTM LSTMCell RNNCell GRUCell 为啥呢？因为dynamic quantization只是把权重参数进行量化，而这些layer一般参数数量很大，在整个模型中参数量占比极高，因此边际效益高。对其它layer进行dynamic quantization几乎没有实际的意义。再来说说这个API的第二个参数：qconfig_spec： qconfig_spec指定了一组qconfig，具体就是哪个op对应哪个qconfig ； 每个qconfig是QConfig类的实例，封装了两个observer； 这两个observer分别是activation的observer和weight的observer； 但是动态量化使用的是QConfig子类QConfigDynamic的实例，该实例实际上只封装了weight的observer； activate就是post process，就是op forward之后的后处理，但在动态量化中不包含； observer用来根据四元组（min_val，max_val，qmin, qmax）来计算2个量化的参数：scale和zero_point； qmin、qmax是算法提前确定好的，min_val和max_val是从输入数据中观察到的，所以起名叫observer。 当qconfig_spec为None的时候就是默认行为，如果想要改变默认行为，则可以： qconfig_spec赋值为一个set，比如：{nn.LSTM, nn.Linear}，意思是指定当前模型中的哪些layer要被dynamic quantization； qconfig_spec赋值为一个dict，key为submodule的name或type，value为QConfigDynamic实例（其包含了特定的Observer，比如MinMaxObserver、MovingAverageMinMaxObserver、PerChannelMinMaxObserver、MovingAveragePerChannelMinMaxObserver、HistogramObserver）。 事实上，当qconfig_spec为None的时候，quantize_dynamic API就会使用如下的默认值： 12345678qconfig_spec = &#123; nn.Linear : default_dynamic_qconfig, nn.LSTM : default_dynamic_qconfig, nn.GRU : default_dynamic_qconfig, nn.LSTMCell : default_dynamic_qconfig, nn.RNNCell : default_dynamic_qconfig, nn.GRUCell : default_dynamic_qconfig, &#125; 这就是Gemfield刚才提到的动态量化只量化Linear和RNN变种的真相。而default_dynamic_qconfig是QConfigDynamic的一个实例，使用如下的参数进行构造： 123default_dynamic_qconfig = QConfigDynamic(activation=default_dynamic_quant_observer, weight=default_weight_observer)default_dynamic_quant_observer = PlaceholderObserver.with_args(dtype=torch.float, compute_dtype=torch.quint8)default_weight_observer = MinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_tensor_symmetric) 其中，用于activation的PlaceholderObserver 就是个占位符，啥也不做；而用于weight的MinMaxObserver就是记录输入tensor中的最大值和最小值，用来计算scale和zp。 对于一个默认行为下的quantize_dynamic调用，你的模型会经历什么变化呢？Gemfield使用一个小网络来演示下： 1234567891011121314class CivilNet(nn.Module): def __init__(self): super(CivilNet, self).__init__() gemfieldin = 1 gemfieldout = 1 self.conv = nn.Conv2d(gemfieldin, gemfieldout, kernel_size=1, stride=1, padding=0, groups=1, bias=False) self.fc = nn.Linear(3, 2,bias=False) self.relu = nn.ReLU(inplace=False) def forward(self, x): x = self.conv(x) x = self.fc(x) x = self.relu(x) return x 原始网络和动态量化后的网络如下所示： 12345678910111213#原始网络CivilNet( (conv): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1), bias=False) (fc): Linear(in_features=3, out_features=2, bias=False) (relu): ReLU())#quantize_dynamic后CivilNet( (conv): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1), bias=False) (fc): DynamicQuantizedLinear(in_features=3, out_features=2, dtype=torch.qint8, qscheme=torch.per_tensor_affine) (relu): ReLU()) 可以看到，除了Linear，其它op都没有变动。而Linear被转换成了DynamicQuantizedLinear，DynamicQuantizedLinear就是torch.nn.quantized.dynamic.modules.linear.Linear类。没错，quantize_dynamic API的本质就是检索模型中op的type，如果某个op的type属于字典DEFAULT_DYNAMIC_QUANT_MODULE_MAPPINGS的key，那么，这个op将被替换为key对应的value： 12345678# Default map for swapping dynamic modulesDEFAULT_DYNAMIC_QUANT_MODULE_MAPPINGS = &#123; nn.GRUCell: nnqd.GRUCell, nn.Linear: nnqd.Linear, nn.LSTM: nnqd.LSTM, nn.LSTMCell: nnqd.LSTMCell, nn.RNNCell: nnqd.RNNCell,&#125; 这里，nnqd.Linear就是DynamicQuantizedLinear就是torch.nn.quantized.dynamic.modules.linear.Linear。 但是，type从key换为value，那这个新的type如何实例化呢？更重要的是，实例化新的type一定是要用之前的权重参数的呀。没错，以Linear为例，该逻辑定义在 nnqd.Linear的from_float()方法中，通过如下方式实例化： 1new_mod = mapping[type(mod)].from_float(mod) from_float做的事情主要就是： 使用MinMaxObserver计算模型中op权重参数中tensor的最大值最小值（这个例子中只有Linear op），缩小量化时原始值的取值范围，提高量化的精度； 通过上述步骤中得到四元组中的min_val和max_val，再结合算法确定的qmin, qmax计算出scale和zp，参考前文“Tensor的量化”小节，计算得到量化后的weight，这个量化过程有torch.quantize_per_tensor和torch.quantize_per_channel两种，默认是前者（因为qchema默认是torch.per_tensor_affine）； 实例化nnqd.Linear，然后使用qlinear.set_weight_bias将量化后的weight和原始的bias设置到新的layer上。其中最后一步还涉及到weight和bias的打包，在源代码中是这样的： 123456789101112#ifdef USE_FBGEMM if (ctx.qEngine() == at::QEngine::FBGEMM) &#123; return PackedLinearWeight::prepack(std::move(weight), std::move(bias)); &#125;#endif#ifdef USE_PYTORCH_QNNPACK if (ctx.qEngine() == at::QEngine::QNNPACK) &#123; return PackedLinearWeightsQnnp::prepack(std::move(weight), std::move(bias)); &#125;#endif TORCH_CHECK(false,&quot;Didn&#x27;t find engine for operation quantized::linear_prepack &quot;,toString(ctx.qEngine())); 也就是说依赖FBGEMM、QNNPACK这些backend。量化完后的模型在推理的时候有什么不一样的呢？在原始网络中，从输入到最终输出是这么计算的： 1234567891011#inputtorch.Tensor([[[[-1,-2,-3],[1,2,3]]]])#经过卷积后（权重为torch.Tensor([[[[-0.7867]]]])）torch.Tensor([[[[ 0.7867, 1.5734, 2.3601],[-0.7867, -1.5734, -2.3601]]]])#经过fc后（权重为torch.Tensor([[ 0.4097, -0.2896, -0.4931], [-0.3738, -0.5541, 0.3243]]) )torch.Tensor([[[[-1.2972, -0.4004], [1.2972, 0.4004]]]])#经过relu后torch.Tensor([[[[0.0000, 0.0000],[1.2972, 0.4004]]]]) 而在动态量化模型中，上述过程就变成了： 1234567891011#inputtorch.Tensor([[[[-1,-2,-3],[1,2,3]]]])#经过卷积后（权重为torch.Tensor([[[[-0.7867]]]])）torch.Tensor([[[[ 0.7867, 1.5734, 2.3601],[-0.7867, -1.5734, -2.3601]]]])#经过fc后（权重为torch.Tensor([[ 0.4085, -0.2912, -0.4911],[-0.3737, -0.5563, 0.3259]], dtype=torch.qint8,scale=0.0043458822183310986,zero_point=0) )torch.Tensor([[[[-1.3038, -0.3847], [1.2856, 0.3969]]]])#经过relu后torch.Tensor([[[[0.0000, 0.0000], [1.2856, 0.3969]]]]) 所以关键点就是这里的Linear op了，因为其它op和量化之前是一模一样的。你可以看到Linear权重的scale为0.0043458822183310986，zero_point为0。scale和zero_point怎么来的呢？由其使用的observer计算得到的，具体来说就是默认的MinMaxObserver，它是怎么工作的呢？还记得前面说过的observer负责根据四元组来计算scale和zp吧： 在各种observer中，计算权重的scale和zp离不开这四个变量：min_val，max_val，qmin, qmax，分别代表op权重数据/input tensor数据分布的最小值和最大值，以及量化后的取值范围的最小、最大值。qmin和qmax的值好确定，基本就是8个bit能表示的范围，这里取的分别是-128和127（更详细的计算方式将会在下文的“静态量化”章节中描述）；Linear op的权重为torch.Tensor([[ 0.4097, -0.2896, -0.4931], [-0.3738, -0.5541, 0.3243]])，因此其min_val和max_val分别为-0.5541 和 0.4097，在这个上下文中，max_val将进一步取这俩绝对值的最大值。由此我们就可以得到： scale = max_val / (float(qmax - qmin) / 2) = 0.5541 / ((127 + 128) / 2) = 0.004345882… zp = 0 scale和zp的计算细节还会在下文的“静态量化”章节中更详细的描述。从上面我们可以得知，权重部分的量化是“静态”的，是提前就转换完毕的，而之所以叫做“动态”量化，就在于前向推理的时候动态的把input的float tensor转换为量化tensor。 在forward的时候，nnqd.Linear会调用torch.ops.quantized.linear_dynamic函数，输入正是上面（pack好后的）量化后的权重和float的bias，而torch.ops.quantized.linear_dynamic函数最终会被PyTorch分发到C++中的apply_dynamic_impl函数，在这里，或者使用FBGEMM的实现（x86-64设备），或者使用QNNPACK的实现（ARM设备上）： 123456789101112131415#ifdef USE_FBGEMMat::Tensor PackedLinearWeight::apply_dynamic_impl(at::Tensor input, bool reduce_range) &#123; ... fbgemm::xxxx ...&#125;#endif // USE_FBGEMM#ifdef USE_PYTORCH_QNNPACKat::Tensor PackedLinearWeightsQnnp::apply_dynamic_impl(at::Tensor input) &#123; ... qnnpack::qnnpackLinearDynamic(xxxx) ...&#125;#endif // USE_PYTORCH_QNNPACK 等等，input还是float32的啊，这怎么运算嘛。别急，在上述的apply_dynamic_impl函数中，会使用下面的逻辑对输入进行量化： 1Tensor q_input = at::quantize_per_tensor(input_contig, q_params.scale, q_params.zero_point, c10::kQUInt8); 也就是说，动态量化的本质就藏身于此：基于运行时对数据范围的观察，来动态确定对输入进行量化时的scale值。这就确保 input tensor的scale因子能够基于输入数据进行优化，从而获得颗粒度更细的信息。 而模型的参数则是提前就转换为了INT8的格式（在使用quantize_dynamic API的时候）。这样，当输入也被量化后，网络中的运算就使用向量化的INT8指令来完成。 而在当前layer输出的时候，我们还需要把结果再重新转换为float32——re-quantization的scale值是依据input、 weight和output scale来确定的，定义如下：requant_scale = input_scale_fp32 * weight_scale_fp32 / output_scale_fp32实际上，在apply_dynamic_impl函数中，requant_scales就是这么实现的： 123auto output_scale = 1.fauto inverse_output_scale = 1.f /output_scale;requant_scales[i] = (weight_scales_data[i] * input_scale) * inverse_output_scale; 这就是为什么在前面Gemfield提到过，经过量化版的fc的输出为torch.Tensor([[[[-1.3038, -0.3847], [1.2856, 0.3969]]]])，已经变回正常的float tensor了。所以动态量化模型的前向推理过程可以概括如下： 123456789#原始的模型，所有的tensor和计算都是浮点型previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32 /linear_weight_fp32#动态量化后的模型，Linear和LSTM的权重是int8previous_layer_fp32 -- linear_int8_w_fp32_inp -- activation_fp32 -- next_layer_fp32 / linear_weight_int8 总结下来，我们可以这么说：Post Training Dynamic Quantization，简称为Dynamic Quantization，也就是动态量化，或者叫作Weight-only的量化，是提前把模型中某些op的参数量化为INT8，然后在运行的时候动态的把输入量化为INT8，然后在当前op输出的时候再把结果requantization回到float32类型。动态量化默认只适用于Linear以及RNN的变种。 Post Training Static Quantization与其介绍post training static quantization是什么，我们不如先来说明下它和dynamic quantization的相同点和区别是什么。相同点就是，都是把网络的权重参数转从float32转换为int8；不同点是，需要把训练集或者和训练集分布类似的数据喂给模型（注意没有反向传播），然后通过每个op输入的分布特点来计算activation的量化参数（scale和zp）——称之为Calibrate（定标）。是的，静态量化包含有activation了，也就是post process，也就是op forward之后的后处理。为什么静态量化需要activation呢？因为静态量化的前向推理过程自(始+1)至(终-1)都是INT计算，activation需要确保一个op的输入符合下一个op的输入。 PyTorch会使用五部曲来完成模型的静态量化： 1，fuse_model合并一些可以合并的layer。这一步的目的是为了提高速度和准确度： 1fuse_modules(model, modules_to_fuse, inplace=False, fuser_func=fuse_known_modules, fuse_custom_config_dict=None) 比如给fuse_modules传递下面的参数就会合并网络中的conv1、bn1、relu1： 1torch.quantization.fuse_modules(gemfield_model, [[&#x27;conv1&#x27;, &#x27;bn1&#x27;, &#x27;relu1&#x27;]], inplace=True) 一旦合并成功，那么原始网络中的conv1就会被替换为新的合并后的module（因为其是list中的第一个元素），而bn1、relu1（list中剩余的元素）会被替换为nn.Identity()，这个模块是个占位符，直接输出输入。举个例子，对于下面的一个小网络： 1234567891011121314class CivilNet(nn.Module): def __init__(self): super(CivilNet, self).__init__() syszuxin = 1 syszuxout = 1 self.conv = nn.Conv2d(syszuxin, syszuxout, kernel_size=1, stride=1, padding=0, groups=1, bias=False) self.fc = nn.Linear(3, 2,bias=False) self.relu = nn.ReLU(inplace=False) def forward(self, x): x = self.conv(x) x = self.fc(x) x = self.relu(x) return x 网络结构如下： 12345CivilNet( (conv): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1), bias=False) (fc): Linear(in_features=3, out_features=2, bias=False) (relu): ReLU()) 经过torch.quantization.fuse_modules(c, [[‘fc’, ‘relu’]], inplace=True)后，网络变成了： 12345678CivilNet( (conv): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1), bias=False) (fc): LinearReLU( (0): Linear(in_features=3, out_features=2, bias=False) (1): ReLU() ) (relu): Identity()) modules_to_fuse参数的list可以包含多个item list，或者是submodule的op list也可以，比如：[ [‘conv1’, ‘bn1’, ‘relu1’], [‘submodule.conv’, ‘submodule.relu’]]。有的人会说了，我要fuse的module被Sequential封装起来了，如何传参？参考下面的代码： 1torch.quantization.fuse_modules(a_sequential_module, [&#x27;0&#x27;, &#x27;1&#x27;, &#x27;2&#x27;], inplace=True) 不是什么类型的op都可以参与合并，也不是什么样的顺序都可以参与合并。就目前来说，截止到pytorch 1.7.1，只有如下的op和顺序才可以： Convolution, Batch normalization Convolution, Batch normalization, Relu Convolution, Relu Linear, Relu Batch normalization, Relu 实际上，这个mapping关系就定义在DEFAULT_OP_LIST_TO_FUSER_METHOD中： 1234567891011121314DEFAULT_OP_LIST_TO_FUSER_METHOD : Dict[Tuple, Union[nn.Sequential, Callable]] = &#123; (nn.Conv1d, nn.BatchNorm1d): fuse_conv_bn, (nn.Conv1d, nn.BatchNorm1d, nn.ReLU): fuse_conv_bn_relu, (nn.Conv2d, nn.BatchNorm2d): fuse_conv_bn, (nn.Conv2d, nn.BatchNorm2d, nn.ReLU): fuse_conv_bn_relu, (nn.Conv3d, nn.BatchNorm3d): fuse_conv_bn, (nn.Conv3d, nn.BatchNorm3d, nn.ReLU): fuse_conv_bn_relu, (nn.Conv1d, nn.ReLU): nni.ConvReLU1d, (nn.Conv2d, nn.ReLU): nni.ConvReLU2d, (nn.Conv3d, nn.ReLU): nni.ConvReLU3d, (nn.Linear, nn.ReLU): nni.LinearReLU, (nn.BatchNorm2d, nn.ReLU): nni.BNReLU2d, (nn.BatchNorm3d, nn.ReLU): nni.BNReLU3d,&#125; 2，设置qconfigqconfig是要设置到模型或者模型的子module上的。前文Gemfield就已经说过，qconfig是QConfig的一个实例，QConfig这个类就是维护了两个observer，一个是activation所使用的observer，一个是op权重所使用的observer。 12345#如果要部署在x86 server上gemfield_model.qconfig = torch.quantization.get_default_qconfig(&#x27;fbgemm&#x27;)#如果要部署在ARM上gemfield_model.qconfig = torch.quantization.get_default_qconfig(&#x27;qnnpack&#x27;) 如果是x86和arm之外呢？抱歉，目前不支持。实际上，这里的get_default_qconfig函数的实现如下所示： 12345678def get_default_qconfig(backend=&#x27;fbgemm&#x27;): if backend == &#x27;fbgemm&#x27;: qconfig = QConfig(activation=HistogramObserver.with_args(reduce_range=True),weight=default_per_channel_weight_observer) elif backend == &#x27;qnnpack&#x27;: qconfig = QConfig(activation=HistogramObserver.with_args(reduce_range=False),weight=default_weight_observer) else: qconfig = default_qconfig return qconfig default_qconfig实际上是QConfig(activation=default_observer, weight=default_weight_observer)，所以gemfield这里总结了一个表格： 量化的backend activation weight fbgemm HistogramObserver (reduce_range=True) PerChannelMinMaxObserver (default_per_channel_weight_observer) qnnpack HistogramObserver (reduce_range=False) MinMaxObserver (default_weight_observer) 默认（非fbgemm和qnnpack） MinMaxObserver (default_observer) MinMaxObserver (default_weight_observer) 3，prepareprepare调用是通过如下API完成的： 1gemfield_model_prepared = torch.quantization.prepare(gemfield_model) prepare用来给每个子module插入Observer，用来收集和定标数据。以activation的observer为例，就是期望其观察输入数据得到四元组中的min_val和max_val，至少观察个几百个迭代的数据吧，然后由这四元组得到scale和zp这两个参数的值。 module上安插activation的observer是怎么实现的呢？还记得https://zhuanlan.zhihu.com/p/53927068一文中说过的“_forward_hooks是通过register_forward_hook来完成注册的。这些hooks是在forward完之后被调用的......”吗？没错，CivilNet模型中的Conv2d、Linear、ReLU、QuantStub这些module的_forward_hooks上都被插入了activation的HistogramObserver，当这些子module计算完毕后，结果会被立刻送到其_forward_hooks中的HistogramObserver进行观察。 这一步完成后，CivilNet网络就被改造成了： 1234567891011121314151617CivilNet( (conv): Conv2d( 1, 1, kernel_size=(1, 1), stride=(1, 1), bias=False (activation_post_process): HistogramObserver() ) (fc): Linear( in_features=3, out_features=2, bias=False (activation_post_process): HistogramObserver() ) (relu): ReLU( (activation_post_process): HistogramObserver() ) (quant): QuantStub( (activation_post_process): HistogramObserver() ) (dequant): DeQuantStub()) 4，喂数据这一步不是训练。是为了获取数据的分布特点，来更好的计算activation的scale和zp。至少要喂上几百个迭代的数据。 123#至少观察个几百迭代for data in data_loader: gemfield_model_prepared(data) 5，转换模型第四步完成后，各个op权重的四元组（min_val，max_val，qmin, qmax）中的min_val，max_val已经有了，各个op activation的四元组（min_val，max_val，qmin, qmax）中的min_val，max_val也已经观察出来了。那么在这一步我们将调用convert API： 1gemfield_model_prepared_int8 = torch.quantization.convert(gemfield_model_prepared) 这个过程和dynamic量化类似，本质就是检索模型中op的type，如果某个op的type属于字典DEFAULT_STATIC_QUANT_MODULE_MAPPINGS的key（注意字典和动态量化的不一样了），那么，这个op将被替换为key对应的value： 1234567891011121314151617181920212223242526272829303132333435363738394041DEFAULT_STATIC_QUANT_MODULE_MAPPINGS = &#123; QuantStub: nnq.Quantize, DeQuantStub: nnq.DeQuantize, nn.BatchNorm2d: nnq.BatchNorm2d, nn.BatchNorm3d: nnq.BatchNorm3d, nn.Conv1d: nnq.Conv1d, nn.Conv2d: nnq.Conv2d, nn.Conv3d: nnq.Conv3d, nn.ConvTranspose1d: nnq.ConvTranspose1d, nn.ConvTranspose2d: nnq.ConvTranspose2d, nn.ELU: nnq.ELU, nn.Embedding: nnq.Embedding, nn.EmbeddingBag: nnq.EmbeddingBag, nn.GroupNorm: nnq.GroupNorm, nn.Hardswish: nnq.Hardswish, nn.InstanceNorm1d: nnq.InstanceNorm1d, nn.InstanceNorm2d: nnq.InstanceNorm2d, nn.InstanceNorm3d: nnq.InstanceNorm3d, nn.LayerNorm: nnq.LayerNorm, nn.LeakyReLU: nnq.LeakyReLU, nn.Linear: nnq.Linear, nn.ReLU6: nnq.ReLU6, # Wrapper Modules: nnq.FloatFunctional: nnq.QFunctional, # Intrinsic modules: nni.BNReLU2d: nniq.BNReLU2d, nni.BNReLU3d: nniq.BNReLU3d, nni.ConvReLU1d: nniq.ConvReLU1d, nni.ConvReLU2d: nniq.ConvReLU2d, nni.ConvReLU3d: nniq.ConvReLU3d, nni.LinearReLU: nniq.LinearReLU, nniqat.ConvBn1d: nnq.Conv1d, nniqat.ConvBn2d: nnq.Conv2d, nniqat.ConvBnReLU1d: nniq.ConvReLU1d, nniqat.ConvBnReLU2d: nniq.ConvReLU2d, nniqat.ConvReLU2d: nniq.ConvReLU2d, nniqat.LinearReLU: nniq.LinearReLU, # QAT modules: nnqat.Linear: nnq.Linear, nnqat.Conv2d: nnq.Conv2d,&#125; 替换的过程也和dynamic一样，使用from_float() API，这个API会使用前面的四元组信息计算出op权重和op activation的scale和zp，然后用于量化。动态量化”章节时Gemfield说过要再详细介绍下scale和zp的计算过程，好了，就在这里。这个计算过程覆盖了如下的几个问题： QuantStub的scale和zp是怎么来的（静态量化需要插入QuantStub，后文有说明）？ conv activation的scale和zp是怎么来的？ conv weight的scale和zp是怎么来的？ fc activation的scale和zp是怎么来的？ fc weight的scale和zp是怎么来的？ relu activation的scale和zp是怎么来的？ relu weight的…等等，relu没有weight。 我们就从conv来说起吧，还记得前面说过的Observer吗？分为activation和weight两种。以Gemfield这里使用的fbgemm后端为例，activation默认的observer是HistogramObserver、weight默认的observer是PerChannelMinMaxObserver。而计算scale和zp所需的四元组都是这些observer观察出来的呀（好吧，其中两个）。 在convert API调用中，pytorch会将Conv2d op替换为对应的QuantizedConv2d，在这个替换的过程中会计算QuantizedConv2d activation的scale和zp以及QuantizedConv2d weight的scale和zp。在各种observer中，计算scale和zp离不开这四个变量：min_val，max_val，qmin, qmax，分别代表输入的数据/权重的数据分布的最小值和最大值，以及量化后的取值范围的最小、最大值。qmin和qmax的值好确定，基本就是8个bit能表示的范围，在pytorch中，qmin和qmax是使用如下方式确定的： 12345678910if self.dtype == torch.qint8: if self.reduce_range: qmin, qmax = -64, 63 else: qmin, qmax = -128, 127else: if self.reduce_range: qmin, qmax = 0, 127 else: qmin, qmax = 0, 255 比如conv的activation的observer(quint8)是HistogramObserver，又是reduce_range的，因此其qmin,qmax = 0 ，127，而conv的weight(qint8)是PerChannelMinMaxObserver，不是reduce_range的，因此其qmin, qmax = -128, 127。那么min_val，max_val又是怎么确定的呢？对于HistogramObserver，其由输入数据 + 权重值根据L2Norm(An approximation for L2 error minimization)确定；对于PerChannelMinMaxObserver来说，其由输入数据的最小值和最大值确定，比如在上述的例子中，值就是-0.7898和-0.7898。既然现在conv weight的min_val，max_val，qmin, qmax 分别为 -0.7898、-0.7898、-128、 127，那如何得到scale和zp呢？PyTorch就是用下面的逻辑进行计算的： 12345678910111213#qscheme 是 torch.per_tensor_symmetric 或者torch.per_channel_symmetric时max_val = torch.max(-min_val, max_val)scale = max_val / (float(qmax - qmin) / 2)scale = torch.max(scale, torch.tensor(self.eps, device=device, dtype=scale.dtype))if self.dtype == torch.quint8: zero_point = zero_point.new_full(zero_point.size(), 128)#qscheme 是 torch.per_tensor_affine时scale = (max_val - min_val) / float(qmax - qmin)scale = torch.max(scale, torch.tensor(self.eps, device=device, dtype=scale.dtype))zero_point = qmin - torch.round(min_val / scale)zero_point = torch.max(zero_point, torch.tensor(qmin, device=device, dtype=zero_point.dtype))zero_point = torch.min(zero_point, torch.tensor(qmax, device=device, dtype=zero_point.dtype)) 由此conv2d weight的谜团就被我们解开了： scale = 0.7898 / ((127 + 128)/2 ) = 0.0062 zp = 0再说说QuantStub的scale和zp是如何计算的。QuantStub使用的是HistogramObserver，根据输入从[-3,3]的分布，HistogramObserver计算得到min_val、max_val分别是-3、2.9971，而qmin和qmax又分别是0、127，其schema为per_tensor_affine，因此套用上面的per_tensor_affine逻辑可得： scale = (2.9971 + 3) / (127 - 0) = 0.0472 zp = 0 - round(-3 /0.0472) = 64 其它计算同理，不再赘述。有了scale和zp，就有了量化版本的module，上面那个CivilNet网络，经过静态量化后，网络的变化如下所示： 12345678910111213#原始的CivilNet网络：CivilNet( (conv): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1), bias=False) (fc): Linear(in_features=3, out_features=2, bias=False) (relu): ReLU())#静态量化后的CivilNet网络：CivilNet( (conv): QuantizedConv2d(1, 1, kernel_size=(1, 1), stride=(1, 1), scale=0.0077941399067640305, zero_point=0, bias=False) (fc): QuantizedLinear(in_features=3, out_features=2, scale=0.002811126410961151, zero_point=14, qscheme=torch.per_channel_affine) (relu): QuantizedReLU()) 静态量化模型如何推理？我们知道，在PyTorch的网络中，前向推理逻辑都是实现在了每个op的forward函数中（参考：Gemfield：详解Pytorch中的网络构造）。而在convert完成后，所有的op被替换成了量化版本的op，那么量化版本的op的forward会有什么不一样的呢？还记得吗？动态量化中可是只量化了op的权重哦，输入的量化所需的scale的值是在推理过程中动态计算出来的。而静态量化中，统统都是提前就计算好的。我们来看一个典型的静态量化模型的推理过程： 123456789101112131415161718192021import torchimport torch.nn as nnclass CivilNet(nn.Module): def __init__(self): super(CivilNet, self).__init__() in_planes = 1 out_planes = 1 self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, groups=1, bias=False) self.fc = nn.Linear(3, 2,bias=False) self.relu = nn.ReLU(inplace=False) self.quant = QuantStub() self.dequant = DeQuantStub() def forward(self, x): x = self.quant(x) x = self.conv(x) x = self.fc(x) x = self.relu(x) x = self.dequant(x) return x 网络forward的开始和结束还必须安插QuantStub和DeQuantStub，如上所示。否则运行时会报错：RuntimeError: Could not run ‘quantized::conv2d.new’ with arguments from the ‘CPU’ backend. ‘quantized::conv2d.new’ is only available for these backends: [QuantizedCPU]。 QuantStub在observer阶段会记录参数值，DeQuantStub在prepare阶段相当于Identity；而在convert API调用过程中，会分别被替换为nnq.Quantize和nnq.DeQuantize。在这个章节要介绍的推理过程中，QuantStub，也就是nnq.Quantize在做什么工作呢？如下所示： 12def forward(self, X): return torch.quantize_per_tensor(X, float(self.scale), int(self.zero_point), self.dtype) 是不是呼应了前文中的“tensor的量化”章节？这里的scale和zero_point的计算方式前文也刚介绍过。而nnq.DeQuantize做了什么呢？很简单，把量化tensor反量化回来。 12def forward(self, Xq): return Xq.dequantize() 是不是又呼应了前文中的“tensor的量化”章节？我们就以上面的CivilNet网络为例，当在静态量化后的模型进行前向推理和原始的模型的区别是什么呢？假设网络的输入为torch.Tensor([[[[-1,-2,-3],[1,2,3]]]])： 123c = CivilNet()t = torch.Tensor([[[[-1,-2,-3],[1,2,3]]]])c(t) 假设conv的权重为torch.Tensor([[[[-0.7867]]]])，假设fc的权重为torch.Tensor([[ 0.4097, -0.2896, -0.4931], [-0.3738, -0.5541, 0.3243]])，那么在原始的CivilNet前向中，从输入到输出的过程依次为： 1234567891011#inputtorch.Tensor([[[[-1,-2,-3],[1,2,3]]]])#经过卷积后（权重为torch.Tensor([[[[-0.7867]]]])）torch.Tensor([[[[ 0.7867, 1.5734, 2.3601],[-0.7867, -1.5734, -2.3601]]]])#经过fc后（权重为torch.Tensor([[ 0.4097, -0.2896, -0.4931], [-0.3738, -0.5541, 0.3243]]) )torch.Tensor([[[[-1.2972, -0.4004], [1.2972, 0.4004]]]])#经过relu后torch.Tensor([[[[0.0000, 0.0000],[1.2972, 0.4004]]]]) 而在静态量化的模型前向中，总体情况如下： 1234567891011121314151617181920#inputtorch.Tensor([[[[-1,-2,-3],[1,2,3]]]])#QuantStub后 (scale=tensor([0.0472]), zero_point=tensor([64]))tensor([[[[-0.9916, -1.9833, -3.0221],[ 0.9916, 1.9833, 3.0221]]]], dtype=torch.quint8, scale=0.04722102731466293, zero_point=64)#经过卷积后（权重为torch.Tensor([[[[-0.7898]]]], dtype=torch.qint8, scale=0.0062, zero_point=0))#conv activation（输入）的scale为0.03714831545948982，zp为64torch.Tensor([[[[ 0.7801, 1.5602, 2.3775],[-0.7801, -1.5602, -2.3775]]]], scale=0.03714831545948982, zero_point=64)#经过fc后（权重为torch.Tensor([[ 0.4100, -0.2901, -0.4951],[-0.3737, -0.5562, 0.3259]], dtype=torch.qint8, scale=tensor([0.0039, 0.0043]),zero_point=tensor([0, 0])) )#fc activation（输入）的scale为0.020418135449290276, zp为64torch.Tensor([[[[-1.3068, -0.3879],[ 1.3068, 0.3879]]]], dtype=torch.quint8, scale=0.020418135449290276, zero_point=64)#经过relu后torch.Tensor([[[[0.0000, 0.0000],[1.3068, 0.3879]]]], dtype=torch.quint8, scale=0.020418135449290276, zero_point=64)#经过DeQuantStub后torch.Tensor([[[[0.0000, 0.0000],[1.3068, 0.3879]]]]) Gemfield这里用原始的python语句来分步骤来介绍下。首先是QuantStub的工作： 12345678910111213141516171819import torchimport torch.nn.quantized as nnq#输入&gt;&gt;&gt; x = torch.Tensor([[[[-1,-2,-3],[1,2,3]]]])&gt;&gt;&gt; xtensor([[[[-1., -2., -3.], [ 1., 2., 3.]]]])#经过QuantStub&gt;&gt;&gt; xq = torch.quantize_per_tensor(x, scale = 0.0472, zero_point = 64, dtype=torch.quint8)&gt;&gt;&gt; xqtensor([[[[-0.9912, -1.9824, -3.0208], [ 0.9912, 1.9824, 3.0208]]]], size=(1, 1, 2, 3), dtype=torch.quint8, quantization_scheme=torch.per_tensor_affine, scale=0.0472, zero_point=64)&gt;&gt;&gt; xq.int_repr()tensor([[[[ 43, 22, 0], [ 85, 106, 128]]]], dtype=torch.uint8) 我们特意在网络前面安插的QuantStub完成了自己的使命，其scale = 0.0472、zero_point = 64是静态量化完毕后就已经知道的，然后通过quantize_per_tensor调用把输入的float tensor转换为了量化tensor，然后送给接下来的Conv2d——量化版本的Conv2d： 123456789101112&gt;&gt;&gt; c = nnq.Conv2d(1,1,1)&gt;&gt;&gt; weight = torch.Tensor([[[[-0.7898]]]])&gt;&gt;&gt; qweight = torch.quantize_per_channel(weight, scales=torch.Tensor([0.0062]).to(torch.double), zero_points = torch.Tensor([0]).to(torch.int64), axis=0, dtype=torch.qint8)&gt;&gt;&gt; c.set_weight_bias(qweight, None)&gt;&gt;&gt; c.scale = 0.03714831545948982&gt;&gt;&gt; c.zero_point = 64&gt;&gt;&gt; x = c(xq)&gt;&gt;&gt; xtensor([[[[ 0.7801, 1.5602, 2.3775], [-0.7801, -1.5602, -2.3775]]]], size=(1, 1, 2, 3), dtype=torch.quint8, quantization_scheme=torch.per_tensor_affine, scale=0.03714831545948982, zero_point=64) 同理，Conv2d的权重的scale=0.0062、zero_points=0是静态量化完毕就已知的，其activation的scale = 0.03714831545948982、zero_point = 64也是量化完毕已知的。然后送给nnq.Conv2d的forward函数（参考：https://zhuanlan.zhihu.com/p/53927068），其forward逻辑为： 12def forward(self, input): return ops.quantized.conv2d(input, self._packed_params, self.scale, self.zero_point) Conv2d计算完了，我们停下来反省一下。如果是按照浮点数计算，那么-0.7898 * -0.9912 大约是0.7828，但这里使用int8的计算方式得到的值是0.7801，这说明已经在引入误差了（大约为0.34%的误差）。这也是前面gemfield说的使用fuse_modules可以提高精度的原因，因为每一层都会引入类似的误差。 后面Linear的计算同理，其forward逻辑为： 12def forward(self, x): return torch.ops.quantized.linear(x, self._packed_params._packed_params, self.scale, self.zero_point) 可以看到，所有以量化方式计算完的值现在需要经过activation的计算。这是静态量化和动态量化的本质区别之一：op和op之间不再需要转换回到float tensor了。通过上面的分析，我们可以把静态量化模型的前向推理过程概括为如下的形式： 123456789#原始的模型，所有的tensor和计算都是浮点型previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32 / linear_weight_fp32#静态量化的模型，权重和输入都是int8previous_layer_int8 -- linear_with_activation_int8 -- next_layer_int8 / linear_weight_int8 最后再来描述下动态量化和静态量化的最大区别： 静态量化的float输入必经QuantStub变为int，此后到输出之前都是int； 动态量化的float输入是经动态计算的scale和zp量化为int，op输出时转换回float。 QAT（Quantization Aware Training）前面两种量化方法都有一个post关键字，意思是模型训练完毕后所做的量化。而QAT则不一样，是指在训练过程中就开启了量化功能。 QAT需要五部曲，说到这里，你可能想到了静态量化，那不妨对比着来看。1，设置qconfig在设置qconfig之前，模型首先设置为训练模式，这很容易理解，因为QAT的着力点就是T嘛： 12cnet = CivilNet()cnet.train() 使用get_default_qat_qconfig API来给要QAT的网络设置qconfig： 1cnet.qconfig = torch.quantization.get_default_qat_qconfig(&#x27;fbgemm&#x27;) 不过，这个qconfig和静态量化中的可不一样啊。前文说过qconfig维护了两个observer，activation的和权重的。QAT的qconfig中，activation和权重的observer都变成了FakeQuantize（和observer是has a的关系，也即包含一个observer），并且参数不一样（qmin、qmax、schema,dtype,qschema,reduce_range这些参数），如下所示： 1234567891011#activation的observer的参数FakeQuantize.with_args(observer=MovingAverageMinMaxObserver,quant_min=0,quant_max=255,reduce_range=True)#权重的observer的参数FakeQuantize.with_args(observer=MovingAveragePerChannelMinMaxObserver, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, reduce_range=False, ch_axis=0) 这里FakeQuantize包含的observer是MovingAverageMinMaxObserver，继承自前面提到过的MinMaxObserver，但是求最小值和最大值的方法有点区别，使用的是如下公式： Xmin、Xmax是当前运行中正在求解和最终求解的最小值、最大值； X是当前输入的tensor； c是一个常数，PyTorch中默认为0.01，也就是最新一次的极值由上一次贡献99%，当前的tensor贡献1%。 MovingAverageMinMaxObserver在求min、max的方式和其基类MinMaxObserver有所区别之外，scale和zero_points的计算则是一致的。那么在包含了上述的observer之后，FakeQuantize的作用又是什么呢？看下面的步骤。 2，fuse_modules和静态量化一样，不再赘述。 3，prepare_qat在静态量化中，我们这一步使用的是prepare API，而在QAT这里使用的是prepare_qat API。最重要的区别有两点： prepare_qat要把qconfig安插到每个op上，qconfig的内容本身就不同，参考五部曲中的第一步； prepare_qat 中需要多做一步转换子module的工作，需要inplace的把模型中的一些子module替换了，替换的逻辑就是从DEFAULT_QAT_MODULE_MAPPINGS的key替换为value，这个字典的定义如下：123456789101112# Default map for swapping float module to qat modulesDEFAULT_QAT_MODULE_MAPPINGS : Dict[Callable, Any] = &#123; nn.Conv2d: nnqat.Conv2d, nn.Linear: nnqat.Linear, # Intrinsic modules: nni.ConvBn1d: nniqat.ConvBn1d, nni.ConvBn2d: nniqat.ConvBn2d, nni.ConvBnReLU1d: nniqat.ConvBnReLU1d, nni.ConvBnReLU2d: nniqat.ConvBnReLU2d, nni.ConvReLU2d: nniqat.ConvReLU2d, nni.LinearReLU: nniqat.LinearReLU&#125; 因此，同静态量化的prepare相比，prepare_qat在多插入fake_quants、又替换了nn.Conv2d、nn.Linear之后，CivilNet网络就被改成了如下的样子：12345678910111213141516171819202122232425262728293031323334353637CivilNet( (conv): QATConv2d( 1, 1, kernel_size=(1, 1), stride=(1, 1), bias=False (activation_post_process): FakeQuantize( fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), scale=tensor([1.]), zero_point=tensor([0]) (activation_post_process): MovingAverageMinMaxObserver(min_val=tensor([]), max_val=tensor([])) ) (weight_fake_quant): FakeQuantize( fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), scale=tensor([1.]), zero_point=tensor([0]) (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([])) ) ) (fc): QATLinear( in_features=3, out_features=2, bias=False (activation_post_process): FakeQuantize( fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), scale=tensor([1.]), zero_point=tensor([0]) (activation_post_process): MovingAverageMinMaxObserver(min_val=tensor([]), max_val=tensor([])) ) (weight_fake_quant): FakeQuantize( fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), scale=tensor([1.]), zero_point=tensor([0]) (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([])) ) ) (relu): ReLU( (activation_post_process): FakeQuantize( fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), scale=tensor([1.]), zero_point=tensor([0]) (activation_post_process): MovingAverageMinMaxObserver(min_val=tensor([]), max_val=tensor([])) ) ) (quant): QuantStub( (activation_post_process): FakeQuantize( fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), scale=tensor([1.]), zero_point=tensor([0]) (activation_post_process): MovingAverageMinMaxObserver(min_val=tensor([]), max_val=tensor([])) ) ) (dequant): DeQuantStub()) 4，喂数据和静态量化完全不同，在QAT中这一步是用来训练的。我们知道，在PyTorch的网络中，前向推理逻辑都是实现在了每个op的forward函数中（参考：Gemfield：详解Pytorch中的网络构造）。而在prepare_qat中，所有的op被替换成了QAT版本的op，那么这些op的forward函数有什么特别的地方呢？ Conv2d被替换为了QATConv2d： 12def forward(self, input): return self.activation_post_process(self._conv_forward(input, self.weight_fake_quant(self.weight))) Linear被替换为了QATLinear: 12def forward(self, input): return self.activation_post_process(F.linear(input, self.weight_fake_quant(self.weight), self.bias)) ReLU还是那个ReLU，不说了。总之，你可以看出来，每个op的输入都需要经过self.weight_fake_quant来处理下，输出又都需要经过self.activation_post_process来处理下，这两个都是FakeQuantize的实例，只是里面包含的observer不一样。以Conv2d为例： 123456789#conv2dweight=functools.partial(&lt;class &#x27;torch.quantization.fake_quantize.FakeQuantize&#x27;&gt;, observer=&lt;class &#x27;torch.quantization.observer.MovingAveragePerChannelMinMaxObserver&#x27;&gt;, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, reduce_range=False, ch_axis=0))activation=functools.partial(&lt;class &#x27;torch.quantization.fake_quantize.FakeQuantize&#x27;&gt;, observer=&lt;class &#x27;torch.quantization.observer.MovingAverageMinMaxObserver&#x27;&gt;, quant_min=0, quant_max=255, reduce_range=True) 而FakeQuantize的forward函数如下所示： 1234567def forward(self, X): if self.observer_enabled[0] == 1: #使用移动平均算法计算scale和zp if self.fake_quant_enabled[0] == 1: X = torch.fake_quantize_per_channel_or_tensor_affine(X...) return X FakeQuantize中的fake_quantize_per_channel_or_tensor_affine实现了quantize和dequantize，用公式表示的话为：out = (clamp(round(x/scale + zero_point), quant_min, quant_max)-zero_point)*scale。也就是说，这是把量化的误差引入到了训练loss之中呀！ 这样，在QAT中，所有的weights和activations就像上面那样被fake quantized了，且参与模型训练中的前向和反向计算。float值被round成了（用来模拟的）int8值，但是所有的计算仍然是通过float来完成的。 这样以来，所有的权重在优化过程中都能感知到量化带来的影响，称之为量化感知训练（支持cpu和cuda），精度也因此更高。 5，转换这一步和静态量化一样，不再赘述。需要注意的是，QAT中，有一些module在prepare中已经转换成新的module了，所以静态量化中所使用的字典包含有如下的条目： 123456DEFAULT_STATIC_QUANT_MODULE_MAPPINGS = &#123; ...... # QAT modules: nnqat.Linear: nnq.Linear, nnqat.Conv2d: nnq.Conv2d,&#125; 总结下来就是： 1234567891011121314# 原始的模型，所有的tensor和计算都是浮点previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32 / linear_weight_fp32# 训练过程中，fake_quants发挥作用previous_layer_fp32 -- fq -- linear_fp32 -- activation_fp32 -- fq -- next_layer_fp32 / linear_weight_fp32 -- fq# 量化后的模型进行推理，权重和输入都是int8previous_layer_int8 -- linear_with_activation_int8 -- next_layer_int8 / linear_weight_int8 总结那么如何更方便的在你的代码中使用PyTorch的量化功能呢？一个比较优雅的方式就是使用deepvac规范——这是一个定义了PyTorch工程标准的项目：https://github.com/DeepVAC/deepvac 原文链接: PyTorch","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://huangzhiyuan.github.io/tags/pytorch/"},{"name":"quantization","slug":"quantization","permalink":"http://huangzhiyuan.github.io/tags/quantization/"}]},{"title":"简要解读内存性能","slug":"memory-brief-introduction","date":"2021-01-10T03:11:53.000Z","updated":"2021-01-10T03:25:30.000Z","comments":true,"path":"2021/01/10/memory-brief-introduction/","link":"","permalink":"http://huangzhiyuan.github.io/2021/01/10/memory-brief-introduction/","excerpt":"一台服务器，不管是物理机还是虚拟机，必不可少的就是内存，内存的性能又是如何来衡量呢。","text":"一台服务器，不管是物理机还是虚拟机，必不可少的就是内存，内存的性能又是如何来衡量呢。 内存与缓存现在比较新的CPU一般都有三级缓存，L1 Cache（32KB-256KB），L2 Cache（128KB-2MB），L3 Cache（1M-32M）。缓存逐渐变大，CPU在取数据的时候，优先从缓存去取数据，取不到才去内存取数据。 内存与时延显然，越靠近CPU，取数据的速度越块，通过LMBench进行了读数延迟的测试。 从上图可以看出： Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz 这款CPU的L1D Cache，L1I Cache为32KB，而L2 Cache为1M，L3为32M； 在对应的Cache中，时延是稳定的； 不同缓存的时延呈现指数级增长； 所以我们在写业务代码的时候，如果想要更快地提高效率，那么使得计算更加贴近CPU则可以获取更好的性能。但是从上图也可以看出，内存的时延都是纳秒为单位，而实际业务中都是毫秒为单位，优化的重点应该是那些以毫秒为单位的运算，而内存时延优化这块则是长尾部分。 内存带宽内存时延与缓存其实可谓是紧密相关，不理解透彻了，则可能测的是缓存时延。同样测试内存带宽，如果不是正确的测试，则测的是缓存带宽了。为了了解内存带宽，有必要去了解下内存与CPU的架构，早期的CPU与内存的架构还需要经过北桥总线，现在CPU与内存直接已经不需要北桥，直接通过CPU的内存控制器（IMC）进行内存读取操作： 那对应的内存带宽是怎样的呢？测试内存带宽有很多很多工具，linux下一般通过stream进行测试。简单介绍下stream的算法： stream算法的原理从上图可以看出非常简单：某个内存块之间的数据读取出来，经过简单的运算放入另一个内存块。那所谓的内存带宽：内存带宽=搬运的内存大小/耗时。通过整机合理的测试，可以测出来内存控制器的带宽。下图是某云产品的内存带宽数据： 1234567-------------------------------------------------------------Function Best Rate MB/s Avg time Min time Max timeCopy: 128728.5 0.134157 0.133458 0.136076Scale: 128656.4 0.134349 0.133533 0.137638Add: 144763.0 0.178851 0.178014 0.181158Triad: 144779.8 0.178717 0.177993 0.180214------------------------------------------------------------- 内存带宽的重要性自然不言而喻，这意味着操作内存的最大数据吞吐量。但是正确合理的测试非常重要，有几个注意事项需要关注： 内存数组大小的设置，必须要远大于L3 Cache的大小，否则就是测试缓存的吞吐性能； CPU数目很有关系，一般来说，一两个核的计算能力，是远远到不了内存带宽的，整机的CPU全部运行起来，才可以有效地测试内存带宽。当然跑单核的stream测试也有意义，可以测试内存的延时。 其他 内存与NUMA的关系：开启NUMA，可以有效地提供内存的吞吐性能，降低内存时延。 stream算法的编译方法选择：通过icc编译，可以有效地提供内存带宽性能分。原因是Intel优化了CPU的指令，通过指令向量化和指令Prefetch操作，加速了数据的读写操作以及指令操作。当然其他C代码都可以通过icc编译的方法，提供指令的效率。 原文链接：阿里云技术","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"Memory","slug":"Memory","permalink":"http://huangzhiyuan.github.io/tags/Memory/"}]},{"title":"容器、docker、虚拟机","slug":"docker-info","date":"2021-01-10T02:36:31.000Z","updated":"2021-01-10T03:03:54.000Z","comments":true,"path":"2021/01/10/docker-info/","link":"","permalink":"http://huangzhiyuan.github.io/2021/01/10/docker-info/","excerpt":"容器技术起源于Linux，是一种内核虚拟化技术，提供轻量级的虚拟化，以便隔离进程和资源。尽管容器技术已经出现很久，却是随着Docker的出现而变得广为人知。Docker是第一个使容器能在不同机器之间移植的系统。它不仅简化了打包应用的流程，也简化了打包应用的库和依赖，甚至整个操作系统的文件系统能被打包成一个简单的可移植的包，这个包可以被用来在任何其他运行Docker的机器上使用。 容器和虚拟机具有相似的资源隔离和分配方式，容器虚拟化了操作系统而不是硬件，更加便携和高效。","text":"容器技术起源于Linux，是一种内核虚拟化技术，提供轻量级的虚拟化，以便隔离进程和资源。尽管容器技术已经出现很久，却是随着Docker的出现而变得广为人知。Docker是第一个使容器能在不同机器之间移植的系统。它不仅简化了打包应用的流程，也简化了打包应用的库和依赖，甚至整个操作系统的文件系统能被打包成一个简单的可移植的包，这个包可以被用来在任何其他运行Docker的机器上使用。 容器和虚拟机具有相似的资源隔离和分配方式，容器虚拟化了操作系统而不是硬件，更加便携和高效。 docker优势相比于使用虚拟机，容器有如下优点： 更高效的利用系统资源由于容器不需要进行硬件虚拟以及运行完整操作系统等额外开销，容器对系统资源的利用率更高。无论是应用执行速度、内存损耗或者文件存储速度，都要比传统虚拟机技术更高效。因此，相比虚拟机技术，一个相同配置的主机，往往可以运行更多数量的应用。 更快速的启动时间传统的虚拟机技术启动应用服务往往需要数分钟，而Docker容器应用，由于直接运行于宿主内核，无需启动完整的操作系统，因此可以做到秒级、甚至毫秒级的启动时间，大大节约了开发、测试、部署的时间。 一致的运行环境开发过程中一个常见的问题是环境一致性问题。由于开发环境、测试环境、生产环境不一致，导致有些问题并未在开发过程中被发现。而Docker的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性。 更轻松的迁移由于Docker确保了执行环境的一致性，使得应用的迁移更加容易。Docker可以在很多平台上运行，无论是物理机、虚拟机，其运行结果是一致的。因此可以很轻易的将在一个平台上运行的应用，迁移到另一个平台上，而不用担心运行环境的变化导致应用无法正常运行的情况。 更轻松的维护和扩展Docker使用的分层存储以及镜像的技术，使得应用重复部分的复用更为容易，也使得应用的维护更新更加简单，基于基础镜像进一步扩展镜像也变得非常简单。此外，Docker团队同各个开源项目团队一起维护了大批高质量的官方镜像，既可以直接在生产环境使用，又可以作为基础进一步定制，大大的降低了应用服务的镜像制作成本。 Docker容器典型使用流程docker容器概念Docker容器有如下三个主要概念： 镜像：Docker镜像里包含了已打包的应用程序及其所依赖的环境。它包含应用程序可用的文件系统和其他元数据，如镜像运行时的可执行文件路径。 镜像仓库：Docker镜像仓库用于存放Docker镜像，以及促进不同人和不同电脑之间共享这些镜像。当编译镜像时，要么可以在编译它的电脑上运行，要么可以先上传镜像到一个镜像仓库，然后下载到另外一台电脑上并运行它。某些仓库是公开的，允许所有人从中拉取镜像，同时也有一些是私有的，仅部分人和机器可接入。 容器：Docker容器通常是一个Linux容器，它基于Docker镜像被创建。一个运行中的容器是一个运行在Docker主机上的进程，但它和主机，以及所有运行在主机上的其他进程都是隔离的。这个进程也是资源受限的，意味着它只能访问和使用分配给它的资源（CPU、内存等）。 典型的使用流程如图2所示： （1）首先开发者在开发环境机器上开发应用并制作镜像。Docker执行命令，构建镜像并存储在机器上。 （2）开发者发送上传镜像命令。Docker收到命令后，将本地镜像上传到镜像仓库。 （3）开发者向生产环境机器发送运行镜像命令。生产环境机器收到命令后，Docker会从镜像仓库拉取镜像到机器上，然后基于镜像运行容器。 使用示例下面使用Docker将基于Nginx镜像打包一个容器镜像，并基于容器镜像运行应用，然后推送到容器镜像仓库。安装DockerDocker几乎支持在所有操作系统上安装，用户可以根据需要选择要安装的Docker版本。在Linux操作系统下，可以使用如下命令快速安装Docker。 12curl -fsSL get.docker.com -o get-docker.shsh get-docker.sh 说明：CentOS 8.0操作系统使用上述脚本安装Docker会出现问题，建议使用如下命令安装较低版本Docker。 1234wget -O /etc/yum.repos.d/docker-ce.repo https://repo.huaweicloud.com/docker-ce/linux/centos/docker-ce.repo sudosed -i &#x27;s+download.docker.com+http://repo.huaweicloud.com/docker-ce+&#x27; /etc/yum.repos.d/docker-ce.repoyum install docker-ce-18.06.3.ce -ysystemctl restart docker Docker打包镜像Docker提供了一种便捷的描述应用打包的方式，叫做Dockerfile，如下所示： 12345678# 使用官方提供的Nginx镜像作为基础镜像FROM nginx:alpine# 执行一条命令修改Nginx镜像index.html的内容RUN echo &quot;hello world&quot; &gt; /usr/share/nginx/html/index.html# 允许外界访问容器的80端口EXPOSE 80 执行docker build命令打包镜像。 1docker build -t hello . 其中-t表示给镜像加一个标签，也就是给镜像取名，这里镜像名为hello。. 表示在当前目录下执行该打包命令。 执行docker images命令查看镜像，可以看到hello镜像已经创建成功。您还可以看到一个Nginx镜像，这个镜像是从镜像仓库下载下来的，作为hello镜像的基础镜像使用。 1234# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEhello latest d120ec16dcea 17 minutes ago 158MBnginx alpine eeb27ee6b893 2 months ago 148MB 本地运行容器镜像有了镜像后，您可以在本地执行docker run命令运行容器镜像。 1# docker run -p 8080:80 hello docker run命令会启动一个容器，命令中-p是将本地机器的8080端口映射到容器的80端口，即本地机器的8080端口的流量会映射到容器的80端口，当您在本地机器访问 http://127.0.0.1:8080时，就会访问到容器中，此时浏览器中返回的内容应该就是“hello world”。 把镜像推送到镜像仓库 华为云提供了容器镜像服务SWR，您也可以将镜像上传到SWR，下面演示如何将镜像推送到SWR。详细的方法请参见客户端上传镜像，本文档后续的示例中将主要使用SWR作为示例。 首先登录SWR控制台，在左侧选择“我的镜像”，然后单击右侧“客户端上传镜像”，在弹出的窗口中单击“生成临时登录指令”，然后复制该指令在本地机器上执行，登录到SWR镜像仓库。 上传镜像前需要给镜像取一个完整的名称，如下所示： 1# docker tag hello swr.cn-east-3.myhuaweicloud.com/container/hello:v1 这里http://swr.cn-east-3.myhuaweicloud.com是仓库地址，每个华为云区域的地址不同，v1则是hello镜像分配的版本号。 http://swr.cn-east-3.myhuaweicloud.com是仓库地址，每个华为云区域的地址不同。container是组织名，组织一般在SWR中创建，如果没有创建则首次上传的时候会自动创建，组织名在单个区域内全局唯一，需要选择合适的组织名称。v1则是hello镜像分配的版本号。 然后执行docker push命令就可以将镜像上传到SWR。 1# docker push swr.cn-east-3.myhuaweicloud.com/container/hello:v1 当需要使用该镜像时，使用docker pull命令拉取（下载）该命令即可。 1# docker pull swr.cn-east-3.myhuaweicloud.com/container/hello:v1 原文链接：华为云技术","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://huangzhiyuan.github.io/tags/Docker/"}]},{"title":"Linux IO原理解析和zero-copy技术揭秘","slug":"Linux-IO-and-zero-copy-tech","date":"2021-01-07T13:38:01.000Z","updated":"2021-01-07T14:56:40.000Z","comments":true,"path":"2021/01/07/Linux-IO-and-zero-copy-tech/","link":"","permalink":"http://huangzhiyuan.github.io/2021/01/07/Linux-IO-and-zero-copy-tech/","excerpt":"原文链接：https://strikefreedom.top/linux-io-and-zero-copy 早就想写篇关于介绍IO操作的技术总结，直到今天看到这篇文章介绍的如此逻辑清晰，也把自己之前掌握的知识串了起来，读完收获颇多。这里就做个搬运工，原文请参见上面链接。如今的网络应用早已从 CPU 密集型转向了 I/O 密集型，网络服务器大多是基于 C-S 模型，也即 客户端 - 服务端 模型，客户端需要和服务端进行大量的网络通信，这也决定了现代网络应用的性能瓶颈：I/O。","text":"原文链接：https://strikefreedom.top/linux-io-and-zero-copy 早就想写篇关于介绍IO操作的技术总结，直到今天看到这篇文章介绍的如此逻辑清晰，也把自己之前掌握的知识串了起来，读完收获颇多。这里就做个搬运工，原文请参见上面链接。如今的网络应用早已从 CPU 密集型转向了 I/O 密集型，网络服务器大多是基于 C-S 模型，也即 客户端 - 服务端 模型，客户端需要和服务端进行大量的网络通信，这也决定了现代网络应用的性能瓶颈：I/O。 传统的 Linux 操作系统的标准 I/O 接口是基于数据拷贝操作的，即 I/O 操作会导致数据在操作系统内核地址空间的缓冲区和用户进程地址空间定义的缓冲区之间进行传输。设置缓冲区最大的好处是可以减少磁盘 I/O 的操作，如果所请求的数据已经存放在操作系统的高速缓冲存储器中，那么就不需要再进行实际的物理磁盘 I/O 操作；然而传统的 Linux I/O 在数据传输过程中的数据拷贝操作深度依赖 CPU，也就是说 I/O 过程需要 CPU 去执行数据拷贝的操作，因此导致了极大的系统开销，限制了操作系统有效进行数据传输操作的能力。I/O 是决定网络服务器性能瓶颈的关键，而传统的 Linux I/O 机制又会导致大量的数据拷贝操作，损耗性能，所以我们亟需一种新的技术来解决数据大量拷贝的问题，这个答案就是零拷贝(Zero-copy)。 计算机存储器既然要分析 Linux I/O，就不能不了解计算机的各类存储器。存储器是计算机的核心部件之一，在完全理想的状态下，存储器应该要同时具备以下三种特性： 速度足够快：存储器的存取速度应当快于 CPU 执行一条指令，这样 CPU 的效率才不会受限于存储器容量足够大： 容量能够存储计算机所需的全部数据价格足够便宜： 价格低廉，所有类型的计算机都能配备但是现实往往是残酷的，我们目前的计算机技术无法同时满足上述的三个条件，于是现代计算机的存储器设计采用了一种分层次的结构： 从顶至底，现代计算机里的存储器类型分别有：寄存器、高速缓存、主存和磁盘，这些存储器的速度逐级递减而容量逐级递增。存取速度最快的是寄存器，因为寄存器的制作材料和 CPU 是相同的，所以速度和 CPU 一样快，CPU 访问寄存器是没有时延的，然而因为价格昂贵，因此容量也极小，一般 32 位的 CPU 配备的寄存器容量是 32 * 32 Bit，64 位的 CPU 则是 64 * 64 Bit，不管是 32 位还是 64 位，寄存器容量都小于 1 KB，且寄存器也必须通过软件自行管理。 第二层是高速缓存，也即我们平时了解的 CPU 高速缓存 L1、L2、L3，一般 L1 是每个 CPU 独享，L3 是全部 CPU 共享，而 L2 则根据不同的架构设计会被设计成独享或者共享两种模式之一，比如 Intel 的多核芯片采用的是共享 L2 模式而 AMD 的多核芯片则采用的是独享 L2 模式。 第三层则是主存，也即主内存，通常称作随机访问存储器（Random Access Memory, RAM）。是与 CPU 直接交换数据的内部存储器。它可以随时读写（刷新时除外），而且速度很快，通常作为操作系统或其他正在运行中的程序的临时资料存储介质。 第三层则是主存，也即主内存，通常称作随机访问存储器（Random Access Memory, RAM）。是与 CPU 直接交换数据的内部存储器。它可以随时读写（刷新时除外），而且速度很快，通常作为操作系统或其他正在运行中的程序的临时资料存储介质。 主内存是操作系统进行 I/O 操作的重中之重，绝大部分的工作都是在用户进程和内核的内存缓冲区里完成的，因此我们接下来需要提前学习一些主存的相关原理。 物理内存我们平时一直提及的物理内存就是上文中对应的第三种计算机存储器，RAM 主存，它在计算机中以内存条的形式存在，嵌在主板的内存槽上，用来加载各式各样的程序与数据以供 CPU 直接运行和使用。 虚拟内存在计算机领域有一句如同摩西十诫般神圣的哲言：”计算机科学领域的任何问题都可以通过增加一个间接的中间层来解决“，从内存管理、网络模型、并发调度甚至是硬件架构，都能看到这句哲言在闪烁着光芒，而虚拟内存则是这一哲言的完美实践之一。 虚拟内存是现代计算机中的一个非常重要的存储器抽象，主要是用来解决应用程序日益增长的内存使用需求：现代物理内存的容量增长已经非常快速了，然而还是跟不上应用程序对主存需求的增长速度，对于应用程序来说内存还是不够用，因此便需要一种方法来解决这两者之间的容量差矛盾。 计算机对多程序内存访问的管理经历了 静态重定位 –&gt; 动态重定位 –&gt; 交换(swapping)技术 –&gt; 虚拟内存，最原始的多程序内存访问是直接访问绝对内存地址，这种方式几乎是完全不可用的方案，因为如果每一个程序都直接访问物理内存地址的话，比如两个程序并发执行以下指令的时候： 123456789mov cx, 2mov bx, 1000Hmov ds, bxmov [0], cx...mov ax, [0]add ax, ax 这一段汇编表示在地址 1000:0 处存入数值 2，然后在后面的逻辑中把该地址的值取出来乘以 2，最终存入 ax 寄存器的值就是 4，如果第二个程序存入 cx 寄存器里的值是 3，那么并发执行的时候，第一个程序最终从 ax 寄存器里得到的值就可能是 6，这就完全错误了，得到脏数据还顶多算程序结果错误，要是其他程序往特定的地址里写入一些危险的指令而被另一个程序取出来执行，还可能会导致整个系统的崩溃。所以，为了确保进程间互不干扰，每一个用户进程都需要实时知晓当前其他进程在使用哪些内存地址，这对于写程序的人来说无疑是一场噩梦。 因此，操作绝对内存地址是完全不可行的方案，那就只能用操作相对内存地址，我们知道每个进程都会有自己的进程地址，从 0 开始，可以通过相对地址来访问内存，但是这同样有问题，还是前面类似的问题，比如有两个大小为 16KB 的程序 A 和 B，现在它们都被加载进了内存，内存地址段分别是 0 ~ 16384，16384 ~ 32768。A 的第一条指令是 jmp 1024，而在地址 1024 处是一条 mov 指令，下一条指令是 add，基于前面的 mov 指令做加法运算，与此同时，B 的第一条指令是 jmp 1028，本来在 B 的相对地址 1028 处应该也是一条 mov 去操作自己的内存地址上的值，但是由于这两个程序共享了段寄存器，因此虽然他们使用了各自的相对地址，但是依然操作的还是绝对内存地址，于是 B 就会跳去执行 add 指令，这时候就会因为非法的内存操作而 crash。 有一种静态重定位的技术可以解决这个问题，它的工作原理非常简单粗暴：当 B 程序被加载到地址 16384 处之后，把 B 的所有相对内存地址都加上 16384，这样的话当 B 执行 jmp 1028 之时，其实执行的是 jmp 1028+16384，就可以跳转到正确的内存地址处去执行正确的指令了，但是这种技术并不通用，而且还会对程序装载进内存的性能有影响。 再往后，就发展出来了存储器抽象：地址空间，就好像进程是 CPU 的抽象，地址空间则是存储器的抽象，每个进程都会分配独享的地址空间，但是独享的地址空间又带来了新的问题：如何实现不同进程的相同相对地址指向不同的物理地址？最开始是使用动态重定位技术来实现，这是用一种相对简单的地址空间到物理内存的映射方法。基本原理就是为每一个 CPU 配备两个特殊的硬件寄存器：基址寄存器和界限寄存器，用来动态保存每一个程序的起始物理内存地址和长度，比如前文中的 A，B 两个程序，当 A 运行时基址寄存器和界限寄存器就会分别存入 0 和 16384，而当 B 运行时则两个寄存器又会分别存入 16384 和 32768。然后每次访问指定的内存地址时，CPU 会在把地址发往内存总线之前自动把基址寄存器里的值加到该内存地址上，得到一个真正的物理内存地址，同时还会根据界限寄存器里的值检查该地址是否溢出，若是，则产生错误中止程序，动态重定位技术解决了静态重定位技术造成的程序装载速度慢的问题，但是也有新问题：每次访问内存都需要进行加法和比较运算，比较运算本身可以很快，但是加法运算由于进位传递时间的问题，除非使用特殊的电路，否则会比较慢。 然后就是 交换（swapping）技术，这种技术简单来说就是动态地把程序在内存和磁盘之间进行交换保存，要运行一个进程的时候就把程序的代码段和数据段调入内存，然后再把程序封存，存入磁盘，如此反复。为什么要这么麻烦？因为前面那两种重定位技术的前提条件是计算机内存足够大，能够把所有要运行的进程地址空间都加载进主存，才能够并发运行这些进程，但是现实往往不是如此，内存的大小总是有限的，所有就需要另一类方法来处理内存超载的情况，第一种便是简单的交换技术： 先把进程 A 换入内存，然后启动进程 B 和 C，也换入内存，接着 A 被从内存交换到磁盘，然后又有新的进程 D 调入内存，用了 A 退出之后空出来的内存空间，最后 A 又被重新换入内存，由于内存布局已经发生了变化，所以 A 在换入内存之时会通过软件或者在运行期间通过硬件（基址寄存器和界限寄存器）对其内存地址进行重定位，多数情况下都是通过硬件。 另一种处理内存超载的技术就是虚拟内存技术了，它比交换（swapping）技术更复杂而又更高效，是目前最新应用最广泛的存储器抽象技术： 虚拟内存的核心原理是：为每个程序设置一段”连续”的虚拟地址空间，把这个地址空间分割成多个具有连续地址范围的页 (page)，并把这些页和物理内存做映射，在程序运行期间动态映射到物理内存。当程序引用到一段在物理内存的地址空间时，由硬件立刻执行必要的映射；而当程序引用到一段不在物理内存中的地址空间时，由操作系统负责将缺失的部分装入物理内存并重新执行失败的指令： 虚拟地址空间按照固定大小划分成被称为页（page）的若干单元，物理内存中对应的则是页框（page frame）。这两者一般来说是一样的大小，如上图中的是 4KB，不过实际上计算机系统中一般是 512 字节到 1 GB，这就是虚拟内存的分页技术。因为是虚拟内存空间，每个进程分配的大小是 4GB (32 位架构)，而实际上当然不可能给所有在运行中的进程都分配 4GB 的物理内存，所以虚拟内存技术还需要利用到前面介绍的交换（swapping）技术，在进程运行期间只分配映射当前使用到的内存，暂时不使用的数据则写回磁盘作为副本保存，需要用的时候再读入内存，动态地在磁盘和内存之间交换数据。 其实虚拟内存技术从某种角度来看的话，很像是糅合了基址寄存器和界限寄存器之后的新技术。它使得整个进程的地址空间可以通过较小的单元映射到物理内存，而不需要为程序的代码和数据地址进行重定位。 进程在运行期间产生的内存地址都是虚拟地址，如果计算机没有引入虚拟内存这种存储器抽象技术的话，则 CPU 会把这些地址直接发送到内存地址总线上，直接访问和虚拟地址相同值的物理地址；如果使用虚拟内存技术的话，CPU 则是把这些虚拟地址通过地址总线送到内存管理单元（Memory Management Unit，MMU），MMU 将虚拟地址映射为物理地址之后再通过内存总线去访问物理内存： 虚拟地址（比如 16 位地址 8196=0010 000000000100）分为两部分：虚拟页号（高位部分）和偏移量（低位部分），虚拟地址转换成物理地址是通过页表（page table）来实现的，页表由页表项构成，页表项中保存了页框号、修改位、访问位、保护位和 “在/不在” 位等信息，从数学角度来说页表就是一个函数，入参是虚拟页号，输出是物理页框号，得到物理页框号之后复制到寄存器的高三位中，最后直接把 12 位的偏移量复制到寄存器的末 12 位构成 15 位的物理地址，即可以把该寄存器的存储的物理内存地址发送到内存总线： 在 MMU 进行地址转换时，如果页表项的 “在/不在” 位是 0，则表示该页面并没有映射到真实的物理页框，则会引发一个缺页中断，CPU 陷入操作系统内核，接着操作系统就会通过页面置换算法选择一个页面将其换出 (swap)，以便为即将调入的新页面腾出位置，如果要换出的页面的页表项里的修改位已经被设置过，也就是被更新过，则这是一个脏页 (dirty page)，需要写回磁盘更新改页面在磁盘上的副本，如果该页面是”干净”的，也就是没有被修改过，则直接用调入的新页面覆盖掉被换出的旧页面即可。 最后，还需要了解的一个概念是转换检测缓冲器（Translation Lookaside Buffer，TLB），也叫快表，是用来加速虚拟地址映射的，因为虚拟内存的分页机制，页表一般是保存内存中的一块固定的存储区，导致进程通过 MMU 访问内存比直接访问内存多了一次内存访问，性能至少下降一半，因此需要引入加速机制，即 TLB 快表，TLB 可以简单地理解成页表的高速缓存，保存了最高频被访问的页表项，由于一般是硬件实现的，因此速度极快，MMU收到虚拟地址时一般会先通过硬件 TLB 查询对应的页表号，若命中且该页表项的访问操作合法，则直接从 TLB 取出对应的物理页框号返回，若不命中则穿透到内存页表里查询，并且会用这个从内存页表里查询到最新页表项替换到现有 TLB 里的其中一个，以备下次缓存命中。 至此，我们介绍完了包含虚拟内存在内的多项计算机存储器抽象技术，虚拟内存的其他内容比如针对大内存的多级页表、倒排页表，以及处理缺页中断的页面置换算法等等，以后有机会再单独写一篇文章介绍，或者各位读者也可以先行去查阅相关资料了解，这里就不再深入了。 用户态和内核态一般来说，我们在编写程序操作 Linux I/O 之时十有八九是在用户空间和内核空间之间传输数据，因此有必要先了解一下 Linux 的用户态和内核态的概念。 首先是用户态和内核态： 从宏观上来看，Linux 操作系统的体系架构分为用户态和内核态（或者用户空间和内核）。内核从本质上看是一种软件 —— 控制计算机的硬件资源，并提供上层应用程序 (进程) 运行的环境。用户态即上层应用程序 (进程) 的运行空间，应用程序 (进程) 的执行必须依托于内核提供的资源，这其中包括但不限于 CPU 资源、存储资源、I/O 资源等等。 现代操作系统都是采用虚拟存储器，那么对 32 位操作系统而言，它的寻址空间（虚拟存储空间）为 2^32 B = 4G。操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核（kernel），保证内核的安全，操心系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。针对 Linux 操作系统而言，将最高的 1G 字节（从虚拟地址 0xC0000000 到 0xFFFFFFFF），供内核使用，称为内核空间，而将较低的 3G 字节（从虚拟地址 0x00000000 到 0xBFFFFFFF），供各个进程使用，称为用户空间。 因为操作系统的资源是有限的，如果访问资源的操作过多，必然会消耗过多的系统资源，而且如果不对这些操作加以区分，很可能造成资源访问的冲突。所以，为了减少有限资源的访问和使用冲突，Unix/Linux 的设计哲学之一就是：对不同的操作赋予不同的执行等级，就是所谓特权的概念。简单说就是有多大能力做多大的事，与系统相关的一些特别关键的操作必须由最高特权的程序来完成。Intel 的 x86 架构的 CPU 提供了 0 到 3 四个特权级，数字越小，特权越高，Linux 操作系统中主要采用了 0 和 3 两个特权级，分别对应的就是内核态和用户态。运行于用户态的进程可以执行的操作和访问的资源都会受到极大的限制，而运行在内核态的进程则可以执行任何操作并且在资源的使用上没有限制。很多程序开始时运行于用户态，但在执行的过程中，一些操作需要在内核权限下才能执行，这就涉及到一个从用户态切换到内核态的过程。比如 C 函数库中的内存分配函数 malloc()，它具体是使用 sbrk() 系统调用来分配内存，当 malloc 调用 sbrk() 的时候就涉及一次从用户态到内核态的切换，类似的函数还有 printf()，调用的是 wirte() 系统调用来输出字符串，等等。 用户进程在系统中运行时，大部分时间是处在用户态空间里的，在其需要操作系统帮助完成一些用户态没有特权和能力完成的操作时就需要切换到内核态。那么用户进程如何切换到内核态去使用那些内核资源呢？答案是：1) 系统调用（trap），2) 异常（exception）和 3) 中断（interrupt）。 系统调用：用户进程主动发起的操作。用户态进程发起系统调用主动要求切换到内核态，陷入内核之后，由操作系统来操作系统资源，完成之后再返回到进程。 异常：被动的操作，且用户进程无法预测其发生的时机。当用户进程在运行期间发生了异常（比如某条指令出了问题），这时会触发由当前运行进程切换到处理此异常的内核相关进程中，也即是切换到了内核态。异常包括程序运算引起的各种错误如除 0、缓冲区溢出、缺页等。 中断：当外围设备完成用户请求的操作后，会向 CPU 发出相应的中断信号，这时 CPU 会暂停执行下一条即将要执行的指令而转到与中断信号对应的处理程序去执行，如果前面执行的指令是用户态下的程序，那么转换的过程自然就会是从用户态到内核态的切换。中断包括 I/O 中断、外部信号中断、各种定时器引起的时钟中断等。中断和异常类似，都是通过中断向量表来找到相应的处理程序进行处理。区别在于，中断来自处理器外部，不是由任何一条专门的指令造成，而异常是执行当前指令的结果。 通过上面的分析，我们可以得出 Linux 的内部层级可分为三大部分： 用户空间； 内核空间； 硬件。 Linux I/OI/O 缓冲区 在 Linux 中，当程序调用各类文件操作函数后，用户数据（User Data）到达磁盘（Disk）的流程如上图所示。 图中描述了 Linux 中文件操作函数的层级关系和内存缓存层的存在位置，中间的黑色实线是用户态和内核态的分界线。 read(2)/write(2) 是 Linux 系统中最基本的 I/O 读写系统调用，我们开发操作 I/O 的程序时必定会接触到它们，而在这两个系统调用和真实的磁盘读写之间存在一层称为 Kernel buffer cache 的缓冲区缓存。在 Linux 中 I/O 缓存其实可以细分为两个：Page Cache 和 Buffer Cache，这两个其实是一体两面，共同组成了 Linux 的内核缓冲区（Kernel Buffer Cache）： 读磁盘：内核会先检查 Page Cache 里是不是已经缓存了这个数据，若是，直接从这个内存缓冲区里读取返回，若否，则穿透到磁盘去读取，然后再缓存在 Page Cache 里，以备下次缓存命中； 写磁盘：内核直接把数据写入 Page Cache，并把对应的页标记为 dirty，添加到 dirty list 里，然后就直接返回，内核会定期把 dirty list 的页缓存 flush 到磁盘，保证页缓存和磁盘的最终一致性。 Page Cache 会通过页面置换算法如 LRU 定期淘汰旧的页面，加载新的页面。可以看出，所谓 I/O 缓冲区缓存就是在内核和磁盘、网卡等外设之间的一层缓冲区，用来提升读写性能的。 在 Linux 还不支持虚拟内存技术之前，还没有页的概念，因此 Buffer Cache 是基于操作系统读写磁盘的最小单位 – 块（block）来进行的，所有的磁盘块操作都是通过 Buffer Cache 来加速，Linux 引入虚拟内存的机制来管理内存后，页成为虚拟内存管理的最小单位，因此也引入了 Page Cache 来缓存 Linux 文件内容，主要用来作为文件系统上的文件数据的缓存，提升读写性能，常见的是针对文件的 read()/write() 操作，另外也包括了通过 mmap() 映射之后的块设备，也就是说，事实上 Page Cache 负责了大部分的块设备文件的缓存工作。而 Buffer Cache 用来在系统对块设备进行读写的时候，对块进行数据缓存的系统来使用，实际上负责所有对磁盘的 I/O 访问： 因为 Buffer Cache 是对粒度更细的设备块的缓存，而 Page Cache 是基于虚拟内存的页单元缓存，因此还是会基于 Buffer Cache，也就是说如果是缓存文件内容数据就会在内存里缓存两份相同的数据，这就会导致同一份文件保存了两份，冗余且低效。另外一个问题是，调用 write 后，有效数据是在 Buffer Cache 中，而非 Page Cache 中。这就导致 mmap 访问的文件数据可能存在不一致问题。为了规避这个问题，所有基于磁盘文件系统的 write，都需要调用 update_vm_cache() 函数，该操作会把调用 write 之后的 Buffer Cache 更新到 Page Cache 去。由于有这些设计上的弊端，因此在 Linux 2.4 版本之后，kernel 就将两者进行了统一，Buffer Cache 不再以独立的形式存在，而是以融合的方式存在于 Page Cache 中： 融合之后就可以统一操作 Page Cache 和 Buffer Cache：处理文件 I/O 缓存交给 Page Cache，而当底层 RAW device 刷新数据时以 Buffer Cache 的块单位来实际处理。 I/O 模式在 Linux 或者其他 Unix-like 操作系统里，I/O 模式一般有三种： 程序控制 I/O 中断驱动 I/O DMA I/O 程序控制 I/O这是最简单的一种 I/O 模式，也叫忙等待或者轮询：用户通过发起一个系统调用，陷入内核态，内核将系统调用翻译成一个对应设备驱动程序的过程调用，接着设备驱动程序会启动 I/O 不断循环去检查该设备，看看是否已经就绪，一般通过返回码来表示，I/O 结束之后，设备驱动程序会把数据送到指定的地方并返回，切回用户态。比如发起系统调用 read()： 中断驱动 I/O第二种 I/O 模式是利用中断来实现的： 流程如下： 用户进程发起一个 read() 系统调用读取磁盘文件，陷入内核态并由其所在的 CPU 通过设备驱动程序向设备寄存器写入一个通知信号，告知设备控制器 (我们这里是磁盘控制器)要读取数据； 磁盘控制器启动磁盘读取的过程，把数据从磁盘拷贝到磁盘控制器缓冲区里； 完成拷贝之后磁盘控制器会通过总线发送一个中断信号到中断控制器，如果此时中断控制器手头还有正在处理的中断或者有一个和该中断信号同时到达的更高优先级的中断，则这个中断信号将被忽略，而磁盘控制器会在后面持续发送中断信号直至中断控制器受理； 中断控制器收到磁盘控制器的中断信号之后会通过地址总线存入一个磁盘设备的编号，表示这次中断需要关注的设备是磁盘； 中断控制器向 CPU 置起一个磁盘中断信号； CPU 收到中断信号之后停止当前的工作，把当前的 PC/PSW 等寄存器压入堆栈保存现场，然后从地址总线取出设备编号，通过编号找到中断向量所包含的中断服务的入口地址，压入 PC 寄存器，开始运行磁盘中断服务，把数据从磁盘控制器的缓冲区拷贝到主存里的内核缓冲区； 最后 CPU 再把数据从内核缓冲区拷贝到用户缓冲区，完成读取操作，read() 返回，切换回用户态。 DMA I/O并发系统的性能高低究其根本，是取决于如何对 CPU 资源的高效调度和使用，而回头看前面的中断驱动 I/O 模式的流程，可以发现第 6、7 步的数据拷贝工作都是由 CPU 亲自完成的，也就是在这两次数据拷贝阶段中 CPU 是完全被占用而不能处理其他工作的，那么这里明显是有优化空间的；第 7 步的数据拷贝是从内核缓冲区到用户缓冲区，都是在主存里，所以这一步只能由 CPU 亲自完成，但是第 6 步的数据拷贝，是从磁盘控制器的缓冲区到主存，是两个设备之间的数据传输，这一步并非一定要 CPU 来完成，可以借助 DMA 来完成，减轻 CPU 的负担。 DMA 全称是 Direct Memory Access，也即直接存储器存取，是一种用来提供在外设和存储器之间或者存储器和存储器之间的高速数据传输。整个过程无须 CPU 参与，数据直接通过 DMA 控制器进行快速地移动拷贝，节省 CPU 的资源去做其他工作。 目前，大部分的计算机都配备了 DMA 控制器，而 DMA 技术也支持大部分的外设和存储器。借助于 DMA 机制，计算机的 I/O 过程就能更加高效： DMA 控制器内部包含若干个可以被 CPU 读写的寄存器：一个主存地址寄存器 MAR（存放要交换数据的主存地址）、一个外设地址寄存器 ADR（存放 I/O 设备的设备码，或者是设备信息存储区的寻址信息）、一个字节数寄存器 WC（对传送数据的总字数进行统计）、和一个或多个控制寄存器。 用户进程发起一个 read() 系统调用读取磁盘文件，陷入内核态并由其所在的 CPU 通过设置 DMA 控制器的寄存器对它进行编程：把内核缓冲区和磁盘文件的地址分别写入 MAR 和 ADR 寄存器，然后把期望读取的字节数写入 WC 寄存器，启动 DMA 控制器； DMA 控制器根据 ADR 寄存器里的信息知道这次 I/O 需要读取的外设是磁盘的某个地址，便向磁盘控制器发出一个命令，通知它从磁盘读取数据到其内部的缓冲区里； 磁盘控制器启动磁盘读取的过程，把数据从磁盘拷贝到磁盘控制器缓冲区里，并对缓冲区内数据的校验和进行检验，如果数据是有效的，那么 DMA 就可以开始了； DMA 控制器通过总线向磁盘控制器发出一个读请求信号从而发起 DMA 传输，这个信号和前面的中断驱动 I/O 小节里 CPU 发给磁盘控制器的读请求是一样的，它并不知道或者并不关心这个读请求是来自 CPU 还是 DMA 控制器； 紧接着 DMA 控制器将引导磁盘控制器将数据传输到 MAR 寄存器里的地址，也就是内核缓冲区； 数据传输完成之后，返回一个 ack 给 DMA 控制器，WC 寄存器里的值会减去相应的数据长度，如果 WC 还不为 0，则重复第 4 步到第 6 步，一直到 WC 里的字节数等于 0； 收到 ack 信号的 DMA 控制器会通过总线发送一个中断信号到中断控制器，如果此时中断控制器手头还有正在处理的中断或者有一个和该中断信号同时到达的更高优先级的中断，则这个中断信号将被忽略，而 DMA 控制器会在后面持续发送中断信号直至中断控制器受理； 中断控制器收到磁盘控制器的中断信号之后会通过地址总线存入一个主存设备的编号，表示这次中断需要关注的设备是主存； 中断控制器向 CPU 置起一个 DMA 中断的信号； CPU 收到中断信号之后停止当前的工作，把当前的 PC/PSW 等寄存器压入堆栈保存现场，然后从地址总线取出设备编号，通过编号找到中断向量所包含的中断服务的入口地址，压入 PC 寄存器，开始运行 DMA 中断服务，把数据从内核缓冲区拷贝到用户缓冲区，完成读取操作，read() 返回，切换回用户态。 传统 I/O 读写模式Linux 中传统的 I/O 读写是通过 read()/write() 系统调用完成的，read() 把数据从存储器 (磁盘、网卡等) 读取到用户缓冲区，write() 则是把数据从用户缓冲区写出到存储器： 1234#include &lt;unistd.h&gt;ssize_t read(int fd, void *buf, size_t count);ssize_t write(int fd, const void *buf, size_t count); 一次完整的读磁盘文件然后写出到网卡的底层传输过程如下： 可以清楚看到这里一共触发了 4 次用户态和内核态的上下文切换，分别是 read()/write() 调用和返回时的切换，2 次 DMA 拷贝，2 次 CPU 拷贝，加起来一共 4 次拷贝操作。 通过引入 DMA，我们已经把 Linux 的 I/O 过程中的 CPU 拷贝次数从 4 次减少到了 2 次，但是 CPU 拷贝依然是代价很大的操作，对系统性能的影响还是很大，特别是那些频繁 I/O 的场景，更是会因为 CPU 拷贝而损失掉很多性能，我们需要进一步优化，降低、甚至是完全避免 CPU 拷贝。 零拷贝 (Zero-copy)Zero-copy 是什么？Wikipedia 的解释如下： 1&quot;Zero-copy&quot; describes computer operations in which the CPU does not perform the task of copying data from one memory area to another. This is frequently used to save CPU cycles and memory bandwidth when transmitting a file over a network. 零拷贝技术是指计算机执行操作时，CPU不需要先将数据从某处内存复制到另一个特定区域。这种技术通常用于通过网络传输文件时节省CPU周期和内存带宽。 Zero-copy 能做什么？ 减少甚至完全避免操作系统内核和用户应用程序地址空间这两者之间进行数据拷贝操作，从而减少用户态 – 内核态上下文切换带来的系统开销。 减少甚至完全避免操作系统内核缓冲区之间进行数据拷贝操作。 帮助用户进程绕开操作系统内核空间直接访问硬件存储接口操作数据。 利用 DMA 而非 CPU 来完成硬件接口和内核缓冲区之间的数据拷贝，从而解放 CPU，使之能去执行其他的任务，提升系统性能。 Zero-copy 的实现方式有哪些？从 zero-copy 这个概念被提出以来，相关的实现技术便犹如雨后春笋，层出不穷。但是截至目前为止，并没有任何一种 zero-copy 技术能满足所有的场景需求，还是计算机领域那句无比经典的名言：”There is no silver bullet”!而在 Linux 平台上，同样也有很多的 zero-copy 技术，新旧各不同，可能存在于不同的内核版本里，很多技术可能有了很大的改进或者被更新的实现方式所替代，这些不同的实现技术按照其核心思想可以归纳成大致的以下三类： 减少甚至避免用户空间和内核空间之间的数据拷贝：在一些场景下，用户进程在数据传输过程中并不需要对数据进行访问和处理，那么数据在 Linux 的 Page Cache 和用户进程的缓冲区之间的传输就完全可以避免，让数据拷贝完全在内核里进行，甚至可以通过更巧妙的方式避免在内核里的数据拷贝。这一类实现一般是通过增加新的系统调用来完成的，比如 Linux 中的 mmap()，sendfile() 以及 splice() 等。 绕过内核的直接 I/O：允许在用户态进程绕过内核直接和硬件进行数据传输，内核在传输过程中只负责一些管理和辅助的工作。这种方式其实和第一种有点类似，也是试图避免用户空间和内核空间之间的数据传输，只是第一种方式是把数据传输过程放在内核态完成，而这种方式则是直接绕过内核和硬件通信，效果类似但原理完全不同。 内核缓冲区和用户缓冲区之间的传输优化：这种方式侧重于在用户进程的缓冲区和操作系统的页缓存之间的 CPU 拷贝的优化。这种方法延续了以往那种传统的通信方式，但更灵活。 减少甚至避免用户空间和内核空间之间的数据拷贝mmap() 1234#include &lt;sys/mman.h&gt;void *mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset);int munmap(void *addr, size_t length); 一种简单的实现方案是在一次读写过程中用 Linux 的另一个系统调用 mmap() 替换原先的 read()，mmap() 也即是内存映射（memory map）：把用户进程空间的一段内存缓冲区（user buffer）映射到文件所在的内核缓冲区（kernel buffer）上。 利用 mmap() 替换 read()，配合 write() 调用的整个流程如下： 用户进程调用 mmap()，从用户态陷入内核态，将内核缓冲区映射到用户缓存区； DMA 控制器将数据从硬盘拷贝到内核缓冲区； mmap() 返回，上下文从内核态切换回用户态； 用户进程调用 write()，尝试把文件数据写到内核里的套接字缓冲区，再次陷入内核态； CPU 将内核缓冲区中的数据拷贝到的套接字缓冲区； DMA 控制器将数据从套接字缓冲区拷贝到网卡完成数据传输； write() 返回，上下文从内核态切换回用户态。 通过这种方式，有两个优点：一是节省内存空间，因为用户进程上的这一段内存是虚拟的，并不真正占据物理内存，只是映射到文件所在的内核缓冲区上，因此可以节省一半的内存占用；二是省去了一次 CPU 拷贝，对比传统的 Linux I/O 读写，数据不需要再经过用户进程进行转发了，而是直接在内核里就完成了拷贝。所以使用 mmap() 之后的拷贝次数是 2 次 DMA 拷贝，1 次 CPU 拷贝，加起来一共 3 次拷贝操作，比传统的 I/O 方式节省了一次 CPU 拷贝以及一半的内存，不过因为 mmap() 也是一个系统调用，因此用户态和内核态的切换还是 4 次。 mmap() 因为既节省 CPU 拷贝次数又节省内存，所以比较适合大文件传输的场景。虽然 mmap() 完全是符合 POSIX 标准的，但是它也不是完美的，因为它并不总是能达到理想的数据传输性能。首先是因为数据数据传输过程中依然需要一次 CPU 拷贝，其次是内存映射技术是一个开销很大的虚拟存储操作：这种操作需要修改页表以及用内核缓冲区里的文件数据汰换掉当前 TLB 里的缓存以维持虚拟内存映射的一致性。但是，因为内存映射通常针对的是相对较大的数据区域，所以对于相同大小的数据来说，内存映射所带来的开销远远低于 CPU 拷贝所带来的开销。此外，使用 mmap() 还可能会遇到一些需要值得关注的特殊情况，例如，在 mmap() –&gt; write() 这两个系统调用的整个传输过程中，如果有其他的进程突然截断了这个文件，那么这时用户进程就会因为访问非法地址而被一个从总线传来的 SIGBUS 中断信号杀死并且产生一个 core dump。有两种解决办法： 设置一个信号处理器，专门用来处理 SIGBUS 信号，这个处理器直接返回， write() 就可以正常返回已写入的字节数而不会被 SIGBUS 中断，errno 错误码也会被设置成 success。然而这实际上是一个掩耳盗铃的解决方案，因为 BIGBUS 信号的带来的信息是系统发生了一些很严重的错误，而我们却选择忽略掉它，一般不建议采用这种方式。 通过内核的文件租借锁（这是 Linux 的叫法，Windows 上称之为机会锁）来解决这个问题，这种方法相对来说更好一些。我们可以通过内核对文件描述符上读/写的租借锁，当另外一个进程尝试对当前用户进程正在进行传输的文件进行截断的时候，内核会发送给用户一个实时信号：RT_SIGNAL_LEASE 信号，这个信号会告诉用户内核正在破坏你加在那个文件上的读/写租借锁，这时 write() 系统调用会被中断，并且当前用户进程会被 SIGBUS 信号杀死，返回值则是中断前写的字节数，errno 同样会被设置为 success。文件租借锁需要在对文件进行内存映射之前设置，最后在用户进程结束之前释放掉。 sendfile()在 Linux 内核 2.1 版本中，引入了一个新的系统调用 sendfile()： 123#include &lt;sys/sendfile.h&gt;ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count); 从功能上来看，这个系统调用将 mmap() + write() 这两个系统调用合二为一，实现了一样效果的同时还简化了用户接口，其他的一些 Unix-like 的系统像 BSD、Solaris 和 AIX 等也有类似的实现，甚至 Windows 上也有一个功能类似的 API 函数 TransmitFile。 out_fd 和 in_fd 分别代表了写入和读出的文件描述符，in_fd 必须是一个指向文件的文件描述符，且要能支持类 mmap() 内存映射，不能是 Socket 类型，而 out_fd 在 Linux 内核 2.6.33 版本之前只能是一个指向 Socket 的文件描述符，从 2.6.33 之后则可以是任意类型的文件描述符。off_t 是一个代表了 in_fd 偏移量的指针，指示 sendfile() 该从 in_fd 的哪个位置开始读取，函数返回后，这个指针会被更新成 sendfile() 最后读取的字节位置处，表明此次调用共读取了多少文件数据，最后的 count 参数则是此次调用需要传输的字节总数。 使用 sendfile() 完成一次数据读写的流程如下： 用户进程调用 sendfile() 从用户态陷入内核态； DMA 控制器将数据从硬盘拷贝到内核缓冲区； CPU 将内核缓冲区中的数据拷贝到套接字缓冲区； DMA 控制器将数据从套接字缓冲区拷贝到网卡完成数据传输； sendfile() 返回，上下文从内核态切换回用户态。 基于 sendfile()， 整个数据传输过程中共发生 2 次 DMA 拷贝和 1 次 CPU 拷贝，这个和 mmap() + write() 相同，但是因为 sendfile() 只是一次系统调用，因此比前者少了一次用户态和内核态的上下文切换开销。读到这里，聪明的读者应该会开始提问了：”sendfile() 会不会遇到和 mmap() + write() 相似的文件截断问题呢？”，很不幸，答案是肯定的。sendfile() 一样会有文件截断的问题，但欣慰的是，sendfile() 不仅比 mmap() + write() 在接口使用上更加简洁，而且处理文件截断时也更加优雅：如果 sendfile() 过程中遭遇文件截断，则 sendfile() 系统调用会被中断杀死之前返回给用户进程其中断前所传输的字节数，errno 会被设置为 success，无需用户提前设置信号处理器，当然你要设置一个进行个性化处理也可以，也不需要像之前那样提前给文件描述符设置一个租借锁，因为最终结果还是一样的。 sendfile() 相较于 mmap() 的另一个优势在于数据在传输过程中始终没有越过用户态和内核态的边界，因此极大地减少了存储管理的开销。即便如此，sendfile() 依然是一个适用性很窄的技术，最适合的场景基本也就是一个静态文件服务器了。而且根据 Linus 在 2001 年和其他内核维护者的邮件列表内容，其实当初之所以决定在 Linux 上实现 sendfile() 仅仅是因为在其他操作系统平台上已经率先实现了，而且有大名鼎鼎的 Apache Web 服务器已经在使用了，为了兼容 Apache Web 服务器才决定在 Linux 上也实现这个技术，而且 sendfile() 实现上的简洁性也和 Linux 内核的其他部分集成得很好，所以 Linus 也就同意了这个提案。 然而 sendfile() 本身是有很大问题的，从不同的角度来看的话主要是： 首先一个是这个接口并没有进行标准化，导致 sendfile() 在 Linux 上的接口实现和其他类 Unix 系统的实现并不相同； 其次由于网络传输的异步性，很难在接收端实现和 sendfile() 对接的技术，因此接收端一直没有实现对应的这种技术； 最后从性能方面考量，因为 sendfile() 在把磁盘文件从内核缓冲区（page cache）传输到到套接字缓冲区的过程中依然需要 CPU 参与，这就很难避免 CPU 的高速缓存被传输的数据所污染。 此外，需要说明下，sendfile() 的最初设计并不是用来处理大文件的，因此如果需要处理很大的文件的话，可以使用另一个系统调用 sendfile64()，它支持对更大的文件内容进行寻址和偏移。 sendﬁle() with DMA Scatter/Gather Copy 上一小节介绍的 sendfile() 技术已经把一次数据读写过程中的 CPU 拷贝的降低至只有 1 次了，但是人永远是贪心和不知足的，现在如果想要把这仅有的一次 CPU 拷贝也去除掉，有没有办法呢？当然有！通过引入一个新硬件上的支持，我们可以把这个仅剩的一次 CPU 拷贝也给抹掉：Linux 在内核 2.4 版本里引入了 DMA 的 scatter/gather – 分散/收集功能，并修改了 sendfile() 的代码使之和 DMA 适配。scatter 使得 DMA 拷贝可以不再需要把数据存储在一片连续的内存空间上，而是允许离散存储，gather 则能够让 DMA 控制器根据少量的元信息：一个包含了内存地址和数据大小的缓冲区描述符，收集存储在各处的数据，最终还原成一个完整的网络包，直接拷贝到网卡而非套接字缓冲区，避免了最后一次的 CPU 拷贝： sendfile() + DMA gather 的数据传输过程如下： 用户进程调用 sendfile()，从用户态陷入内核态； DMA 控制器使用 scatter 功能把数据从硬盘拷贝到内核缓冲区进行离散存储； CPU 把包含内存地址和数据长度的缓冲区描述符拷贝到套接字缓冲区，DMA 控制器能够根据这些信息生成网络包数据分组的报头和报尾 DMA 控制器根据缓冲区描述符里的内存地址和数据大小，使用 scatter-gather 功能开始从内核缓冲区收集离散的数据并组包，最后直接把网络包数据拷贝到网卡完成数据传输； sendfile() 返回，上下文从内核态切换回用户态。 基于这种方案，我们就可以把这仅剩的唯一一次 CPU 拷贝也给去除了（严格来说还是会有一次，但是因为这次 CPU 拷贝的只是那些微乎其微的元信息，开销几乎可以忽略不计），理论上，数据传输过程就再也没有 CPU 的参与了，也因此 CPU 的高速缓存再不会被污染了，也不再需要 CPU 来计算数据校验和了，CPU 可以去执行其他的业务计算任务，同时和 DMA 的 I/O 任务并行，此举能极大地提升系统性能。 splice() sendfile() + DMA Scatter/Gather 的零拷贝方案虽然高效，但是也有两个缺点： 这种方案需要引入新的硬件支持； 虽然 sendfile() 的输出文件描述符在 Linux kernel 2.6.33 版本之后已经可以支持任意类型的文件描述符，但是输入文件描述符依然只能指向文件。 这两个缺点限制了 sendfile() + DMA Scatter/Gather 方案的适用场景。为此，Linux 在 2.6.17 版本引入了一个新的系统调用 splice()，它在功能上和 sendfile() 非常相似，但是能够实现在任意类型的两个文件描述符时之间传输数据；而在底层实现上，splice()又比 sendfile() 少了一次 CPU 拷贝，也就是等同于 sendfile() + DMA Scatter/Gather，完全去除了数据传输过程中的 CPU 拷贝。 splice() 系统调用函数定义如下： 1234567#include &lt;fcntl.h&gt;#include &lt;unistd.h&gt;int pipe(int pipefd[2]);int pipe2(int pipefd[2], int flags);ssize_t splice(int fd_in, loff_t *off_in, int fd_out, loff_t *off_out, size_t len, unsigned int flags); fd_in 和 fd_out 也是分别代表了输入端和输出端的文件描述符，这两个文件描述符必须有一个是指向管道设备的，这也是一个不太友好的限制，虽然 Linux 内核开发的官方从这个系统调用推出之时就承诺未来可能会重构去掉这个限制，然而他们许下这个承诺之后就如同石沉大海，如今 14 年过去了，依旧杳无音讯…off_in 和 off_out 则分别是 fd_in 和 fd_out 的偏移量指针，指示内核从哪里读取和写入数据，len 则指示了此次调用希望传输的字节数，最后的 flags 是系统调用的标记选项位掩码，用来设置系统调用的行为属性的，由以下 0 个或者多个值通过『或』操作组合而成： SPLICE_F_MOVE：指示 splice() 尝试仅仅是移动内存页面而不是复制，设置了这个值不代表就一定不会复制内存页面，复制还是移动取决于内核能否从管道中移动内存页面，或者管道中的内存页面是否是完整的；这个标记的初始实现有很多 bug，所以从 Linux 2.6.21 版本开始就已经无效了，但还是保留了下来，因为在未来的版本里可能会重新被实现。 SPLICE_F_NONBLOCK：指示 splice() 不要阻塞 I/O，也就是使得 splice() 调用成为一个非阻塞调用，可以用来实现异步数据传输，不过需要注意的是，数据传输的两个文件描述符也最好是预先通过 O_NONBLOCK 标记成非阻塞 I/O，不然 splice() 调用还是有可能被阻塞。 SPLICE_F_MORE：通知内核下一个 splice() 系统调用将会有更多的数据传输过来，这个标记对于输出端是 socket 的场景非常有用。 splice() 是基于 Linux 的管道缓冲区 (pipe buffer) 机制实现的，所以 splice() 的两个入参文件描述符才要求必须有一个是管道设备，一个典型的 splice() 用法是： 123456789int pfd[2];pipe(pfd);ssize_t bytes = splice(file_fd, NULL, pfd[1], NULL, 4096, SPLICE_F_MOVE);assert(bytes != -1);bytes = splice(pfd[0], NULL, socket_fd, NULL, bytes, SPLICE_F_MOVE | SPLICE_F_MORE);assert(bytes != -1); 数据传输过程图： 使用 splice() 完成一次磁盘文件到网卡的读写过程如下： 用户进程调用 pipe()，从用户态陷入内核态，创建匿名单向管道，pipe() 返回，上下文从内核态切换回用户态； 用户进程调用 splice()，从用户态陷入内核态； DMA 控制器将数据从硬盘拷贝到内核缓冲区，从管道的写入端”拷贝”进管道，splice() 返回，上下文从内核态回到用户态； 用户进程再次调用 splice()，从用户态陷入内核态； 内核把数据从管道的读取端”拷贝”到套接字缓冲区，DMA 控制器将数据从套接字缓冲区拷贝到网卡； splice() 返回，上下文从内核态切换回用户态。 相信看完上面的读写流程之后，读者肯定会非常困惑：说好的 splice() 是 sendfile() 的改进版呢？sendfile() 好歹只需要一次系统调用，splice() 居然需要三次，这也就罢了，居然中间还搞出来一个管道，而且还要在内核空间拷贝两次，这算个毛的改进啊？我最开始了解 splice() 的时候，也是这个反应，但是深入学习它之后，才渐渐知晓个中奥妙，且听我细细道来：先来了解一下 pipe buffer 管道，管道是 Linux 上用来供进程之间通信的信道，管道有两个端：写入端和读出端，从进程的视角来看，管道表现为一个 FIFO 字节流环形队列： 管道本质上是一个内存中的文件，也就是本质上还是基于 Linux 的 VFS，用户进程可以通过 pipe() 系统调用创建一个匿名管道，创建完成之后会有两个 VFS 的 file 结构体的 inode 分别指向其写入端和读出端，并返回对应的两个文件描述符，用户进程通过这两个文件描述符读写管道；管道的容量单位是一个虚拟内存的页，也就是 4KB，总大小一般是 16 个页，基于其环形结构，管道的页可以循环使用，提高内存利用率。 Linux 中以 pipe_buffer 结构体封装管道页，file 结构体里的 inode 字段里会保存一个 pipe_inode_info 结构体指代管道，其中会保存很多读写管道时所需的元信息，环形队列的头部指针页，读写时的同步机制如互斥锁、等待队列等： 123456789101112131415161718192021222324struct pipe_buffer &#123; struct page *page; // 内存页结构 unsigned int offset, len; // 偏移量，长度 const struct pipe_buf_operations *ops; unsigned int flags; unsigned long private;&#125;;struct pipe_inode_info &#123; struct mutex mutex; wait_queue_head_t wait; unsigned int nrbufs, curbuf, buffers; unsigned int readers; unsigned int writers; unsigned int files; unsigned int waiting_writers; unsigned int r_counter; unsigned int w_counter; struct page *tmp_page; struct fasync_struct *fasync_readers; struct fasync_struct *fasync_writers; struct pipe_buffer *bufs; struct user_struct *user;&#125;; pipe_buffer 中保存了数据在内存中的页、偏移量和长度，以这三个值来定位数据，注意这里的页不是虚拟内存的页，而用的是物理内存的页框，因为管道时跨进程的信道，因此不能使用虚拟内存来表示，只能使用物理内存的页框定位数据；管道的正常读写操作是通过 pipe_write()/pipe_read() 来完成的，通过把数据读取/写入环形队列的 pipe_buffer 来完成数据传输。 splice() 是基于 pipe buffer 实现的，但是它在通过管道传输数据的时候却是零拷贝，因为它在写入读出时并没有使用 pipe_write()/pipe_read() 真正地在管道缓冲区写入读出数据，而是通过把数据在内存缓冲区中的物理内存页框指针、偏移量和长度赋值给前文提及的 pipe_buffer 中对应的三个字段来完成数据的”拷贝”，也就是其实只拷贝了数据的内存地址等元信息。splice() 在 Linux 内核源码中的内部实现是 do_splice() 函数，而写入读出管道则分别是通过 do_splice_to() 和 do_splice_from()，这里我们重点来解析下写入管道的源码，也就是 do_splice_to()，我现在手头的 Linux 内核版本是 v4.8.17，我们就基于这个版本来分析，至于读出的源码函数 do_splice_from()，原理是相通的，大家举一反三即可。splice() 写入数据到管道的调用链式：do_splice() –&gt; do_splice_to() –&gt; splice_read() 1234567891011121314151617181920212223242526272829303132static long do_splice(struct file *in, loff_t __user *off_in, struct file *out, loff_t __user *off_out, size_t len, unsigned int flags)&#123;... // 判断是写出 fd 是一个管道设备，则进入数据写入的逻辑 if (opipe) &#123; if (off_out) return -ESPIPE; if (off_in) &#123; if (!(in-&gt;f_mode &amp; FMODE_PREAD)) return -EINVAL; if (copy_from_user(&amp;offset, off_in, sizeof(loff_t))) return -EFAULT; &#125; else &#123; offset = in-&gt;f_pos; &#125; // 调用 do_splice_to 把文件内容写入管道 ret = do_splice_to(in, &amp;offset, opipe, len, flags); if (!off_in) in-&gt;f_pos = offset; else if (copy_to_user(off_in, &amp;offset, sizeof(loff_t))) ret = -EFAULT; return ret; &#125; return -EINVAL;&#125; 进入 do_splice_to() 之后，再调用 splice_read()： 123456789101112131415161718192021222324252627static long do_splice_to(struct file *in, loff_t *ppos, struct pipe_inode_info *pipe, size_t len, unsigned int flags)&#123; ssize_t (*splice_read)(struct file *, loff_t *, struct pipe_inode_info *, size_t, unsigned int); int ret; if (unlikely(!(in-&gt;f_mode &amp; FMODE_READ))) return -EBADF; ret = rw_verify_area(READ, in, ppos, len); if (unlikely(ret &lt; 0)) return ret; if (unlikely(len &gt; MAX_RW_COUNT)) len = MAX_RW_COUNT; // 判断文件的文件的 file 结构体的 f_op 中有没有可供使用的、支持 splice 的 splice_read 函数指针 // 因为是 splice() 调用，因此内核会提前给这个函数指针指派一个可用的函数 if (in-&gt;f_op-&gt;splice_read) splice_read = in-&gt;f_op-&gt;splice_read; else splice_read = default_file_splice_read; return splice_read(in, ppos, pipe, len, flags);&#125; in-&gt;f_op-&gt;splice_read 这个函数指针根据文件描述符的类型不同有不同的实现，比如这里的 in 是一个文件，因此是 generic_file_splice_read()，如果是 socket 的话，则是 sock_splice_read()，其他的类型也会有对应的实现，总之我们这里将使用的是 generic_file_splice_read() 函数，这个函数会继续调用内部函数 __generic_file_splice_read 完成以下工作： 在 page cache 页缓存里进行搜寻，看看我们要读取这个文件内容是否已经在缓存里了，如果是则直接用，否则如果不存在或者只有部分数据在缓存中，则分配一些新的内存页并进行读入数据操作，同时会增加页框的引用计数； 基于这些内存页，初始化 splice_pipe_desc 结构，这个结构保存会保存文件数据的地址元信息，包含有物理内存页框地址，偏移、数据长度，也就是 pipe_buffer 所需的三个定位数据的值； 最后，调用 splice_to_pipe()，splice_pipe_desc 结构体实例是函数入参。 12345678910111213141516171819202122232425262728293031323334353637383940414243ssize_t splice_to_pipe(struct pipe_inode_info *pipe, struct splice_pipe_desc *spd)&#123;... for (;;) &#123; if (!pipe-&gt;readers) &#123; send_sig(SIGPIPE, current, 0); if (!ret) ret = -EPIPE; break; &#125; if (pipe-&gt;nrbufs &lt; pipe-&gt;buffers) &#123; int newbuf = (pipe-&gt;curbuf + pipe-&gt;nrbufs) &amp; (pipe-&gt;buffers - 1); struct pipe_buffer *buf = pipe-&gt;bufs + newbuf; // 写入数据到管道，没有真正拷贝数据，而是内存地址指针的移动， // 把物理页框、偏移量和数据长度赋值给 pipe_buffer 完成数据入队操作 buf-&gt;page = spd-&gt;pages[page_nr]; buf-&gt;offset = spd-&gt;partial[page_nr].offset; buf-&gt;len = spd-&gt;partial[page_nr].len; buf-&gt;private = spd-&gt;partial[page_nr].private; buf-&gt;ops = spd-&gt;ops; if (spd-&gt;flags &amp; SPLICE_F_GIFT) buf-&gt;flags |= PIPE_BUF_FLAG_GIFT; pipe-&gt;nrbufs++; page_nr++; ret += buf-&gt;len; if (pipe-&gt;files) do_wakeup = 1; if (!--spd-&gt;nr_pages) break; if (pipe-&gt;nrbufs &lt; pipe-&gt;buffers) continue; break; &#125; ...&#125; 这里可以清楚地看到 splice() 所谓的写入数据到管道其实并没有真正地拷贝数据，而是玩了个 tricky 的操作：只进行内存地址指针的拷贝而不真正去拷贝数据。所以，数据 splice() 在内核中并没有进行真正的数据拷贝，因此 splice() 系统调用也是零拷贝。还有一点需要注意，前面说过管道的容量是 16 个内存页，也就是 16 * 4KB = 64 KB，也就是说一次往管道里写数据的时候最好不要超过 64 KB，否则的话会 splice() 会阻塞住，除非在创建管道的时候使用的是 pipe2() 并通过传入 O_NONBLOCK 属性将管道设置为非阻塞。 即使 splice() 通过内存地址指针避免了真正的拷贝开销，但是算起来它还要使用额外的管道来完成数据传输，也就是比 sendfile() 多了两次系统调用，这不是又增加了上下文切换的开销吗？为什么不直接在内核创建管道并调用那两次 splice()，然后只暴露给用户一次系统调用呢？实际上因为 splice() 利用管道而非硬件来完成零拷贝的实现比 sendfile() + DMA Scatter/Gather 的门槛更低，因此后来的 sendfile() 的底层实现就已经替换成 splice() 了。 至于说 splice() 本身的 API 为什么还是这种使用模式，那是因为 Linux 内核开发团队一直想把基于管道的这个限制去掉，但不知道因为什么一直搁置，所以这个 API 也就一直没变化，只能等内核团队哪天想起来了这一茬，然后重构一下使之不再依赖管道，在那之前，使用 splice() 依然还是需要额外创建管道来作为中间缓冲，如果你的业务场景很适合使用 splice()，但又是性能敏感的，不想频繁地创建销毁 pipe buffer 管道缓冲区，那么可以参考一下 HAProxy 使用 splice() 时采用的优化方案：预先分配一个 pipe buffer pool 缓存管道，每次调用 spclie() 的时候去缓存池里取一个管道，用完就放回去，循环利用，提升性能。 send() with MSG_ZEROCOPYLinux 内核在 2017 年的 v4.14 版本接受了来自 Google 工程师 Willem de Bruijn 在 TCP 网络报文的通用发送接口 send() 中实现的 zero-copy 功能 (MSG_ZEROCOPY) 的 patch，通过这个新功能，用户进程就能够把用户缓冲区的数据通过零拷贝的方式经过内核空间发送到网络套接字中去，这个新技术和前文介绍的几种零拷贝方式相比更加先进，因为前面几种零拷贝技术都是要求用户进程不能处理加工数据而是直接转发到目标文件描述符中去的。Willem de Bruijn 在他的论文里给出的压测数据是：采用 netperf 大包发送测试，性能提升 39%，而线上环境的数据发送性能则提升了 5%~8%，官方文档陈述说这个特性通常只在发送 10KB 左右大包的场景下才会有显著的性能提升。一开始这个特性只支持 TCP，到内核 v5.0 版本之后才支持 UDP。这个功能的使用模式如下： 1234if (setsockopt(socket_fd, SOL_SOCKET, SO_ZEROCOPY, &amp;one, sizeof(one))) error(1, errno, &quot;setsockopt zerocopy&quot;);ret = send(socket_fd, buffer, sizeof(buffer), MSG_ZEROCOPY); 首先第一步，先给要发送数据的 socket 设置一个 SOCK_ZEROCOPY option，然后在调用 send() 发送数据时再设置一个 MSG_ZEROCOPY option，其实理论上来说只需要调用 setsockopt() 或者 send() 时传递这个 zero-copy 的 option 即可，两者选其一，但是这里却要设置同一个 option 两次，官方的说法是为了兼容 send() API 以前的设计上的一个错误：send() 以前的实现会忽略掉未知的 option，为了兼容那些可能已经不小心设置了 MSG_ZEROCOPY option 的程序，故而设计成了两步设置。不过我猜还有一种可能：就是给使用者提供更灵活的使用模式，因为这个新功能只在大包场景下才可能会有显著的性能提升，但是现实场景是很复杂的，不仅仅是全部大包或者全部小包的场景，有可能是大包小包混合的场景，因此使用者可以先调用 setsockopt() 设置 SOCK_ZEROCOPY option，然后再根据实际业务场景中的网络包尺寸选择是否要在调用 send() 时使用 MSG_ZEROCOPY 进行 zero-copy 传输。 因为 send() 可能是异步发送数据，因此使用 MSG_ZEROCOPY 有一个需要特别注意的点是：调用 send() 之后不能立刻重用或释放 buffer，因为 buffer 中的数据不一定已经被内核读走了，所以还需要从 socket 关联的错误队列里读取一下通知消息，看看 buffer 中的数据是否已经被内核读走了： 1234567891011121314151617181920212223242526272829pfd.fd = fd;pfd.events = 0;if (poll(&amp;pfd, 1, -1) != 1 || pfd.revents &amp; POLLERR == 0) error(1, errno, &quot;poll&quot;);ret = recvmsg(fd, &amp;msg, MSG_ERRQUEUE);if (ret == -1) error(1, errno, &quot;recvmsg&quot;);read_notification(msg);uint32_t read_notification(struct msghdr *msg)&#123; struct sock_extended_err *serr; struct cmsghdr *cm; cm = CMSG_FIRSTHDR(msg); if (cm-&gt;cmsg_level != SOL_IP &amp;&amp; cm-&gt;cmsg_type != IP_RECVERR) error(1, 0, &quot;cmsg&quot;); serr = (void *) CMSG_DATA(cm); if (serr-&gt;ee_errno != 0 || serr-&gt;ee_origin != SO_EE_ORIGIN_ZEROCOPY) error(1, 0, &quot;serr&quot;); return serr-&gt;ee _ data;&#125; 这个技术是基于 redhat 红帽在 2010 年给 Linux 内核提交的 virtio-net zero-copy 技术之上实现的，至于底层原理，简单来说就是通过 send() 把数据在用户缓冲区中的分段指针发送到 socket 中去，利用 page pinning 页锁定机制锁住用户缓冲区的内存页，然后利用 DMA 直接在用户缓冲区通过内存地址指针进行数据读取，实现零拷贝；具体的细节可以通过阅读 Willem de Bruijn 的论文 (PDF) 深入了解。 目前来说，这种技术的主要缺陷有： 只适用于大文件 (10KB 左右) 的场景，小文件场景因为 page pinning 页锁定和等待缓冲区释放的通知消息这些机制，甚至可能比直接 CPU 拷贝更耗时； 因为可能异步发送数据，需要额外调用 poll() 和 recvmsg() 系统调用等待 buffer 被释放的通知消息，增加代码复杂度，以及会导致多次用户态和内核态的上下文切换； MSG_ZEROCOPY 目前只支持发送端，接收端暂不支持。 绕过内核的直接 I/O可以看出，前面种种的 zero-copy 的方法，都是在想方设法地优化减少或者去掉用户态和内核态之间以及内核态和内核态之间的数据拷贝，为了实现避免这些拷贝可谓是八仙过海，各显神通，采用了各种各样的手段，那么如果我们换个思路：其实这么费劲地去消除这些拷贝不就是因为有内核在掺和吗？如果我们绕过内核直接进行 I/O 不就没有这些烦人的拷贝问题了吗？这就是绕过内核直接 I/O 技术： 这种方案有两种实现方式： 用户直接访问硬件这种技术赋予用户进程直接访问硬件设备的权限，这让用户进程能有直接读写硬件设备，在数据传输过程中只需要内核做一些虚拟内存配置相关的工作。这种无需数据拷贝和内核干预的直接 I/O，理论上是最高效的数据传输技术，但是正如前面所说的那样，并不存在能解决一切问题的银弹，这种直接 I/O 技术虽然有可能非常高效，但是它的适用性也非常窄，目前只适用于诸如 MPI 高性能通信、丛集计算系统中的远程共享内存等有限的场景。 这种技术实际上破坏了现代计算机操作系统最重要的概念之一 —— 硬件抽象，我们之前提过，抽象是计算机领域最最核心的设计思路，正式由于有了抽象和分层，各个层级才能不必去关心很多底层细节从而专注于真正的工作，才使得系统的运作更加高效和快速。此外，网卡通常使用功能较弱的 CPU，例如只包含简单指令集的 MIPS 架构处理器（没有不必要的功能，如浮点数计算等），也没有太多的内存来容纳复杂的软件。因此，通常只有那些基于以太网之上的专用协议会使用这种技术，这些专用协议的设计要比远比 TCP/IP 简单得多，而且多用于局域网环境中，在这种环境中，数据包丢失和损坏很少发生，因此没有必要进行复杂的数据包确认和流量控制机制。而且这种技术还需要定制的网卡，所以它是高度依赖硬件的。 与传统的通信设计相比，直接硬件访问技术给程序设计带来了各种限制：由于设备之间的数据传输是通过 DMA 完成的，因此用户空间的数据缓冲区内存页必须进行 page pinning（页锁定），这是为了防止其物理页框地址被交换到磁盘或者被移动到新的地址而导致 DMA 去拷贝数据的时候在指定的地址找不到内存页从而引发缺页错误，而页锁定的开销并不比 CPU 拷贝小，所以为了避免频繁的页锁定系统调用，应用程序必须分配和注册一个持久的内存池，用于数据缓冲。 用户直接访问硬件的技术可以得到极高的 I/O 性能，但是其应用领域和适用场景也极其的有限，如集群或网络存储系统中的节点通信。它需要定制的硬件和专门设计的应用程序，但相应地对操作系统内核的改动比较小，可以很容易地以内核模块或设备驱动程序的形式实现出来。直接访问硬件还可能会带来严重的安全问题，因为用户进程拥有直接访问硬件的极高权限，所以如果你的程序设计没有做好的话，可能会消耗本来就有限的硬件资源或者进行非法地址访问，可能也会因此间接地影响其他正在使用同一设备的应用程序，而因为绕开了内核，所以也无法让内核替你去控制和管理。 内核控制访问硬件 相较于用户直接访问硬件技术，通过内核控制的直接访问硬件技术更加的安全，它比前者在数据传输过程中会多干预一点，但也仅仅是作为一个代理人这样的角色，不会参与到实际的数据传输过程，内核会控制 DMA 引擎去替用户进程做缓冲区的数据传输工作。同样的，这种方式也是高度依赖硬件的，比如一些集成了专有网络栈协议的网卡。这种技术的一个优势就是用户集成去 I/O 时的接口不会改变，就和普通的 read()/write() 系统调用那样使用即可，所有的脏活累活都在内核里完成，用户接口友好度很高，不过需要注意的是，使用这种技术的过程中如果发生了什么不可预知的意外从而导致无法使用这种技术进行数据传输的话，则内核会自动切换为最传统 I/O 模式，也就是性能最差的那种模式。 这种技术也有着和用户直接访问硬件技术一样的问题：DMA 传输数据的过程中，用户进程的缓冲区内存页必须进行 page pinning 页锁定，数据传输完成后才能解锁。CPU 高速缓存内保存的多个内存地址也会被冲刷掉以保证 DMA 传输前后的数据一致性。这些机制有可能会导致数据传输的性能变得更差，因为 read()/write() 系统调用的语义并不能提前通知 CPU 用户缓冲区要参与 DMA 数据传输传输，因此也就无法像内核缓冲区那样可依提前加载进高速缓存，提高性能。由于用户缓冲区的内存页可能分布在物理内存中的任意位置，因此一些实现不好的 DMA 控制器引擎可能会有寻址限制从而导致无法访问这些内存区域。一些技术比如 AMD64 架构中的 IOMMU，允许通过将 DMA 地址重新映射到内存中的物理地址来解决这些限制，但反过来又可能会导致可移植性问题，因为其他的处理器架构，甚至是 Intel 64 位 x86 架构的变种 EM64T 都不具备这样的特性单元。此外，还可能存在其他限制，比如 DMA 传输的数据对齐问题，又会导致无法访问用户进程指定的任意缓冲区内存地址。 内核缓冲区和用户缓冲区之间的传输优化到目前为止，我们讨论的 zero-copy 技术都是基于减少甚至是避免用户空间和内核空间之间的 CPU 数据拷贝的，虽然有一些技术非常高效，但是大多都有适用性很窄的问题，比如 sendfile()、splice() 这些，效率很高，但是都只适用于那些用户进程不需要直接处理数据的场景，比如静态文件服务器或者是直接转发数据的代理服务器。 现在我们已经知道，硬件设备之间的数据可以通过 DMA 进行传输，然而却并没有这样的传输机制可以应用于用户缓冲区和内核缓冲区之间的数据传输。不过另一方面，广泛应用在现代的 CPU 架构和操作系统上的虚拟内存机制表明，通过在不同的虚拟地址上重新映射页面可以实现在用户进程和内核之间虚拟复制和共享内存，尽管一次传输的内存颗粒度相对较大：4KB 或 8KB。 因此如果要在实现在用户进程内处理数据（这种场景比直接转发数据更加常见）之后再发送出去的话，用户空间和内核空间的数据传输就是不可避免的，既然避无可避，那就只能选择优化了，因此本章节我们要介绍两种优化用户空间和内核空间数据传输的技术： 动态重映射与写时拷贝 (Copy-on-Write) 缓冲区共享 (Buffer Sharing) 动态重映射与写时拷贝 (Copy-on-Write) 前面我们介绍过利用内存映射技术来减少数据在用户空间和内核空间之间的复制，通常简单模式下，用户进程是对共享的缓冲区进行同步阻塞读写的，这样不会有 data race 问题，但是这种模式下效率并不高，而提升效率的一种方法就是异步地对共享缓冲区进行读写，而这样的话就必须引入保护机制来避免数据冲突问题，写时复制 (Copy on Write) 就是这样的一种技术。 写入时复制（Copy-on-write，COW）是一种计算机程序设计领域的优化策略。其核心思想是，如果有多个调用者（callers）同时请求相同资源（如内存或磁盘上的数据存储），他们会共同获取相同的指针指向相同的资源，直到某个调用者试图修改资源的内容时，系统才会真正复制一份专用副本（private copy）给该调用者，而其他调用者所见到的最初的资源仍然保持不变。这过程对其他的调用者都是透明的。此作法主要的优点是如果调用者没有修改该资源，就不会有副本（private copy）被创建，因此多个调用者只是读取操作时可以共享同一份资源。 举一个例子，引入了 COW 技术之后，用户进程读取磁盘文件进行数据处理最后写到网卡，首先使用内存映射技术让用户缓冲区和内核缓冲区共享了一段内存地址并标记为只读 (read-only)，避免数据拷贝，而当要把数据写到网卡的时候，用户进程选择了异步写的方式，系统调用会直接返回，数据传输就会在内核里异步进行，而用户进程就可以继续其他的工作，并且共享缓冲区的内容可以随时再进行读取，效率很高，但是如果该进程又尝试往共享缓冲区写入数据，则会产生一个 COW 事件，让试图写入数据的进程把数据复制到自己的缓冲区去修改，这里只需要复制要修改的内存页即可，无需所有数据都复制过去，而如果其他访问该共享内存的进程不需要修改数据则可以永远不需要进行数据拷贝。 COW 是一种建构在虚拟内存冲映射技术之上的技术，因此它需要 MMU 的硬件支持，MMU 会记录当前哪些内存页被标记成只读，当有进程尝试往这些内存页中写数据的时候，MMU 就会抛一个异常给操作系统内核，内核处理该异常时为该进程分配一份物理内存并复制数据到此内存地址，重新向 MMU 发出执行该进程的写操作。 COW 最大的优势是节省内存和减少数据拷贝，不过却是通过增加操作系统内核 I/O 过程复杂性作为代价的。当确定采用 COW 来复制页面时，重要的是注意空闲页面的分配位置。许多操作系统为这类请求提供了一个空闲的页面池。当进程的堆栈或堆要扩展时或有写时复制页面需要管理时，通常分配这些空闲页面。操作系统分配这些页面通常采用称为按需填零的技术。按需填零页面在需要分配之前先填零，因此会清除里面旧的内容。 COW 这种零拷贝技术比较适用于那种多读少写从而使得 COW 事件发生较少的场景，因为 COW 事件所带来的系统开销要远远高于一次 CPU 拷贝所产生的。此外，在实际应用的过程中，为了避免频繁的内存映射，可以重复使用同一段内存缓冲区，因此，你不需要在只用过一次共享缓冲区之后就解除掉内存页的映射关系，而是重复循环使用，从而提升性能，不过这种内存页映射的持久化并不会减少由于页表往返移动和 TLB 冲刷所带来的系统开销，因为每次接收到 COW 事件之后对内存页而进行加锁或者解锁的时候，页面的只读标志 (read-ony) 都要被更改为 (write-only)。 缓冲区共享 (Buffer Sharing)从前面的介绍可以看出，传统的 Linux I/O接口，都是基于复制/拷贝的：数据需要在操作系统内核空间和用户空间的缓冲区之间进行拷贝。在进行 I/O 操作之前，用户进程需要预先分配好一个内存缓冲区，使用 read() 系统调用时，内核会将从存储器或者网卡等设备读入的数据拷贝到这个用户缓冲区里；而使用 write() 系统调用时，则是把用户内存缓冲区的数据拷贝至内核缓冲区。 为了实现这种传统的 I/O 模式，Linux 必须要在每一个 I/O 操作时都进行内存虚拟映射和解除。这种内存页重映射的机制的效率严重受限于缓存体系结构、MMU 地址转换速度和 TLB 命中率。如果能够避免处理 I/O 请求的虚拟地址转换和 TLB 刷新所带来的开销，则有可能极大地提升 I/O 性能。而缓冲区共享就是用来解决上述问题的一种技术 最早支持 Buffer Sharing 的操作系统是 Solaris。后来，Linux 也逐步支持了这种 Buffer Sharing 的技术，但时至今日依然不够完整和成熟。 操作系统内核开发者们实现了一种叫 fbufs 的缓冲区共享的框架，也即快速缓冲区（ Fast Buffers ），使用一个 fbuf 缓冲区作为数据传输的最小单位，使用这种技术需要调用新的操作系统 API，用户区和内核区、内核区之间的数据都必须严格地在 fbufs 这个体系下进行通信。fbufs 为每一个用户进程分配一个 buffer pool，里面会储存预分配 (也可以使用的时候再分配) 好的 buffers，这些 buffers 会被同时映射到用户内存空间和内核内存空间。fbufs 只需通过一次虚拟内存映射操作即可创建缓冲区，有效地消除那些由存储一致性维护所引发的大多数性能损耗。 传统的 Linux I/O 接口是通过把数据在用户缓冲区和内核缓冲区之间进行拷贝传输来完成的，这种数据传输过程中需要进行大量的数据拷贝，同时由于虚拟内存技术的存在，I/O 过程中还需要频繁地通过 MMU 进行虚拟内存地址到物理内存地址的转换，高速缓存的汰换以及 TLB 的刷新，这些操作均会导致性能的损耗。而如果利用 fbufs 框架来实现数据传输的话，首先可以把 buffers 都缓存到 pool 里循环利用，而不需要每次都去重新分配，而且缓存下来的不止有 buffers 本身，而且还会把虚拟内存地址到物理内存地址的映射关系也缓存下来，也就可以避免每次都进行地址转换，从发送接收数据的层面来说，用户进程和 I/O 子系统比如设备驱动程序、网卡等可以直接传输整个缓冲区本身而不是其中的数据内容，也可以理解成是传输内存地址指针，这样就就避免了大量的数据内容拷贝：用户进程/ IO 子系统通过发送一个个的 fbuf 写出数据到内核而非直接传递数据内容，相对应的，用户进程/ IO 子系统通过接收一个个的 fbuf 而从内核读入数据，这样就能减少传统的 read()/write() 系统调用带来的数据拷贝开销： 发送方用户进程调用 uf_allocate 从自己的 buffer pool 获取一个 fbuf 缓冲区，往其中填充内容之后调用 uf_write 向内核区发送指向 fbuf 的文件描述符； I/O 子系统接收到 fbuf 之后，调用 uf_allocb 从接收方用户进程的 buffer pool 获取一个 fubf 并用接收到的数据进行填充，然后向用户区发送指向 fbuf 的文件描述符； 接收方用户进程调用 uf_get 接收到 fbuf，读取数据进行处理，完成之后调用 uf_deallocate 把 fbuf 放回自己的 buffer pool。 fbufs 的缺陷共享缓冲区技术的实现需要依赖于用户进程、操作系统内核、以及 I/O 子系统 (设备驱动程序，文件系统等)之间协同工作。比如，设计得不好的用户进程容易就会修改已经发送出去的 fbuf 从而污染数据，更要命的是这种问题很难 debug。虽然这个技术的设计方案非常精彩，但是它的门槛和限制却不比前面介绍的其他技术少：首先会对操作系统 API 造成变动，需要使用新的一些 API 调用，其次还需要设备驱动程序配合改动，还有由于是内存共享，内核需要很小心谨慎地实现对这部分共享的内存进行数据保护和同步的机制，而这种并发的同步机制是非常容易出 bug 的从而又增加了内核的代码复杂度，等等。因此这一类的技术还远远没有到发展成熟和广泛应用的阶段，目前大多数的实现都还处于实验阶段 总结本文中我主要讲解了 Linux I/O 底层原理，然后介绍并解析了 Linux 中的 Zero-copy 技术，并给出了 Linux 对 I/O 模块的优化和改进思路。 Linux 的 Zero-copy 技术可以归纳成以下三大类： 减少甚至避免用户空间和内核空间之间的数据拷贝：在一些场景下，用户进程在数据传输过程中并不需要对数据进行访问和处理，那么数据在 Linux 的 Page Cache 和用户进程的缓冲区之间的传输就完全可以避免，让数据拷贝完全在内核里进行，甚至可以通过更巧妙的方式避免在内核里的数据拷贝。这一类实现一般是是通过增加新的系统调用来完成的，比如 Linux 中的 mmap()，sendfile() 以及 splice() 等。 绕过内核的直接 I/O：允许在用户态进程绕过内核直接和硬件进行数据传输，内核在传输过程中只负责一些管理和辅助的工作。这种方式其实和第一种有点类似，也是试图避免用户空间和内核空间之间的数据传输，只是第一种方式是把数据传输过程放在内核态完成，而这种方式则是直接绕过内核和硬件通信，效果类似但原理完全不同。 内核缓冲区和用户缓冲区之间的传输优化：这种方式侧重于在用户进程的缓冲区和操作系统的页缓存之间的 CPU 拷贝的优化。这种方法延续了以往那种传统的通信方式，但更灵活。 本文从虚拟内存、I/O 缓冲区，用户态&amp;内核态以及 I/O 模式等等知识点全面而又详尽地剖析了 Linux 系统的 I/O 底层原理，分析了 Linux 传统的 I/O 模式的弊端，进而引入 Linux Zero-copy 零拷贝技术的介绍和原理解析，通过将零拷贝技术和传统的 I/O 模式进行区分和对比，带领读者经历了 Linux I/O 的演化历史，通过帮助读者理解 Linux 内核对 I/O 模块的优化改进思路，相信不仅仅是让读者了解 Linux 底层系统的设计原理，更能对读者们在以后优化改进自己的程序设计过程中能够有所启发。 参考&amp;延伸阅读 MODERN OPERATING SYSTEMS Zero Copy I: User-Mode Perspective Message Passing for Gigabit/s Networks with “Zero-Copy” under Linux ZeroCopy: Techniques, Benefits and Pitfalls Zero-copy networking Driver porting: Zero-copy user-space access sendmsg copy avoidance with MSG_ZEROCOPY It’s all about buffers: zero-copy, mmap and Java NIO Linux Zero Copy Two new system calls: splice() and sync_file_range() Circular pipes The future of the page cache Provide a zero-copy method on KVM virtio-net","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"zero-copy","slug":"zero-copy","permalink":"http://huangzhiyuan.github.io/tags/zero-copy/"}]},{"title":"OpenMP 总结","slug":"guide-into-openmp","date":"2020-08-26T05:14:17.000Z","updated":"2020-08-26T07:13:54.000Z","comments":true,"path":"2020/08/26/guide-into-openmp/","link":"","permalink":"http://huangzhiyuan.github.io/2020/08/26/guide-into-openmp/","excerpt":"本文档尝试快速介绍 OpenMP（如版本 4.5），这是一个简单的 C/C++/Fortran 编译器扩展 C++，它允许在现有源代码中添加并行性，而无需重写它。","text":"本文档尝试快速介绍 OpenMP（如版本 4.5），这是一个简单的 C/C++/Fortran 编译器扩展 C++，它允许在现有源代码中添加并行性，而无需重写它。 引言 多线程的重要性随着 CPU 速度不再像以前那样显著提高，多核系统正变得越来越流行。要利用这种功能，程序员在并行编程中知识化变得非常重要。本文档试图快速介绍 OpenMP，这是一个简单的 C/C++/Fortran 编译器扩展，允许将并行性添加到现有源代码中，而无需完全重写它。 多个编译器支持 GCC (GNU Compiler Collection) Clang++ Solaris Studio ICC (Intel C Compiler) Microsoft Visual C++ C++中的OpenMP介绍OpenMP 由一组编译器#pragmas控制程序工作原理的编译器。pragmas的设计使即使编译器不支持它们，程序仍将产生正确的行为，但没有任何并行性。下面是演示 OpenMP 的两个简单的示例程序。你可以像这样编译它们： 1g++ tmp.cpp -fopenmp Example: Initializing a table in parallel (multiple threads)此代码将表初始化划分为多个线程，这些线程同时运行。每个线程初始化表的一部分 123456789101112#include &lt;cmath&gt;int main()&#123; const int size = 256; double sinTable[size]; #pragma omp parallel for for(int n=0; n&lt;size; ++n) sinTable[n] = std::sin(2 * M_PI * n / size); // the table is now initialized&#125; Example: Initializing a table in parallel (single thread, SIMD)此版本需要至少对 OpenMP 4.0 的编译器支持，并使用并行浮点库，如 AMD ACML 或英特尔SVML(可以在GCC中使用通过添加‑mveclibabi=svml）。 123456789101112#include &lt;cmath&gt;int main()&#123; const int size = 256; double sinTable[size]; #pragma omp simd for(int n=0; n&lt;size; ++n) sinTable[n] = std::sin(2 * M_PI * n / size); // the table is now initialized&#125; Example: Initializing a table in parallel (multiple threads on another device)OpenMP 4.0 增加了对将代码卸载到不同设备（如 GPU）的支持。因此，单个程序中可以有三层并行性：单个线程处理多个数据;多个线程同时运行;和多个设备同时运行同一程序。 123456789101112#include &lt;cmath&gt;int main()&#123; const int size = 256; double sinTable[size]; #pragma omp target teams distribute parallel for map(from:sinTable[0:256]) for(int n=0; n&lt;size; ++n) sinTable[n] = std::sin(2 * M_PI * n / size); // the table is now initialized&#125; 综上所述，程序中很少指示它并行运行。如果删除 #pragma 行，则结果仍然是运行并执行预期操作。#pragma的有效 C++ 程序。它确实同时计算 N 个值，其中 N 是线程数。在 GCC 中，libgomp 从处理器数目确定这个N的值。在C 和 C++ 标准中，如果编译器遇到#pragma但它不支持，它将忽略它。因此，添加 OMP 语句可以安全地完成，而不会破坏与旧编译器的兼容性。还有一个运行时库，可以通过 omp.h 访问，但它不太经常需要。如果需要，可以在不支持 OpenMP #define _OPENMP检查条件编译的格式。 OpenMP语法解析C 和 C++的所有 OpenMP 构造都用 #pragma，后跟参数，以新行结尾。#pragma通常仅适用于紧接着它的语句，但barrier和flush命令除外，它们没有关联的语句。 The parallel constructparallel构造用parallel关键字启动并行块。它创建一组N个数目线程（其中 N 在运行时确定，通常来自 CPU 内核的数量，但可能受到一些因素的影响），所有这些线程都执行下一个语句（或下一个块，如果语句是 […] - 存储模块）。语句之后，线程将重新联接到一个线程中。 12345#pragma omp parallel&#123; // Code inside this region runs in parallel. printf(&quot;Hello!\\n&quot;);&#125; 在内部，GCC 通过创建一个magic函数并移动关联的代码到该函数中实现此功能，使该块内声明的所有变量成为该函数的局部变量（从而成为每个线程的局部变量）。另一方面，ICC 使用类似于fork()的机制，并且不创建一个magic函数。当然，这两个实现都是有效和语义上相同的。从上下文共享的变量以透明方式处理，有时通过传递引用，有时通过使用在并行块末尾刷新的寄存器变量（或每当执行flush时）。 Parallelism conditionality clause: if通过在并行命令中包括 if 子句，可以使并行性成为条件，例如： 1234extern int parallelism_enabled;#pragma omp parallel for if(parallelism_enabled)for(int c=0; c&lt;n; ++c) handle(c); 在这种情况下，如果parallelism_enabled值为零，则for循环中开启的线程数将始终为一个。 Loop construct: forfor关键字把for-loop拆分，因此每个线程都可以单独处理在循环里面不同的部分。 123456#pragma omp forfor(int n=0; n&lt;10; ++n)&#123; printf(&quot; %d&quot;, n);&#125;printf(&quot;.\\n&quot;); 此循环将输出 0…9 一次。但是，它可以按任意顺序执行。它可以输出，例如 10 5 6 7 1 8 2 3 4 9. 事实上，上述代码和如下代码效果等同： 12345int this_thread = omp_get_thread_num(), num_threads = omp_get_num_threads();int my_start = (this_thread ) * 10 / num_threads;int my_end = (this_thread+1) * 10 / num_threads;for(int n=my_start; n&lt;my_end; ++n) printf(&quot; %d&quot;, n); 因此每个线程都会得到不同的部分代码，并且它们并行的执行只属于自己的部分。 也可以显示的指定线程数目，通过使用关键字num_threads。 123456789#pragma omp parallel num_threads(3)&#123; // This code will be executed by three threads. // Chunks of this loop will be divided amongst // the (three) threads of the current team. #pragma omp for for(int n=0; n&lt;10; ++n) printf(&quot; %d&quot;, n);&#125; 注意OpenMP同样对C语言适用。在C里面，你需要显示的指定循环变量使用关键字private，因为C不允许在for循环体内声明变量。 1234int n;#pragma omp for private(n)for(n=0; n&lt;10; ++n) printf(&quot; %d&quot;, n);printf(&quot;.\\n&quot;); parallel, for and a team概念 parallel for ：parallel for是两个命令的组合，parallel和for 。：parallel 创建一个新team，for为该team拆分以处理循环的不同部分。 for：用于在当前team的线程之间划分 for 循环的工作。它不创建线程，它只将工作划分到当前正在执行团队的线程之间。 team: 是当前执行所有线程的组合。在程序开始的时候，team里面只包含一个线程。parallel关键字把当前线程切分为多个线程team，直到执行完毕 调度策略for-loop 的调度算法可以显式控制。 123#pragma omp for schedule(static)for(int n=0; n&lt;10; ++n) printf(&quot; %d&quot;, n);printf(&quot;.\\n&quot;) 共有5种调度算法：static, dynamic, guided, auto, runtime （OpenMP 4.0之后）。之外在OpenMP4.5之后，又添加了新的3种monotonic, nonmonotonic, simd。 使用如下demo验证： 12345678910111213141516#include &lt;omp.h&gt;#include &lt;iostream&gt;#define COUNT 48int main()&#123;#pragma omp parallel for schedule(static)/* #pragma omp parallel for schedule(static, 2) */ for(int i = 0;i &lt; COUNT; i++) &#123; printf(&quot;Thread: %d, Iteration: %d\\n&quot;, omp_get_thread_num(), i); &#125; return 0;&#125; Build and Run: 12$ g++ omp.cpp -fopenmp$ ./a.out 本机配置： 1234567891011121314151617181920212223242526(base) huang@mlp:~/taichi/omp$ lscpuArchitecture: x86_64CPU op-mode(s): 32-bit, 64-bitByte Order: Little EndianCPU(s): 12On-line CPU(s) list: 0-11Thread(s) per core: 2Core(s) per socket: 6Socket(s): 1NUMA node(s): 1Vendor ID: GenuineIntelCPU family: 6Model: 158Model name: Intel(R) Core(TM) i7-8700K CPU @ 3.70GHzStepping: 10CPU MHz: 3907.882CPU max MHz: 4700.0000CPU min MHz: 800.0000BogoMIPS: 7399.70Virtualization: VT-xL1d cache: 32KL1i cache: 32KL2 cache: 256KL3 cache: 12288KNUMA node0 CPU(s): 0-11 staticstatic是默认的调度算法。在上面的所有例子中，每个线程独立的决定它们处理哪个loop片段。schedule(static, chunk-size)子句指定 for 循环具有静态调度类型。OpenMP 将迭代划分为大小块大小的块，并将区块按循环顺序分发到线程。当chunk-size没有指定的时候OpenMP将迭代划分为大小大致相等的区块，并且最多将一个区块分发到每个线程。下面是关于static调度的例子。 12345schedule(static):**************** **************** **************** **************** 实际效果如下：这个分配是静态的，“静态”体现在这个分配过程跟实际的运行是无关的，可以从逻辑上推断出哪几次迭代会在哪几个线程上运行。具体而言，对于一个N次迭代，使用M个线程，那么，[0,size-1]的size次的迭代是在第一个线程上运行，[size, size + size -1]是在第二个线程上运行，依次类推。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950(base) huang@mlp:~/omp$ ./a.outThread: 11, Iteration: 44Thread: 11, Iteration: 45Thread: 11, Iteration: 46Thread: 5, Iteration: 20Thread: 5, Iteration: 21Thread: 5, Iteration: 22Thread: 5, Iteration: 23Thread: 1, Iteration: 4Thread: 1, Iteration: 5Thread: 1, Iteration: 6Thread: 1, Iteration: 7Thread: 10, Iteration: 40Thread: 10, Iteration: 41Thread: 10, Iteration: 42Thread: 10, Iteration: 43Thread: 7, Iteration: 28Thread: 7, Iteration: 29Thread: 7, Iteration: 30Thread: 7, Iteration: 31Thread: 8, Iteration: 32Thread: 8, Iteration: 33Thread: 8, Iteration: 34Thread: 2, Iteration: 8Thread: 11, Iteration: 47Thread: 6, Iteration: 24Thread: 6, Iteration: 25Thread: 6, Iteration: 26Thread: 6, Iteration: 27Thread: 4, Iteration: 16Thread: 4, Iteration: 17Thread: 4, Iteration: 18Thread: 4, Iteration: 19Thread: 2, Iteration: 9Thread: 2, Iteration: 10Thread: 2, Iteration: 11Thread: 9, Iteration: 36Thread: 9, Iteration: 37Thread: 9, Iteration: 38Thread: 9, Iteration: 39Thread: 8, Iteration: 35Thread: 0, Iteration: 0Thread: 0, Iteration: 1Thread: 0, Iteration: 2Thread: 0, Iteration: 3Thread: 3, Iteration: 12Thread: 3, Iteration: 13Thread: 3, Iteration: 14Thread: 3, Iteration: 15 12345schedule(static, 4):**** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **** 12345schedule(static, 8):******** ******** ******** ******** ******** ******** ******** ******** 让我解释一下这些例子。我们使用 64 次迭代并行化 for 循环，并且使用四个线程并行化 for 循环。示例中的每一行星数表示一个线程。每列表示迭代。当所有迭代具有相同的计算成本时，静态调度类型是适当的。 dynamicschedule(dynamic, chunk-size)子句指定 for 循环具有动态调度类型。OpenMP 将迭代划分为大小块大小的块。每个线程执行一个迭代区块，然后请求另一个区块，直到不再有可用的区块。没有将区块以特定顺序分布到线程。每次执行 for 循环时，顺序会更改。如果没有指定chunk-size，默认为1。下面是一些动态调度的例子。 12345schedule(dynamic):* ** ** * * * * * * ** * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ** * * * * * 对于dynamic，没有size参数的情况下，每个线程按先执行完先分配的方式执行1次循环。实际运行情况如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849Thread: 5, Iteration: 4Thread: 5, Iteration: 12Thread: 5, Iteration: 13Thread: 5, Iteration: 14Thread: 5, Iteration: 15Thread: 5, Iteration: 16Thread: 5, Iteration: 17Thread: 5, Iteration: 18Thread: 5, Iteration: 19Thread: 5, Iteration: 20Thread: 5, Iteration: 21Thread: 5, Iteration: 22Thread: 5, Iteration: 23Thread: 5, Iteration: 24Thread: 5, Iteration: 25Thread: 5, Iteration: 26Thread: 5, Iteration: 27Thread: 5, Iteration: 28Thread: 5, Iteration: 29Thread: 5, Iteration: 30Thread: 5, Iteration: 31Thread: 5, Iteration: 32Thread: 5, Iteration: 33Thread: 5, Iteration: 34Thread: 5, Iteration: 35Thread: 5, Iteration: 36Thread: 5, Iteration: 37Thread: 5, Iteration: 38Thread: 5, Iteration: 39Thread: 5, Iteration: 40Thread: 5, Iteration: 41Thread: 5, Iteration: 42Thread: 5, Iteration: 43Thread: 5, Iteration: 44Thread: 5, Iteration: 45Thread: 5, Iteration: 46Thread: 5, Iteration: 47Thread: 9, Iteration: 1Thread: 0, Iteration: 6Thread: 3, Iteration: 2Thread: 7, Iteration: 0Thread: 6, Iteration: 5Thread: 8, Iteration: 3Thread: 10, Iteration: 8Thread: 1, Iteration: 7Thread: 11, Iteration: 9Thread: 4, Iteration: 10Thread: 2, Iteration: 11 12345schedule(dynamic, 1): * * * * * * * * * * * * * ** * * * * * * * * * * * *** * * * * * * * * ** * * * * * * * * * * * * * ** * * * * * * * * * * 12345schedule(dynamic, 4): **** **** ******** **** **** **** **** **** **** **** **** **** **** **** **** 实际运行情况如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849(base) huang@mlp:~/taichi/omp$ ./a.outThread: 0, Iteration: 12Thread: 0, Iteration: 13Thread: 0, Iteration: 14Thread: 0, Iteration: 15Thread: 3, Iteration: 28Thread: 3, Iteration: 29Thread: 3, Iteration: 30Thread: 3, Iteration: 31Thread: 8, Iteration: 8Thread: 8, Iteration: 9Thread: 8, Iteration: 10Thread: 8, Iteration: 11Thread: 4, Iteration: 4Thread: 4, Iteration: 5Thread: 4, Iteration: 6Thread: 4, Iteration: 7Thread: 11, Iteration: 36Thread: 11, Iteration: 37Thread: 11, Iteration: 38Thread: 11, Iteration: 39Thread: 5, Iteration: 24Thread: 5, Iteration: 25Thread: 7, Iteration: 32Thread: 2, Iteration: 44Thread: 9, Iteration: 16Thread: 10, Iteration: 0Thread: 10, Iteration: 1Thread: 10, Iteration: 2Thread: 10, Iteration: 3Thread: 7, Iteration: 33Thread: 7, Iteration: 34Thread: 7, Iteration: 35Thread: 9, Iteration: 17Thread: 9, Iteration: 18Thread: 9, Iteration: 19Thread: 6, Iteration: 20Thread: 6, Iteration: 21Thread: 6, Iteration: 22Thread: 6, Iteration: 23Thread: 5, Iteration: 26Thread: 5, Iteration: 27Thread: 2, Iteration: 45Thread: 2, Iteration: 46Thread: 2, Iteration: 47Thread: 1, Iteration: 40Thread: 1, Iteration: 41Thread: 1, Iteration: 42Thread: 1, Iteration: 43 12345schedule(dynamic, 8): ******** ******** ******** **************** ******** ******** ******** 当迭代需要不同的计算成本时，动态调度类型是合适的。这意味着迭代之间的平衡很差。动态计划类型比静态计划类型具有更高的开销，因为它在运行时动态分布迭代。 GuidedGuided调度算法类似于动态计划类型。OpenMP 再次将迭代划分为块。每个线程执行一个迭代块，然后请求另一个区块，直到不再有可用的区块。与动态调度类型的区别在于chunk-size。区块的大小与未分配迭代数除以线程数成正比。因此，区块的大小减小。区块的最小大小由chunk-size设置。我们在调度确定它：schedule(guided, chunk-size)。但是，包含上次迭代的区块的大小可能小于chunk-size。如果我们不指定chunk-size，则默认为 1。对应示例如下。 1234567891011121314151617181920212223schedule(guided): ********* * ************ ******* *** ******* ***************** ***** ** *schedule(guided, 2): ************ **** ** ******* *** ** ************************* ***** ** **schedule(guided, 4): ******* ************ **** **** ************************* ***** **** ***schedule(guided, 8): ************ ******** ******************* ******** ********* ******** 我们可以看出chunk的大小一直在递减。第一个chunk总是有16个iteration，这是因为我们有64个iteration共有4个线程去并行这个loop。我们还可以看到，最小区块大小在计划子句中确定。唯一的例外是最后一个区块。其大小可能低于规定的最小大小。 当迭代之间不平衡时，引导调度类型是合适的。初始区块较大，因为它们减少了开销。较小的区块在计算结束时填充计划并改善负载平衡。当在计算接近尾声时发生负载平衡不佳的情况时，此调度类型尤其合适。 AutoAuto调度算法将调度决策委托给编译器和runtime。在下面的示例中，编译器/系统确定静态调度。 12345schedule(auto):**************** **************** **************** **************** RuntimeRuntime调度策略将有关计划的决定推迟到运行时。本例中，我们已经描述了指定计划类型的不同方法。一个选项是环境变量OMP_SCHEDULE，另一个选项是函数omp_set_schedule。 Default如果我们不指定任何调度策略： 123#pragma omp parallel forfor (...)&#123; ... &#125; OpenMP使用默认的调度策略，由内部的控制变量def-sched-var决定。在我的机器上面结果如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748Thread: 5, Iteration: 20Thread: 5, Iteration: 21Thread: 5, Iteration: 22Thread: 5, Iteration: 23Thread: 1, Iteration: 4Thread: 1, Iteration: 5Thread: 1, Iteration: 6Thread: 1, Iteration: 7Thread: 0, Iteration: 0Thread: 0, Iteration: 1Thread: 0, Iteration: 2Thread: 0, Iteration: 3Thread: 6, Iteration: 24Thread: 6, Iteration: 25Thread: 6, Iteration: 26Thread: 6, Iteration: 27Thread: 9, Iteration: 36Thread: 9, Iteration: 37Thread: 9, Iteration: 38Thread: 9, Iteration: 39Thread: 10, Iteration: 40Thread: 4, Iteration: 16Thread: 4, Iteration: 17Thread: 4, Iteration: 18Thread: 4, Iteration: 19Thread: 2, Iteration: 8Thread: 2, Iteration: 9Thread: 2, Iteration: 10Thread: 2, Iteration: 11Thread: 8, Iteration: 32Thread: 8, Iteration: 33Thread: 8, Iteration: 34Thread: 8, Iteration: 35Thread: 3, Iteration: 12Thread: 3, Iteration: 13Thread: 3, Iteration: 14Thread: 3, Iteration: 15Thread: 7, Iteration: 28Thread: 7, Iteration: 29Thread: 7, Iteration: 30Thread: 7, Iteration: 31Thread: 10, Iteration: 41Thread: 10, Iteration: 42Thread: 10, Iteration: 43Thread: 11, Iteration: 44Thread: 11, Iteration: 45Thread: 11, Iteration: 46Thread: 11, Iteration: 47 参考：https://bisqwit.iki.fi/story/howto/openmp/https://www.openmp.org/wp-content/uploads/omp-hands-on-SC08.pdfhttps://stackoverflow.com/questions/1448318/omp-parallel-vs-omp-parallel-forhttp://jakascorner.com/blog/2016/06/omp-for-scheduling.html","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"OpenMP","slug":"OpenMP","permalink":"http://huangzhiyuan.github.io/tags/OpenMP/"}]},{"title":"Hello World的魔幻打印","slug":"Obfuscating-hello-world","date":"2020-08-20T12:57:53.000Z","updated":"2020-08-21T01:57:30.000Z","comments":true,"path":"2020/08/20/Obfuscating-hello-world/","link":"","permalink":"http://huangzhiyuan.github.io/2020/08/20/Obfuscating-hello-world/","excerpt":"今天遇到了个很好玩的话题，怎么模（魔）糊（幻）打印Hello World。和平常第一次接触编程语言的Hello World不一样，平常一行就能完成的功能这里实现的能够异常复杂，并且生成了一堆看似乱码的字符，结果用python解析运行却能得到和Hello World一样的结果，原理我到现在还没完全搞明白，先做个记录后面慢慢理解吧。","text":"今天遇到了个很好玩的话题，怎么模（魔）糊（幻）打印Hello World。和平常第一次接触编程语言的Hello World不一样，平常一行就能完成的功能这里实现的能够异常复杂，并且生成了一堆看似乱码的字符，结果用python解析运行却能得到和Hello World一样的结果，原理我到现在还没完全搞明白，先做个记录后面慢慢理解吧。 先看要求：Task: Create an obfuscated program that prints Hello World! (exactly like that). Your program may not have any strings in it. Rules: You can use any programming language you like.Make it as obfuscated as possibleThis is a popularity-contest, so the answer with the most upvotes wins. 如下答案高票得选： 1(lambda _, __, ___, ____, _____, ______, _______, ________: getattr(__import__(True.__class__.__name__[_] + [].__class__.__name__[__]), ().__class__.__eq__.__class__.__name__[:__] + ().__iter__().__class__.__name__[_____:________])(_, (lambda _, __, ___: _(_, __, ___))(lambda _, __, ___: chr(___ % __) + _(_, __, ___ // __) if ___ else (lambda: _).func_code.co_lnotab, _ &lt;&lt; ________, (((_____ &lt;&lt; ____) + _) &lt;&lt; ((___ &lt;&lt; _____) - ___)) + (((((___ &lt;&lt; __) - _) &lt;&lt; ___) + _) &lt;&lt; ((_____ &lt;&lt; ____) + (_ &lt;&lt; _))) + (((_______ &lt;&lt; __) - _) &lt;&lt; (((((_ &lt;&lt; ___) + _)) &lt;&lt; ___) + (_ &lt;&lt; _))) + (((_______ &lt;&lt; ___) + _) &lt;&lt; ((_ &lt;&lt; ______) + _)) + (((_______ &lt;&lt; ____) - _) &lt;&lt; ((_______ &lt;&lt; ___))) + (((_ &lt;&lt; ____) - _) &lt;&lt; ((((___ &lt;&lt; __) + _) &lt;&lt; __) - _)) - (_______ &lt;&lt; ((((___ &lt;&lt; __) - _) &lt;&lt; __) + _)) + (_______ &lt;&lt; (((((_ &lt;&lt; ___) + _)) &lt;&lt; __))) - ((((((_ &lt;&lt; ___) + _)) &lt;&lt; __) + _) &lt;&lt; ((((___ &lt;&lt; __) + _) &lt;&lt; _))) + (((_______ &lt;&lt; __) - _) &lt;&lt; (((((_ &lt;&lt; ___) + _)) &lt;&lt; _))) + (((___ &lt;&lt; ___) + _) &lt;&lt; ((_____ &lt;&lt; _))) + (_____ &lt;&lt; ______) + (_ &lt;&lt; ___))))(*(lambda _, __, ___: _(_, __, ___))((lambda _, __, ___: [__(___[(lambda: _).func_code.co_nlocals])] + _(_, __, ___[(lambda _: _).func_code.co_nlocals:]) if ___ else []), lambda _: _.func_code.co_argcount, (lambda _: _, lambda _, __: _, lambda _, __, ___: _, lambda _, __, ___, ____: _, lambda _, __, ___, ____, _____: _, lambda _, __, ___, ____, _____, ______: _, lambda _, __, ___, ____, _____, ______, _______: _, lambda _, __, ___, ____, _____, ______, _______, ________: _))) 第一印象，什么鬼？？第二眼还是看不懂。来看看作者怎么解释的。 12345678Here is a more readable version: http://codepad.org/UzSmoxF2Notes:One line, single expression (i.e. no print statement).No strings, no ints; only functions, attribute access, lists, tuples, basic math, one True, and one star-args.Minimal builtin usage (__import__, getattr, and chr once each).The payload can be changed easily. Here is the program I wrote to generate it.Edit: I wrote a fairly substantial explanation of how this works on my blog. 编写generate.py脚本通过对输入字符串加密： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# How to use?# python generate.py &quot;the str needed to be encry&quot;#from math import ceil, logimport sysdef encode_string(s): bytes = [ord(c) for c in s] num = sum(bytes[i] * 256 ** i for i in range(len(bytes))) return reduce(num, 0)def reduce(num, depth): def _encode(num, depth): if num == 0: return &quot;_ - _&quot; if num in range(9): return &quot;_&quot; * num return &quot;(&quot; + reduce(num, depth + 1) + &quot;)&quot; result = &quot;&quot; while num: best_base = best_shift = 0 best = num span = int(ceil(log(abs(num), 1.5))) + (16 &gt;&gt; depth) for base in range(span): for shift in range(span): diff = abs(num) - (base &lt;&lt; shift) if abs(diff) &lt; abs(best): best = diff best_base = base best_shift = shift if result: result += &quot; + &quot; if num &gt; 0 else &quot; - &quot; elif num &lt; 0: best_base = -best_base if best_shift == 0: result += _encode(best_base, depth) else: result += &quot;(%s &lt;&lt; %s)&quot; % (_encode(best_base, depth), _encode(best_shift, depth)) num = best if num &gt; 0 else -best return result# print(len(sys.argv))# print(&#x27; &#x27;.join(sys.argv[1:]))result = encode_string(str(&#x27; &#x27;.join(sys.argv[1:]))+&quot;\\n&quot;)print(&quot;Encrypted Text: &quot;)print(result) Run tese case: 123(base) huang@mlp:~/taichi/intel$ python generate.py huang zhiyuanEncrypted Text:(((_______ &lt;&lt; ___) - _) &lt;&lt; ((___ &lt;&lt; _____) + _)) + (((___ &lt;&lt; _____) + _) &lt;&lt; ((((___ &lt;&lt; __) - _) &lt;&lt; ___))) + (((((_ &lt;&lt; ____) - _) &lt;&lt; ___) - ___) &lt;&lt; ((_____ &lt;&lt; ____))) + (((((_ &lt;&lt; ____) - _) &lt;&lt; ___) + _) &lt;&lt; (((((_ &lt;&lt; ___) + _)) &lt;&lt; ___))) + (((((___ &lt;&lt; __) + _) &lt;&lt; ___) + _) &lt;&lt; ((_ &lt;&lt; ______))) + ((((___ &lt;&lt; __) + _)) &lt;&lt; ((((_ &lt;&lt; ____) - _) &lt;&lt; __) - _)) + (((((_ &lt;&lt; ____) - _) &lt;&lt; __) + _) &lt;&lt; ((___ &lt;&lt; ____) + _)) + (((_ &lt;&lt; ______) + _) &lt;&lt; ((_____ &lt;&lt; ___) - _)) - (((___ &lt;&lt; ____) + _) &lt;&lt; ((_ &lt;&lt; _____) - _)) - ((((((_ &lt;&lt; ___) + _)) &lt;&lt; __) - _) &lt;&lt; ((___ &lt;&lt; ___) - _)) - (((_ &lt;&lt; _____) - _) &lt;&lt; ((_ &lt;&lt; ____))) + (((_______ &lt;&lt; __) + _) &lt;&lt; ((_____ &lt;&lt; _))) + ((((___ &lt;&lt; __) - _)) &lt;&lt; _____) + (_ &lt;&lt; ___) 利用上面生成的加密字符填充运行脚本run.py： 1234567891011121314151617181920212223242526272829303132333435363738huang@mlp:~/taichi/intel$ cat run.py(lambda _, __, ___, ____, _____, ______, _______, ________: getattr( __import__(True.__class__.__name__[_] + [].__class__.__name__[__]), ().__class__.__eq__.__class__.__name__[:__] + ().__iter__().__class__.__name__[_____:________] )( _, (lambda _, __, ___: _(_, __, ___))( lambda _, __, ___: chr(___ % __) + _(_, __, ___ // __) if ___ else (lambda: _).func_code.co_lnotab, _ &lt;&lt; ________, ## fill the encrypted text here (((_____ &lt;&lt; _____) + _______) &lt;&lt; ((___ &lt;&lt; _____) + (_ &lt;&lt; __))) - ((((___ &lt;&lt; __) + _)) &lt;&lt; ((___ &lt;&lt; _____) - ___)) + (((___ &lt;&lt; _____) - ___) &lt;&lt; ((_____ &lt;&lt; ____) + (_ &lt;&lt; _))) + (((___ &lt;&lt; ____) - _) &lt;&lt; (((((_ &lt;&lt; ___) + _)) &lt;&lt; ___) + ___)) + (((((___ &lt;&lt; __) - _) &lt;&lt; __) + _) &lt;&lt; ((((_ &lt;&lt; ____) + _) &lt;&lt; __) - _)) + (((((___ &lt;&lt; __) - _) &lt;&lt; __) + _) &lt;&lt; ((((_ &lt;&lt; ____) - _) &lt;&lt; __) - _)) + (((((_ &lt;&lt; ____) - _) &lt;&lt; __) + _) &lt;&lt; ((___ &lt;&lt; ____) + _)) + (((_ &lt;&lt; ______) + _) &lt;&lt; ((_____ &lt;&lt; ___) - _)) - (((___ &lt;&lt; ____) + _) &lt;&lt; ((_ &lt;&lt; _____) - _)) - ((((((_ &lt;&lt; ___) + _)) &lt;&lt; __) - _) &lt;&lt; ((___ &lt;&lt; ___) - _)) - (((_ &lt;&lt; _____) - _) &lt;&lt; ((_ &lt;&lt; ____))) + (((_______ &lt;&lt; __) + _) &lt;&lt; ((_____ &lt;&lt; _))) + ((((___ &lt;&lt; __) - _)) &lt;&lt; _____) + (_ &lt;&lt; ___) ) ))( *(lambda _, __, ___: _(_, __, ___))( (lambda _, __, ___: [__(___[(lambda: _).func_code.co_nlocals])] + _(_, __, ___[(lambda _: _).func_code.co_nlocals:]) if ___ else [] ), lambda _: _.func_code.co_argcount, ( lambda _: _, lambda _, __: _, lambda _, __, ___: _, lambda _, __, ___, ____: _, lambda _, __, ___, ____, _____: _, lambda _, __, ___, ____, _____, ______: _, lambda _, __, ___, ____, _____, ______, _______: _, lambda _, __, ___, ____, _____, ______, _______, ________: _ ) )) 验（zhuang）证（bi）结（shi）果（ke）： 12(base) huang@mlp:~/taichi/intel$ python2 run.pyhuang zhiyuan 版本目前来看只支持python2，否则会报error,从python2到python3有些built-in函数attribute命名规则发生了改变。 1The function attributes named func_X have been renamed to use the __X__ form, freeing up these names in the function attribute namespace for user-defined attributes. To wit, func_closure, func_code, func_defaults, func_dict, func_doc, func_globals, func_name were renamed to __closure__, __code__, __defaults__, __dict__, __doc__, __globals__, __name__, respectively. 123456789(base) huang@mlp:~/taichi/intel$ python run.pyTraceback (most recent call last): File &quot;run.py&quot;, line 32, in &lt;module&gt; lambda _, __, ___, ____, _____, ______, _______, ________: _ File &quot;run.py&quot;, line 18, in &lt;lambda&gt; *(lambda _, __, ___: _(_, __, ___))( File &quot;run.py&quot;, line 21, in &lt;lambda&gt; _(_, __, ___[(lambda _: _).func_code.co_nlocals:]) if ___ else []AttributeError: &#x27;function&#x27; object has no attribute &#x27;func_code&#x27; 参考链接：https://codegolf.stackexchange.com/questions/22533/weirdest-obfuscated-hello-worldhttps://benkurtovic.com/2014/06/01/obfuscating-hello-world.htmlhttp://codepad.org/UzSmoxF2http://codepad.org/oVuFVcB5","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"加密","slug":"加密","permalink":"http://huangzhiyuan.github.io/tags/%E5%8A%A0%E5%AF%86/"}]},{"title":"分布式训练理论篇","slug":"distributed-training-theory","date":"2020-08-12T07:00:45.000Z","updated":"2020-08-12T08:32:00.000Z","comments":true,"path":"2020/08/12/distributed-training-theory/","link":"","permalink":"http://huangzhiyuan.github.io/2020/08/12/distributed-training-theory/","excerpt":"我们为什么需要分布式，一方面是不得已而为之，例如 数据量太大，数据无法加载模型太复杂，一个GPU放不下或者我们也可以利用分布式提高我们的训练速度","text":"我们为什么需要分布式，一方面是不得已而为之，例如 数据量太大，数据无法加载模型太复杂，一个GPU放不下或者我们也可以利用分布式提高我们的训练速度 算法分类Parameter Server机器分为Parameter Server 和 woker 两类，Parameter server 负责整合梯度，更新参数，woker负责计算，训练网络。 从上图可以很清楚的看到PS的流程，共分为4步。 Task Scheduler：加载数据，并将数据分发给不同的workers Workers： 1.计算梯度（1. compute）加载训练数据; 从servers拉取最新的参数（4. Pull）2.将梯度push到servers（2.Push）从servers拉取最新的参数（4. Pull） Servers： 1.聚合梯度2.更新参数（3. Update） 通信成本为： O(d/b(2m-1)), 通信时间和GPU个数成正比。m: number of GPUs.d: number of parameters.b: network bandwidth. Ring ALL-Reducereduce 和 all-reduce的区别： reduce：server 获取reduce之后的结果（sum/avg/mean/count），其他节点并不知道reduce之后的结果。例如我们需要进行reduce_sum 的操作，那么所有的workers的值都会传给server，然后server将这些值进行相加。 all reduce：所有的节点都获取reduce之后的结果，不单单server有。all-reduce是reduce+broadcast。例如我们需要进行all-reduce_sum 的操作，那么所有的workers的值都会传给server，然后server将这些值进行相加，然后server会将相加的和broadcast给所有的workers。 all-to-all communication。或者说如果没有 server 机器，那么我们可以通过all-to-all 通信，让每个节点都获取其他节点的值，然后再在各自机器上进行reduce。 首先我们有4台机器GPU 0:4, 我们的目标是将每个GPU计算出来的梯度 [公式] 进行相加，使得每个GPU都有所有GPU的梯度。 Ring All-Reduce Native Approach首先我们想到的方式是，我们将每个GPU的目前加和的梯度都向下传递，然后最后一个GPU可以得到最后的加和，然后它再将所有的结果传递给其他的没有获得结果的GPU。整个过程如下： GPU0 将 g0 传给GPU1，GPU1得到 g0+g1 的结果 GPU1 将 g0+g1 传给 GPU2，GPU2得到 g0+g1+g2 的结果 GPU2 将 g0+g1+g2 传给 GPU3，GPU3得到 g0+g1+g2+g3 的结果 GPU3 将 g=g0+g1+g2+g3 传给GPU0， GPU0 得到 g GPU0 将 g 传给GPU1， GPU1 得到 g GPU1 将 g 传给GPU3， GPU1 得到 g 最后每个worker自己做参数更新。 我们可以发现，这样子做的话，每次的数据传输都只用了一个通道，即只有两台GPU在进行传输，其他的3个通道都没有使用，造成了资源的浪费。我们假设有 m 台机器，每台机器传输的参数量为 d, 传输带宽为 b， 我们可以得到传输的时间为 O(d*m/b)，传输的速度和GPU数目成正比，这极大地限制了集群的可扩展性。 Ring All-Reduce (Efficient Approach)为了充分利用起其他的通道，我们可以采取 Ring All-reduce的方式进行传输。其主要的作用就是将传输的时间不再正比于GPU的数目。 首先我们将传输的梯度进行 m 等分split，每一次的传输传的参数量为 d/m。 如下图。 首先我们得到GPU_i 计算出来的梯度倍等切成后的 gi=[ai;bi;ci;di]（本例子为4等分），然后我们可以发现所有 ai 加起来是第一部分的加和，所有 bi 加起来是第2部分的加和，所有 ci 加起来是第3部分的加和，所有 di 加起来是第4部分的加和。并且 GPU0 可以传输给GPU1，GPU1 可以传输给GPU2，GPU2可以传输给GPU3，GPU3可以传输给GPU0，形成环状网络。 这个环状传输是同时进行的，其保证了每次传输的并行（纵向），且保证了每次的reduce计算都只在机器的一个部分，不会同一个机器多个部分同时进行（横向）。 首先进行第一次传输， GPU0 –a0–&gt; GPU1，GPU1 –b1–&gt; GPU2, GPU2 –c2–&gt; GPU3, GPU3 –d3–&gt; GPU0, 注意这边的初始传输值不是 a0, a1, a2,a3 第二次传输，就是将目前得到的每个部门的reduce结果，传输给下个device。GPU1 –a0+a1–&gt; GPU2，GPU2 –b1+b2–&gt; GPU3, GPU3 –c2+c3–&gt; GPU0, GPU0 –d0+d3–&gt; GPU1 第三次传输，还是将上次传输的结果传给下一个device，注意这边传的是结果而不是中间过程，即例如传输 a0+a1+a2 是传输这个结果而不是三个a0,a1,a2参数,否则传输的参数量就增加了。GPU2 –a0+a1+a2–&gt; GPU3，GPU3 –b1+b2+b3–&gt; GPU0, GPU0 –c0+c2+c3–&gt; GPU1, GPU1 –d0+d1+d3–&gt; GPU2 第四次传输,还是将上次传输的结果传给下一个device，此时，每个部分都已经有一个机器获得了最终的结果。 我们令a=a0+a1+a2+a3, b=b0+b1+b2+b3, c=c0+c1+c2+c3, d=d0+d1+d2+d3GPU3 –a–&gt; GPU0，GPU0 –b–&gt; GPU1, GPU1 –d–&gt; GPU2, GPU2 –d–&gt; GPU3 第五次传输，如下 第六次传输 得到最终的结果： 我们可以计算一下时间的开销：O(d/m *m/b) = O(d/m) ，传输的时间和GPU的个数无关！ 更新方式分类同步式和异步式的区别在于参数的更新，是否需要等其他的worker的梯度计算完成。同步式需要，而异步式不需要。 同步式我们以 MultiworkerMirroredStrategy 作为例子：首先是数据并行，数据会被分发到不同的devices（下图的A/B/C）上，然后每个device 各自进行梯度计算，最后Parameter Device(s) 会等所有的梯度都计算完之后，再更新参数，最后将参数传给所有的devices。同步式的更新的优缺点：优点： 同步避免过多的通信（因为参数传输需要通信） 效果会比异步并行略好 缺点： 有的机器计算可能快，有的机器计算慢，这样就导致，个别机器会有计算资源的空耗，导致浪费。（单机多卡一般不会） 异步式我们以 ParameterServerStrategy 作为例子：首先我们的所有的数据还是进行并行化，分发到不同的devices（下图的A/B/C）上，每个 worker device 各自进行了梯度的计算后，传给parameter server，而参数服务器在收到了梯度之后，不用等其他机器的结果，直接去更新参数，得到新的参数值，传给各个worker。 异步式的更新的优缺点：优点： 参数更新不需要等其他机器，所有更高效，避免短板效应（多机多卡） 异步的计算会增加模型的泛化能力,因为异步不是严格正确的，所以模型更容忍错误 缺点： 异步式更新的次数多了，会导致更多的通信时间 每个GPU计算完梯度后就各自更新，速度快，但是最终模型的性能略低于同步数据并行方式。 并行方式分类数据并行数据同时分发给各个worker，并进行梯度计算。 模型并行当我们的模型过大，不能放在一个GPU中，那么需要将模型分成多个部分，在各个worker 中。 数据和模型并行把数据和模型同时并行，进行训练。其中最出名的就是微软的ZeRO &amp; DeepSpeed: New system optimizations enable training models with over 100 billion parameters算法。它的出现使得训练超级大模型成了可能。微软用这个算法，训练出了 Turing-NLG: A 17-billion-parameter language model by Microsoft 170亿的参数，直接刷新世界观（壕无人性） 该ZeRO算法，主要如下图。 数据被分为n+1份，并行处理。然后整个模型结构被过大，不能但在单一的GPU中，它被存放在n+1个GPU中，每个GPU存放三个部分 蓝色的部分是模型的参数 黄色的部分是模型的梯度 绿色的部分是模型梯度更新中间产生的数据，例如adam的中间产生数据。 那么模型要如何进行训练呢？首先我们知道模型在进行更新的过程是先将数据正向传播，计算loss和梯度，再反向传播进行更新。例如下图，我们的模型存放在两个GPU中，在正向传播过程中我们必须等到数据经过GPU1，才能到GPU2，在反向传播过程中，我们需要保证GPU2的参数都计算完成，才能传到GPU1，这就导致了每次都有一个机器是闲置的。 这是因为我们的每个GPU都缺少了部分的模型参数，但是我们发现其实模型的参数并不大，大的是模型的 optimizer 的中间参数， 所以我们就是可以通过将所需要的其他部分的模型参数通过通信传输，获得，使得所有的部分都可以并行。 在进行前向传播的时候，我们以 GPU0 做为例子，在data0 flow的时候，如果需要任何的参数，都可以通过通信的过程获得，比如说刚开始先forward flow完成GPU0的模型结构，之后需要GPU1的模型结构，GPU1 会将其的参数传给GPU0，之后GPU2会将其的参数传给GPU0… 以此类推，直至前行传播完成获得loss。计算出梯度。注意此时DATA1，…DATAn+1 都在并行进行前向传播。 再反向传播的过程中，我们以 GPU0 做为例子，首先会计算出n+1 部分模型的梯度Gn+1（黄色的部分）, GPU0 不会储存这个梯度，而是将它直接传输给GPU_{n+1}, 因为它负责更新n+1 部分模型（注意，此时所有的其他GPU计算出来的这部分的梯度都会传给它，最后在有它进行reduce），之后计算会计算出第n 部分模型的梯度Gn（黄色的部分），并传给 GPU_n，以此类推，算出得到第0部分模型的梯度G0（黄色的部分），然后并且收到了其他GPU关于此部分的梯度，并按照optimizer paramters进行第0部分的参数的更新。 引用https://fyubang.com/2019/07/08/distributed-training/https://zhuanlan.zhihu.com/p/129912419https://www.youtube.com/watch%3Fv%3DtC01FRB0M7w","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"GPU","slug":"GPU","permalink":"http://huangzhiyuan.github.io/tags/GPU/"}]},{"title":"Copy issue in ATS GPU","slug":"copy-issue-in-ATS","date":"2020-08-11T08:53:34.000Z","updated":"2020-08-11T09:13:16.000Z","comments":true,"path":"2020/08/11/copy-issue-in-ATS/","link":"","permalink":"http://huangzhiyuan.github.io/2020/08/11/copy-issue-in-ATS/","excerpt":"Performance of copy using different data types.Got a performance issue on ATS when copy a continuous buffer of u16 data type. It is almost 2x slower than copy a buffer of same byte-length but using u32 data type. Similarly, copy of u8 is even slower. Below is a test case of copy a same buffer using different data type of u32/u16/u8.","text":"Performance of copy using different data types.Got a performance issue on ATS when copy a continuous buffer of u16 data type. It is almost 2x slower than copy a buffer of same byte-length but using u32 data type. Similarly, copy of u8 is even slower. Below is a test case of copy a same buffer using different data type of u32/u16/u8. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131// Build with DPC++ Compiler:// clang++ -std=c++14 -fsycl -O3 copy.cpp -o copy//// Run:// ./copy 65536 512//#include &lt;iostream&gt;#include &lt;cmath&gt;#include &lt;chrono&gt;#include &quot;CL/sycl.hpp&quot;// Timer#define _(x) x#define __tstart(n) _(std::chrono::high_resolution_clock::time_point __s##n = \\ std::chrono::high_resolution_clock::now());#define __tend(n) \\ _(std::chrono::high_resolution_clock::time_point __e##n = \\ std::chrono::high_resolution_clock::now()); \\ _(printf(&quot;time: %s, %.2f ms\\n&quot;, #n, \\ std::chrono::duration&lt;float, std::milli&gt;(__e##n - __s##n).count()));namespace sycl = cl::sycl;int main(int argc, char * argv[])&#123; if (argc &lt; 3) &#123; std::cerr &lt;&lt; &quot;Usage: copy &lt;M length&gt; &lt;N length&gt;&quot; &lt;&lt; std::endl; return argc; &#125; size_t M = std::atoi(argv[1]); size_t N = std::atoi(argv[2]); if (N % 4 != 0) &#123; std::cerr &lt;&lt; &quot;Usage: N must be times of 4&quot; &lt;&lt; std::endl; &#125; sycl::queue q(sycl::default_selector&#123;&#125;); auto ctx = q.get_context(); auto dev = q.get_device(); uint8_t *Y = static_cast&lt;uint8_t*&gt;(sycl::malloc_shared(M * N, dev, ctx)); uint8_t *Z = static_cast&lt;uint8_t*&gt;(sycl::malloc_shared(M * N, dev, ctx)); uint32_t *Z32 = (uint32_t*)Z; uint32_t *Y32 = (uint32_t*)Y; uint16_t *Z16 = (uint16_t*)Z; uint16_t *Y16 = (uint16_t*)Y; size_t N32 = N / 4; size_t N16 = N / 2; for (size_t i = 0; i &lt; M * N; i++) &#123; Y[i] = i % 255; &#125; // warmup &#123; for (size_t j = 0; j &lt; 5; j++) &#123; q.submit([&amp;](sycl::handler&amp; h) &#123; h.parallel_for&lt;class u32_warmup&gt;( sycl::range&lt;2&gt;&#123;M, N32&#125;, [=] (sycl::id&lt;2&gt; it) &#123; const int m = it[0]; const int n = it[1]; Z32[m * N32 + n] = Y32[m * N32 + n]; &#125;); &#125;); q.wait(); &#125; &#125; __tstart(copy_as_u32); &#123; for (size_t j = 0; j &lt; 100; j++) &#123; q.submit([&amp;](sycl::handler&amp; h) &#123; h.parallel_for&lt;class u32&gt;( sycl::range&lt;2&gt;&#123;M, N32&#125;, [=] (sycl::id&lt;2&gt; it) &#123; const int m = it[0]; const int n = it[1]; Z32[m * N32 + n] = Y32[m * N32 + n]; &#125;); &#125;); q.wait(); &#125; &#125; __tend(copy_as_u32); // warmup &#123; for (size_t j = 0; j &lt; 5; j++) &#123; q.submit([&amp;](sycl::handler&amp; h) &#123; h.parallel_for&lt;class u16_warmup&gt;( sycl::range&lt;2&gt;&#123;M, N16&#125;, [=] (sycl::id&lt;2&gt; it) &#123; const int m = it[0]; const int n = it[1]; Z16[m * N16 + n] = Y16[m * N16 + n]; &#125;); &#125;); q.wait(); &#125; &#125; __tstart(copy_as_u16); &#123; for (size_t j = 0; j &lt; 100; j++) &#123; q.submit([&amp;](sycl::handler&amp; h) &#123; h.parallel_for&lt;class u16&gt;( sycl::range&lt;2&gt;&#123;M, N16&#125;, [=] (sycl::id&lt;2&gt; it) &#123; const int m = it[0]; const int n = it[1]; Z16[m * N16 + n] = Y16[m * N16 + n]; &#125;); &#125;); q.wait(); &#125; &#125; __tend(copy_as_u16); // warmup &#123; for (size_t j = 0; j &lt; 5; j++) &#123; q.submit([&amp;](sycl::handler&amp; h) &#123; h.parallel_for&lt;class u8_warmup&gt;( sycl::range&lt;2&gt;&#123;M, N&#125;, [=] (sycl::id&lt;2&gt; it) &#123; const int m = it[0]; const int n = it[1]; Z[m * N + n] = Y[m * N + n]; &#125;); &#125;); q.wait(); &#125; &#125; __tstart(copy_as_u8); &#123; for (size_t j = 0; j &lt; 100; j++) &#123; q.submit([&amp;](sycl::handler&amp; h) &#123; h.parallel_for&lt;class u8&gt;( sycl::range&lt;2&gt;&#123;M, N&#125;, [=] (sycl::id&lt;2&gt; it) &#123; const int m = it[0]; const int n = it[1]; Z[m * N + n] = Y[m * N + n]; &#125;); &#125;); q.wait(); &#125; &#125; __tend(copy_as_u8); sycl::free(Y, ctx); sycl::free(Z, ctx); return 0;&#125; Build with DPC++: 1$ clang++ -std=c++14 -fsycl -O3 copy.cpp -o copy Run the bench on ATS 480EU: 1234$ ./copy 65536 512time: copy_as_u32, 37.74 mstime: copy_as_u16, 64.26 mstime: copy_as_u8, 114.77 ms It shows copy_as_u16 is 1.7x slower than copy_as_u32, copy_as_u8 is 3x slower than copy_as_u32 when running in a bench case of 100 iterations for each data type. Checking with the generated code, copy_as_u32 is in a quite compact form: 12345678Address Source Line Assembly0x9a0 0 send.dc1 (16|M0) r45 r41 null 0x0 0x4205E01 &#123;$5&#125; [, msg-length:2, resp-length:2, header:no, func-control:5e01]0x9b0 0 sync.nop null &#123;Compacted, I@1&#125;0x9b8 0 send.dc1 (16|M16) r47 r43 null 0x0 0x4205E01 &#123;$6&#125; [, msg-length:2, resp-length:2, header:no, func-control:5e01]...0xde8 0 send.dc1 (16|M0) null r81 r45 0x80 0x4025E02 &#123;$5&#125; [, msg-length:2, resp-length:0, header:no, func-control:25e02]0xdf8 0 sync.nop null &#123;Compacted, I@1&#125;0xe00 0 send.dc1 (16|M16) null r83 r47 0x80 0x4025E02 &#123;$6&#125; [, msg-length:2, resp-length:0, header:no, func-control:25e02] However for copy_as_u16, the generated code is using only the low 16bit of the registers with the higher 16bit kept empty. And it issues 2x of send with additional movs. 1234567891011Address Source Line Assembly0x9a0 0 send.dc0 (16|M0) r45 r30 null 0x0 0x4210501 &#123;$5&#125; [, msg-length:2, resp-length:2, header:no, func-control:10501]0x9b0 0 sync.nop null &#123;Compacted, I@1&#125;0x9b8 0 send.dc0 (16|M16) r47 r32 null 0x0 0x4210501 &#123;$6&#125; [, msg-length:2, resp-length:2, header:no, func-control:10501]…0xdd0 0 mov (16|M0) r85.0&lt;1&gt;:ud r45.0&lt;2;1,0&gt;:uw &#123;$5.dst&#125;0xde0 0 mov (16|M16) r87.0&lt;1&gt;:ud r47.0&lt;2;1,0&gt;:uw &#123;$6.dst&#125;…0xe08 0 send.dc0 (16|M0) null r81 r85 0x80 0x4030502 &#123;$7&#125; [, msg-length:2, resp-length:0, header:no, func-control:30502]0xe18 0 sync.nop null &#123;Compacted, I@1&#125;0xe20 0 send.dc0 (16|M16) null r83 r87 0x80 0x4030502 &#123;$8&#125; [, msg-length:2, resp-length:0, header:no, func-control:30502] This seems to be a quite common performance penalty as in FP16 inference/training, most of math kernels uses the FP16 data type passed from the framework front-end. A lot of OPs (like embedding, concat, index and so on) have copy semantics. INT8 inference will have similar issue. We expect in case of contiguous copy, u8/u16 is same fast as u32. the same issue can also be reproduced on Gen9: 1234$./copy 65536 512time: copy_as_u32, 379.55 mstime: copy_as_u16, 579.60 mstime: copy_as_u8, 764.89 ms","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"GPU","slug":"GPU","permalink":"http://huangzhiyuan.github.io/tags/GPU/"}]},{"title":"mlperf-v0.7","slug":"mlperf-v0-7","date":"2020-08-10T01:45:19.000Z","updated":"2020-08-10T01:59:00.000Z","comments":true,"path":"2020/08/10/mlperf-v0-7/","link":"","permalink":"http://huangzhiyuan.github.io/2020/08/10/mlperf-v0-7/","excerpt":"MLPerf是业内首套衡量机器学习软硬件性能的通用基准，由图灵奖得主David Patterson联合谷歌和几所著名高校于2018年发起。MLPerf 是AI芯片的一个基准测试，主要包括：Training 和Inference两个方面的性能测试。Training是于测量系统将模型训练到目标质量指标的速度；Inference是用于测试系统使用训练有素的模型处理输入和产生结果的速度。MLPerf基准联盟现有83家成员，包括谷歌、英伟达、微软、Facebook、阿里巴巴等73家企业和斯坦福、哈佛、多伦多大学等10所高校","text":"MLPerf是业内首套衡量机器学习软硬件性能的通用基准，由图灵奖得主David Patterson联合谷歌和几所著名高校于2018年发起。MLPerf 是AI芯片的一个基准测试，主要包括：Training 和Inference两个方面的性能测试。Training是于测量系统将模型训练到目标质量指标的速度；Inference是用于测试系统使用训练有素的模型处理输入和产生结果的速度。MLPerf基准联盟现有83家成员，包括谷歌、英伟达、微软、Facebook、阿里巴巴等73家企业和斯坦福、哈佛、多伦多大学等10所高校 随着AI技术的进步，今年的测试基准进一步加大了难度。 MLPerf训练测试基准包括图像分类、翻译、推荐系统和围棋等8个机器学习任务中，最终结果是这8项任务的训练时间，速度越快则性能越强。 具体的8项任务内容如下： 其中后三项是新加入或重新制定的标准： 1、BERT：用Wikipedia语料库训练BERT，这是首次将BERT引入MLPerf测试基准。 2、DLRM：用Criteo AI Lab的Terabyte点击率数据集训练的深度学习推荐模型（DLRM），广泛用于在线购物推荐、搜索结果和社交媒体内容排序。 3、Mini-Go：之前的MLPerf v0.5和v0.6也有训练围棋的强化学习任务，但却是迷你棋盘，此次v0.7将棋盘扩大为19×19全尺寸，这更能反映研究成果。 感谢zixuan同学的整理，现将部分汇总结论整理如下： Submitter – Software Relationship MxNet, Pytorch, Tensorflow are still the mainstream of deep learning framework. Customized frameworks, i.e. Huawei MindSpore, Nvidia Merlin are also entering public view. Submitter – Field Relationship Nvidia and Google are active in all the deep learning fields.Image Classification benchmark is popular and is adopted by almost all the company. RL, Recommendation, NLP received less attention. Intel has no submission for NLP and Object detection. China mainland companies, i.e. Alibaba, Inspur, Shenzhen*, needs more efforts to become remarkable Software – Field Relationship Tensorflow is adopted in all the fields. MxNet has the submissions for image classification and object detection (because gluon-cv toolkit is much more developed than others, like gluon-nlp?) Combination of two frameworks (mxnet + pytorch) is also an option. 参考链接：https://xueqiu.com/1097649362/155335766https://mlperf.org/training-results-0-7","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"MLPerf","slug":"MLPerf","permalink":"http://huangzhiyuan.github.io/tags/MLPerf/"}]},{"title":"PyTorch 模型参数可视化","slug":"pytorch-model-parameters-visualization","date":"2020-07-02T07:12:14.000Z","updated":"2020-07-02T09:12:44.000Z","comments":true,"path":"2020/07/02/pytorch-model-parameters-visualization/","link":"","permalink":"http://huangzhiyuan.github.io/2020/07/02/pytorch-model-parameters-visualization/","excerpt":"最近在分析不同的数据类型在深度学习过程中的应用，看CUDA的doc发现有篇文章是关于FP16数据类型对模型训练，达到节省带宽和内存的目的。基于数据模型的精度损失问题，需要分析模型参数的数值分布规律，做到量化和缩放操作避免损失模型精度。此文用来使用jupyter notebook 和matplotlib 可视化模型参数的具体过程。","text":"最近在分析不同的数据类型在深度学习过程中的应用，看CUDA的doc发现有篇文章是关于FP16数据类型对模型训练，达到节省带宽和内存的目的。基于数据模型的精度损失问题，需要分析模型参数的数值分布规律，做到量化和缩放操作避免损失模型精度。此文用来使用jupyter notebook 和matplotlib 可视化模型参数的具体过程。 本文以Resnet50和Mobilenet 网络为例，这两个都是常用的CNN网络，一般用来作为benchmark的baseline，模型结果相对简单，参数不算太多，适合做量化分析，从整体上大致掌握数值的分布规律。 加载Renet50 模型12345678910import numpy as npimport models as modelsmodel_arch = &quot;resnet50&quot;print(&quot;=&gt; using pre-trained model &#x27;&#123;&#125;&#x27;&quot;.format(model_arch))model = models.__dict__[model_arch](pretrained=True)# print model archprint(model) model结构如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178=&gt; using pre-trained model &#x27;resnet50&#x27;ResNet( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (layer1): Sequential( (0): Bottleneck( (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (layer2): Sequential( (0): Bottleneck( (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (3): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (layer3): Sequential( (0): Bottleneck( (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (3): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (4): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (5): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (layer4): Sequential( (0): Bottleneck( (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0) (fc): Linear(in_features=2048, out_features=1000, bias=True)) load model parameters123456789101112131415# print model parameterslayer_list = []params_list = []for name, param in model.named_parameters(): if param.requires_grad:# print(name, param.data.type()) layer_list.append(name) params_list += param.data.view(-1, 1)params_list = [round(float(i.numpy().tolist()[0]), 3) for i in params_list]nonzero_norm_list=[i for i in params_list if i!=0]zero_num = 0for k in params_list: if k == 0: zero_num += 1 获取paramaters大致轮廓123456789101112print(&quot;all parameters: &quot;, len(params_list))print(&quot;all non zero parameters: &quot;, len(nonzero_norm_list))print(&quot;max value: &quot;, max(params_list))print(&quot;min value: &quot;, min(params_list))print(&quot;zero count: &quot;, zero_num)# print(&quot;top 10&quot;, params_list[:10])all parameters: 25557032all non zero parameters: 24695549max value: 1.32min value: -0.782zero count: 861483 可视化参数由于parameter大多在[0,1]之间，所以这里只取部分数据来说明分布： 1234567from matplotlib import pyplot as pltplt.hist(params_list[:], bins=300)plt.title(&quot;Resnet50 parameter value range&quot;)plt.xlabel(&quot;Value Range&quot;)plt.ylabel(&quot;Counts&quot;)plt.show() 注意上图Y轴数量级（10e6），由于参数数量级较大，横轴值的分布较为集中， 大都分布在0左右, 极值范围在[-1, 1.5]。也可以只打印部分数值分析。 1plt.hist(params_list[-8000:], bins=300)","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://huangzhiyuan.github.io/tags/pytorch/"}]},{"title":"机器翻译评价指标之BLEU解析","slug":"BLEU-in-NLP","date":"2020-06-11T05:52:54.000Z","updated":"2020-08-14T07:36:14.000Z","comments":true,"path":"2020/06/11/BLEU-in-NLP/","link":"","permalink":"http://huangzhiyuan.github.io/2020/06/11/BLEU-in-NLP/","excerpt":"如何衡量翻译的好坏？ 机器翻译越接近专业的人工翻译，它就越好。 这是我们的提议背后的核心理念。 为了判断机器翻译的质量，可以根据一个数值指标来衡量其与一个或多个人工参考翻译的接近程度。 因此，我们的MT评估系统需要两个成分： 一个“翻译接近度”数值指标 一个高质量的人工参考翻译语料库 BLEU（Bilingual Evaluation Understudy），相信大家对这个评价指标的概念已经很熟悉，随便百度谷歌就有相关介绍。原论文为BLEU: a Method for Automatic Evaluation of Machine Translation，IBM出品。 本文通过一个例子详细介绍BLEU是如何计算以及NLTK nltk.align.bleu_score模块的源码。 首先祭出公式： 其中，注意这里的BLEU值是针对一条翻译（一个样本）来说的。","text":"如何衡量翻译的好坏？ 机器翻译越接近专业的人工翻译，它就越好。 这是我们的提议背后的核心理念。 为了判断机器翻译的质量，可以根据一个数值指标来衡量其与一个或多个人工参考翻译的接近程度。 因此，我们的MT评估系统需要两个成分： 一个“翻译接近度”数值指标 一个高质量的人工参考翻译语料库 BLEU（Bilingual Evaluation Understudy），相信大家对这个评价指标的概念已经很熟悉，随便百度谷歌就有相关介绍。原论文为BLEU: a Method for Automatic Evaluation of Machine Translation，IBM出品。 本文通过一个例子详细介绍BLEU是如何计算以及NLTK nltk.align.bleu_score模块的源码。 首先祭出公式： 其中，注意这里的BLEU值是针对一条翻译（一个样本）来说的。 NLTKnltk.align.bleu_score模块实现了这里的公式，主要包括三个函数，两个私有函数分别计算P和BP，一个函数整合计算BLEU值。 12345678# 计算BLEU值def bleu(candidate, references, weights)# （1）私有函数，计算修正的n元精确率（Modified n-gram Precision）def _modified_precision(candidate, references, n)# （2）私有函数，计算BP惩罚因子def _brevity_penalty(candidate, references) 例子候选译文（Predicted）：It is a guide to action which ensures that the military always obeys the commands of the party 参考译文（Gold Standard）1：It is a guide to action that ensures that the military will forever heed Party commands2：It is the guiding principle which guarantees the military forces always being under the command of the Party3：It is the practical guide for the army always to heed the directions of the party Modified n-gram Precision计算（也即是Pn）123456789101112131415def _modified_precision(candidate, references, n): counts = Counter(ngrams(candidate, n)) if not counts: return 0 max_counts = &#123;&#125; for reference in references: reference_counts = Counter(ngrams(reference, n)) for ngram in counts: max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram]) clipped_counts = dict((ngram, min(count, max_counts[ngram])) for ngram, count in counts.items()) return sum(clipped_counts.values()) / sum(counts.values()) 我们这里n取值为4，也就是从1-gram计算到4-gram。 Modified 1-gram precision首先统计候选译文里每个词出现的次数，然后统计每个词在参考译文中出现的次数，Max表示3个参考译文中的最大值，Min表示候选译文和Max两个的最小值。 Modified 2-gram precision Modified 3-gram precision Modified 4-gram precision Brevity Penalty 计算123456789101112def _brevity_penalty(candidate, references): c = len(candidate) ref_lens = (len(reference) for reference in references) #这里有个知识点是Python中元组是可以比较的，如(0,1)&gt;(1,0)返回False，这里利用元组比较实现了选取参考翻译中长度最接近候选翻译的句子，当最接近的参考翻译有多个时，选取最短的。例如候选翻译长度是10，两个参考翻译长度分别为9和11，则r=9. r = min(ref_lens, key=lambda ref_len: (abs(ref_len - c), ref_len)) print &#x27;r:&#x27;,r if c &gt; r: return 1 else: return math.exp(1 - r / c) 下面计算BP（Brevity Penalty），翻译过来就是“过短惩罚”。由BP的公式可知取值范围是(0,1]，候选句子越短，越接近0。 候选翻译句子长度为18，参考翻译分别为：16，18，16。所以c = 18，r=18（参考翻译中选取长度最接近候选翻译的作为rr） 所以BP = e^0 =1 整合最终BLEU = 1 ⋅ exp(−0.684055269517) = 0.504566684006 BLEU的取值范围是[0,1]，0最差，1最好。 通过计算过程，我们可以看到，BLEU值其实也就是“改进版的n-gram”加上“过短惩罚因子”。 参考：https://link.zhihu.com/?target=https%3A//www.yiyibooks.cn/yiyibooks/BLEU_a_Method_for_Automatic_Evaluation_of_Machine_Translation/index.htmlhttps://blog.csdn.net/guolindonggld/java/article/details/56966200https://www.zhihu.com/question/304798594/answer/567383628","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"mAP","slug":"mAP","permalink":"http://huangzhiyuan.github.io/tags/mAP/"}]},{"title":"使用Vtune工具反汇编SYCL代码样例","slug":"sycl-assemble-vtune","date":"2020-05-22T01:44:26.000Z","updated":"2020-05-22T02:28:08.000Z","comments":true,"path":"2020/05/22/sycl-assemble-vtune/","link":"","permalink":"http://huangzhiyuan.github.io/2020/05/22/sycl-assemble-vtune/","excerpt":"最近需要验证一个bit_cast转换函数在GPU kernel里面的底层实现形式，之前一直猜想是Mov指令完成的，CPU端的代码往往通过gdb反汇编很容易看到每行代码对应个汇编语言，但是对于GPU的kernel，我却一直没有什么经验。刚好可以利用Intel最近准备release的OneAPI开发工具包，借助里面的gdb-oneapi和VTune工具来实现。本文就是记录这次踩坑过程。","text":"最近需要验证一个bit_cast转换函数在GPU kernel里面的底层实现形式，之前一直猜想是Mov指令完成的，CPU端的代码往往通过gdb反汇编很容易看到每行代码对应个汇编语言，但是对于GPU的kernel，我却一直没有什么经验。刚好可以利用Intel最近准备release的OneAPI开发工具包，借助里面的gdb-oneapi和VTune工具来实现。本文就是记录这次踩坑过程。 这里使用到的就是Intel vtune profiler工具。该工具官方解释：Locate performance bottlenecks fast. Advanced sampling and profiling techniques quickly analyze your code, isolate issues, and deliver insights for optimizing performance on modern processors. Source Code先上测试代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788#pragma OPENCL EXTENSION cl_khr_fp16 : enable#include &lt;iostream&gt;#include &lt;CL/sycl.hpp&gt;//class kernel1;class kernel2;namespace sycl = cl::sycl;template &lt;typename To, typename From&gt; To bit_cast(const From &amp;from) &#123;#if __cpp_lib_bit_cast return std::bit_cast&lt;To&gt;(from);#else#if __has_builtin(__builtin_bit_cast) return __builtin_bit_cast(To, from); // clang path#else To to; detail::memcpy(&amp;to, &amp;from, sizeof(To)); return to;#endif // __has_builtin(__builtin_bit_cast)#endif // __cpp_lib_bit_cast&#125;void add() &#123; sycl::float4 a = &#123;1.0, 2.0, 3.0, 4.0&#125;; sycl::float4 b = &#123;4.0, 3.0, 2.0, 3.0&#125;; sycl::float4 c = &#123;.0, 0.0, 0.0, 0.0&#125;; sycl::default_selector device_selector; sycl::queue queue(device_selector); std::cout &lt;&lt; &quot;Running on &quot; &lt;&lt; queue.get_device().get_info&lt;sycl::info::device::name&gt;() &lt;&lt; &quot;\\n&quot;; &#123; sycl::buffer&lt;sycl::float4, 1&gt; a_sycl(&amp;a, sycl::range&lt;1&gt;(1)); sycl::buffer&lt;sycl::float4, 1&gt; b_sycl(&amp;b, sycl::range&lt;1&gt;(1)); sycl::buffer&lt;sycl::float4, 1&gt; c_sycl(&amp;c, sycl::range&lt;1&gt;(1)); queue.submit([&amp;] (sycl::handler&amp; cgh) &#123; auto a_acc = a_sycl.get_access&lt;sycl::access::mode::read&gt;(cgh); auto b_acc = b_sycl.get_access&lt;sycl::access::mode::read&gt;(cgh); auto c_acc = c_sycl.get_access&lt;sycl::access::mode::discard_write&gt;(cgh); cgh.single_task&lt;class vector_addition&gt;([=] () &#123; c_acc[0] = a_acc[0] + b_acc[0]; &#125;); &#125;); &#125; std::cout &lt;&lt; &quot; A &#123; &quot; &lt;&lt; a.x() &lt;&lt; &quot;, &quot; &lt;&lt; a.y() &lt;&lt; &quot;, &quot; &lt;&lt; a.z() &lt;&lt; &quot;, &quot; &lt;&lt; a.w() &lt;&lt; &quot; &#125;\\n&quot; &lt;&lt; &quot;+ B &#123; &quot; &lt;&lt; b.x() &lt;&lt; &quot;, &quot; &lt;&lt; b.y() &lt;&lt; &quot;, &quot; &lt;&lt; b.z() &lt;&lt; &quot;, &quot; &lt;&lt; b.w() &lt;&lt; &quot; &#125;\\n&quot; &lt;&lt; &quot;------------------\\n&quot; &lt;&lt; &quot;= C &#123; &quot; &lt;&lt; c.x() &lt;&lt; &quot;, &quot; &lt;&lt; c.y() &lt;&lt; &quot;, &quot; &lt;&lt; c.z() &lt;&lt; &quot;, &quot; &lt;&lt; c.w() &lt;&lt; &quot; &#125;&quot; &lt;&lt; std::endl;&#125;void bitCast() &#123; // sycl::gpu_selector device_selector; // sycl::queue queue(device_selector); cl::sycl::queue queue; std::cout &lt;&lt; &quot;Running on &quot; &lt;&lt; queue.get_device().get_info&lt;cl::sycl::info::device::name&gt;() &lt;&lt; &quot;\\n&quot;; constexpr size_t LENGTH = 64; unsigned short res[64] = &#123;0&#125;; cl::sycl::range&lt;1&gt; data_range &#123;LENGTH&#125;; &#123; cl::sycl::buffer&lt;unsigned short, 1&gt; buf_res(res, 1); cl::sycl::half tmp(0.0f); queue.submit([&amp;] (sycl::handler&amp; cgh) &#123; auto a_acc = buf_res.get_access&lt;sycl::access::mode::discard_write&gt;(cgh); cgh.parallel_for&lt;class kernel2&gt;(data_range, [=](cl::sycl::id&lt;1&gt; index) &#123; a_acc[index] = bit_cast&lt;unsigned short, cl::sycl::half&gt;(tmp); // a_acc[index] = 0x42; //a_acc[0] = __builtin_bit_cast(unsigned short, tmp); &#125;); &#125;); queue.wait_and_throw(); &#125; std::cout &lt;&lt; &quot;bit_cast &quot; &lt;&lt; res[0] &lt;&lt; std::endl;&#125;int main() &#123; add(); bitCast(); return 0;&#125; 这段code比较简单，借助SYCL环境，有两个functions，add()``用来验证简单的scalar 加法，bitCast()用来验证`__builtin_bit_cast() function（需要clang++编译器支持）。这里直接安装整个Intel OneAPI工具包即可。这里的kernel block块是下面，需要验证a_acc[index] = bit_cast&lt;unsigned short, cl::sycl::half&gt;(tmp);对应的汇编代码实现。 1234567891011121314cl::sycl::range&lt;1&gt; data_range &#123;LENGTH&#125;;&#123; cl::sycl::buffer&lt;unsigned short, 1&gt; buf_res(res, 1); cl::sycl::half tmp(0.0f); queue.submit([&amp;] (sycl::handler&amp; cgh) &#123; auto a_acc = buf_res.get_access&lt;sycl::access::mode::discard_write&gt;(cgh); cgh.parallel_for&lt;class kernel2&gt;(data_range, [=](cl::sycl::id&lt;1&gt; index) &#123; a_acc[index] = bit_cast&lt;unsigned short, cl::sycl::half&gt;(tmp); // a_acc[index] = 0x42; //a_acc[0] = __builtin_bit_cast(unsigned short, tmp); &#125;); &#125;); queue.wait_and_throw();&#125; Buildbuild步采用makefile来编译。安装并配置oneapie环境变量。 1source /home/zhiyuanh/intel/inteloneapi/setvars.sh makefile文件夹内容如下： 123456789101112131415ONE_API_ROOT := /home/zhiyuanh/intel/inteloneapiSYCL_ROOT := $&#123;ONE_API_ROOT&#125;/compiler/latest/linuxSYCLCXX := $(SYCL_ROOT)/bin/clang++CXXFLAGS := -I $(SYCL_ROOT)/lib/clang/10.0.0/include -fsycl -g -O0LDFLAGS := -L $(SYCL_ROOT)/lib -lOpenCL -fsycl -gfirst.o: first.cpp $(SYCLCXX) -std=c++11 $(CXXFLAGS) -c first.cppclean: rm -f first.exe first.o 之后make生成first目标文件。 Vtune采集导入vtune并选择GPU offload（preview）模式。 查看对应的source code和生成的汇编code。 这里猜想77行内的源代码对应的是右边标红的mov指令，因为cl::sycl::half共有16个bit，对应的汇编符号是UW。该mov指令将一个UW写到r9寄存器，然后再最后send发射出去。中间没有任何一个环节有修改r9的指令。 这里做一小测试，将77行注释，换成一行a_acc[index] = 0x42常量赋值，如果真是对应那条mov指令，那么现在的新的指令里面会有0x42这个立即数。 1234567891011121314cl::sycl::range&lt;1&gt; data_range &#123;LENGTH&#125;;&#123; cl::sycl::buffer&lt;unsigned short, 1&gt; buf_res(res, 1); cl::sycl::half tmp(0.0f); queue.submit([&amp;] (sycl::handler&amp; cgh) &#123; auto a_acc = buf_res.get_access&lt;sycl::access::mode::discard_write&gt;(cgh); cgh.parallel_for&lt;class kernel2&gt;(data_range, [=](cl::sycl::id&lt;1&gt; index) &#123; //a_acc[index] = bit_cast&lt;unsigned short, cl::sycl::half&gt;(tmp); a_acc[index] = 0x42; //a_acc[0] = __builtin_bit_cast(unsigned short, tmp); &#125;); &#125;); queue.wait_and_throw();&#125; 果然和我们猜想一样。 结论bit_cast转换函数在GPU kernel里面的底层实现形式，之前一直猜想是Mov指令完成的，但也有一些其他额外的指令比如mul, or, shl这些可能是算地址偏移量相关。","categories":[{"name":"GPU","slug":"GPU","permalink":"http://huangzhiyuan.github.io/categories/GPU/"}],"tags":[{"name":"SYCL","slug":"SYCL","permalink":"http://huangzhiyuan.github.io/tags/SYCL/"},{"name":"Vtune","slug":"Vtune","permalink":"http://huangzhiyuan.github.io/tags/Vtune/"}]},{"title":"Atomic Operations in CUDA","slug":"atomic-operations","date":"2020-05-18T06:06:21.000Z","updated":"2020-05-18T06:16:54.000Z","comments":true,"path":"2020/05/18/atomic-operations/","link":"","permalink":"http://huangzhiyuan.github.io/2020/05/18/atomic-operations/","excerpt":"This tutorial will discuss how to perform atomic operations in CUDA, which are often essential for many algorithms. Atomic operations are easy to use, and extremely useful in many applications. Atomic operations help avoid race conditions and can be used to make code simpler to write.","text":"This tutorial will discuss how to perform atomic operations in CUDA, which are often essential for many algorithms. Atomic operations are easy to use, and extremely useful in many applications. Atomic operations help avoid race conditions and can be used to make code simpler to write. What are atomic operations?Atomic operations are operations which are performed without interference from any other threads. Atomic operations are often used to prevent race conditions which are common problems in mulithreaded applications. For example, suppose you have two threads named A and B. Now suppose each thread wants to increase the value of memory location 0x1234 by one. Suppose the value at memory location 0x1234 is 5. If A and B both want to increase the value at location 0x1234 at the same time, each thread will first have to read the value. Depending on when the reads occur, it is possible that both A and B will read a value of 5. After adding a value of 1, both A and B will want to write 6 into the memory location, which is not correct! The value, 5, should have been increased twice (once by each thread), but instead, the value was only increased once! This is called a race condition, and can happen in any multi-threaded program if the programmer is not careful. How to avoid race conditionsFortunately, race conditions are easy to avoid in CUDA. An atomic operation is capable of reading, modifying, and writing a value back to memory without the interference of any other threads, which guarentees that a race condition won’t occur. Atomic operations in CUDA generally work for both shared memory and global memory. Atomic operations in shared memory are generally used to prevent race conditions between different threads within the same thread block. Atomic operations in global memory are used to prevent race conditions between two different threads regaurdless of which thread block they are in. Please note that shared memory is generally much faster than global memory. Example: 1int atomicAdd(int* address, int val); This atomicAdd function can be called within a kernel. When a thread executes this operation, a memory address is read, has the value of ‘val’ added to it, and the result is written back to memory. The original value of the memory at location ‘address’ is returned to the thread. Many algorithms which require atomic operations will not need to use the original value at the memory location. For a full list of available atomic functions, please read a CUDA programming guide version 1.1 or later. Performance notesThere are a couple things to beware of when using atomic operations. As mentioned before, shared memory is much faster than global memory, so atomic operations in shared memory tend to complete faster than atomic operations in global memory. While atomic operations are often necessary in some algorithms, it is important to minimize their usage when possible, especially with global memory accesses. Also beware of serialization. If two threads perform an atomic operation at the same memory address at the same time, those operations will be serialized. The order in which the operations complete is undefined, which is fine, but the serialization can be quite costly. Example of SLOW code: 1234567__shared__ totalSum;if (threadIdx.x == 0) totalSum = 0;__syncthreads();int localVal = pValues[blockIdx.x * blockDim.x + threadIdx.x];atomicAdd(&amp;totalSum, 1);__syncthreads(); The code you see above is very simple. Each thread reads a value from memory sequentially, which is quite fast. If the sum of all those numbers is needed, you might think it would be okay to simply use an atomicAdd operations. This would effectively calculate the final sum in one line of code, which might seem great. Unfortunately, all of those operations will be serialized, which is extremely slow. If you have 512 threads per thread block, each block would have to do 512 sequential additions. However, using a reduction method discussed in a previous tutorial would be able to accomplish the same task with just 511 additions. The key here is that these additions can be done in parallel. Generally, 16 or 32 additions can be done completely parallel, making it much faster than using an atomicAdd for a reduction problem such as this. So whenever you use atomic operations, be sure to program such that there won’t need to be too many sequential operations. Failure to do so will result in a dramatic loss of parallelism, and thus a dramatic loss in performance. However, when atomic operations are used correctly, they are extremely useful. Compatibility notesWhen nVidia released their first CUDA capable cards, the original 8800GTX with 768MB memory and the 8800GTS with 640 MB of memory, CUDA was a new technology. These original CUDA capable cards are the only ones which do not support atomic operations. Every nVidia GPU that is a core 84 or higher supports CUDA 1.1 or higher, and thus supports atomic operations. Even after the introduction of atomic operations with CUDA 1.1, there are still a couple atomic operations which were added later, such as 64-bit atomic operations, etc. Because there are a lot of CUDA 1.1 cards in consumer hands right now, I would recommend only using atomic operations with 32-bit integers and 32-bit unsigned integers. This will ensure that your application will work on the largest number of graphics cards available in the market today. For full compatibility with all CUDA devices including those with compute capability 1.0, you may wish to you ifdefs in your code. This way, when your program executes on a device which supports atomic operations, they will be used, but your program will still be able to execute alternate, less efficient code if the device only has compute capability 1.0. When you compile to support atomic operations, the constant, CUDA_NO_SM_11_ATOMIC_INTRINSICS will be defined. Compiler issuesWith Visual Studio, you shouldn’t have any trouble compiling programs with atomic intrinsics. You may want to use the histogram64 program as a template for starting your own. However, if you are working on Linux or Mac OS, you may need to add “-arch=sm_11” as a compiler flag for nvcc. You can tell if it’s working by placing the following code inside the main function of your program: 123#ifndef CUDA_NO_SM_11_ATOMIC_INTRINSICS printf(&quot;WARNING! Not using atomics!\\n&quot;);#endif Refer to this link: http://supercomputingblog.com/cuda/cuda-tutorial-4-atomic-operations/","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"atomic","slug":"atomic","permalink":"http://huangzhiyuan.github.io/tags/atomic/"}]},{"title":"PyTorch Hook用法解析","slug":"hook-in-pytorch","date":"2020-05-12T08:43:39.000Z","updated":"2020-05-12T09:12:52.000Z","comments":true,"path":"2020/05/12/hook-in-pytorch/","link":"","permalink":"http://huangzhiyuan.github.io/2020/05/12/hook-in-pytorch/","excerpt":"hook在维基百科中定义：钩子编程（hooking），也称作“挂钩”，是计算机程序设计术语，指通过拦截软件模块间的函数调用、消息传递、事件传递来修改或扩展操作系统、应用程序或其他软件组件的行为的各种技术。处理被拦截的函数调用、事件、消息的代码，被称为钩子（hook）。 Hook 是 PyTorch 中一个十分有用的特性。利用它，我们可以不必改变网络输入输出的结构，方便地获取、改变网络中间层变量的值和梯度。这个功能被广泛用于可视化神经网络中间层的 feature、gradient，从而诊断神经网络中可能出现的问题，分析网络有效性。本文将结合代码，由浅入深地介绍 pytorch 中 hook 的用法。本文分为三部分： Hook for Tensors ：针对 Tensor 的 hook Hook for Modules：针对例如 nn.Conv2dnn.Linear等网络模块的 hook Guided Backpropagation：利用 Hook 实现的一段神经网络可视化代码","text":"hook在维基百科中定义：钩子编程（hooking），也称作“挂钩”，是计算机程序设计术语，指通过拦截软件模块间的函数调用、消息传递、事件传递来修改或扩展操作系统、应用程序或其他软件组件的行为的各种技术。处理被拦截的函数调用、事件、消息的代码，被称为钩子（hook）。 Hook 是 PyTorch 中一个十分有用的特性。利用它，我们可以不必改变网络输入输出的结构，方便地获取、改变网络中间层变量的值和梯度。这个功能被广泛用于可视化神经网络中间层的 feature、gradient，从而诊断神经网络中可能出现的问题，分析网络有效性。本文将结合代码，由浅入深地介绍 pytorch 中 hook 的用法。本文分为三部分： Hook for Tensors ：针对 Tensor 的 hook Hook for Modules：针对例如 nn.Conv2dnn.Linear等网络模块的 hook Guided Backpropagation：利用 Hook 实现的一段神经网络可视化代码 Hook for Tensors在pytorch docs搜索hook，可以发现有四个hook相关的函数，分别为register_hook，register_backward_hook，register_forward_hook，register_forward_pre_hook。其中register_hook属于tensor类，而后面三个属于moudule类。 在 PyTorch 的计算图（computation graph）中，只有叶子结点（leaf nodes）的变量会保留梯度。而所有中间变量的梯度只被用于反向传播，一旦完成反向传播，中间变量的梯度就将自动释放，从而节约内存。如下面这段代码所示： 12345678910111213141516171819202122232425262728293031X---------- W ------ - - -----&gt; + ---&gt; Z ---&gt; * ---&gt; O -Y----------import torchx = torch.Tensor([0, 1, 2, 3]).requires_grad_()y = torch.Tensor([4, 5, 6, 7]).requires_grad_()w = torch.Tensor([1, 2, 3, 4]).requires_grad_()z = x+y# z.retain_grad()o = w.matmul(z)o.backward()# o.retain_grad()print(&#x27;x.requires_grad:&#x27;, x.requires_grad) # Trueprint(&#x27;y.requires_grad:&#x27;, y.requires_grad) # Trueprint(&#x27;z.requires_grad:&#x27;, z.requires_grad) # Trueprint(&#x27;w.requires_grad:&#x27;, w.requires_grad) # Trueprint(&#x27;o.requires_grad:&#x27;, o.requires_grad) # Trueprint(&#x27;x.grad:&#x27;, x.grad) # tensor([1., 2., 3., 4.])print(&#x27;y.grad:&#x27;, y.grad) # tensor([1., 2., 3., 4.])print(&#x27;w.grad:&#x27;, w.grad) # tensor([ 4., 6., 8., 10.])print(&#x27;z.grad:&#x27;, z.grad) # Noneprint(&#x27;o.grad:&#x27;, o.grad) # None 由于 z 和 o 为中间变量（并非直接指定数值的变量，而是由别的变量计算得到的变量），它们虽然 requires_grad 的参数都是 True，但是反向传播后，它们的梯度并没有保存下来，而是直接删除了，因此是 None。如果想在反向传播之后保留它们的梯度，则需要特殊指定：把上面代码中的z.retain_grad() 和 o.retain_grad的注释去掉，可以得到它们对应的梯度，运行结果如下所示： 12345678910x.requires_grad: Truey.requires_grad: Truez.requires_grad: Truew.requires_grad: Trueo.requires_grad: Truex.grad: tensor([1., 2., 3., 4.])y.grad: tensor([1., 2., 3., 4.])w.grad: tensor([ 4., 6., 8., 10.])z.grad: tensor([1., 2., 3., 4.])o.grad: tensor(1.) 但是，这种加 retain_grad() 的方案会增加内存占用，并不是个好办法，对此的一种替代方案，就是用 hook 保存中间变量的梯度。 对于中间变量z，hook 的使用方式为：z.register_hook(hook_fn)，其中 hook_fn 为一个用户自定义的函数，其签名为： 1hook_fn(grad) -&gt; Tensor or None 它的输入为变量 z 的梯度，输出为一个 Tensor 或者是 None （None 一般用于直接打印梯度）。反向传播时，梯度传播到变量 z，再继续向前传播之前，将会传入 hook_fn。如果 hook_fn的返回值是 None，那么梯度将不改变，继续向前传播，如果 hook_fn的返回值是 Tensor 类型，则该 Tensor 将取代 z 原有的梯度，向前传播。 下面的示例代码中 hook_fn 不改变梯度值，仅仅是打印梯度： 123456789101112131415161718192021222324import torchx = torch.Tensor([0, 1, 2, 3]).requires_grad_()y = torch.Tensor([4, 5, 6, 7]).requires_grad_()w = torch.Tensor([1, 2, 3, 4]).requires_grad_()z = x+y# ===================def hook_fn(grad): print(grad)z.register_hook(hook_fn)# ===================o = w.matmul(z)print(&#x27;=====Start backprop=====&#x27;)o.backward()print(&#x27;=====End backprop=====&#x27;)print(&#x27;x.grad:&#x27;, x.grad)print(&#x27;y.grad:&#x27;, y.grad)print(&#x27;w.grad:&#x27;, w.grad)print(&#x27;z.grad:&#x27;, z.grad) 运行结果如下： 1234567=====Start backprop=====tensor([1., 2., 3., 4.])=====End backprop=====x.grad: tensor([1., 2., 3., 4.])y.grad: tensor([1., 2., 3., 4.])w.grad: tensor([ 4., 6., 8., 10.])z.grad: None 我们发现，z 绑定了hook_fn后，梯度反向传播时将会打印出 o 对 z 的偏导，和上文中 z.retain_grad()方法得到的 z 的偏导一致。 接下来可以试一下，在 hook_fn 中改变梯度值，看看会有什么结果。 12345678910111213141516171819202122232425262728import torchx = torch.Tensor([0, 1, 2, 3]).requires_grad_()y = torch.Tensor([4, 5, 6, 7]).requires_grad_()w = torch.Tensor([1, 2, 3, 4]).requires_grad_()z = x + y# ===================def hook_fn(grad): g = 2 * grad print(g) return gz.register_hook(hook_fn)# ===================o = w.matmul(z)print(&#x27;=====Start backprop=====&#x27;)o.backward()print(&#x27;=====End backprop=====&#x27;)print(&#x27;x.grad:&#x27;, x.grad)print(&#x27;y.grad:&#x27;, y.grad)print(&#x27;w.grad:&#x27;, w.grad)print(&#x27;z.grad:&#x27;, z.grad) 运行结果如下： 1234567=====Start backprop=====tensor([2., 4., 6., 8.])=====End backprop=====x.grad: tensor([2., 4., 6., 8.])y.grad: tensor([2., 4., 6., 8.])w.grad: tensor([ 4., 6., 8., 10.])z.grad: None 发现 z 的梯度变为两倍后，受其影响，x和y的梯度也都变成了原来的两倍。在实际代码中，为了方便，也可以用 lambda 表达式来代替函数，简写为如下形式： 12345678910111213141516171819202122import torchx = torch.Tensor([0, 1, 2, 3]).requires_grad_()y = torch.Tensor([4, 5, 6, 7]).requires_grad_()w = torch.Tensor([1, 2, 3, 4]).requires_grad_()z = x + y# ===================z.register_hook(lambda x: 2*x)z.register_hook(lambda x: print(x))# ===================o = w.matmul(z)print(&#x27;=====Start backprop=====&#x27;)o.backward()print(&#x27;=====End backprop=====&#x27;)print(&#x27;x.grad:&#x27;, x.grad)print(&#x27;y.grad:&#x27;, y.grad)print(&#x27;w.grad:&#x27;, w.grad)print(&#x27;z.grad:&#x27;, z.grad) 运行结果和上面的代码相同，我们发现一个变量可以绑定多个 hook_fn，反向传播时，它们按绑定顺序依次执行。例如上面的代码中，第一个绑定的 hook_fn把 z的梯度乘以2，第二个绑定的 hook_fn打印z的梯度。因此反向传播时，也是按照这个顺序执行的，打印出来的 z的梯度值，是其原本梯度值的两倍。 特征图打印应用：直接利用pytorch已有的resnet18进行特征图打印，只打印卷积层的特征图， 1234567891011121314151617181920212223242526272829303132333435363738394041424344import torchfrom torchvision.models import resnet18import torch.nn as nnfrom torchvision import transformsimport matplotlib.pyplot as pltdef viz(module, input): x = input[0][0] #最多显示4张图 min_num = np.minimum(4, x.size()[0]) for i in range(min_num): plt.subplot(1, 4, i+1) plt.imshow(x[i].cpu()) plt.show()import cv2import numpy as npdef main(): t = transforms.Compose([transforms.ToPILImage(), transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ]) device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) model = resnet18(pretrained=True).to(device) for name, m in model.named_modules(): # if not isinstance(m, torch.nn.ModuleList) and \\ # not isinstance(m, torch.nn.Sequential) and \\ # type(m) in torch.nn.__dict__.values(): # 这里只对卷积层的feature map进行显示 if isinstance(m, torch.nn.Conv2d): m.register_forward_pre_hook(viz) img = cv2.imread(&#x27;./cat.jpeg&#x27;) img = t(img).unsqueeze(0).to(device) with torch.no_grad(): model(img)if __name__ == &#x27;__main__&#x27;: main() 第一层卷积层输入 第四层卷积层的输入 模型大小，算力计算, 同样的用法，可以直接参考pytorch-summary这个项目。 Hook for Modules网络模块 module 不像上一节中的 Tensor，拥有显式的变量名可以直接访问，而是被封装在神经网络中间。我们通常只能获得网络整体的输入和输出，对于夹在网络中间的模块，我们不但很难得知它输入/输出的梯度，甚至连它输入输出的数值都无法获得。除非设计网络时，在 forward 函数的返回值中包含中间 module 的输出，或者用很麻烦的办法，把网络按照 module 的名称拆分再组合，让中间层提取的 feature 暴露出来。 为了解决这个麻烦，PyTorch 设计了两种 hook：register_forward_hook 和 register_backward_hook，分别用来获取正/反向传播时，中间层模块输入和输出的 feature/gradient，大大降低了获取模型内部信息流的难度。具体用法参考半小时学会 PyTorch Hook。 参考：https://zhuanlan.zhihu.com/p/73868323https://github.com/sksq96/pytorch-summaryhttps://oldpan.me/archives/pytorch-autograd-hookhttps://pytorch.org/docs/stable/search.html?q=hook&amp;check_keywords=yes&amp;area=default","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"hook","slug":"hook","permalink":"http://huangzhiyuan.github.io/tags/hook/"}]},{"title":"CPU threads affinity hyperthreading","slug":"cpu-threads-affinity-hyperthreadign","date":"2020-05-07T06:37:52.000Z","updated":"2020-05-07T07:23:28.000Z","comments":true,"path":"2020/05/07/cpu-threads-affinity-hyperthreadign/","link":"","permalink":"http://huangzhiyuan.github.io/2020/05/07/cpu-threads-affinity-hyperthreadign/","excerpt":"This post is not a tutorial on C++11 threads, but it uses them as the main threading mechanism to demonstrate its points. It starts with a basic example but then quickly veers off into the specialized area of thread affinities, hardware topologies and performance implications of hyperthreading. It does as much as feasible in portable C++, clearly marking the deviations into platform-specific calls for the really specialized stuff.","text":"This post is not a tutorial on C++11 threads, but it uses them as the main threading mechanism to demonstrate its points. It starts with a basic example but then quickly veers off into the specialized area of thread affinities, hardware topologies and performance implications of hyperthreading. It does as much as feasible in portable C++, clearly marking the deviations into platform-specific calls for the really specialized stuff. CPU Socket: refers to a physical connector on a motherboard that accepts a single physical chip. It is commonplace for modern CPUs to provide multiple physical cores which are exposed to the operating system as logical CPUs that can perform parallel execution streams. This document refers to a socket and physical CPU synonymously. See Also: CPU SocketNUMA: Non-niform Memory Access, refers to the commonplace architecture in which machines with multiple CPU sockets divide the memory banks of RAM into nodes on a per-socket basis. Access to memory on a socket’s “local” memory node is faster than accessing memory on a remote node tied to a different socket. See Also: NumaCPU Core: Contemporary CPUs are likely to run multiple cores which are exposed to the underlying OS as a CPU. See Also: Multi-core processingHyper-threading: Intel technology to make a single core appear logically as multiple cores on the same chip to improve the performanceLogical CPU: What the operating system sees as a CPU. The number of CPUs available to the OS is num sockets * cores per socket * hyper threads per core.Processor Affinity: Refers to the act of restricting the set of logical CPUs on which a particular program thread can execute. Benefits of thread affinitizationPinning a thread to a particular CPU ensures that the OS won’t reschedule the thread to another core and incur a context switch that would force the thread to reload its working state from main memory which results in jitter. When all critical threads in the processing pipeline are pinned to their own CPU and busy spinning, the OS scheduler is less likely to schedule another thread onto that core, keeping the threads’ processor caches hot. Logical CPUs, cores and threadsMost modern machines are multi-CPU. Whether these CPUs are divided into sockets and hardware cores depends on the machine, of course, but the OS sees a number of “logical” CPUs that can execute tasks concurrently. The easiest way to get this information on Linux is to cat /proc/cpuinfo, which lists the system’s CPUs in order, providing some infromation about each (such as current frequency, cache size, etc). On my (8-CPU) machine: 123456789101112131415161718192021222324252627$ cat /proc/cpuinfoprocessor : 0vendor_id : GenuineIntelcpu family : 6model : 60model name : Intel(R) Core(TM) i7-4771 CPU @ 3.50GHz[...]stepping : 3microcode : 0x7cpu MHz : 3501.000cache size : 8192 KBphysical id : 0siblings : 8core id : 0cpu cores : 4apicid : 0[...]processor : 1vendor_id : GenuineIntelcpu family : 6[...][...]processor : 7vendor_id : GenuineIntelcpu family : 6 A summary output can be obtained from lscpu: 12345678910111213141516171819202122$ lscpuArchitecture: x86_64CPU op-mode(s): 32-bit, 64-bitByte Order: Little EndianCPU(s): 8On-line CPU(s) list: 0-7Thread(s) per core: 2Core(s) per socket: 4Socket(s): 1NUMA node(s): 1Vendor ID: GenuineIntelCPU family: 6Model: 60Stepping: 3CPU MHz: 3501.000BogoMIPS: 6984.09Virtualization: VT-xL1d cache: 32KL1i cache: 32KL2 cache: 256KL3 cache: 8192KNUMA node0 CPU(s): 0-7 Here it’s also very easy to see that the machine has 4 cores, each having two HW threads (see hyperthreading). And yet the OS sees them as 8 “CPUs” numbered 0-7. Launching a thread per CPUThe C++11 threading library gracefully made available a utility function that we can use to find out how many CPUs the machine has, so that we could plan our parallelism strategy. The function is called hardware_concurrency, and here is a complete example that uses it to launch an appropriate number of threads. The following is just a code snippet; full code samples for this post, along with a Makefile for Linux can be found in this repository. 123456789101112131415161718192021222324252627int main(int argc, const char** argv) &#123; unsigned num_cpus = std::thread::hardware_concurrency(); std::cout &lt;&lt; &quot;Launching &quot; &lt;&lt; num_cpus &lt;&lt; &quot; threads\\n&quot;; // A mutex ensures orderly access to std::cout from multiple threads. std::mutex iomutex; std::vector&lt;std::thread&gt; threads(num_cpus); for (unsigned i = 0; i &lt; num_cpus; ++i) &#123; threads[i] = std::thread([&amp;iomutex, i] &#123; &#123; // Use a lexical scope and lock_guard to safely lock the mutex only for // the duration of std::cout usage. std::lock_guard&lt;std::mutex&gt; iolock(iomutex); std::cout &lt;&lt; &quot;Thread #&quot; &lt;&lt; i &lt;&lt; &quot; is running\\n&quot;; &#125; // Simulate important work done by the tread by sleeping for a bit... std::this_thread::sleep_for(std::chrono::milliseconds(200)); &#125;); &#125; for (auto&amp; t : threads) &#123; t.join(); &#125; return 0;&#125; A std::thread is a thin wrapper around a platform-specific thread object; this is something we’ll use to our advantage shortly. So when we launch a std::thread, and actual OS thread is launched. This is fairly low-level thread control, but in this article I won’t detour into higher-level constructs like task-based parallelism, leaving this to some future post. Thread affinitySo we know how to query the system for the number of CPUs it has, and how to launch any number of threads. Now let’s do something a bit more advanced. All modern OSes support setting CPU affinity per thread. Affinity means that instead of being free to run the thread on any CPU it feels like, the OS scheduler is asked to only schedule a given thread to a single CPU or a pre-defined set of CPUs. By default, the affinity covers all logical CPUs in the system, so the OS can pick any of them for any thread, based on its scheduling considerations. In addition, the OS will sometimes migrate threads between CPUs if it makes sense to the scheduler (though it should try to miminize migrations because of the loss of warm caches on the core from which the thread was migrated). Let’s observe this in action with another code sample: 1234567891011121314151617181920212223242526int main(int argc, const char** argv) &#123; constexpr unsigned num_threads = 4; // A mutex ensures orderly access to std::cout from multiple threads. std::mutex iomutex; std::vector&lt;std::thread&gt; threads(num_threads); for (unsigned i = 0; i &lt; num_threads; ++i) &#123; threads[i] = std::thread([&amp;iomutex, i] &#123; while (1) &#123; &#123; // Use a lexical scope and lock_guard to safely lock the mutex only // for the duration of std::cout usage. std::lock_guard&lt;std::mutex&gt; iolock(iomutex); std::cout &lt;&lt; &quot;Thread #&quot; &lt;&lt; i &lt;&lt; &quot;: on CPU &quot; &lt;&lt; sched_getcpu() &lt;&lt; &quot;\\n&quot;; &#125; // Simulate important work done by the tread by sleeping for a bit... std::this_thread::sleep_for(std::chrono::milliseconds(900)); &#125; &#125;); &#125; for (auto&amp; t : threads) &#123; t.join(); &#125; return 0;&#125; This sample launches four threads that loop infinitely, sleeping and reporting which CPU they run on. The reporting is done via the sched_getcpu function (glibc specific - other platforms will have other APIs with similar functionality). Here’s a sample run: 12345678910111213141516171819202122$ ./launch-threads-report-cpuThread #0: on CPU 5Thread #1: on CPU 5Thread #2: on CPU 2Thread #3: on CPU 5Thread #0: on CPU 2Thread #1: on CPU 5Thread #2: on CPU 3Thread #3: on CPU 5Thread #0: on CPU 3Thread #2: on CPU 7Thread #1: on CPU 5Thread #3: on CPU 0Thread #0: on CPU 3Thread #2: on CPU 7Thread #1: on CPU 5Thread #3: on CPU 0Thread #0: on CPU 3Thread #2: on CPU 7Thread #1: on CPU 5Thread #3: on CPU 0^C Some observations: the threads are sometimes scheduled onto the same CPU, and sometimes onto different CPUs. Also, there’s quite a bit of migration going on. Eventually, the scheduler managed to place each thread onto a different CPU, and keep it there. Different constraints (such as system load) could result in a different scheduling, of course. Now let’s rerun the same sample, but this time using taskset to restrict the affinity of the process to only two CPUs - 5 and 6: 123456789101112131415161718$ taskset -c 5,6 ./launch-threads-report-cpuThread #0: on CPU 5Thread #2: on CPU 6Thread #1: on CPU 5Thread #3: on CPU 6Thread #0: on CPU 5Thread #2: on CPU 6Thread #1: on CPU 5Thread #3: on CPU 6Thread #0: on CPU 5Thread #1: on CPU 5Thread #2: on CPU 6Thread #3: on CPU 6Thread #0: on CPU 5Thread #1: on CPU 6Thread #2: on CPU 6Thread #3: on CPU 6^C As expected, though there’s some migration happening here, all threads remain faithfully locked to CPUs 5 and 6, as instructed. Setting CPU affinity programaticallyAs we’ve seen earlier, command-line tools like taskset let us control the CPU affinity of a whole process. Sometimes, however, we’d like to do something more fine-grained and set the affinities of specific threads from within the program. How do we do that? On Linux, we can use the pthread-specific pthread_setaffinity_np function. Here’s an example that reproduces what we did before, but this time from inside the program. In fact, let’s go a bit more fancy and pin each thread to a single known CPU by setting its affinity: 1234567891011121314151617181920212223242526272829303132333435363738int main(int argc, const char** argv) &#123; constexpr unsigned num_threads = 4; // A mutex ensures orderly access to std::cout from multiple threads. std::mutex iomutex; std::vector&lt;std::thread&gt; threads(num_threads); for (unsigned i = 0; i &lt; num_threads; ++i) &#123; threads[i] = std::thread([&amp;iomutex, i] &#123; std::this_thread::sleep_for(std::chrono::milliseconds(20)); while (1) &#123; &#123; // Use a lexical scope and lock_guard to safely lock the mutex only // for the duration of std::cout usage. std::lock_guard&lt;std::mutex&gt; iolock(iomutex); std::cout &lt;&lt; &quot;Thread #&quot; &lt;&lt; i &lt;&lt; &quot;: on CPU &quot; &lt;&lt; sched_getcpu() &lt;&lt; &quot;\\n&quot;; &#125; // Simulate important work done by the tread by sleeping for a bit... std::this_thread::sleep_for(std::chrono::milliseconds(900)); &#125; &#125;); // Create a cpu_set_t object representing a set of CPUs. Clear it and mark // only CPU i as set. cpu_set_t cpuset; CPU_ZERO(&amp;cpuset); CPU_SET(i, &amp;cpuset); int rc = pthread_setaffinity_np(threads[i].native_handle(), sizeof(cpu_set_t), &amp;cpuset); if (rc != 0) &#123; std::cerr &lt;&lt; &quot;Error calling pthread_setaffinity_np: &quot; &lt;&lt; rc &lt;&lt; &quot;\\n&quot;; &#125; &#125; for (auto&amp; t : threads) &#123; t.join(); &#125; return 0;&#125; Note how we use the native_handle method discussed earlier in order to pass the underlying native handle to the pthread call (it takes a pthread_t ID as its first argument). The output of this program on my machine is: 1234567891011121314$ ./set-affinityThread #0: on CPU 0Thread #1: on CPU 1Thread #2: on CPU 2Thread #3: on CPU 3Thread #0: on CPU 0Thread #1: on CPU 1Thread #2: on CPU 2Thread #3: on CPU 3Thread #0: on CPU 0Thread #1: on CPU 1Thread #2: on CPU 2Thread #3: on CPU 3^C The threads get pinned to single CPUs exactly as requested. refers:https://pages.tacc.utexas.edu/~eijkhout/pcse/html/omp-affinity.html#Threadbindinghttps://docs.neeveresearch.com/display/TALONDOC/Tuning+Thread+Affinitization+and+NUMAhttps://eli.thegreenplace.net/2016/c11-threads-affinity-and-hyperthreading/","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"affinity","slug":"affinity","permalink":"http://huangzhiyuan.github.io/tags/affinity/"},{"name":"hyperthreading","slug":"hyperthreading","permalink":"http://huangzhiyuan.github.io/tags/hyperthreading/"},{"name":"NUMA","slug":"NUMA","permalink":"http://huangzhiyuan.github.io/tags/NUMA/"}]},{"title":"deep learning compiler调研报告","slug":"deep-learning-compiler-survery","date":"2020-04-29T12:43:36.000Z","updated":"2020-08-17T08:05:36.000Z","comments":true,"path":"2020/04/29/deep-learning-compiler-survery/","link":"","permalink":"http://huangzhiyuan.github.io/2020/04/29/deep-learning-compiler-survery/","excerpt":"在不同DL硬件上部署各种深度学习模型，促进了社区深度学习编译器的研发。从行业和学术界已经提出了几个DL编译器，如Tensorflow XLA和TVM。同样，DL编译器将不同DL框架中描述的DL模型作为输入，然后为不同的DL硬件生成优化的代码作为输出。然而，现有的调查没有全面分析DL-编译器的独特设计。在本文中，我们对现有DL编译器进行了全面调查，对常见设计进行了详细剖析，重点介绍了面向DL的多级IR和前端后端优化。从不同方面，我们对现有DL和编译器进行了全面的比较。此外，我们还详细介绍了多级IR设计和编译器优化技术。最后几个见解被突出显示为DL编译器的潜在研究方向。这是一份重点介绍DL编译器独特设计的最值得关注的调查论文，我们希望，这能为未来DL编译器的研究铺平道路。","text":"在不同DL硬件上部署各种深度学习模型，促进了社区深度学习编译器的研发。从行业和学术界已经提出了几个DL编译器，如Tensorflow XLA和TVM。同样，DL编译器将不同DL框架中描述的DL模型作为输入，然后为不同的DL硬件生成优化的代码作为输出。然而，现有的调查没有全面分析DL-编译器的独特设计。在本文中，我们对现有DL编译器进行了全面调查，对常见设计进行了详细剖析，重点介绍了面向DL的多级IR和前端后端优化。从不同方面，我们对现有DL和编译器进行了全面的比较。此外，我们还详细介绍了多级IR设计和编译器优化技术。最后几个见解被突出显示为DL编译器的潜在研究方向。这是一份重点介绍DL编译器独特设计的最值得关注的调查论文，我们希望，这能为未来DL编译器的研究铺平道路。 Introduction深度学习的发展对各种科学型产品产生了深远的影响。 它不仅在自然语言处理（NLP）和计算机视觉（CV）等艺术智能（NLP）和计算机视觉（CV）方面表现出非凡的价值，而且在电子商务、智能城市和药物发现等更广泛的应用方面也取得了巨大成功。随着卷积神经网络（CNN）、循环神经网络（RNN）、长期短期内存（LSTM）和生成对抗性网络等多功能深度学习模型的出现，简化各种DL模型的编程以实现其广泛采用至关重要。 在业界和学术界的不断努力下，提出了几种流行的DL编程框架，如TensorFlow、PyTorch、MXNet和CNTK，以简化各种DL模型的应用。尽管上述DL编程框架中存在优势和弱点，具体取决于其设计中的权衡，但当互操作性支持现有DL模型中新出现的DL模型时，模型在不同的FWK直接转化对于减少冗余工程工作变得非常重要。为了改进转换的简便性，ONNX已提出定义用于表示DL模型的开源格式。（Open Neural Network Exchange） 同时，矩阵乘法GEMM等独特的计算特性激发了芯片架构师设计定制DL芯片更高效率的热情。互联网巨头（例如谷歌TPU，Hisilicon NPU， Apple Bonic），处理器供应商（例如，NVIDIA 图灵，英特尔 NNP），服务提供商（例如，亚马逊，阿里巴巴寒光），甚至初创公司（例如，Cambricon，Graphcore）正在投入庞大的劳人力物力研发。一般来说，DL芯片的类别包括： 通用芯片与软件硬件共同设计; 专为DL机型定制的专用芯片; 受生物脑和科学启发的神经形态芯片。 通用芯片（例如 CPU、GPU）添加了特殊的硬件和组件，如AVX512矢量单元和Tensor core以加速DL模型。而对于专用芯片（如 Google Tensor处理单元（TPU），应用规格集成电路（例如矩阵乘法引擎和高带宽内存）的设计旨在将性能和能量耗竭提升到极致。在可预见的将来，DL芯片的设计将更加多样化。 为了加速不同DL芯片上的DL模型，将计算映射到DL芯片非常重要。在通用芯片上，高度优化的线性代数库（如基本线性代数子程序 （BLAS） 库（例如 MKL 和 cuBLAS）是 DL 模型的”精效计算”的基础。以卷积convolution为例，DL 框架将卷积转换为矩阵乘法，然后在 BLAS+ 库中调用 GEMM 函数。此外，芯片供应商还发布了专为DL计算（例如 MKL-DNN 和 cuDNN）量身定制的优化加速库，包括forward和backward convolution、pooling、normlization和activation。还开发了更先进的工具，以进一步加快DL操作。以TensorRT为例，它支持图优化（例如层融合）和低位量化(int8/half)，并大量收集高度优化的GPU内核。在专用DL芯片上，供应商还提供类似的库以及工具链，以便有效地执行DL+模型。但是依靠上述库和工具在不同DL芯片上映射DL模型的缺点是，它们通常落后于DL模型的快速发展，因此未能有效地利用DL芯片。 为了解决DL库和工具的缺点，以及减轻手动优化每个DL芯片上的DL模型的负担，DL社区采用了域规范 + 编译器技术进行弥补徐。很快已经提出了几个流行的DL编译器，如[TVM，Tensor Comprehensive，Glow，nGraph和XLA，都是来自工业和学术界。DL编译器将DL框架中描述的模型定义的描述视为输入，并在各种DL芯片上生成最成熟的代码实现作为输出。模型定义和规范代码实现之间的转换是高度优化的目标 - 模型规范和硬件体系结构。例如，DL编译器集成了面向DL的优化，如层和运算符融合，可实现高度code gen。此外现有的DL编译器还利用了通用编译器（例如LLVM）的成熟工具链，它在不同的硬件体系结构中提供了更好的可移植性。但是DL编译器的独特性在于多层IR和DL特定优化的设计。 本文我们提供一个对现有的DL编译器的综合调查，具体包括前端、多层IR和后端，并特别强调IR的设计和优化方法。据我们所知，这是一篇对DL编译器设计进行全面调查的论文。本文的贡献如下： 我们从硬件支持、DL框架支持、代码生成和优化等各个方面对现有DL编译器进行了全面比较，这些代码生成和优化可用作为最终用户选择合适DL编译器的指南。 我们剖析了现有DL编译器的一般设计，并详细分析了多级IR设计和编译器优化技术，如数据流优化、硬件指令映射、内存延迟隐藏和数据并行化技术。 我们为DL编译器的未来发展提供了一些见解，包括auto-tuning、多面体编译器、量化、差异化编程和隐私保护，我们希望推动DL编译器社区的研究。 本文的剩余部分按如下方式组织。第 2 节介绍了 DL 编译器的背景，包括 DL 框架、DL 芯片以及特定于硬件（FPGA）的DL编译器。第 3 节提供了现有 DL 编译器之间的详细比较。第 4 节介绍了 DL 编译器的一般设计，重点介绍IR和前后端优化。第 5 节总结了本文，并重点介绍了未来的方向。 BackgroundDeep Learning Frameworks在这部分，我们将概述流行的深度学习框架。讨论可能并非详尽无遗，但旨在为DL从业人员提供一个准则。下图介绍了DL框架的总况，包括当前流行的框架、历史框架和 ONNX 支持的框架。 TensorFlowTensorFlow在2015年被Google首次发布，在所有的深度学习框架里面，TF有最为广泛支持的语言接口，包括C++, Python, Java, GO, R和Haskell。TensorFlow™是一个基于数据流编程（dataflow programming）的符号数学系统，被广泛应用于各类机器学习（machine learning）算法的编程实现，其前身是谷歌的神经网络算法库DistBelief。TensorFlow Lite是为移动端和嵌入式设备设计的神经网络API而设计。为了降低使用的复杂程度使用户更加容易上手，Google把Keras作为TF的前端内核。除此之外和PyTorch支持动态计算题相比，TF eager-model提供了类似的功能。 KerasKeras于2015年3月首次发布，由Google支持。Keras 是一个高级网络库，用于快速构建 DL 模型，该模型是纯 Python 编写的。虽然Keras本身不是DL框架，但它提供了一个与TensorFlow、MXNet、Theano 和 CNTK 集成的高级别 API。借助 Keras，DL 开发人员只需几行代码就构建神经网络。此外，Keras 可以集成其他常见的 DL 包，例如Python的scikit学习。但是由于过度封装，Keras 不够灵活，因此很难添加operator算子或获取Low-level数据信息。 PyTorchPyTorch于2017年初由Facebook推出。Facebook 在Python中重写了基于Lua DL框架Torch，并在Tensor级别上重构了所有模块。此外最流行的动态框架，PyTorch嵌入了用于在Python中构造动态数据流图的基元，其中控制流在Python解释器中执行。PyTorch1.0集成了PyTorch 0.4 和 Caffe2的代码库，以创建一个统一的框架。这使得 PyTorch 能够吸收 Caffe2 的好处，以支持高效的图形执行和移动部署。FastAI是基于 PyTorch 的上层封装的高级 API 层。它完全借用 Keras 来提高 Pytorch 的易用性。 Caffe/Caffe2Caffe专为2014年加州大学伯克利分校的深度学习和分类而设计。Caffe 支持命令行、Python和MATLAB API。Caffe的一个重要功能是无需编写代码即可训练和部署模型。Caffe 的简单框架使其代码易于扩展，适合开发人员进行深入分析。因此，Caffe主要定位在研究上，从起源于至今，它流行起来。Caffe2基于原始的 Caffe 项目，支持移动（例如iOS和Android）和服务器（例如 Linux、Windows 和 Mac）构建平台。Caffe2 在结构上与 TensorFlow 类似，尽管具有较轻的 API，并且使访问计算图中的中间结果更加容易。 MXNetApache MXNet支持多种语言 AP，包括 Python、C++、R、Scala、Julia、Matlab 和 JavaScript。该项目于 2015 年 9 月开始，版本 1.0.0 于 2017 年 12 月发布。MXNet 旨在可扩展，从系统角度设计，以减少数据加载和 I/O 复杂性。MXNet提供了不同的范例：像 Caffe 和 Tensorflow 这样的命令性编程，以及像 PyTorch 这样的命令性编程。2017 年 12 月，亚马逊和微软联合发布了Gluon，这是一个类似于基于 MXNet 的 Keras 和 FastAI 的高级界面。Gluon 的最大特点是它既支持灵活的动态图形，又支持高效的静态图形。Gluon现在可用在Apache MXNet 和微软认知工具包CNTK。 CNTK微软认知工具包（也称为 CNTK）于 2015 年 10 月开发。CNTK 可以通过 Python、C++ 和 C# API 或它自己的脚本语言（即 BrainScript）使用。CNTK 设计为易于使用和生产就绪，可用于大型生产规模数据，在 Linux 和 Windows 上受支持。但是，CNTK 尚不支持 ARM 体系结构，它限制了其在移动设备上的使用。CNTK 使用类似于 TensorFlow 和 Caffe 的静态计算图，其中神经网络被视为通过定向图形的一系列计算步骤。 PaddlePaddle2016 年 8 月，百度开源的 PadlePadle，一个 DL 在内部使用多年。PadlePadle可应用于自然语言处理、图像识别、推荐引擎等。PadlePadle 的原始设计类似于 Caffe，其中每个模型都可以表示为一组图层。2017年4月，百度推出了Padpadle v2，它增加了运营商的概念，参照TensorFlow，将层分解为粒度更细的运营商，从而支持更复杂的网络结构。此外，PadlePadle 于 2017 年后期推出。流体与PyTorch类似，因为它提供自己的解释器，以便不受Python性能限制。 ONNX开放神经网络交换（ONNX）由微软和2017 年 9 月发布。ONNX 定义了一个可扩展的计算图形模型，由不同的 DL 框架构建的计算图可以转换为该模型。ONNX 使在 DL 框架之间转换模型更加容易。例如，它允许开发人员构建 MXNet 模型，然后使用 PyTorch 运行模型进行推理。如图所示，ONNX 已集成到 PyTorch、MXNet、PadlePadle 等中。对于尚未直接支持的几个 DL 框架（例如，TensorFlow 和 Keras），ONNX 会向它们添加转换器。 Historical Frameworks由于DL社区的迅速发展，许多历史DL框架不再存在或活跃。例如，Facebook提议的PyTorch已经取代了Torch。作为最古老的 DL 框架之一，Theano不再受维护。Deeplearning4J基于Java和Scala的分布式DL框架，但是由于缺乏大型开发人员社区（如PyTorch），它变得不活跃。Chainer曾经是动态计算图形的首选框架，但被具有类似功能的 MXNet、PyTorch 和 TensorFlow 所取代。 先前的工作比较了 DL 框架在不同应用程序（例如计算机视觉和图像分类）和不同硬件目标（例如 CPU、GPU 和 TPU）上的性能。有关每个 DL 框架的详细调查。本调查侧重于 DL 编译器的研究工作，这些编译器提供了更通用的方法，可在各种硬件上高效执行各种 DL 模型。 Deep Learning HardwaresDL硬件可以根据通用性分为三类：（注，原文里面是four categories，应该是笔误 ） 通用硬件（Neuromorphic Hardware），可以通过硬件和软件优化支持DL工作负载; 专用硬件（Dedicated Hardware），专注于通过完全定制的电路设计加速 DL 工作负载; 通过模仿人脑功能的神经形态硬件（Neuromorphic Hardware） Neuromorphic HardwareDL模型最具代表性的通用硬件是图形处理单元 （GPU），它实现了与多核架构的高并行性。Tensor core可以同时加速混合精度矩阵乘积计算，在训练和推理过程中，DL 模型中广泛使用。NVIDIA 还与硬件一起优化，还推出了高度优化的 DL 库和工具，如 cuDNN和 TensorRT，以进一步加快 DL 模型的计算。 Dedicated Hardware专用硬件完全定制用于 DL 计算，可将性能和能效提高至极致。DL 应用程序和算法的快速扩展促使许多初创公司开发专用 DL 硬件（例如，Graphcore GC2、Cambricon MLU270）。此外，传统硬件公司（如英特尔NNP、高通云AI 100）和云服务提供商（如谷歌TPU、亚马逊和阿里巴巴汉光）也在此领域投资。最有名的专用DL硬件是谷歌的TPU系列。TPU 包括矩阵乘法单元 （MXU）、统一缓冲区 （UB） 和激活单元 （AU），由主机处理器使用 CISC 指令驱动。MXU 主要由收缩数组组成，该阵列针对执行矩阵乘法时功率和面积效率进行了优化。与 CPU 和 GPU 相比，TPU 仍然是可编程的，但使用矩阵作为基元而不是矢量或标量。亚马逊推断最近也引起了关注。该芯片有四个神经核心，专为张量级操作而设计，并且具有较大的片上缓存，以避免频繁的主内存访问。 Neuromorphic Hardware神经形态芯片使用电子技术来模拟生物大脑。这种具有代表性的产品是IBM的ThiNorth和英特尔的Loihi。神经形态芯片（例如，TrueNorth）在人工神经元之间具有非常高的连接性。神经形态芯片还可以复制类似于脑组织的结构：神经元可以同时存储和处理数据。传统芯片在不同的位置分配处理器和内存，但神经形态芯片通常有许多微处理器，每个微处理器都有少量的本地内存。与 TrueNorth 相比，Loihi 的学习能力与大脑更相似。Loihi 引入了脉冲时间依赖突触可塑性模型 （STDP），这是一种机制，通过突触前脉冲和突触后脉冲的相对时间来调节突触强度。然而，神经形态芯片离大规模商业生产还很远。尽管如此，在计算机科学领域，神经形态芯片可以帮助捕捉快速，终身学习的过程，这是忽略定期DL模型，并在神经学领域，他们有助于找出大脑的各个部分如何协同工作，创造思想，感觉，甚至意识。 硬件特定的 DL 编译器FPGA是可重新编程的集成电路，包含一系列可编程逻辑块。开发人员可以在制造后配置它们。除了可重新编程的性质外，FPGA的低功率和高性能性质使其广泛应用于通信、医疗、图像处理和 ASIC 原型设计等众多领域。至于深度学习领域，高性能 CPU 和 GPU 可高度可重新编程，但功耗高，而高能效的 ASIC 专用于固定应用。然而，FPGA可以弥补CPU/GPU和SIC之间的差距，这使得FPGA成为一个有吸引力的深度学习平台。 High-Level Synthesis（HLS） 编程模型使 FPGA 程序员能够使用高级语言（如 C 和高级语言）方便地生成有效的硬件C++。它避免编写大量 Verilog 或 VHDL 描述，从而降低了编程阈值并减少了长设计圈。Xilinx Vivado HLS 和英特尔 OpenCL 的 FPGA SDK 是两种针对他们自己的 FPGA 的流行的 HLS 工具。但是，即使使用 HLS，将 DL 模型映射到 FPGA 仍是一项复杂的工作，因为 1） DL 模型通常由 DL 框架的语言而不是裸露的心理 C/C++ 代码描述，并且 2） DL 特定信息和优化很难被利用。 针对 FPGA 的特定于硬件的 DL 编译器以 DL 模型或其特定于域的语言 （DSL） 作为输入，进行特定于域（关于 FPGA 和 DL）的优化和映射，然后生成 HLS 或 Verilog/DLVH，最后生成比特流。根据基于 FPGA 的加速器生成的体系结构，它们可以分为两类：处理器体系结构和流式处理体系结构。 处理器体系结构（The processor architecture）与通用处理器具有相似性。此体系结构的 FPGA 加速器通常由多个处理单元 （PUs） 组成，这些单元由片上缓冲器和多个较小的处理引擎 （PEs） 组成。它通常具有虚拟指令集 （ISA），硬件的控制和执行的调度应该由软件确定。更重要的是，静态调度方法避免了冯·诺伊曼执行的开销（包括指令提取和解码）。硬件模板是具有可的通用和通用实现。针对此体系结构的 DL 编译器采用硬件模板自动生成加速器设计。借助模板的可配置参数，编译器实现了可扩展性和灵活性。可伸缩性意味着编译器可以生成从高性能到高能效的 FPGA 设计，灵活性意味着编译器可以生成具有不同层类型和参数的各种 DL 模型的设计。每个 PU 的 PU 数和 PEs 的数量是重要的模板参数。此外，tile size和batch size也是将DL模型映射到P、PEs的基本调度参数。所有这些参数通常由设计空间探索使用各种策略确定，例如组合性能模型和自动调整。DNN Weaver， Angel-Eye，阿拉莫， FP-DNN， SysArrayAccel是典型的 FPGA DL 编译器，针对处理器体系结构。此外，PUs 和 PEs 通常负责粗粒度基本操作，如矩阵矢量乘法、矩阵矩阵乘法、池化和某些元素操作。这些基本操作的优化主要以并行和数据重用之间的权衡为指导，这与一般优化类似。 流式处理体系结构（The streaming architecture）与管道（pipelines）有相似之处。此体系结构的 FPGA 加速器由多个不同的硬件块组成，并且它几乎为输入 DL 模型的每一层有一个硬件块。使用 DL 模型的输入数据，此类加速器会以与图层相同的顺序通过不同的硬件块处理数据。此外，使用流式处理输入数据，所有硬件块都可以以管道方式充分利用。但是，流式处理体系结构通常遵循一个初始假设，即目标 FPGA 上的芯片内存计算资源足以适应 DL 模型，这带来了部署具有复杂层的深度模型的障碍。针对此体系结构的 DL 编译器可以通过利用 FPGA 的可重构性或采用动态控制流来解决此问题。单个块的进一步优化类似于处理器体系结构的基本操作。fpgaConvNet， DeepBurning，Haddoc2 和AutoCodeGen是典型的对应的 DL 编译器。 COMPARISON OF DL COMPILERS 在本节中，我们比较了几个流行的DL编译器，包括TVM、TC、Glow、nGraph、PlaidML和XLA。上表显示了不同 DL 编译器从各个方面的详细比较，其中”+”表示受支持，”+”表示不支持，”+”表示正在开发中。请注意，我们使用 TVM 来表示VTA、中继和autoTVM的工作。此外，PlaidML 与 nGraph 紧密耦合，因此我们在比较过程中将它们一起考虑。此外，对于DL编译器的性能比较。 核心/编程语言 所有 DL 编译器的核心语言C++，C++在设计中强调性能、效率和使用灵活性。然而，Python 由于其简单性和可用性，越来越受程序员的欢迎。对于大多数成熟的DL编译器（例如，TVM、TC、nGraph、PlaidML 和 XLA），其 Python 接口几乎涵盖了所有核心功能。 支持的硬件 所有 DL 编译器都支持英特尔和 AMD CPU 以及 NVIDIA GPU。TC 的官方版本目前不为 AMD GPU 提供支持。请注意，nGraph 与 PlaidML 集成，以加速更多硬件目标。nGraph 可以通过调用现有的内核库（例如 cuDNN 和 MKL-DNN）来支持目标硬件。此外，PlaidML 还提供广泛的硬件目标支持，因为它能够生成代码。TVM 可以使用 VTA 体系结构和运行时将工作负载映射到FPGA。DL 编译器的本机支持的专用 DL 芯片通常与开发人员相关。例如，nGragh 可以通过调用 NNP 库来支持英特尔 Nervana 神经网络处理器 （NNP）。XLA 可以通过直接生成二进制文件来支持 Google TPUs。MLIR 可以利用 XLA ÂĂŹS的编译能力。除了 TC 和 nGraph 之外，所有 DL 编译器都可以通过基于 LLVM 开发接口来支持自定义硬件目标。例如，Glow 使用自动代码生成技术（即基于 LLVM 的 ClassGen）来定义指令和节点，编译器研究人员可以调用接口来支持新的硬件目标。 支持的 DL 框架 目前 TensorFlow 和 PyTorch 是两个最流行的 DL 框架。支持DL框架的方法有三种：1） DL编译器集成到DL框架中;2） DL 框架已启动官方包以支持 DL 编译器;3） DL 编译器使用转换器部署 DL 模型。下面是说明这三种方法的示例。对于 1），XLA 与 TensorFlow 集成，而 TC 和 Glow 则与 PyTorch 和 Caffe2 提供轻量级集成。对于 2），PyTorch 将受益于直接利用编译器堆栈。为此，PyTorch 现在拥有基于 TVM 和 XLA 的官方软件包（即torch_tvm和torch_xla）。与 1） 和 2） 相比， 3） 更常见。例如，nGraph 通过使用”桥接”来维护其编程或用户界面，支持 TensorFlow 和 PadlePadle。TVM 可以部署 DL 框架生成的模型，然后优化模型推理的性能。随着越来越多的 DL 框架支持导出 ONNX 模型（第 2.1 节），DL 编译器支持 ONNX 进行未来开发非常重要。目前，三个DL编译器（即TVM、Glow和nGraph）能够加载、编译和执行预先训练的 ONNX 模型。 支持的代码目标 所有 DL 编译器都使用 LLVM 作为其低级 IR。与其他低级编译器（例如 GCC 和 ICC）相比，LLVM 的优势是统一IR、高模块化和快速自定义。借助 LLVM，编译器研究人员可以快速为特定于域的应用程序编写优化的通道，并通过 TableGen 模块生成各种目标代码（例如 ARM、x86、PTX）。如表 1 所示，ngraph 只能为 CPU 后端生成 LLVM 代码。CUDA 和 OpenCL 都用于实现异构并行计算。OpenCL 可用于对 NVIDIA 和 AMD GPU 进行编程，而 CUDA 特定于 NVIDIA GPU。尽管 OpenCL 承诺为 GPU 编程提供便携式语言，但其通用性可能会降低性能。两个 DL 编译器（即 TVM 和 XLA）支持同时生成 CUDA 和 OpenCL 代码。只有 TVM 和 PlaidML 支持生成 OpenGL 的代码，这是一个处理渲染图形的跨平台 API。然而，在WWDC 2018，苹果宣布弃用 OpenGL 和 OpenCL 的新系统。相反，Apple 使用金属 API 进行图形渲染和通用计算。目前只有 TVM 和 PlaidML 支持生成Metal代码。 支持的编译 所有 DL 编译器都支持实时编译 （JIT），以提高程序执行的效率。四个 DL 编译器（即 TVM、Glow、nGraph 和 XLA）支持提前编译 （AOT），其中 nGraph 仅在官方版本中启动 AOT（在 Beta 版本中不支持）。TVM/Relay 的 AOT 编译器生成给定中继表达式的本机库，并在 Python 中动态加载该库。Glow 可以生成提前编译的可执行捆绑包，这些捆绑包是自包含的编译网络模型，可用于在独立模式下执行。XLA使用 tfcompile 将 TensorFlow 图形编译为可执行代码谷歌建议 JAX 支持 XLA，将 Autograd 和 XLA 组合在一起进行高性能机器学习研究，而不是添加自动区分（自动分级）支持。 支持的DL优化 至于low-bit inference，目前有四个DL编译器（即TVM、发光、nGraph和MLIR）支持量化。目前，单XLA无法解决量化问题：当重写的TensorFlow 图形减少到量化 XLA 图形时，量化重新编写器缺少部分。TVM 的自动分化、量化和training仍在开发中。此外，TVM v0.6 版本中还提供具有梯度支持的运算符计数。支持动态形状需要更改运行时，这对 DL 编译器来说是一个很大的挑战。此时，两个 DL 编译器（即 TVM 和 nGraph）支持动态形状。TC 和 XLA 仅支持内部静态尺寸，以提供自动形状和绑定推理。TVM 和 TC 支持自动调整，通过调整可用的映射选项来优化性能。TC 只能在 NVIDIA GPU 上执行自动调谐，而 TVM 可以对 CPU（x86 和 ARM）、移动 GPU 和 NVIDIA GPU应用自动fine tuen。TVM 和 TC 使用不同的调优方法：TC 使用遗传搜索，TVM 使用两种机器学习模型（即 GBT 和 TreeGRU）。PlaidML 只能对平铺（自动平铺）应用自动调整，它使用假设的成本模型探索切片大小的空间。 DL编译器的常见设计设计概述DL 编译器的常见设计主要包含两个部分：编译器前端和编译器后端，如图2所示。中间表示形式 （IR） 分布在前端和后端。通常，IR 是程序的抽象，用于程序优化。具体来说，DL模型在DL编译器中转换为多级IR，其中高级IR驻留在前端，低级IR驻留在后端。表2中列出了不同DL编译器中的IR实现。基于高级IR，编译器前端负责独立于硬件的转换和优化。基于低级IR，编译器后端负责特定于硬件的优化、代码生成和编译。DL编译器中的前端、后端和多级IR的功能简要描述如下：The high-level IR 也叫graph IR 表示计算图和控制流，并且与硬件无关。high-level IR的设计挑战是计算和控制流的抽象能力，它能够捕获和表达不同的DL模型。高级 IR 的目标是建立运算符和数据之间的控制流和依赖关系，并为图形级优化提供接口。它还包含丰富的语义信息，用于编译，也为自定义运算符提供了可扩展性。第 4.2 节介绍了对高级别 IR 的详细讨论。 Low-level IR 用于针对不同硬件目标进行特定于硬件的优化和代码生成。因此，低级 IR 的细粒度应足以反映硬件特性并表示特定于硬件的优化。它还允许在编译器后端（如 Halide、多面体模型 和 LLVM）中使用成熟的第三方工具链。第 4.3 节介绍了对低级红外的详细讨论。 前端(The Frontend)将现有DL框架中的DL模型作为输入，然后将模型转换为计算图形表示形式（即graph IR）。为了支持不同框架中的各种格式，前端需要实现各种格式转换。计算图形优化结合了通用编译器和DL特定优化的优化技术，减少了冗余，提高了图形IR的效率。此类优化可分为节点级（例如，节点消除和零分量张量消除）、块级（例如代数简化、运算符融合和运算符下沉）和数据流级别（例如 CSE、DCE、静态内存规划和布局转换）。前端之后，将生成优化的计算图并传递到后端。请注意，某些 DL 编译器将进一步将优化的计算图转换为操作IR。第 4.4 节介绍了对前端的详细讨论。 后端(The Backend)将high-level IR转换为low-level IR，并同时执行特定于硬件的优化。一方面，它可以直接将high-level IR转换为第三方工具链（如 LLVM IR）以利用 LLVM 基础结构进行通用优化和 CPU/GPU 代码生成。另一方面，它可以利用 DL 模型和硬件特性的先前知识，通过自定义的编译过程生成更高效的代码。常用的硬件优化包括硬件内部映射、内存分配和提取、内存延迟隐藏、并行化和面向环路的优化。为了解决上述优化引入的大型解决方案空间，现有 DL 编译器广泛采用两种方法，如自动调度（例如多面体模型）和auto-tuning例如 AutoTVM）。优化的low-level IR使用 JIT 或 AOT 编译，以生成不同硬件目标的代码。第 4.5 节介绍了对后端的详细讨论。 High-Level IR为了克服传统编译器中限制DL模型使用的复杂计算的IR限制，现有的DL编译器利用图IR与专门设计的数据结构进行高效的代码优化。为了更好地了解DL编译器中使用的图形IR，我们描述图形IR的语义和表示形式如下。 Low-Level IR原文链接：The Deep Learning Compiler: A Comprehensive Survey","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"compiler","slug":"compiler","permalink":"http://huangzhiyuan.github.io/tags/compiler/"}]},{"title":"软件开发license问题","slug":"software-license","date":"2020-04-28T12:57:02.000Z","updated":"2020-04-28T13:11:24.000Z","comments":true,"path":"2020/04/28/software-license/","link":"","permalink":"http://huangzhiyuan.github.io/2020/04/28/software-license/","excerpt":"现今存在的开源协议很多，而经过Open Source Initiative组织通过批准的开源协议目前有58种。我们现在常见的开源协议如BSD, Apache,GPL, LGPL,MIT等都是OSI批准的协议。如果要开源自己的代码，最好也是选择这些被批准的开源协议。今天介绍几种常见的开源软件协议。","text":"现今存在的开源协议很多，而经过Open Source Initiative组织通过批准的开源协议目前有58种。我们现在常见的开源协议如BSD, Apache,GPL, LGPL,MIT等都是OSI批准的协议。如果要开源自己的代码，最好也是选择这些被批准的开源协议。今天介绍几种常见的开源软件协议。 BSD (original BSD license、FreeBSD license、Original BSD license)BSD开源协议是一个给于使用者很大自由的协议。基本上使用者可以”为所欲为”，可以自由的使用，修改源代码，也可以将修改后的代码作为开源或者专有软件再发布。 但”为所欲为”的前提当你发布使用了BSD协议的代码，或则以BSD协议代码为基础做二次开发自己的产品时，需要满足三个条件： 如果再发布的产品中包含源代码，则在源代码中必须带有原来代码中的BSD协议。如果再发布的只是二进制类库/软件，则需要在类库/软件的文档和版权声明中包含原来代码中的BSD协议。 不可以用开源代码的作者/机构名字和原来产品的名字做市场推广。 BSD代码鼓励代码共享，但需要尊重代码作者的著作权。BSD由于允许使用者修改和重新发布代码，也允许使用或在BSD代码上开发商业软件发布和销售，因此是对商业集成很友好的协议。而很多的公司企业在选用开源产品的时候都首选BSD协议，因为可以完全控制这些第三方的代码，在必要的时候可以修改或者二次开发。 Apache Licence 2.0Apache Licence是著名的非盈利开源组织Apache采用的协议。该协议和BSD类似，同样鼓励代码共享和尊重原作者的著作权，同样允许代码修改，再发布(作为开源或商业软件)。需要满足的条件也和BSD类似： 需要给代码的用户一份Apache Licence，如果你修改了代码，需要在被修改的文件中说明。在延伸的代码中(修改和有源代码衍生的代码中)需要带有原来代码中的协议，商标，专利声明和其他原来作者规定需要包含的说明。 如果再发布的产品中包含一个Notice文件，则在Notice文件中需要带有Apache Licence。你可以在Notice中增加自己的许可，但不可以表现为对Apache Licence构成更改。 Apache Licence也是对商业应用友好的许可。使用者也可以在需要的时候修改代码来满足需要并作为开源或商业产品发布/销售。 GPL(GNU General Public License)我们很熟悉的Linux就是采用了GPL。GPL协议和BSD，Apache Licence等鼓励代码重用的许可很不一样。GPL的出发点是代码的开源/免费使用和引用/修改/衍生代码的开源/免费使用，但不允许修改后和衍生的代码做为闭源的商业软件发布和销售。这也就是为什么我们能用免费的各种linux，包括商业公司的linux和linux上各种各样的由个人，组织，以及商业软件公司开发的免费软件了。 GPL协议的主要内容是只要在一个软件中使用(”使用”指类库引用，修改后的代码或者衍生代码)GPL协议的产品，则该软件产品必须也采用GPL协议，既必须也是开源和免费。这就是所谓的”传染性”。GPL协议的产品作为一个单独的产品使用没有任何问题，还可以享受免费的优势。 由于GPL严格要求使用了GPL类库的软件产品必须使用GPL协议，对于使用GPL协议的开源代码，商业软件或者对代码有保密要求的部门就不适合集成/采用作为类库和二次开发的基础。 其它细节如再发布的时候需要伴随GPL协议等和BSD/Apache等类似。 LGPL(GNU Lesser General Public License)LGPL是GPL的一个为主要为类库使用设计的开源协议。和GPL要求任何使用/修改/衍生之GPL类库的的软件必须采用GPL协议不同。LGPL允许商业软件通过类库引用(link)方式使用LGPL类库而不需要开源商业软件的代码。这使得采用LGPL协议的开源代码可以被商业软件作为类库引用并发布和销售。 但是如果修改LGPL协议的代码或者衍生，则所有修改的代码，涉及修改部分的额外代码和衍生的代码都必须采用LGPL协议。因此LGPL协议的开源代码很适合作为第三方类库被商业软件引用，但不适合希望以LGPL协议代码为基础，通过修改和衍生的方式做二次开发的商业软件采用。 GPL/LGPL都保障原作者的知识产权，避免有人利用开源代码复制并开发类似的产品 MITMIT是和BSD一样宽范的许可协议，作者只想保留版权，而无任何其他了限制.也就是说，你必须在你的发行版里包含原许可协议的声明，无论你是以二进制发布的还是以源代码发布的。","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"license","slug":"license","permalink":"http://huangzhiyuan.github.io/tags/license/"}]},{"title":"topk in GPU","slug":"topk-in-sycl","date":"2020-04-22T09:41:13.000Z","updated":"2020-04-22T10:06:42.000Z","comments":true,"path":"2020/04/22/topk-in-sycl/","link":"","permalink":"http://huangzhiyuan.github.io/2020/04/22/topk-in-sycl/","excerpt":"topk(input, k, dim=None, largest=True, sorted=True, out=None) -&gt; (Tensor, LongTensor)Semantics: Returns the :attr:k largest elements of the given :attr:input tensor, a long a given dimension.","text":"topk(input, k, dim=None, largest=True, sorted=True, out=None) -&gt; (Tensor, LongTensor)Semantics: Returns the :attr:k largest elements of the given :attr:input tensor, a long a given dimension. example: 123456&gt;&gt;input = torch.tensor([[-3., -1., -2., -8., -7., -4., -9., -6.], [ 3., 1., 2., 8., 7., 4., 9., 6.]], dtype=float)&gt;&gt;output_data,output_index = input.topk(5, 1, True, True)&gt;&gt;print(output_data)&gt;&gt;tensor([-1., -2., -3., -4., -5.], [ 9., 8., 7., 5., 4.]) Select topk value – Radix selectRadix select is not a comparison select but a counting select algorithm. When we select n bit keys, 2n counts are prepared for each number. Simple Example: 12# Get top5 data(0, 3, 2, 2, 3, 2, 0, 3, 2, 1) Step0: count[0x11 &amp; input] ++;Step1: count[0x11] = 3, remain= 2Step2: count[0x10] = 3, count &gt; remain – found 5th topk valueStep3: has_topk = (input &gt;= 5th topkvalue) convert fp32 to uint32Radix select assume all the data is unit32 type, so we need to convert float32 to unint32, double to uint64 first. test file: 123456789101112131415161718192021222324252627#include &lt;stdio.h&gt;#include &lt;stdint.h&gt;int main()&#123; float v = 1; uint32_t x, mask, out; x = *((uint32_t*)&amp;v); mask = (x &amp; 0x80000000) ? 0xffffffff : 0x80000000; out = x ^ mask; v = 2.0; x = *((uint32_t*)&amp;v); mask = (x &amp; 0x80000000) ? 0xffffffff : 0x80000000; out = x ^ mask; v = -1.0; x = *((uint32_t*)&amp;v); mask = (x &amp; 0x80000000) ? 0xffffffff : 0x80000000; out = x ^ mask; v = -2.0; x = *((uint32_t*)&amp;v); mask = (x &amp; 0x80000000) ? 0xffffffff : 0x80000000; out = x ^ mask; return 0;&#125; run: 123456789$ g++ -g float.c -o float(gdb) x/4b &amp;mask0x7fffffffdf70: 0x00 0x00 0x00 0x80(gdb) x/4tb &amp;x0x7fffffffdf6c: 00000000 00000000 00000000 11000000(gdb) x/4tb &amp;mask0x7fffffffdf70: 11111111 11111111 11111111 11111111(gdb) x/4tb &amp;out0x7fffffffdf74: 11111111 11111111 11111111 00111111 Select topk value – Radix selectexclusive prefix scanNow, we got topk th value: desired value got from previous step, but we don’t know at what index to write out the resulting values.Inorder to get this, we performance an exclusive prefix sum of “hasTopk”, this will return the resulting index into which we need to write the result, if a thread has a result. 1bool hasTopK = (input_value &gt;= topKValue); Store hasTopK into shared local memory: smem[thread_id] = hasTopK 123if (hasTopk) &#123; output[index-1] = input_value; &#125; Sort for the top k valueBitonic sorterBitonic sort is a sorting algorithm designed specially for parallel machines.sequence is called Bitonic if it is first increasing, then decreasing. In other words, an array arr[0..n-i] is Bitonic if there exists an index i where 0&lt;=i&lt;=n-1 such that **x0 &lt;= x1 …..&lt;= xi and xi &gt;= xi+1….. &gt;= xn-1 **given a bitonic sequence, if we apply recursively these operations we get a sorted sequence. Bitonic sorterUnsorted squence -&gt; bitornic squence -&gt; sorted sequence","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"GPU","slug":"GPU","permalink":"http://huangzhiyuan.github.io/tags/GPU/"},{"name":"topk","slug":"topk","permalink":"http://huangzhiyuan.github.io/tags/topk/"}]},{"title":"Programming GPUs with SYCL","slug":"programming-gpus-with-sycl","date":"2020-04-15T06:39:39.000Z","updated":"2020-04-15T08:34:50.000Z","comments":true,"path":"2020/04/15/programming-gpus-with-sycl/","link":"","permalink":"http://huangzhiyuan.github.io/2020/04/15/programming-gpus-with-sycl/","excerpt":"Introduction to GPGPU Why program GPUsCPU VS GPU architectureGeneral GPU programming tips SYCL for OpenCL OverviewFeatures SYCL example Vector add","text":"Introduction to GPGPU Why program GPUsCPU VS GPU architectureGeneral GPU programming tips SYCL for OpenCL OverviewFeatures SYCL example Vector add Introduction to GPGPUWhy program GPUs Need for parallellism to gain performance “Free lunch” provided by Moore’s law is Overadding even more CPU cores is showing diminishing returns GPUs are extremely efficient for data parallel tasksArithmetic heavy computations CPU VS GPU architecture CPU: task parallellism small number of large cores seperate instrustions on each core independently higher power consumption lower memory bandwidth random memory access GPU: data parallellism large number of small execution Units single instrustions on all multiple execution units in lock-step lower power consumption higher memory bandwidth sequential memory access common CPU Architecture common GPU Architecture common system architecture GPUs execute in lock-step GPUs access memory sequentially General GPU programming tips ensure the task is suitableGPUs are most efficient for data parallel tasksperformance gain from prforming computing &gt; cost of moving data avoid branchingwaves of processing elements execute in lock-stepboth sides of branches execute with the other masked avoid non-coalesced memory accessGPUs access memory more efficiently if accessed as contiguous blocks avoid exponsive data movementthe bottleneck in GPU programming is data movement between CPU and GPU memoryit’s important to have data as clse to the procesing as possible SYCL for OpenCLwhat is OpenCL allows you to write kernels that execute on accelerators allows you to copy data between the host CPU and accelerators supports a wide range of devices comes in two componentsHost side C API for en-queueing kernels and copying DataDevice side OpenCL C language for writing kernels Motivation of SYCL make heterogeneous programming more accessibleprovide a foundation for efficient and portable templeate algorithms create a C++ for OpenCL ecosystemdefine an open portable standardprovide the performance and portability of OpenCLbase only on standard C++ provide a high-level shared source modelprovide a high-level abstraction over OpenCL boiler plate Codeallow C++ template libraries to target OpenCLallow type safety across host and device how does shared source work? Regular C++ (Single Source) how does shared source work? OpenCL (separate Source) how does shared source work? SYCL (shared Source) Suported Subset of C++ in Device CodeSuported Features static polymorphism lambdas classes operator overloading templeates placement new Non-Suported Features dynamic polymorphism dynamic allocation exception handling RTTI static variables function pointers SYCL exampleVector add","categories":[{"name":"GPU","slug":"GPU","permalink":"http://huangzhiyuan.github.io/categories/GPU/"}],"tags":[{"name":"SYCL","slug":"SYCL","permalink":"http://huangzhiyuan.github.io/tags/SYCL/"}]},{"title":"深度学习面试常见问题集锦","slug":"ML-interview","date":"2020-04-14T13:30:56.000Z","updated":"2020-04-24T05:52:36.000Z","comments":true,"path":"2020/04/14/ML-interview/","link":"","permalink":"http://huangzhiyuan.github.io/2020/04/14/ML-interview/","excerpt":"这篇文章用来记录总结深度学习领域常见面试题。题目来源于各大门户网站和招聘网站。一来用于了解当前面试常考热点问题，二来在工作闭门造车的同时，弥补下自己深度学习理论方面的不足。","text":"这篇文章用来记录总结深度学习领域常见面试题。题目来源于各大门户网站和招聘网站。一来用于了解当前面试常考热点问题，二来在工作闭门造车的同时，弥补下自己深度学习理论方面的不足。 深度学习模型中参数量的计算？全连接层假设输入层大小i，隐藏层h，输出层o： 则参数量为各层之间的参数+每层的偏差=(ih+ho)+(h+o) 例如输入大小3，隐藏层大小5，输出大小2： 则参数个数为：（3×5 + 5×2）+（5 + 2） = 32 对于rnn网络首先我们定义三个参数：g:门的数量(RNN有1个门，GRU有3个，LSTM有4个)h:隐藏单元大小 ，i:输出层大小每个门中的权重实际上是一个输入大小(h + i)（解释:上一个时刻的隐状态和当前输入的拼接）和输出大小为h（解释：当前时刻的隐状态）的FFNN。所以每个门都有h（h + i）+ h个参数。即在RNN中参数数量为：g × [ h（h + i）+ h ]注意：输出我们只关心h，不关心其上层的全连接或者softmax，因为这已经不是当前rnn的参数了。举例：具有2个隐藏单元和输入尺寸3的LSTM： 则参数量为：g ×[ h（h + i）+ h ]= 4 ×[2（2 + 3）+ 2] = 48 具有5个隐藏单元和输入大小为8的堆叠双向GRU +具有50个隐藏单元的LSTM的参数数量为： 第一层参数：2 × g ×[ h(h + i)+ h ] = 2 ×3×[5(5 + 8)+ 5] = 420第二层参数： g ×[ h(h + i)+ h ]= 4×[50(50 + 10)+ 50] = 12200则总参数量为： 420 + 12200 = 12620 对于CNN网络首先我们定义三个参数：i:输入尺寸，f:卷积核的大小，o:输出大小 则每个滤波器对应的输出映射参数为：num_params =权重+偏差= [ i×(f×f)×o ] + o 例如带有1 × 1滤波器的灰度图像，输出3个通道 参数数量为： [ i×(f×f)×o ] + o= [1 ×(2 × 2)× 3] + 3= 15 例如具有2×2滤波器的RGB图像，1通道的输出。则每个输入像素通道都有1个滤波器。产生的卷积按元素叠加，并且向每个元素添加偏差项。 最终得到参数数量为： [ i×(f×f)×o ] + o= [3 ×(2×2)×1] + 1 = 13 卷积神经网络在max pooling处怎么反向传播误差无论max pooling还是mean pooling，都没有需要学习的参数。因此，在卷积神经网络的训练中，Pooling层需要做的仅仅是将误差项传递到上一层，而没有梯度的计算。（1）max pooling层：对于max pooling，下一层的误差项的值会原封不动的传递到上一层对应区块中的最大值所对应的神经元，而其他神经元的误差项的值都是0；（2）mean pooling层：对于mean pooling，下一层的误差项的值会平均分配到上一层对应区块中的所有神经元。 怎么防止模型过拟合？包括但不限于以下： 提前终止（当验证集上的效果变差的时候） 正则化（Regularization）L1正则化L2正则化 数据集扩增（Data augmentation） Dropout GN，BN，LN和IN的区别？ BN是在batch上，对N，H，W做归一化，对小batchsize效果不好，应优先考虑 LN在通道方向上，对C，H，W做归一化，主要用于RNN IN在图像像素上，对H， W做归一化，主要用于风格迁移， GAN GN将通道分组，然后归一化，适合mini-batch很小的情况 SN将BN，LN，IN结合，赋予权重，让网络自适应归一化层 基于机器学习领域的一个重要的假设：iidiidiid假设，即假设训练数据和测试数据是满足相同的分布的，这是通过训练数据集获得的模型能够在测试集上获得好的效果的一个基本保障。 Batch Normalization那么BN，简单的说，就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同的分布。 但是也有不少问题： 如果batch size太小，则BN效果明显下降 对于有些像素级图片生成任务，效果不佳 不适合RNN等动态网络，不方便且效果不佳 训练时和推理时统计量不一致 Layer Normalization为了能够在只有当前一个训练实例下，找到一个合理的统计范围，一个直接的想法是：把同一个卷积层作为集合，求均值和方差；对于RNN，就是把同层隐层神经元的响应值（不同时刻）作为集合，再求均值和方差。 Instance NormalizationLN是对所有的通道，那对一个通道做归一化是怎样呢！IN就是这么做的，单个通道的feature map作为集合，并在此集合上求均值和方差，这种像素级上的操作，使得IN对于一些图片生成类的任务效果明显优于BN，比如图片风格转换。 Group NormalizationLN是对于所有的通道，IN是对于单个通道，如果把所有通道分组，再各个组上做归一化又会怎样呢？这就是GN啦。通道分组也是CNN常用的优化技巧。GN在mini-batch比较小的场景或者物体检测、视频分类等场景下效果优于BN。 Switchable Normalization归一化，虽能提高模型的泛化能力，但BN，LN， IN， GN都是人工设计的，是否可以放开手脚，让网络自适应呢！这是个更加generic的做法。 python浅拷贝和深拷贝的区别在python中，对象赋值实际上是对象的引用。当创建一个对象，然后把它赋给另一个变量的时候，python并没有拷贝这个对象，而只是拷贝了这个对象的引用。一般有3种方法： 1alist=[1,2,3,[&quot;a&quot;,&quot;b&quot;]] （1）直接赋值,默认浅拷贝传递对象的引用而已,原始列表改变，被赋值的b也会做相同的改变 1234567&gt;&gt;&gt; b=alist&gt;&gt;&gt; print b[1, 2, 3, [&#x27;a&#x27;, &#x27;b&#x27;]]&gt;&gt;&gt; alist.append(5)&gt;&gt;&gt; print alist;print b[1, 2, 3, [&#x27;a&#x27;, &#x27;b&#x27;], 5][1, 2, 3, [&#x27;a&#x27;, &#x27;b&#x27;], 5] （2）copy浅拷贝，没有拷贝子对象，所以原始数据改变，子对象会改变 1234567891011121314151617&gt;&gt;&gt; import copy&gt;&gt;&gt; c=copy.copy(alist)&gt;&gt;&gt; print alist;print c[1, 2, 3, [&#x27;a&#x27;, &#x27;b&#x27;]][1, 2, 3, [&#x27;a&#x27;, &#x27;b&#x27;]]&gt;&gt;&gt; alist.append(5)&gt;&gt;&gt; print alist;print c[1, 2, 3, [&#x27;a&#x27;, &#x27;b&#x27;], 5][1, 2, 3, [&#x27;a&#x27;, &#x27;b&#x27;]]&gt;&gt;&gt; alist[3][&#x27;a&#x27;, &#x27;b&#x27;]&gt;&gt;&gt; alist[3].append(&#x27;cccc&#x27;)&gt;&gt;&gt; print alist;print c[1, 2, 3, [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;cccc&#x27;], 5][1, 2, 3, [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;cccc&#x27;]] 里面的子对象被改变了 （3）深拷贝，包含对象里面的自对象的拷贝，所以原始对象的改变不会造成深拷贝里任何子元素的改变 12345678910111213141516&gt;&gt;&gt; import copy&gt;&gt;&gt; d=copy.deepcopy(alist)&gt;&gt;&gt; print alist;print d[1, 2, 3, [&#x27;a&#x27;, &#x27;b&#x27;]][1, 2, 3, [&#x27;a&#x27;, &#x27;b&#x27;]]始终没有改变&gt;&gt;&gt; alist.append(5)&gt;&gt;&gt; print alist;print d[1, 2, 3, [&#x27;a&#x27;, &#x27;b&#x27;], 5][1, 2, 3, [&#x27;a&#x27;, &#x27;b&#x27;]]始终没有改变&gt;&gt;&gt; alist[3][&#x27;a&#x27;, &#x27;b&#x27;]&gt;&gt;&gt; alist[3].append(&quot;ccccc&quot;)&gt;&gt;&gt; print alist;print d[1, 2, 3, [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;ccccc&#x27;], 5][1, 2, 3, [&#x27;a&#x27;, &#x27;b&#x27;]] 始终没有改变 为什么2个3 * 3跟1个5 * 5感受野相同？我们假设图片是28 * 28的，我们使用5 * 5的卷积核对其卷积，步长为1，得到的结果是:(28-5)/1+1=24，然后我们使用2个卷积核为3 * 3的，这里的两个是指2层：第一层3 * 3：得到的结果是(28-3)/1+1=26，第二层3 * 3：，得到的结果是(26-3)/1+1=24，所以我们的最终结果和5 * 5的卷积核是一样。 请解释为什么两层3x3的卷积核感受野为5x5，三层3x3的卷积核感受野为7x7呢？角度一：在stride=1,padding=0的情况下，两层3x3的卷积核感受野为5x5，如下 三层3x3的卷积核感受野为7x7，如下 角度二：我们假设图片是28 * 28的，我们使用5 * 5的卷积核对其卷积，步长为1，得到的结果是:(28-5)/1+1=24。然后我们使用2个卷积核为3 * 3的，这里的两个是指2层：第一层3 * 3：得到的结果是(28-3)/1+1=26，第二层3 * 3：得到的结果是(26-3)/1+1=24，所以我们的最终结果和5 * 5的卷积核是一样的；其他同理。 图片直观感受法：10x10的图片使用3x3的卷积核卷积，默认stride=1，padding=0 2层3x3卷积核——output2中1x1的像素已经可以感受原图中的5x5区域了3层3x3卷积核——output3中1x1的像素已经可以感受原图中的7x7区域了 为什么使用3x3的卷积核？(1)3x3是最小的能够捕获像素八邻域信息的尺寸。(2)两个3x3的堆叠卷基层的有限感受野是5x5；三个3x3的堆叠卷基层的感受野是7x7，故可以通过小尺寸卷积层的堆叠替代大尺寸卷积层，并且感受野大小不变。(3)多个3x3的卷基层比一个大尺寸filter卷基层有更多的非线性（更多层的非线性函数，使用了3个非线性激活函数）。(4)多个3x3的卷积层比一个大尺寸的filter有更少的参数，如三个3x3的卷积层参数个数3x((3x3xC)xC)=27C2；一个（7x7xC）xC的卷积层参数为49C2。唯一的不足是，在进行反向传播时，中间的卷积层可能会导致占用更多的内存； batch normalization的均值和方差怎么更新的？Batchnorm是归一化的一种手段，极限来说，这种方式会减小图像之间的绝对差异，突出相对差异，加快训练速度，如果我们每一个batch输入的数据都具有不同的分布，显然会给网络的训练带来困难。另一方面，数据经过一层层网络计算后，其数据分布也在发生着变化，会给下一层的网络学习带来困难。batchnorm就是为了解决这个分布变化问题。BatchNorm就是对神经网络的每一层进行归一化，假设将每一层输出后的数据都归一化到0均值，1方差，满足正太分布，但是，此时有一个问题，每一层的数据分布都是标准正太分布，导致其完全学习不到输入数据的特征，因为，费劲心思学习到的特征分布被归一化了，因此，直接对每一层做归一化显然是不合理的。但是如果稍作修改，加入可训练的参数做归一化，那就是BatchNorm实现的了之前也说过如果直接做归一化不做其他处理，神经网络是学不到任何东西的，但是加入这两个参数后，事情就不一样了，先考虑特殊情况下，如果γ和β分别等于此batch的标准差和均值，那么 不就还原到归一化前的x了吗，也即是缩放平移到了归一化前的分布，相当于batchnorm没有起作用，β和γ分别称之为 平移参数和缩放参数 。这样就保证了每一次数据经过归一化后还保留的有学习来的特征，同时又能完成归一化这个操作，加速训练。batchnorm是在输入值和激活函数之间进行的，每次训练给一个批量，然后计算批量的均值方差，但是在测试的时候可不是这样，测试的时候每次只输入一张图片，这怎么计算批量的均值和方差，于是，在训练的时候实现计算好mean var测试的时候直接拿来用就可以了，不用计算均值和方差 ResNet旁路跟卷积后的结果相加具体是怎么做的？1*1的卷积核有什么用1×1卷积核用于升维、降维：如果卷积的输入、输出都仅有一个平面，那么1×1卷积核并没有啥意义。它完全不考虑像素与其周围像素之间的关系！但通常卷积的输入、输出都是立方体（多通道的），此时1×1卷积核实际上是对每个像素点在不同通道（channels）上线性组合（信息整合），且保留了图片的原有平面结构，仅仅是改变channels的数量，进而达到升维和降维的功能！ 上左图显示了1×1卷积核用于降维。输入为4×4的平面，共3个通道。用两个1×1的卷积核（卷积层）作用之后，变为4×4的平面，共2通道。同理，上右图展示了升维的操作。 若卷积核数量如果与输入保持相同，则单纯地向网络增加非线性。 1、降维（通道数） 通过控制卷积核的数量达到通道数大小的放缩。而池化层只能改变高度和宽度，无法改变通道数。2、增加非线性 如上所述，1×1卷积核的卷积过程相当于全连接层的计算过程，并且还加入了非线性激活函数，从而可以增加网络的非线性，使得网络可以表达更加复杂的特征。3、减少卷积核参数，运算的复杂度 在Inception Network中，由于需要进行较多的卷积运算，计算量很大，可以通过引入1×1确保效果的同时减少计算量。具体可以通过下面例子量化比较。 未引入1×1卷积的卷积操作：卷积核参数：5x5x32x192引入1×1卷积的卷积操作：卷积核参数为1×1×192×16+5×5×16×32 深度学习中压缩模型的方法？（1）设计轻量级的模型（SqueezeNet，MobileNet，ShuffleNet等）：不需要压缩；（2）模型结构/内存优化：剪枝、权值量化等；（3）模型蒸馏 softmax与交叉熵(Cross Entropy)的区别与联系参考：https://blog.csdn.net/weixin_42423701/article/details/90553196 随机森林+过拟合？相对于单个的Decision Tree，Random Forest不太容易over-fitting。Over-fitting的主要原因是因为模型学习了太多样本中的随机误差。因为Random Forest随机选择了样本和特征，并且将很多这样的随机树进行了平均，这些随机误差也随之被平均，乃至相互抵消了。但是这不代表它不会很多人说Random Forest不会over-fitting。相信很多人也亲身经历过，我自己也见识过过RandomForest over-fitting了。防止RandomFroest过拟合，一个方法是控制每个树的深度，深的树有可能会过拟合；另外一个是对模型进行交叉验证。 可迭代对象、迭代器、生成器、装饰器： 容器是一系列元素的集合，str、list、set、dict、file、sockets对象都可以看作是容器，容器都可以被迭代（用在for，while等语句中），因此他们被称为可迭代对象。 可迭代对象实现了__iter__方法，该方法返回一个迭代器对象。 迭代器持有一个内部状态的字段，用于记录下次迭代返回值，它实现了__next__和__iter__方法，迭代器不会一次性把所有元素加载到内存，而是需要的时候才生成返回结果。 生成器是一种特殊的迭代器，它的返回值不是通过return而是用yield。 装饰器可以让已有的函数不做任何改动的情况下增加功能。 数据不均衡如何处理？ 对较多的那个类别进行**欠采样(under-sampling)**，舍弃一部分数据，使其与较少类别的数据相当；（注释：已经改变了数据的分布，浪费了部分数据） 对较少的类别进行**过采样(over-sampling)**，重复使用一部分数据，使其与较多类别的数据相当；（注释：已经改变了分布，数据重复生成，容易过拟合） 阈值调整（threshold moving），将原本默认为0.5的阈值调整到 较少类别/（较少类别+较多类别）即可或者使用现有的集成学习分类器，如随机森林或者xgboost，并调整分类阈值。（推荐这种） 有1千万条短信，有重复，以文本文件的形式保存，一行一条，有重复。请用5分钟时间，找出重复出现最多的前10？ 方法1：可以用哈希表的方法对1千万条短信分成若干组进行边扫描边建散列表。第一次扫描，取首字节，尾字节，中间随便两字节作为Hash Code，插入到hash table中。并记录其地址和信息长度和重复次数，1千万条信息，记录这几个信息还放得下。同Hash Code且等长就疑似相同，比较一下。相同记录只加1次进hash table，但将重复次数加1。一次扫描以后，已经记录各自的重复次数，进行第二次hash table的处理。用线性时间选择可在O（n）的级别上完成前10条的寻找。分组后每份中的top10必须保证各不相同，可hash来保证，也可直接按hash值的大小来分类。 方法2：可以采用从小到大排序的方法，根据经验，除非是群发的过节短信，否则字数越少的短信出现重复的几率越高。建议从字数少的短信开始找起，比如一开始搜一个字的短信，找出重复出现的top10并分别记录出现次数，然后搜两个字的，依次类推。对于对相同字数的比较常的短信的搜索，除了hash之类的算法外，可以选择只抽取头、中和尾等几个位置的字符进行粗判，因为此种判断方式是为了加快查找速度但未能得到真正期望的top10，因此需要做标记；如此搜索一遍后，可以从各次top10结果中找到备选的top10，如果这top10中有刚才做过标记的，则对其对应字数的所有短信进行精确搜索以找到真正的top10并再次比较。 方法3：可以采用内存映射的办法，首先1千万条短信按现在的短信长度将不会超过1G空间，使用内存映射文件比较合适。可以一次映射（当然如果更大的数据量的话，可以采用分段映射），由于不需要频繁使用文件I/O和频繁分配小内存，这将大大提高数据的加载速度。其次，对每条短信的第i（i从0到70）个字母按ASCII嘛进行分组，其实也就是创建树。i是树的深度，也是短信第i个字母。 为什么正则化能阻止overfiting？过拟合是由于高方差引起的，高方差是由于参数太多，特征值过于敏感导致的，正则化能够减少参数数量，降低特征值敏感度，所以能阻止过拟合。 神经网络训练中的梯度消失与梯度爆炸层数比较多的神经网络模型在训练时也是会出现一些问题的，其中就包括梯度消失问题（gradient vanishing problem）和梯度爆炸问题（gradient exploding problem）。梯度消失问题和梯度爆炸问题一般随着网络层数的增加会变得越来越明显。例如，对于下图所示的含有3个隐藏层的神经网络，梯度消失问题发生时，接近于输出层的hidden layer 3等的权值更新相对正常，但前面的hidden layer 1的权值更新会变得很慢，导致前面的层权值几乎不变，仍接近于初始化的权值，这就导致hidden layer 1相当于只是一个映射层，对所有的输入做了一个同一映射，这是此深层网络的学习就等价于只有后几层的浅层网络的学习了。 而这种问题为何会产生呢？以下图的反向传播为例（假设每一层只有一个神经元且对于每一层可以推导出: 而sigmoid的导数如下图: 其实梯度爆炸和梯度消失问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。对于更普遍的梯度消失问题，可以考虑用ReLU激活函数取代sigmoid激活函数。另外，LSTM的结构设计也可以改善RNN中的梯度消失问题。 参考：神经网络训练中的梯度消失与梯度爆炸 传统机器学习考察点：1、bias与variance的含义，并结合ensemble method问哪种方法降低bias，哪种方法降低variance2、lr与svm的区别与联系3、gbdt与adaboost的区别与联系4、手推svm，svm麻雀虽小五脏俱全5、pca与lda的区别与联系，并推导6、白化的原理与作用7、给一个算法，例如lr，问这个算法的model、evaluate、optimization分别是啥 深度学习考察点1、手推bp2、梯度消失/爆炸原因，以及解决方法3、bn的原理，与白化的联系4、防止过拟合有哪些方法5、dnn、cnn、rnn的区别与联系6、机器学习与深度学习的联系7、batch size大小会怎么影响收敛速度 CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用CNN解出来？为什么AlphaGo里也用了CNN？这几个不相关的问题的相似性在哪里？CNN通过什么手段抓住了这个共性？以上几个不相关问题的相关性在于，都存在局部与整体的关系，由低层次的特征经过组合，组成高层次的特征，并且得到不同特征之间的空间相关性。如下图：低层次的直线／曲线等特征，组合成为不同的形状，最后得到汽车的表示。CNN抓住此共性的手段主要有四个：局部连接／权值共享／池化操作／多层次结构。局部连接使网络可以提取数据的局部特征；权值共享大大降低了网络的训练难度，一个Filter只提取一个特征，在整个图片（或者语音／文本） 中进行卷积；池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成为较高层次的特征，从而对整个图片进行表示。 对所有优化问题来说, 有没有可能找到比現在已知算法更好的算法?没有免费的午餐定理 对于训练样本（黑点），不同的算法A/B在不同的测试样本（白点）中有不同的表现，这表示：对于一个学习算法A，若它在某些问题上比学习算法 B更好，则必然存在一些问题，在那里B比A好。 也就是说：对于所有问题，无论学习算法A多聪明，学习算法 B多笨拙，它们的期望性能相同。 但是：没有免费午餐定力假设所有问题出现几率相同，实际应用中，不同的场景，会有不同的问题分布，所以，在优化算法时，针对具体问题进行分析，是算法优化的核心所在。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://huangzhiyuan.github.io/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"2020读书笔记","slug":"2020-reading-notes","date":"2020-04-13T12:41:16.000Z","updated":"2020-07-26T14:11:08.000Z","comments":true,"path":"2020/04/13/2020-reading-notes/","link":"","permalink":"http://huangzhiyuan.github.io/2020/04/13/2020-reading-notes/","excerpt":"这篇用来记录2020年读的书籍和看的有意义的纪录片。电影则单独另作一文。希望每月抽出时间静下心来用心读一本书，内容可涉及各行各业，不必都是专业相关。一来扩宽专业领域，多去探索未知领域，激发好奇心，不要局限在自己局限的世界观内。二来也希望在这个节奏快、功利心强，一心想要追逐都立竿见影效果的社会，慢慢沉淀自己，时时放慢自己的脚本，思索下生活，享受这份难得的静谧。","text":"这篇用来记录2020年读的书籍和看的有意义的纪录片。电影则单独另作一文。希望每月抽出时间静下心来用心读一本书，内容可涉及各行各业，不必都是专业相关。一来扩宽专业领域，多去探索未知领域，激发好奇心，不要局限在自己局限的世界观内。二来也希望在这个节奏快、功利心强，一心想要追逐都立竿见影效果的社会，慢慢沉淀自己，时时放慢自己的脚本，思索下生活，享受这份难得的静谧。 书籍棋王-树王 《棋王》是当代作家阿城的一部短篇小说。小说被视作是新时期“寻根文学”的发轫之作。故事讲述了在文革时代，知青“棋呆子”王一生四处寻找对手下棋、拼棋的故事。小说语言抛弃了20世纪80年代惯有的语言逻辑转而回归宋明小说的语境之中，朴实而飘逸俊美。（节选自百度百科）第一次读小说记得源自高中语文读本里面节选的一段。也就是王一生错过象棋比赛报名，最终终有机会和各个高手同时下盲棋的那段。至今记忆深刻。再次读这本小时，也是一口气读完的。不禁佩服作者娴熟的文字驾驭能力和将故事娓娓道来的本事。尽管没有经历过那个时代，但也能从坐着的笔下仿佛又回到了那个知青上山下乡的年代，随着书中的人物一起生活，一起围观下棋。物质的空虚不能引发精神的空虚，人活着总要为点什么东西而活。一个人的力量对于整个大的社会背景而言是在是太渺小了，尽管外界嘈杂纷纭，但还是要守住自己内心的那片宁静和祥和。说什么生不逢时，说什么时无英雄，唯有强大自己的内心，寻找精神世界的充实和平衡。 解读中国经济作者郎咸平。前半本当时读完之后收获很大，对于了解新中国成立以来中国经济发展的现状、原因都进行了详细的阐述，即使没有太多经济背景的人读完也能学到知识。后半部分读的匆匆，后面有机会再来细读添加更深的感悟吧。 血腥的大唐这本书是在读完当年明月的那本《明朝那些事儿》之后，系统推荐类似的解释整个唐朝历史的一本小说。小说文字篇幅较长，比正史简洁，没有《明朝那些事儿》语言风趣幽默可读性强。当时也尽可能的还原了我心中盛唐的风度。目前只读到了安史之乱，即唐朝后期由盛转衰的转折点。年轻人读书都这样，喜欢君贤臣忠，朝代繁盛，文治武功。排斥君昏臣奸，天下不安，生灵涂炭，却没有大英雄来挽救这个时代。中国的下次鼎盛就是大宋赵家了。 纪录片今天偶尔收到推荐的是如下几个： 无节制消费的元凶但是还有书籍简介：纪录片《但是还有书籍》由哔哩哔哩和北京小河文化传媒有限公司联合出品，该系列共有五集，每集25-30分钟。《但是还有书籍》以书为题材，力图在阅读多样化、碎片化的当下，记录这个时代形形色色的爱书之人，捕捉和书有关的那些精彩故事。希望以新鲜有趣的视角和故事，点燃观众对于书的热爱，为人们提供一份在快时代里的阅读指南。评分9.8。阅读是一座随身携带的避难所，让我们在这个世界以外，还拥有无数个平行世界。我们希望以这个这个片子，记录这个时代形形色色的爱书人，捕捉和书有关的那些精彩故事，向编舟者致敬，为爱书人点赞。借用会长的话来表达一下我们的愿望：十万个人里面有一个人因为这个片子而重新拿起书，重新点燃起对阅读的兴趣，那我们做这个片子就值了。那也是我们作为纪录片工作者的高光时刻了。第一集提到的比较重要的书： 袁哲生《寂寞的游戏》黄锦树《雨》黄国峻《度外》童伟格《无伤时代》《西北雨》《王考》郑天挺《郑天挺西南联大日记》科塔萨尔《万火归一》 看完这集最大的感触就是赶快扔下手机，去拿本书来细细的翻看。之前也买过好多纸质的书籍，也专业书籍，也有文学小说。除了小说过了一遍，专业书籍也只是用来参考用，其他大部分时间都是用来吃灰了。总会有种错觉，买到手的就是自己拥有的。殊不知这些被作者编辑们苦心著作编撰的图书精魂与思想远远没有达到被自己吸收份上。只会成为下次搬家的累赘。在这个快节奏，被扑面而来的新闻消息覆盖的时代，充斥的头条，抖音，快手，b站，游戏等各种各样消耗掉你碎片化时间的手机APP，很难再回到以前车马都很慢，一生只够爱一个人的时代了。纸质书亦是如此。希望自己能够坚持吧！就从坚持今年这篇读书笔记开始。 万物与虚无抽象：设计的艺术积极心理学粒子狂热维度：数学漫步公司的力量混沌之谜人生果实","categories":[{"name":"读书","slug":"读书","permalink":"http://huangzhiyuan.github.io/categories/%E8%AF%BB%E4%B9%A6/"}],"tags":[{"name":"读书","slug":"读书","permalink":"http://huangzhiyuan.github.io/tags/%E8%AF%BB%E4%B9%A6/"}]},{"title":"深度学习中的batch的大小对学习效果有何影响？","slug":"batch-size-ml-training","date":"2020-04-11T14:07:07.000Z","updated":"2020-04-13T12:31:38.000Z","comments":true,"path":"2020/04/11/batch-size-ml-training/","link":"","permalink":"http://huangzhiyuan.github.io/2020/04/11/batch-size-ml-training/","excerpt":"最近在跑某个深度学习的模型训练时，发现训练时采用的batch size对训练的效果有一定的影响，因此这里想要做个总结。这篇文章也是参考了知乎上面几位网友的答案：原文链接：https://www.zhihu.com/question/32673260/answer/71137399","text":"最近在跑某个深度学习的模型训练时，发现训练时采用的batch size对训练的效果有一定的影响，因此这里想要做个总结。这篇文章也是参考了知乎上面几位网友的答案：原文链接：https://www.zhihu.com/question/32673260/answer/71137399 目前深度学习模型多采用批量随机梯度下降算法进行优化，随机梯度下降算法的原理如下， n是批量大小(batchsize)，η是学习率(learning rate)。可知道除了梯度本身，这两个因子直接决定了模型的权重更新，从优化本身来看它们是影响模型性能收敛最重要的参数。学习率直接影响模型的收敛状态，batchsize则影响模型的泛化性能，两者又是分子分母的直接关系，相互也可影响，因此这一次来详述它们对模型性能的影响。模型性能对batchsize虽然没有学习率那么敏感，但是在进一步提升模型性能时，batchsize就会成为一个非常关键的参数。 大的batchsize减少训练时间，提高稳定性这是肯定的，同样的epoch数目，大的batchsize需要的batch数目减少了，所以可以减少训练时间，目前已经有多篇公开论文在1小时内训练完ImageNet数据集。另一方面，大的batch size梯度的计算更加稳定，因为模型训练曲线会更加平滑。在微调的时候，大的batch size可能会取得更好的结果。 大的batchsize导致模型泛化能力下降 在一定范围内，增加batchsize有助于收敛的稳定性，但是随着batchsize的增加，模型的性能会下降，如下图。 这是研究者们普遍观测到的规律，虽然可以通过一些技术缓解。这个导致性能下降的batch size在上图就是8000左右。 那么这是为什么呢？研究表明大的batchsize收敛到sharp minimum，而小的batchsize收敛到flat minimum，后者具有更好的泛化能力。两者的区别就在于变化的趋势，一个快一个慢，如下图，造成这个现象的主要原因是小的batchsize带来的噪声有助于逃离sharp minimum。 Hoffer[7]等人的研究表明，大的batchsize性能下降是因为训练时间不够长，本质上并不少batchsize的问题，在同样的epochs下的参数更新变少了，因此需要更长的迭代次数。总之batchsize在变得很大(超过一个临界点)时，会降低模型的泛化能力。在这个临界点之下，模型的性能变换随batch size通常没有学习率敏感。 学习率和batchsize的关系通常当我们增加batchsize为原来的N倍时，要保证经过同样的样本后更新的权重相等，按照线性缩放规则，学习率应该增加为原来的N倍[5]。但是如果要保证权重的方差不变，则学习率应该增加为原来的sqrt(N)倍[7]，目前这两种策略都被研究过，使用前者的明显居多。 从两种常见的调整策略来看，学习率和batchsize都是同时增加的。学习率是一个非常敏感的因子，不可能太大，否则模型会不收敛。同样batchsize也会影响模型性能，那实际使用中都如何调整这两个参数呢？ 研究[8]表明，衰减学习率可以通过增加batchsize来实现类似的效果，这实际上从SGD的权重更新式子就可以看出来两者确实是等价的，文中通过充分的实验验证了这一点。 研究[9]表明，对于一个固定的学习率，存在一个最优的batchsize能够最大化测试精度，这个batchsize和学习率以及训练集的大小正相关。 对此实际上是有两个建议： 如果增加了学习率，那么batch size最好也跟着增加，这样收敛更稳定。 尽量使用大的学习率，因为很多研究都表明更大的学习率有利于提高泛化能力。如果真的要衰减，可以尝试其他办法，比如增加batch size，学习率对模型的收敛影响真的很大，慎重调整。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"batch-size","slug":"batch-size","permalink":"http://huangzhiyuan.github.io/tags/batch-size/"}]},{"title":"专访罗永浩：我今年四十八岁，还可以承受无数次的失败","slug":"interview-laoluo","date":"2020-04-10T07:35:14.000Z","updated":"2020-04-10T08:31:02.000Z","comments":true,"path":"2020/04/10/interview-laoluo/","link":"","permalink":"http://huangzhiyuan.github.io/2020/04/10/interview-laoluo/","excerpt":"原文链接：http://baijiahao.baidu.com/s?id=1663496784360695202 罗永浩的回归依然万众瞩目。4月1日直播首秀结束后，比起他自己的内心世界，外界对他的情感和评价或许更丰富也更复杂：支持、感动、激动，鄙夷、质疑、嘲讽，这些字眼全都混杂到一起，重新建立罗永浩的舆论形象。他的主播身份越来越鲜明，但不疑有他的是，关注他的人一刻也没有忘记他走过的那条路，以及路途上发生的故事。","text":"原文链接：http://baijiahao.baidu.com/s?id=1663496784360695202 罗永浩的回归依然万众瞩目。4月1日直播首秀结束后，比起他自己的内心世界，外界对他的情感和评价或许更丰富也更复杂：支持、感动、激动，鄙夷、质疑、嘲讽，这些字眼全都混杂到一起，重新建立罗永浩的舆论形象。他的主播身份越来越鲜明，但不疑有他的是，关注他的人一刻也没有忘记他走过的那条路，以及路途上发生的故事。 4月10日晚八点，罗永浩将带着他的团队展开第二场直播，这是一次助销湖北产品的专场。老罗在公众号表示，上一场直播得到的360多万元打赏，将全部用于补贴湖北当地的果农。尽管这是一场带着公益属性的直播，但对于主播罗永浩而言，仍将被视作一场“二战”。上一场首秀结束后，铺天盖地的正负面评价他是否已经完全消化吸收、以在新一场直播中有更好的表现？近日，界面新闻对罗永浩进行了专访，提到了许多外界都在好奇的问题：他对于直播的决心、怎么看待自己首秀的表现、团队选品的标准，以及对留下深刻印痕的手机行业还有什么看法？新的一场直播开始前，不妨借此再了解他一次。 以下为采访实录（略有编辑）： 界面新闻：4月1日直播结束以后，大家评论说李佳琪和薇娅的直播间更让人亢奋，你和朱萧木带货确实有些无聊，你认可吗？但这个评价其实和你的个人形象特别不符，你觉得问题出在哪里？ 罗永浩：完全认可。我们经验不够，准备时间也严重不足，所以第一次确实做得很业余，对得起消费者，但对不起观众。再做几次就好了。 界面新闻：1.1个亿交易额，4800万观看人数，这个首秀成绩你满意吗？十分满分的话，对自己的表现和首秀的成绩各打几分？ 罗永浩：传统上，很多平台统计的是下单金额，由于电商直播的冲动消费比例高，所以很多人会在下单后冷静一下又放弃支付。按下单金额，我们的成绩是1.7亿左右。但抖音平台统计的是成交金额，也就是实际最终支付的金额。按成交金额算，我们的成绩是1.1亿。 作为电商带货的首播，他们说这个数字应该是破了世界纪录的。对这个数字，我们团队基本上是满意的，但觉得通过坚持、改进和努力，提升空间还很大。因为经验严重不足，公司筹备时间也异常紧张，所以整体表现很不理想，我自己打分的话，肯定是不及格的，不过相信很快就会明显改善。 界面新闻：一些人反映第一次直播的部分商品价格和其他平台平日里的差不多，力度并不是很大，原因是什么？按照首播的表现和成果，你们接下来准备如何拉到力度更大的优惠？ 罗永浩：我们只追求每次都要厂商给我们最低价，但其实并不追求低很多。我们希望直播室的用户每次能买到最低价，但不希望厂商因此赔钱赚吆喝。 挟流量和关注度的优势对厂商进行破坏性开采，不利于厂商、销售渠道商、和消费者之间本应有的长期共生共赢关系。 界面新闻：简单讲讲你们选品的流程吧？如果只把首秀上的商品分成科技类和食品类，它们目前获得你首肯的标准是什么？ 罗永浩：流程大概是这样：商务对接，产品经理跟进试用和评价，用户评价/口碑和销量的调研，供应链及其他背景调查，双方市场团队对接，商务和法务协同跟进，确定合同协议，上线直播。 食品类的主要标准（这里排序不分先后）：1.大品牌/国民品牌（这在人手不足的前期，主要是为了安全），或在垂直品类里取得了足够大的成就，但还没扩张成全民皆知（对这部分厂商，我们的品牌宣传价值非常高）；2.好吃/好喝；3.高性价比。 科技类的主要标准（这里排序不分先后）：1.供应链我们可以基本上了解到的，卖出了足够大的销量的，有足够好的口碑，并且没有出现成规模的质量问题的品牌；2.好用/实用/解决用户需求和痛点；3.高性价比。 界面新闻：老实讲，你觉得直播带货这件事本身让你觉得有趣和有激情吗？你特别擅长比喻，可以打个比方让大家知道做手机和做直播，在你心里各是什么地位和感受吗？ 罗永浩：一切创造价值的工作都会让我有兴趣，我现在卖货卖得很开心。我算着它的收入，算着它什么时候能帮我还完债务，算着它后续的可能商业前景，每天都很高兴。 但交个朋友科技有限公司当然不会止于卖货，止于MCN机构。直播电商这块业务，未来会是我们自有品牌的一个重要销售渠道，但远不是我们业务的全部。 刘润老师说，“直播是很多人的梦想，但只是老罗通往梦想的盘缠。” 是这样的。我做产品的热情永远高于其他工作。对我来说，我做产品不是为了创业，我是为了做理想中的伟大产品，才不得不创业，并承受创业所需要的一切。 但人生常常就是这样：你努力做到了世界级的东西，因为种种原因，不但商业上没有成功，还被很多人分析成是东西不灵（不幸的是，我们已经有了至少几百万的知音，已经没法像那些孤独的天才一样郁郁而终了）；而在另外一些方面，你甚至还没来得及努力就能赚钱，虽然这钱只是实现理想的手段，而不是目的，但很多人已经在赞美你的“成功”了。 界面新闻：大家还是很好奇你是如何下定决心来做直播带货的？ 罗永浩：我过去只是对它缺乏兴趣，并没有其他的不良看法。其实，我甚至没“下定”什么“决心”，我只是因为做电商的朋友总跟我聊，所以有所了解。疫情期间没事儿干，在家做了调研，发现它有价值，然后跟几个合伙人吃了顿饭，就开始做了。 界面新闻：你现在对手机行业还关注吗？疫情发生后，手机行业在今年会遭遇一些销售和供应上的困境，厂商多少受到些影响，作为曾经的同行，你的感受是什么？你还能试着给出些建议吗？ 罗永浩：不关注。我们被迫卖掉手机业务之后，我在做其他事情赚钱准备杀回智能设备领域的时候，关注的是手机的下一代计算平台，不是手机。 界面新闻：支持你的人依然不计其数，在他们眼里你还是那个坚持自我的理想主义者，只是短暂地因为现实需求做出改变。你现在怎么看待自己？接下来的目标是什么？怎么看待未来可能出现的成功或是失败？ 罗永浩：我和“不计其数”的人看法基本一致，但我甚至不认为我做了什么“改变”。你走在追求梦想的道路上，不小心欠了钱，你继续追求梦想之前，先抽空赚些钱还债，这不能叫“改变”。 接下来可能的成功？它应该会帮我实现产品上面的那些理想吧。可能的失败？没太关心过这个，我今年四十八岁，时间上，事业的可操作性上，应该还能承受五六次的失败都没问题。至于心理上，我可以承受无数次的失败。","categories":[{"name":"其他","slug":"其他","permalink":"http://huangzhiyuan.github.io/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"新闻","slug":"新闻","permalink":"http://huangzhiyuan.github.io/tags/%E6%96%B0%E9%97%BB/"}]},{"title":"First PyTorch PR merged","slug":"first-pytorch-PR-merged","date":"2020-04-10T01:20:55.000Z","updated":"2020-04-10T01:42:34.000Z","comments":true,"path":"2020/04/10/first-pytorch-PR-merged/","link":"","permalink":"http://huangzhiyuan.github.io/2020/04/10/first-pytorch-PR-merged/","excerpt":"在等待了22天之后，第一个PyTorch pull request终于merge进去! PR虽小，但也是新的尝试，这也是继MXnet之后的第二个做过Contribution的深度学习框架了。写文记之，再接再厉！","text":"在等待了22天之后，第一个PyTorch pull request终于merge进去! PR虽小，但也是新的尝试，这也是继MXnet之后的第二个做过Contribution的深度学习框架了。写文记之，再接再厉！ MXNet Contributors link","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://huangzhiyuan.github.io/tags/pytorch/"}]},{"title":"volatile in cplusplus","slug":"volatile-in-cplusplus","date":"2020-04-02T14:48:44.000Z","updated":"2020-04-02T14:53:50.000Z","comments":true,"path":"2020/04/02/volatile-in-cplusplus/","link":"","permalink":"http://huangzhiyuan.github.io/2020/04/02/volatile-in-cplusplus/","excerpt":"volatile关键字是一种类型修饰符，用它声明的类型变量表示可以被某些编译器未知的因素更改，比如：操作系统、硬件或者其它线程等。遇到这个关键字声明的变量，编译器对访问该变量的代码就不再进行优化，从而可以提供对特殊地址的稳定访问。","text":"volatile关键字是一种类型修饰符，用它声明的类型变量表示可以被某些编译器未知的因素更改，比如：操作系统、硬件或者其它线程等。遇到这个关键字声明的变量，编译器对访问该变量的代码就不再进行优化，从而可以提供对特殊地址的稳定访问。 使用该关键字的例子如下： 1int volatile nVint; 当要求使用volatile 声明的变量的值的时候，系统总是重新从它所在的内存读取数据，即使它前面的指令刚刚从该处读取过数据。而且读取的数据立刻被保存。例如： 123456volatile int i=10;int a = i;...//其他代码，并未明确告诉编译器，对i进行过操作int b = i; volatile 指出 i是随时可能发生变化的，每次使用它的时候必须从i的地址中读取，因而编译器生成的汇编代码会重新从i的地址读取数据放在b中。而优化做法是，由于编译器发现两次从i读数据的代码之间的代码没有对i进行过操作，它会自动把上次读的数据放在b中。而不是重新从i里面读。这样以来，如果i是一个寄存器变量或者表示一个端口数据就容易出错，所以说volatile可以保证对特殊地址的稳定访问。 123456789101112131415#include &lt;stdio.h&gt;void main()&#123; int i=10; int a = i; printf(&quot;i= %d\\n&quot;,a); //下面汇编语句的作用就是改变内存中i的值，但是又不让编译器知道 __asm &#123; mov dword ptr [ebp-4], 20h &#125; int b = i; printf(&quot;i= %d\\n&quot;,b);&#125; 然后，在调试版本模式运行程序，输出结果如下： 12i = 10i = 32 然后，在release版本模式运行程序，输出结果如下： 12i = 10i = 10 输出的结果明显表明，release模式下，编译器对代码进行了优化，第二次没有输出正确的i值。下面，我们把 i的声明加上volatile关键字，看看有什么变化： 1234567891011121314#include &lt;stdio.h&gt;void main()&#123; volatile int i=10; int a = i; printf(&quot;i= %d\\n&quot;,a); __asm &#123; mov dword ptr [ebp-4], 20h &#125; int b = i; printf(&quot;i= %d\\n&quot;,b);&#125; 分别在调试版本和release版本运行程序，输出都是： 12i = 10i = 32","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"volatile","slug":"volatile","permalink":"http://huangzhiyuan.github.io/tags/volatile/"}]},{"title":"sycl vector add example","slug":"sycl-vector-add-example","date":"2020-03-30T13:49:47.000Z","updated":"2020-03-30T14:11:52.000Z","comments":true,"path":"2020/03/30/sycl-vector-add-example/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/30/sycl-vector-add-example/","excerpt":"在安装好SYCL和ComputeCpp环境之后，按照codeplay发布的Hello SYCL步骤，想要通过测试最简单的向量加法来测试本地开发环境。结果总是Build完成之后遇到Runtime error， 具体参考Issue #230。不得不说同行们的效率还是挺高，在较短的时间内给与了回复，不仅指出了出错原因，也指明了当前Hello SYCL存在的问题。为community维护的社区程序员点赞！现将此次问题总结如下：","text":"在安装好SYCL和ComputeCpp环境之后，按照codeplay发布的Hello SYCL步骤，想要通过测试最简单的向量加法来测试本地开发环境。结果总是Build完成之后遇到Runtime error， 具体参考Issue #230。不得不说同行们的效率还是挺高，在较短的时间内给与了回复，不仅指出了出错原因，也指明了当前Hello SYCL存在的问题。为community维护的社区程序员点赞！现将此次问题总结如下： 本地开发环境12345678910111213141516171819202122232425262728293031323334353637383940414243444546(base) huang@mlt:~/compute$ /usr/local/computecpp/bin/computecpp_info********************************************************************************ComputeCpp Info (CE 1.0.3)SYCL 1.2.1 revision 3********************************************************************************Toolchain information:GLIBC version: 2.23GLIBCXX: 20160609This version of libstdc++ is supported.********************************************************************************Device Info:Discovered 2 devices matching: platform : &lt;any&gt; device type : &lt;any&gt;--------------------------------------------------------------------------------Device 0: Device is supported : UNTESTED - Device not tested on this OS CL_DEVICE_NAME : Intel(R) Gen9 HD Graphics NEO CL_DEVICE_VENDOR : Intel(R) Corporation CL_DRIVER_VERSION : 19.20.13008 CL_DEVICE_TYPE : CL_DEVICE_TYPE_GPU--------------------------------------------------------------------------------Device 1: Device is supported : YES - Tested internally by Codeplay Software Ltd. CL_DEVICE_NAME : Intel(R) Core(TM) i7-8700K CPU @ 3.70GHz CL_DEVICE_VENDOR : Intel(R) Corporation CL_DRIVER_VERSION : 1.2.0.10 CL_DEVICE_TYPE : CL_DEVICE_TYPE_CPUIf you encounter problems when using any of these OpenCL devices, please consultthis website for known issues:https://computecpp.codeplay.com/releases/v1.0.3/platform-support-notes******************************************************************************** Source Codehttps://developer.codeplay.com/products/computecpp/ce/guides/sycl-guide/hello-sycl 1234567891011121314151617181920212223242526272829303132333435363738394041#include &lt;iostream&gt;#include &lt;CL/sycl.hpp&gt;class vector_addition;namespace sycl = cl::sycl;int main() &#123; sycl::float4 a = &#123;1.0, 2.0, 3.0, 4.0&#125;; sycl::float4 b = &#123;4.0, 3.0, 2.0, 3.0&#125;; sycl::float4 c = &#123;.0, 0.0, 0.0, 0.0&#125;; sycl::default_selector device_selector; sycl::queue queue(device_selector); std::cout &lt;&lt; &quot;Running on &quot; &lt;&lt; queue.get_device().get_info&lt;sycl::info::device::name&gt;() &lt;&lt; &quot;\\n&quot;; &#123; sycl::buffer&lt;sycl::float4, 1&gt; a_sycl(&amp;a, sycl::range&lt;1&gt;(1)); sycl::buffer&lt;sycl::float4, 1&gt; b_sycl(&amp;b, sycl::range&lt;1&gt;(1)); sycl::buffer&lt;sycl::float4, 1&gt; c_sycl(&amp;c, sycl::range&lt;1&gt;(1)); queue.submit([&amp;] (sycl::handler&amp; cgh) &#123; auto a_acc = a_sycl.get_access&lt;sycl::access::mode::read&gt;(cgh); auto b_acc = b_sycl.get_access&lt;sycl::access::mode::read&gt;(cgh); auto c_acc = c_sycl.get_access&lt;sycl::access::mode::discard_write&gt;(cgh); cgh.single_task&lt;class vector_addition&gt;([=] () &#123; c_acc[0] = a_acc[0] + b_acc[0]; &#125;); &#125;); &#125; std::cout &lt;&lt; &quot; A &#123; &quot; &lt;&lt; a.x() &lt;&lt; &quot;, &quot; &lt;&lt; a.y() &lt;&lt; &quot;, &quot; &lt;&lt; a.z() &lt;&lt; &quot;, &quot; &lt;&lt; a.w() &lt;&lt; &quot; &#125;\\n&quot; &lt;&lt; &quot;+ B &#123; &quot; &lt;&lt; b.x() &lt;&lt; &quot;, &quot; &lt;&lt; b.y() &lt;&lt; &quot;, &quot; &lt;&lt; b.z() &lt;&lt; &quot;, &quot; &lt;&lt; b.w() &lt;&lt; &quot; &#125;\\n&quot; &lt;&lt; &quot;------------------\\n&quot; &lt;&lt; &quot;= C &#123; &quot; &lt;&lt; c.x() &lt;&lt; &quot;, &quot; &lt;&lt; c.y() &lt;&lt; &quot;, &quot; &lt;&lt; c.z() &lt;&lt; &quot;, &quot; &lt;&lt; c.w() &lt;&lt; &quot; &#125;&quot; &lt;&lt; std::endl; return 0;&#125; How to buildBuild.sh 123456789#!/bin/bashSOUR_FILE=$1OBJ_FILE=$2compute++ -g -I/usr/local/computecpp/include $&#123;SOUR_FILE&#125; -sycl-driver -no-serial-memop -L/usr/local/computecpp/lib -lComputeCpp -o $&#123;OBJ_FILE&#125;echo &quot;Build Done!&quot;echo $2 &quot;was generated successfully!&quot; Note: 出错原因就是因为忘记添加-sycl-driver选项，导致kernel在Host端而不是Device端build。 sample code存在的一个潜在的问题是，计算完的结果（30行）并不会被写回到Host端，一个简单避免这个问题的方式就是添加一个大的括号将这段buffer写操作圈起来（19行，33行），这样可以在退出时buffer的析构函数自动触发copy back到Host端机制。 Run123456789(base) huang@mlt:~/compute$ ./build.sh hello_sycl.cpp helloBuild Done!hello was generated successfully!(base) huang@mlt:~/compute$ ./helloRunning on Intel(R) Gen9 HD Graphics NEO A &#123; 1, 2, 3, 4 &#125;+ B &#123; 4, 3, 2, 3 &#125;------------------= C &#123; 5, 5, 5, 7 &#125;","categories":[{"name":"GPU","slug":"GPU","permalink":"http://huangzhiyuan.github.io/categories/GPU/"}],"tags":[{"name":"SYCL","slug":"SYCL","permalink":"http://huangzhiyuan.github.io/tags/SYCL/"}]},{"title":"如何运行tensorboard在远程主机","slug":"how-to-run-tensorboard-on-remote-server","date":"2020-03-30T03:59:46.000Z","updated":"2020-03-30T04:08:02.000Z","comments":true,"path":"2020/03/30/how-to-run-tensorboard-on-remote-server/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/30/how-to-run-tensorboard-on-remote-server/","excerpt":"在做网络可视化的时候，经常会用到tensorboard。tensorflow程序运行在远程服务器，tensorboard启动后访问地址为：0.0.0.0:6006。这样没法用自己机器上的浏览器访问tensorboard。 方法1)使用ssh命令登录时候，添加-L选项重定向port 6006到本地端口16006 1$ ssh -L 16006:127.0.0.1:6006 olivier@my_server_ip 2)启动tensorboard命令使用默认端口号： 12tensorboard --logdir=/logpath# log with the default 6006 port 3)在本地浏览器栏输入地址：http://127.0.0.1:16006 Reference","text":"在做网络可视化的时候，经常会用到tensorboard。tensorflow程序运行在远程服务器，tensorboard启动后访问地址为：0.0.0.0:6006。这样没法用自己机器上的浏览器访问tensorboard。 方法1)使用ssh命令登录时候，添加-L选项重定向port 6006到本地端口16006 1$ ssh -L 16006:127.0.0.1:6006 olivier@my_server_ip 2)启动tensorboard命令使用默认端口号： 12tensorboard --logdir=/logpath# log with the default 6006 port 3)在本地浏览器栏输入地址：http://127.0.0.1:16006 Reference","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"tensorboard","slug":"tensorboard","permalink":"http://huangzhiyuan.github.io/tags/tensorboard/"}]},{"title":"cs179/GPU Machine Learning and cuDNN II","slug":"cs179-GPU-Programming-week6","date":"2020-03-29T08:40:48.000Z","updated":"2020-04-10T05:56:34.000Z","comments":true,"path":"2020/03/29/cs179-GPU-Programming-week6/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/29/cs179-GPU-Programming-week6/","excerpt":"The use of Graphics Processing Units for rendering is well known, but their power for general parallel computation has only recently been explored. Parallel algorithms running on GPUs can often achieve up to 100x speedup over similar CPU algorithms, with many existing applications for physics simulations, signal processing, financial modeling, neural networks, and countless other fields.This course will cover programming techniques for the GPU. The course will introduce NVIDIA’s parallel computing language, CUDA. Beyond covering the CUDA programming model and syntax, the course will also discuss GPU architecture, high performance computing on GPUs, parallel algorithms, CUDA libraries, and applications of GPU computing. Problem sets will cover performance optimization and specific GPU applications such as numerical mathematics, medical imaging, finance, and other fields. Course Link","text":"The use of Graphics Processing Units for rendering is well known, but their power for general parallel computation has only recently been explored. Parallel algorithms running on GPUs can often achieve up to 100x speedup over similar CPU algorithms, with many existing applications for physics simulations, signal processing, financial modeling, neural networks, and countless other fields.This course will cover programming techniques for the GPU. The course will introduce NVIDIA’s parallel computing language, CUDA. Beyond covering the CUDA programming model and syntax, the course will also discuss GPU architecture, high performance computing on GPUs, parallel algorithms, CUDA libraries, and applications of GPU computing. Problem sets will cover performance optimization and specific GPU applications such as numerical mathematics, medical imaging, finance, and other fields. Course Link","categories":[{"name":"CUDA","slug":"CUDA","permalink":"http://huangzhiyuan.github.io/categories/CUDA/"}],"tags":[{"name":"cuda","slug":"cuda","permalink":"http://huangzhiyuan.github.io/tags/cuda/"},{"name":"cs179","slug":"cs179","permalink":"http://huangzhiyuan.github.io/tags/cs179/"}]},{"title":"cs179/GPU Machine Learning and cuDNN I","slug":"cs179-GPU-Programming-week5","date":"2020-03-29T08:40:44.000Z","updated":"2020-05-12T08:46:44.000Z","comments":true,"path":"2020/03/29/cs179-GPU-Programming-week5/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/29/cs179-GPU-Programming-week5/","excerpt":"The use of Graphics Processing Units for rendering is well known, but their power for general parallel computation has only recently been explored. Parallel algorithms running on GPUs can often achieve up to 100x speedup over similar CPU algorithms, with many existing applications for physics simulations, signal processing, financial modeling, neural networks, and countless other fields.This course will cover programming techniques for the GPU. The course will introduce NVIDIA’s parallel computing language, CUDA. Beyond covering the CUDA programming model and syntax, the course will also discuss GPU architecture, high performance computing on GPUs, parallel algorithms, CUDA libraries, and applications of GPU computing. Problem sets will cover performance optimization and specific GPU applications such as numerical mathematics, medical imaging, finance, and other fields. Course Link","text":"The use of Graphics Processing Units for rendering is well known, but their power for general parallel computation has only recently been explored. Parallel algorithms running on GPUs can often achieve up to 100x speedup over similar CPU algorithms, with many existing applications for physics simulations, signal processing, financial modeling, neural networks, and countless other fields.This course will cover programming techniques for the GPU. The course will introduce NVIDIA’s parallel computing language, CUDA. Beyond covering the CUDA programming model and syntax, the course will also discuss GPU architecture, high performance computing on GPUs, parallel algorithms, CUDA libraries, and applications of GPU computing. Problem sets will cover performance optimization and specific GPU applications such as numerical mathematics, medical imaging, finance, and other fields. Course Link whoat is ML GOOD for? Handwriting recognition Spam detection Teaching a rebot how to do a backflip Predicting the performance of a stock portfolio What is MLWhat do these problems have in common? Some pattern we want to learn No good closed-form model for it LOTS of data What can we do?Use data to learn a statistical model for the pattern we are interested in. One data point is a vector 𝑥 in ℝ^𝑑 A 30×30 pixel image is a 900-dimensional vector (one component per pixel intensity) If we are classifying an email as spam or not spam, set 𝑑= number of words in dictionaryCount the number of times 𝑛_𝑖 that a word 𝑖 appears in an email and set 𝑥_𝑖=𝑛_𝑖 what are we trying to doGiven an input 𝑥∈ℝ^𝑑, produce an output 𝑦. What is 𝑦? Could be a real number, e.g. predicted return of a given stock portfolio Could be 0 or 1, e.g. spam or not spam Could be a vector in ℝ^𝑚, e.g. telling a robot how to move each of its 𝑚 joints example of (x, y) pairs: notation statistical models Given (𝐗, 𝐘) (𝑁 pairs of (𝑥^((𝑖)), 𝑦^((𝑖) ) ) data), how do we accurately predict an output 𝑦 given an input 𝑥? One solution: a model 𝑓(𝑥) parametrized by a vector (or matrix) 𝑤, denoted as 𝑓(𝑥;𝑤).The task is finding a set of optimal parameters 𝑤","categories":[{"name":"CUDA","slug":"CUDA","permalink":"http://huangzhiyuan.github.io/categories/CUDA/"}],"tags":[{"name":"cuda","slug":"cuda","permalink":"http://huangzhiyuan.github.io/tags/cuda/"},{"name":"cs179","slug":"cs179","permalink":"http://huangzhiyuan.github.io/tags/cs179/"}]},{"title":"cs179/GPU cuBLAS and Graphics","slug":"cs179-GPU-Programming-week4","date":"2020-03-29T08:40:39.000Z","updated":"2020-04-10T05:55:34.000Z","comments":true,"path":"2020/03/29/cs179-GPU-Programming-week4/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/29/cs179-GPU-Programming-week4/","excerpt":"The use of Graphics Processing Units for rendering is well known, but their power for general parallel computation has only recently been explored. Parallel algorithms running on GPUs can often achieve up to 100x speedup over similar CPU algorithms, with many existing applications for physics simulations, signal processing, financial modeling, neural networks, and countless other fields.This course will cover programming techniques for the GPU. The course will introduce NVIDIA’s parallel computing language, CUDA. Beyond covering the CUDA programming model and syntax, the course will also discuss GPU architecture, high performance computing on GPUs, parallel algorithms, CUDA libraries, and applications of GPU computing. Problem sets will cover performance optimization and specific GPU applications such as numerical mathematics, medical imaging, finance, and other fields. Course Link","text":"The use of Graphics Processing Units for rendering is well known, but their power for general parallel computation has only recently been explored. Parallel algorithms running on GPUs can often achieve up to 100x speedup over similar CPU algorithms, with many existing applications for physics simulations, signal processing, financial modeling, neural networks, and countless other fields.This course will cover programming techniques for the GPU. The course will introduce NVIDIA’s parallel computing language, CUDA. Beyond covering the CUDA programming model and syntax, the course will also discuss GPU architecture, high performance computing on GPUs, parallel algorithms, CUDA libraries, and applications of GPU computing. Problem sets will cover performance optimization and specific GPU applications such as numerical mathematics, medical imaging, finance, and other fields. Course Link Goals for this week: How do we use cuBLAS to accelerate linear algebra computations with already optimized implementations of the Basic Linear Algebra Subroutines (BLAS). How can we use cuBLAS to perform multiple computations in parallel. Learn about the cuBLAS API and why it sucks to read. Learn to use cuBLAS to write optimized cuda kernels for graphics, which we will also use later for machine learning. What is BLAS?https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms BLAS defines a set of common functions we would want to apply to scalars, vectors, and matrices. Libraries that implement it exist in almost all major languages. The names of these functions are opaque and hard to follow, so keeping an index nearby is useful. There are different functions for different number types Some cuBLAS functions cublasIsamax() : Is - a - max. finds the smallest (first) index in a vector that is a maximum for that vector cublasSgemm() : generalized matrix matrix multiplication with single precision floats. Also how you do Vector Vector multiplication. cublasDtrmm() : triangular matrix matrix multiplication with double precision floats. See what I mean? The symbols used throughout these slides will be consistent to the following: Scalars: 𝛂, 𝜷 Vectors: 𝛘, 𝛄 Matrices: A, B, C BLAS (Basic Linear Algebra Subprograms) was written for FORTRAN and cuBLAS follows its conventions. Matrices are indexed column major. There are 3 “levels” of functionality: Level 1: Scalar and Vector, Vector and Vector operations, 𝛄 → 𝛂𝛘 + 𝛄 Level 2: Vector and Matrix operations, 𝛄 → 𝛂A𝛘 + 𝜷𝛄 Level 3: Matrix and Matrix operations, C → 𝛂AB + 𝜷C What is cuBLAS good for?Anything that uses heavy linear algebra computations (on dense matrices) can benefit from GPU acceleration Graphics Machine learning (this will be covered next week) Computer vision Physical simulations Finance etc….. The various cuBLAS typesAll of the functions defined in cuBLAS have four versions which correspond to the four types of numbers in CUDA C S, s : single precision (32 bit) real float D, d : double precision (64 bit) real float C, c : single precision (32 bit) complex float (implemented as a float2) Z, z : double precision (64 bit) complex float H, h : half precision (16 bit) real float cuBLAS function typescublasSgemm → cublas S gemm cublas : the prefix S : single precision real float gemm : general matrix-matrix multiplication cublasHgemm Same as before except half precision cublasDgemv → cublas D gemv D : double precision real float gemv : general matrix vector multiplication Numpy vs math vs cuBLAS Error Checking Like CUDA and cuFFT, cuBLAS has a similar but slightly different status return type. cublasStatus_t We will use a similar macro to gpuErrchk and the cuFFT version to check for cuBLAS errors. Streaming Parallelism Use cudaStreamCreate() to create a stream for computation. Assign the stream to a cublas library routine using cublasSetStream() When you call the routine it will operate in parallel (asynchronously) with other streaming cublas calls to maximize parallelism. Should pass your constant scalars by reference to help maximize this benefit. Cublas Example123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;/* Includes, cuda */#include &lt;cuda_runtime.h&gt;#include &lt;cublas_v2.h&gt;#include &lt;helper_cuda.h&gt;#define IDX2C(i,j,ld) (((j)*(ld))+(i))/* Matrix size */#define N (275)a[IDX2C(0, 1, 50)]/* Host implementation of a simple version of sgemm */static void simple_sgemm(int n, float alpha, const float *A, const float *B, float beta, float *C)&#123; int i; int j; int k; for (i = 0; i &lt; n; ++i) &#123; for (j = 0; j &lt; n; ++j) &#123; float prod = 0; for (k = 0; k &lt; n; ++k) &#123; prod += A[k * n + i] * B[j * n + k]; &#125; C[j * n + i] = alpha * prod + beta * C[j * n + i]; &#125; &#125;&#125;/* Main */int main(int argc, char **argv)&#123; cublasStatus_t status; float *h_A; float *h_B; float *h_C; float *h_C_ref; float *d_A = 0; float *d_B = 0; float *d_C = 0; float alpha = 1.0f; float beta = 0.0f; int n2 = N * N; int i; float error_norm; float ref_norm; float diff; cublasHandle_t handle; status = cublasCreate(&amp;handle); /* Allocate host memory for the matrices */ h_A = (float *)malloc(n2 * sizeof(h_A[0])); h_B = (float *)malloc(n2 * sizeof(h_B[0])); h_C = (float *)malloc(n2 * sizeof(h_C[0])); /* Fill the matrices with test data */ for (i = 0; i &lt; n2; i++) &#123; h_A[i] = rand() / (float)RAND_MAX; h_B[i] = rand() / (float)RAND_MAX; h_C[i] = rand() / (float)RAND_MAX; &#125; /* Allocate device memory for the matrices */ cudaMalloc((void **)&amp;d_A, n2 * sizeof(d_A[0])); cudaMalloc((void **)&amp;d_B, n2 * sizeof(d_B[0])); cudaMalloc((void **)&amp;d_C, n2 * sizeof(d_C[0])); /* Initialize the device matrices with the host matrices */ status = cublasSetVector(n2, sizeof(h_A[0]), h_A, 1, d_A, 1); status = cublasSetVector(n2, sizeof(h_B[0]), h_B, 1, d_B, 1); status = cublasSetVector(n2, sizeof(h_C[0]), h_C, 1, d_C, 1); /* Performs operation using plain C code */ simple_sgemm(N, alpha, h_A, h_B, beta, h_C); h_C_ref = h_C; /* Performs operation using cublas */ status = cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, N, N, N, &amp;alpha, d_A, N, d_B, N, &amp;beta, d_C, N); /* Allocate host memory for reading back the result from device memory */ h_C = (float *)malloc(n2 * sizeof(h_C[0])); /* Read the result back */ status = cublasGetVector(n2, sizeof(h_C[0]), d_C, 1, h_C, 1); /* Check result against reference */ error_norm = 0; ref_norm = 0; for (i = 0; i &lt; n2; ++i) &#123; diff = h_C_ref[i] - h_C[i]; error_norm += diff * diff; ref_norm += h_C_ref[i] * h_C_ref[i]; &#125; error_norm = (float)sqrt((double)error_norm); ref_norm = (float)sqrt((double)ref_norm); /* Memory clean up */ free(h_A); free(h_B); free(h_C); free(h_C_ref); cudaFree(d_A); cudaFree(d_B); cudaFree(d_C); /* Shutdown */ status = cublasDestroy(handle);&#125;","categories":[{"name":"CUDA","slug":"CUDA","permalink":"http://huangzhiyuan.github.io/categories/CUDA/"}],"tags":[{"name":"cuda","slug":"cuda","permalink":"http://huangzhiyuan.github.io/tags/cuda/"},{"name":"cs179","slug":"cs179","permalink":"http://huangzhiyuan.github.io/tags/cs179/"}]},{"title":"cs179/GPU Reductions, FFT","slug":"cs179-GPU-Programming-week3","date":"2020-03-29T08:40:35.000Z","updated":"2020-04-10T05:35:54.000Z","comments":true,"path":"2020/03/29/cs179-GPU-Programming-week3/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/29/cs179-GPU-Programming-week3/","excerpt":"The use of Graphics Processing Units for rendering is well known, but their power for general parallel computation has only recently been explored. Parallel algorithms running on GPUs can often achieve up to 100x speedup over similar CPU algorithms, with many existing applications for physics simulations, signal processing, financial modeling, neural networks, and countless other fields.This course will cover programming techniques for the GPU. The course will introduce NVIDIA’s parallel computing language, CUDA. Beyond covering the CUDA programming model and syntax, the course will also discuss GPU architecture, high performance computing on GPUs, parallel algorithms, CUDA libraries, and applications of GPU computing. Problem sets will cover performance optimization and specific GPU applications such as numerical mathematics, medical imaging, finance, and other fields. Course Link","text":"The use of Graphics Processing Units for rendering is well known, but their power for general parallel computation has only recently been explored. Parallel algorithms running on GPUs can often achieve up to 100x speedup over similar CPU algorithms, with many existing applications for physics simulations, signal processing, financial modeling, neural networks, and countless other fields.This course will cover programming techniques for the GPU. The course will introduce NVIDIA’s parallel computing language, CUDA. Beyond covering the CUDA programming model and syntax, the course will also discuss GPU architecture, high performance computing on GPUs, parallel algorithms, CUDA libraries, and applications of GPU computing. Problem sets will cover performance optimization and specific GPU applications such as numerical mathematics, medical imaging, finance, and other fields. Course Link","categories":[{"name":"CUDA","slug":"CUDA","permalink":"http://huangzhiyuan.github.io/categories/CUDA/"}],"tags":[{"name":"cuda","slug":"cuda","permalink":"http://huangzhiyuan.github.io/tags/cuda/"},{"name":"cs179","slug":"cs179","permalink":"http://huangzhiyuan.github.io/tags/cs179/"}]},{"title":"cs179/GPU Programming Shared Memory","slug":"cs179-GPU-Programming-week2","date":"2020-03-29T08:40:30.000Z","updated":"2020-04-10T05:35:46.000Z","comments":true,"path":"2020/03/29/cs179-GPU-Programming-week2/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/29/cs179-GPU-Programming-week2/","excerpt":"The use of Graphics Processing Units for rendering is well known, but their power for general parallel computation has only recently been explored. Parallel algorithms running on GPUs can often achieve up to 100x speedup over similar CPU algorithms, with many existing applications for physics simulations, signal processing, financial modeling, neural networks, and countless other fields.This course will cover programming techniques for the GPU. The course will introduce NVIDIA’s parallel computing language, CUDA. Beyond covering the CUDA programming model and syntax, the course will also discuss GPU architecture, high performance computing on GPUs, parallel algorithms, CUDA libraries, and applications of GPU computing. Problem sets will cover performance optimization and specific GPU applications such as numerical mathematics, medical imaging, finance, and other fields. Course Link","text":"The use of Graphics Processing Units for rendering is well known, but their power for general parallel computation has only recently been explored. Parallel algorithms running on GPUs can often achieve up to 100x speedup over similar CPU algorithms, with many existing applications for physics simulations, signal processing, financial modeling, neural networks, and countless other fields.This course will cover programming techniques for the GPU. The course will introduce NVIDIA’s parallel computing language, CUDA. Beyond covering the CUDA programming model and syntax, the course will also discuss GPU architecture, high performance computing on GPUs, parallel algorithms, CUDA libraries, and applications of GPU computing. Problem sets will cover performance optimization and specific GPU applications such as numerical mathematics, medical imaging, finance, and other fields. Course Link Lecture 4 GPU Memory SystemsLatency and Throughput Latency is the delay caused by the physical speed of the hardware Throughput is the maximum rate of production/processing For CPU: CPU = low latency, low throughput CPU clock = 3 GHz (3 clocks/ns) CPU main memory latency: ~100+ ns CPU arithmetic instruction latency: ~1+ ns For GPU: GPU = high latency, high throughput GPU clock = 1 GHz (1 clock/ns) GPU main memory latency: ~300+ ns GPU arithmetic instruction latency: ~10+ ns Above numbers were for Kepler GPUs (e.g. GTX 700 series)For Fermi, latencies tend to be double that of Kepler GPUs Compute &amp; IO ThroughputGeForce GTX Titan Black (GK110 based) Compute throughput 5 TFLOPS (single precision) Global memory bandwidth 336 GB/s (84 Gfloat/s) GPU is very IO limited! IO is very often the throughput bottleneck, so its important to be smart about IO. If you want to get beyond ~900 GFLOPS, need to do multiple FLOPs per shared memory load. 1234567891011TFLOP = Teraflop; a way of measuring the power of a computer based on mathematical capability. (Capability of a processor to calculate one trillion floating-point operations per second)Memory bandwidth = rate at which data can be read from or stored into a semiconductor memory by a processorDue to design of GPU, IO (memory access) and compute happen at the same time. IO is generally the bottleneck.Based on these numbers, will GPUs perform better on algorithms will high computational density or low computational density?This is why bank conflicts matter.80% utilization on GPU is great. We’re up to around 93% utilization for single threaded CPU matrix multiplication (probably the most tuned optimized code). GPU Memory SystemsCacheA cache is a chunk of memory that sits in between a larger pool of memory and the processor Often times implemented at hardware level Has much faster access speed than the larger pool of memory When memory is requested, extra memory near the requested memory is read into a cache Amount read is cache and memory pool specificRegions of memory that will always be cached together are called cache lines This makes future accesses likely to be found in the cacheSuch accesses are called cache hits and allow much faster accessIf an access is not found in the cache, it’s called a cache miss (and there is obviously no performance gain) GPU Memory Breakdown Registers: Fast, only for one thread. The fastest form of memory on the multi-processor. Is only accessible by the thread. Has the lifetime of the thread. Local memory: For what doesn’t fit in registers, slow but cached, one thread. Resides in global memory and can be 150x slower than register or shared memory. Is only accessible by the thread. Has the lifetime of the thread. Global memory: Slow and uncached, all threads. Potentially 150x slower than register or shared memory – watch out for uncoalesced reads and writes. Accessible from either the host or device. Has the lifetime of the application—that is, it persistent between kernel launches. Shared memory: Fast, bank conflicts; limited; threads in block. Can be as fast as a register when there are no bank conflicts or when reading from the same address. Accessible by any thread of the block from which it was created. Has the lifetime of the block. L1/L2/L3 cache Constant memory: (read only) Slow, cached, all threads Texture memory: (read only) Cache optimized for 2D access, all threads Read-only cache (CC 3.5+) Global MemoryGlobal memory is separate hardware from the GPU core (containing SM’s, caches, etc). The vast majority of memory on a GPU is global memory If data doesn’t fit into global memory, you are going to have process it in chunks that do fit in global memory. GPUs have .5 - 24GB of global memory, with most now having ~2GB.Global memory latency is ~300ns on Kepler and ~600ns on Fermi Green box is GK110, red lines are global memory Accessing global memory efficientlyGlobal memory IO is the slowest form of IO on GPU except for accessing host memory (duh…)Because of this, we want to access global memory as little as possibleAccess patterns that play nicely with GPU hardware are called coalesced memory accesses. Coalesced memory accesses minimize the number of cache lines read in through these memory transactions. GPU cache lines are 128 bytes and are aligned. Misalignment can cause non-coalesced access: A coalesced access: Bank conflictsShared memory is setup as 32 banks If you divide the shared memory into 4 byte-long elements, element i lies in bank i % 32. A bank conflict occurs when 2 threads in a warp access different elements in the same bank.Bank conflicts cause serial memory accesses rather than parallelSerial anything in GPU programming = bad for performance Bank conflict examples L1 Cache Fermi - caches local &amp; global memory Kepler, Maxwell - only caches local memory same hardware as shared memory Nvidia used to allow a configurable size (16, 32, 48KB), but dropped that in recent generations each SM has its own L1 cache L2 Cache caches all global &amp; local memory accesses ~1MB in size shared by all SM’s L3 Cache Another level of cache above L2 cache Slightly slower (increased latency) than L2 cache but also larger. Constant Memory Constant memory is global memory with a special cacheUsed for constants that cannot be compiled into programConstants must be set from host before running kernel.~64KB for user, ~64KB for compiler kernel arguments are passed through constant memory Lecture 5: Synchronization and ILPSynchronizationIdeal case for parallelism: no resources shared between threads no communication needed between threads However, many algorithms that require shared resources can still be accelerated by massive parallelism of the GPU.On a CPU, you can solve synchronization issues using Locks, Semaphores, Condition Variables, etc.On a GPU, these solutions introduce too much memory and process overhead. We have simpler solutions better suited for parallel programs Use the __syncthreads() function to sync threads within a block Only works at the block levelSMs are separate from each other so can’t do better than this Similar to barrier() function in C/C++ This __synchthreads() call is very useful for kernels using shared memory. Atomic OperationsAtomic Operations are operations that ONLY happen in sequence.For example, if you want to add up N numbers by adding the numbers to a variable that starts in 0, you must add one number at a time. Don’t do this though. We’ll talk about better ways to do this in the next lecture. Only use when you have no other options. CUDA provides built in atomic operations: Use the functions: atomic&lt;op&gt;(float *address, float val); Replace with one of: Add, Sub, Exch, Min, Max, Inc, Dec, And, Or, Xor. e.g. atomicAdd(float *address, float val) for atomic addition These functions are all implemented using a function called atomicCAS(int *address, int compare, int val). CAS stands for compare and swap. The function compares *address to compare and swaps the value to val if the values are different Instruction Level Parallelism (ILP)Instruction Level Parallelism is when you avoid performances losses caused by instruction dependencies Idea: we do not have to wait until instruction n has finished to start instruction n + 1 In CUDA, also removes performances losses caused by how certain operations are handled by the hardware ILP Example:The second half of the code can’t start execution until the first half completes Sequential nature of the code due to instruction dependency has been minimized. Additionally, this code minimizes the number of memory transactions required","categories":[{"name":"CUDA","slug":"CUDA","permalink":"http://huangzhiyuan.github.io/categories/CUDA/"}],"tags":[{"name":"cuda","slug":"cuda","permalink":"http://huangzhiyuan.github.io/tags/cuda/"},{"name":"cs179","slug":"cs179","permalink":"http://huangzhiyuan.github.io/tags/cs179/"}]},{"title":"cs179/GPU Programming Introduction","slug":"cs179-GPU-Programming-week1","date":"2020-03-29T08:40:26.000Z","updated":"2020-03-29T09:56:58.000Z","comments":true,"path":"2020/03/29/cs179-GPU-Programming-week1/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/29/cs179-GPU-Programming-week1/","excerpt":"The use of Graphics Processing Units for rendering is well known, but their power for general parallel computation has only recently been explored. Parallel algorithms running on GPUs can often achieve up to 100x speedup over similar CPU algorithms, with many existing applications for physics simulations, signal processing, financial modeling, neural networks, and countless other fields.This course will cover programming techniques for the GPU. The course will introduce NVIDIA’s parallel computing language, CUDA. Beyond covering the CUDA programming model and syntax, the course will also discuss GPU architecture, high performance computing on GPUs, parallel algorithms, CUDA libraries, and applications of GPU computing. Problem sets will cover performance optimization and specific GPU applications such as numerical mathematics, medical imaging, finance, and other fields. Course Link","text":"The use of Graphics Processing Units for rendering is well known, but their power for general parallel computation has only recently been explored. Parallel algorithms running on GPUs can often achieve up to 100x speedup over similar CPU algorithms, with many existing applications for physics simulations, signal processing, financial modeling, neural networks, and countless other fields.This course will cover programming techniques for the GPU. The course will introduce NVIDIA’s parallel computing language, CUDA. Beyond covering the CUDA programming model and syntax, the course will also discuss GPU architecture, high performance computing on GPUs, parallel algorithms, CUDA libraries, and applications of GPU computing. Problem sets will cover performance optimization and specific GPU applications such as numerical mathematics, medical imaging, finance, and other fields. Course Link Lecture 1GPU Computing: Step by Step Setup inputs on the host (CPU-accessible memory) Allocate memory for outputs on the host Allocate memory for inputs on the GPU Allocate memory for outputs on the GPU Copy inputs from host to GPU Start GPU kernel (function that executed on gpu) Copy output from GPU to host NOTE: Copying can be asynchronous, and unified memory management is available The Kernels Our “parallel” function Given to each thread Simple implementation: Indexing calling the kernel Lecture 2 Intro to the simd lifestyle and GPU internalsCan use GPU to solve highly parallelizable problems. Looked at the a[] + b[] -&gt; c[] example CUDA is a straightforward extension to C++: Separate CUDA code into .cu and .cuh files We compile with nvcc, NVIDIA’s compiler for CUDA, to create object files (.o files) NVCC and g++CUDA is simply an extension of other bits of code you write!!!! Evident in .cu/.cuh vs .cpp/.hpp distinction .cu/.cuh is compiled by nvcc to produce a .o file Since CUDA 7.0 / 9.0 there’s support by NVCC for most C++11 / C++14 language features, but make sure to read restrictions for device codehttps://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#c-cplusplus-language-support .cpp/.hpp is compiled by g++ and the .o file from the CUDA code is simply linked in using a “#include xxx.cuh” call. (No different from how you link in .o files from normal C++ code) Thread Block OrganizationKeywords you MUST know to code in CUDA: Thread - Distributed by the CUDA runtime (threadIdx) Block - A user defined group of 1 to ~512 threads (blockIdx) Grid - A group of one or more blocks. A grid is created for each CUDA kernel function called. Imagine thread organization as an array of thread indices For many parallelizeable problems involving arrays, it’s useful to think of multidimensional arrays. E.g. linear algebra, physical modelling, etc, where we want to assign unique thread indices over a multidimensional object So, CUDA provides built in multidimensional thread indexing capabilities with a struct called dim3! Dimsdim3 is a struct (defined in vector_types.h) to define your Grid and Block dimensions. Works for dimensions 1, 2, and 3: dim3 grid(256); // defines a grid of 256 x 1 x 1 blocks dim3 block(512, 512); // defines a block of 512 x 512 x 1 threads foo&lt;&lt;&lt;grid, block&gt;&gt;&gt;(…); Grid/Block/Thread Visualized Single Instruction, Multiple Data (SIMD) SIMD describes a class of instructions which perform the same operation on multiple registers simultaneously. Example: Add some scalar to 3 registers, storing the output for each addition in those registers. Used to increase the brightness of a pixel CPUs also have SIMD instructions and are very important for applications that need to do a lot of number crunching. Video codecs like x264/x265 make extensive use of SIMD instructions to speed up video encoding and decoding. SIMD continuedConverting an algorithm to use SIMD is usually called “Vectorizing” Not every algorithm can benefit from this or even be vectorized at all, e.x. Parsing. Using SIMD instructions is not always beneficial though. 1) Even using the SIMD hardware requires additional power, and thus waste heat. 2) If the gains are small it probably isn’t worth the additional complexity. Optimizing compilers like GCC and LLVM are still being trained to be able to vectorize code usefully, though there has been many exciting developments on this front in the last 2 years and is an active area of study. https://polly.llvm.org/ SIMT (Single Instruction, Multiple Thread) ArchitectureA looser extension of SIMD which is what CUDA’s computational model uses. Key differences: Single instruction, multiple register setsWastes some registers, but mostly necessary for following two points Single instruction, multiple addresses (i.e. parallel memory access!)Memory access conflicts! Will discuss next week. Single instruction, multiple flow paths (i.e. if statements are allowed!!!)Introduces slowdowns, called ‘warp-divergence.’ Good description of differenceshttps://yosefk.com/blog/simd-simt-smt-parallelism-in-nvidia-gpus.htmlhttps://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation Important CUDA Hardware Keywords Streaming Multiprocessor (SM), Each contains (usually) 128 single precision CUDA cores (which execute a thread) and their associated cache. This is a standard based on your machines Compute Capability. Warp – A unit of up to 32 threads (all within the same block). Each SM creates and manages multiple warps via the block abstraction. Assigns to each warp a Warp Scheduler to schedule the execution of instructions in each warp. Warp Divergence. A condition where threads within a warp need to execute different instructions in order to continue executing their kernel. 1)In order to maintain multiple flow path per instruction, threads in different ‘execution branches’ during an instruction are given no-ops.2) Causes threads to execute sequentially, in most cases ruining parallel performance.3) As of the Kepler (2012) architecture each Warp can have at most 2 branches, starting with Volta (2017) this condition has been nearly eliminated. For this class assume your code must only branch at most twice as we are not yet allocating Volta GPUs to this class. Inpendent Thread Scheduling fixes this problem by maintianing an execution state per thread. See Compute Capability 7.x What a modern GPU looks like inside a GPUThink of Device Memory (we will also refer to it as Global Memory) as a RAM for your GPU Faster than getting memory from the actual RAM but still have other options Will come back to this in future lectures GPUs have many Streaming Multiprocessors (SMs) Each SM has multiple processors but only one instruction unit (each thread shares program counter) Groups of processors must run the exact same set of instructions at any given time with in a single SM When a kernel (the thing you define in .cu files) is called, the task is divided up into threads Each thread handles a small portion of the given task The threads are divided into a Grid of Blocks. Both Grids and Blocks are 3 dimensionale.g. 123dim3 dimBlock(8, 8, 8);dim3 dimGrid(100, 100, 1);Kernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(…); However, we’ll often only work with 1 dimensional grids and blockse.g. 1Kernel&lt;&lt;&lt;block_count, block_size&gt;&gt;&gt;(…); Maximum number of threads per block count is usually 512 or 1024 depending on the machineMaximum number of blocks per grid is usually 65535. If you go over either of these numbers your GPU will just give up or output garbage data Much of GPU programming is dealing with this kind of hardware limitations! Get used to it This limitation also means that your Kernel must compensate for the fact that you may not have enough threads to individually allocate to your data points. Each block is assigned to an SMInside the SM, the block is divided into Warps of threads Warps consist of 32 threads. CUDA defined constant in cuda_runtime.h All 32 threads MUST run the exact same set of instructions at the same time. Due to the fact that there is only one instruction unit Warps are run concurrently in an SM If your Kernel tries to have threads do different things in a single warp (using if statements for example), the two tasks will be run sequentially. Called Warp Divergence (NOT GOOD) In Fermi Architecture (i.e. GPUs with Compute Capability 2.x), each SM has 32 cores, later architectures have more. e.g. GTX 400, 500 series 32 cores is not what makes each warp have 32 threads. Previous architecture also had 32 threads per warp but had less than 32 cores per SM Some early Pascal (2016) GPUs (GP100) had 64 cores per SM, but later chips in that generation (GP104) had 128 core model. Turing (2018) maintains 128 core standard Shown here is a Pascal GP104 GPU Streaming Multiprocessor that can be found in a GTX1080 graphics card. The exact amount of Cache and Shared Memory differ between GPU models, and even more so between different architectures. Whitepapers with exact information can be gotten from Nvidia (use Google)https://international.download.nvidia.com/geforce-com/international/pdfs/GeForce_GTX_1080_Whitepaper_FINAL.pdfhttp://www.nvidia.com/content/PDF/product-specifications/GeForce_GTX_680_Whitepaper_FINAL.pdf","categories":[{"name":"CUDA","slug":"CUDA","permalink":"http://huangzhiyuan.github.io/categories/CUDA/"}],"tags":[{"name":"cuda","slug":"cuda","permalink":"http://huangzhiyuan.github.io/tags/cuda/"},{"name":"cs179","slug":"cs179","permalink":"http://huangzhiyuan.github.io/tags/cs179/"}]},{"title":"cuda-c-programming-guide-chapter-5","slug":"cuda-c-programming-guide-chapter-5","date":"2020-03-28T10:03:08.000Z","updated":"2020-03-28T10:03:10.000Z","comments":true,"path":"2020/03/28/cuda-c-programming-guide-chapter-5/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/28/cuda-c-programming-guide-chapter-5/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"cuda-c-programming-guide-chapter-4","slug":"cuda-c-programming-guide-chapter-4","date":"2020-03-28T10:03:03.000Z","updated":"2020-03-28T10:03:04.000Z","comments":true,"path":"2020/03/28/cuda-c-programming-guide-chapter-4/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/28/cuda-c-programming-guide-chapter-4/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"CUDA-Programming Inferface","slug":"cuda-c-programming-guide-chapter-3","date":"2020-03-28T10:02:59.000Z","updated":"2020-04-01T14:07:32.000Z","comments":true,"path":"2020/03/28/cuda-c-programming-guide-chapter-3/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/28/cuda-c-programming-guide-chapter-3/","excerpt":"CUDA C++ provides a simple path for users familiar with the C++ programming language to easily write programs for execution by the device. It consists of a minimal set of extensions to the C++ language and a runtime library. The core language extensions have been introduced in Programming Model. They allow programmers to define a kernel as a C++ function and use some new syntax to specify the grid and block dimension each time the function is called. A complete description of all extensions can be found in C++ Language Extensions. Any source file that contains some of these extensions must be compiled with nvcc as outlined in Compilation with NVCC. Five parts are included. Compilation with NVCC CUDA Runtime External Resource Interoperability Versioning and Compatibility compute Modes Mode Switches Tesla Compute Cluster Mode for windows","text":"CUDA C++ provides a simple path for users familiar with the C++ programming language to easily write programs for execution by the device. It consists of a minimal set of extensions to the C++ language and a runtime library. The core language extensions have been introduced in Programming Model. They allow programmers to define a kernel as a C++ function and use some new syntax to specify the grid and block dimension each time the function is called. A complete description of all extensions can be found in C++ Language Extensions. Any source file that contains some of these extensions must be compiled with nvcc as outlined in Compilation with NVCC. Five parts are included. Compilation with NVCC CUDA Runtime External Resource Interoperability Versioning and Compatibility compute Modes Mode Switches Tesla Compute Cluster Mode for windows The runtime is introduced in CUDA Runtime. It provides C and C++ functions that execute on the host to allocate and deallocate device memory, transfer data between host memory and device memory, manage systems with multiple devices, etc. A complete description of the runtime can be found in the CUDA reference manual.","categories":[{"name":"CUDA","slug":"CUDA","permalink":"http://huangzhiyuan.github.io/categories/CUDA/"}],"tags":[{"name":"cuda","slug":"cuda","permalink":"http://huangzhiyuan.github.io/tags/cuda/"}]},{"title":"CUDA-Programming Model","slug":"cuda-c-programming-guide-chapter-2","date":"2020-03-28T10:02:54.000Z","updated":"2020-03-28T12:45:12.000Z","comments":true,"path":"2020/03/28/cuda-c-programming-guide-chapter-2/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/28/cuda-c-programming-guide-chapter-2/","excerpt":"This chapter introduces the main concepts behind the CUDA programming model by outlining how they are exposed in C++. An extensive description of CUDA C++ is given in Programming Interface.Full code for the vector addition example used in this chapter and the next can be found in the vectorAdd CUDA sample.Five parts are included. Kernels Thread hierarchy Memory hierarchy Heterogeneous Programming compute capability","text":"This chapter introduces the main concepts behind the CUDA programming model by outlining how they are exposed in C++. An extensive description of CUDA C++ is given in Programming Interface.Full code for the vector addition example used in this chapter and the next can be found in the vectorAdd CUDA sample.Five parts are included. Kernels Thread hierarchy Memory hierarchy Heterogeneous Programming compute capability KernelsCUDA C++ extends C++ by allowing the programmer to define C++ functions, called kernels, that, when called, are executed N times in parallel by N different CUDA threads, as opposed to only once like regular C++ functions. A kernel is defined using the **global* declaration specifier and the number of CUDA threads that execute that kernel for a given kernel call is specified using a new &lt;&lt;&lt;…&gt;&gt;&gt; execution configuration syntax (see C++ Language Extensions). Each thread that executes the kernel is given a unique thread ID that is accessible within the kernel through built-in variables. As an illustration, the following sample code, using the built-in variable threadIdx, adds two vectors A and B of size N and stores the result into vector C: 1234567891011121314// Kernel definition__global__ void VecAdd(float* A, float* B, float* C)&#123; int i = threadIdx.x; C[i] = A[i] + B[i];&#125;int main()&#123; ... // Kernel invocation with N threads VecAdd&lt;&lt;&lt;1, N&gt;&gt;&gt;(A, B, C); ...&#125; Here, each of the N threads that execute `VecAdd()`` performs one pair-wise addition. Thread hierarchyFor convenience, threadIdx is a 3-component vector, so that threads can be identified using a one-dimensional, two-dimensional, or three-dimensional thread index, forming a one-dimensional, two-dimensional, or three-dimensional block of threads, called a thread block. This provides a natural way to invoke computation across the elements in a domain such as a vector, matrix, or volume. The index of a thread and its thread ID relate to each other in a straightforward way: For a one-dimensional block, they are the same; for a two-dimensional block of size (Dx, Dy),the thread ID of a thread of index (x, y) is (x + y Dx); for a three-dimensional block of size (Dx, Dy, Dz), the thread ID of a thread of index (x, y, z) is (x + y Dx + z Dx Dy). As an example, the following code adds two matrices A and B of size NxN and stores the result into matrix C: 123456789101112131415161718// Kernel definition__global__ void MatAdd(float A[N][N], float B[N][N], float C[N][N])&#123; int i = threadIdx.x; int j = threadIdx.y; C[i][j] = A[i][j] + B[i][j];&#125;int main()&#123; ... // Kernel invocation with one block of N * N * 1 threads int numBlocks = 1; dim3 threadsPerBlock(N, N); MatAdd&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(A, B, C); ...&#125; There is a limit to the number of threads per block, since all threads of a block are expected to reside on the same processor core and must share the limited memory resources of that core. On current GPUs, a thread block may contain up to 1024 threads. However, a kernel can be executed by multiple equally-shaped thread blocks, so that the total number of threads is equal to the number of threads per block times the number of blocks. Blocks are organized into a one-dimensional, two-dimensional, or three-dimensional grid of thread blocks as illustrated by Figure 6. The number of thread blocks in a grid is usually dictated by the size of the data being processed, which typically exceeds the number of processors in the system. Figure 6. Grid of Thread Blocks The number of threads per block and the number of blocks per grid specified in the &lt;&lt;&lt;…&gt;&gt;&gt; syntax can be of type int or dim3. Two-dimensional blocks or grids can be specified as in the example above. Each block within the grid can be identified by a one-dimensional, two-dimensional, or three-dimensional unique index accessible within the kernel through the built-in blockIdx variable. The dimension of the thread block is accessible within the kernel through the built-in blockDim variable. Extending the previous MatAdd() example to handle multiple blocks, the code becomes as follows. 12345678910111213141516171819// Kernel definition__global__ void MatAdd(float A[N][N], float B[N][N],float C[N][N])&#123; int i = blockIdx.x * blockDim.x + threadIdx.x; int j = blockIdx.y * blockDim.y + threadIdx.y; if (i &lt; N &amp;&amp; j &lt; N) C[i][j] = A[i][j] + B[i][j];&#125;int main()&#123; ... // Kernel invocation dim3 threadsPerBlock(16, 16); dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y); MatAdd&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(A, B, C); ...&#125; A thread block size of 16x16 (256 threads), although arbitrary in this case, is a common choice. The grid is created with enough blocks to have one thread per matrix element as before. For simplicity, this example assumes that the number of threads per grid in each dimension is evenly divisible by the number of threads per block in that dimension, although that need not be the case. Thread blocks are required to execute independently: It must be possible to execute them in any order, in parallel or in series. This independence requirement allows thread blocks to be scheduled in any order across any number of cores as illustrated by Figure 5, enabling programmers to write code that scales with the number of cores. Threads within a block can cooperate by sharing data through some shared memory and by synchronizing their execution to coordinate memory accesses. More precisely, one can specify synchronization points in the kernel by calling the __syncthreads() intrinsic function; __syncthreads() acts as a barrier at which all threads in the block must wait before any is allowed to proceed. Shared Memory gives an example of using shared memory. In addition to __syncthreads(), the Cooperative Groups API provides a rich set of thread-synchronization primitives. For efficient cooperation, the shared memory is expected to be a low-latency memory near each processor core (much like an L1 cache) and __syncthreads() is expected to be lightweight. Memory hierarchyCUDA threads may access data from multiple memory spaces during their execution as illustrated by Figure 7. Each thread has private local memory. Each thread block has shared memory visible to all threads of the block and with the same lifetime as the block. All threads have access to the same global memory. There are also two additional read-only memory spaces accessible by all threads: the constant and texture memory spaces. The global, constant, and texture memory spaces are optimized for different memory usages (see Device Memory Accesses). Texture memory also offers different addressing modes, as well as data filtering, for some specific data formats (see Texture and Surface Memory). The global, constant, and texture memory spaces are persistent across kernel launches by the same application. Figure 7. Memory Hierarchy Heterogeneous ProgrammingAs illustrated by Figure 8, the CUDA programming model assumes that the CUDA threads execute on a physically separate device that operates as a coprocessor to the host running the C++ program. This is the case, for example, when the kernels execute on a GPU and the rest of the C++ program executes on a CPU. The CUDA programming model also assumes that both the host and the device maintain their own separate memory spaces in DRAM, referred to as host memory and device memory, respectively. Therefore, a program manages the global, constant, and texture memory spaces visible to kernels through calls to the CUDA runtime (described in Programming Interface). This includes device memory allocation and deallocation as well as data transfer between host and device memory. Unified Memory provides managed memory to bridge the host and device memory spaces. Managed memory is accessible from all CPUs and GPUs in the system as a single, coherent memory image with a common address space. This capability enables oversubscription of device memory and can greatly simplify the task of porting applications by eliminating the need to explicitly mirror data on host and device. See Unified Memory Programming for an introduction to Unified Memory. Figure 8. Heterogeneous ProgrammingNote: Serial code executes on the host while parallel code executes on the device. compute capabilityThe compute capability of a device is represented by a version number, also sometimes called its “SM version”. This version number identifies the features supported by the GPU hardware and is used by applications at runtime to determine which hardware features and/or instructions are available on the present GPU. The compute capability comprises a major revision number X and a minor revision number Y and is denoted by X.Y. Devices with the same major revision number are of the same core architecture. The major revision number is 7 for devices based on the Volta architecture, 6 for devices based on the Pascal architecture, 5 for devices based on the Maxwell architecture, 3 for devices based on the Kepler architecture, 2 for devices based on the Fermi architecture, and 1 for devices based on the Tesla architecture. The minor revision number corresponds to an incremental improvement to the core architecture, possibly including new features. Turing is the architecture for devices of compute capability 7.5, and is an incremental update based on the Volta architecture. CUDA-Enabled GPUs lists of all CUDA-enabled devices along with their compute capability. Compute Capabilities gives the technical specifications of each compute capability. Note: The compute capability version of a particular GPU should not be confused with the CUDA version (e.g., CUDA 7.5, CUDA 8, CUDA 9), which is the version of the CUDA software platform. The CUDA platform is used by application developers to create applications that run on many generations of GPU architectures, including future GPU architectures yet to be invented. While new versions of the CUDA platform often add native support for a new GPU architecture by supporting the compute capability version of that architecture, new versions of the CUDA platform typically also include software features that are independent of hardware generation.The Tesla and Fermi architectures are no longer supported starting with CUDA 7.0 and CUDA 9.0, respectively.","categories":[{"name":"CUDA","slug":"CUDA","permalink":"http://huangzhiyuan.github.io/categories/CUDA/"}],"tags":[{"name":"cuda","slug":"cuda","permalink":"http://huangzhiyuan.github.io/tags/cuda/"}]},{"title":"CUDA-Introduction","slug":"cuda-c-programming-guide-chapter-1","date":"2020-03-28T10:02:20.000Z","updated":"2020-03-28T10:37:06.000Z","comments":true,"path":"2020/03/28/cuda-c-programming-guide-chapter-1/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/28/cuda-c-programming-guide-chapter-1/","excerpt":"This series is about CUDA C programming guide. The post is chapter one.Four parts are included. from graphics processing to general purpose parallel computing CUDA - A General-Purpose Parallel Computing Platform and Programming Model A scalable programming Model document structure","text":"This series is about CUDA C programming guide. The post is chapter one.Four parts are included. from graphics processing to general purpose parallel computing CUDA - A General-Purpose Parallel Computing Platform and Programming Model A scalable programming Model document structure From Graphics Processing to General Purpose Parallel ComputingDriven by the insatiable market demand for realtime, high-definition 3D graphics,the programmable Graphic Processor Unit or GPU has evolved into a highly parallel,multithreaded, manycore processor with tremendous computational horsepower andvery high memory bandwidth.The reason behind the discrepancy in floating-point capability between the CPU and the GPU is that the GPU is specialized for highly parallel computation - exactly what graphics rendering is about - and therefore designed such that more transistors are devoted to data processing rather than data caching and flow control, as schematically illustrated by Figure 3. This conceptually works for highly parallel computations because the GPU can hide memory access latencies with computation instead of avoiding memory access latencies through large data caches and flow control. Data-parallel processing maps data elements to parallel processing threads. Many applications that process large data sets can use a data-parallel programming model to speed up the computations. In 3D rendering, large sets of pixels and vertices are mapped to parallel threads. Similarly, image and media processing applications such aspost-processing of rendered images, video encoding and decoding, image scaling, stereovision, and pattern recognition can map image blocks and pixels to parallel processing threads. In fact, many algorithms outside the field of image rendering and processing are accelerated by data-parallel processing, from general signal processing or physics simulation to computational finance or computational biology. CUDA - A General-Purpose Parallel Computing Platform and Programming ModelIn November 2006, NVIDIA introduced CUDA®, a general purpose parallel computing platform and programming model that leverages the parallel compute engine in NVIDIA GPUs to solve many complex computational problems in a more efficient way than on a CPU.CUDA comes with a software environment that allows developers to use C++ as a highlevel programming language. As illustrated by Figure 4, other languages, application programming interfaces, or directives-based approaches are supported, such as FORTRAN, DirectCompute, OpenACC. A scalable programming ModelThe advent of multicore CPUs and manycore GPUs means that mainstream processor chips are now parallel systems. The challenge is to develop application software that transparently scales its parallelism to leverage the increasing number of processor cores, much as 3D graphics applications transparently scale their parallelism to manycore GPUs with widely varying numbers of cores.The CUDA parallel programming model is designed to overcome this challenge while maintaining a low learning curve for programmers familiar with standard programming languages such as C. At its core are three key abstractions - a hierarchy of thread groups, shared memories, and barrier synchronization - that are simply exposed to the programmer as a minimal set of language extensions. These abstractions provide fine-grained data parallelism and thread parallelism, nested within coarse-grained data parallelism and task parallelism. They guide the programmer to partition the problem into coarse sub-problems that can be solved independently in parallel by blocks of threads, and each sub-problem into finer pieces that can be solved cooperatively in parallel by all threads within the block. A GPU is built around an array of Streaming Multiprocessors (SMs) (see Hardware Implementation for more details). A multithreaded program is partitioned into blocks of threads that execute independently from each other, so that a GPU with more multiprocessors will automatically execute the program in less time than a GPU with fewer multiprocessors. document structureThis document is organized into the following chapters: Chapter Introduction is a general introduction to CUDA. Chapter Programming Model outlines the CUDA programming model. Chapter Programming Interface describes the programming interface. Chapter Hardware Implementation describes the hardware implementation. Chapter Performance Guidelines gives some guidance on how to achieve maximum performance. Appendix CUDA-Enabled GPUs lists all CUDA-enabled devices. Appendix C++ Language Extensions is a detailed description of all extensions to the C++ language. Appendix Cooperative Groups describes synchronization primitives for various groups of CUDA threads. Appendix CUDA Dynamic Parallelism describes how to launch and synchronize one kernel from another. Appendix Mathematical Functions lists the mathematical functions supported in CUDA. Appendix C++ Language Support lists the C++ features supported in device code. Appendix Texture Fetching gives more details on texture fetching Appendix Compute Capabilities gives the technical specifications of various devices, as well as more architectural details. Appendix Driver API introduces the low-level driver API. Appendix CUDA Environment Variables lists all the CUDA environment variables. Appendix Unified Memory Programming introduces the Unified Memoryprogramming model.","categories":[{"name":"CUDA","slug":"CUDA","permalink":"http://huangzhiyuan.github.io/categories/CUDA/"}],"tags":[{"name":"cuda","slug":"cuda","permalink":"http://huangzhiyuan.github.io/tags/cuda/"}]},{"title":"在SYCL device调用DNNL library纯C++文件实现","slug":"batchnorm-sycl-mkldnn-ut","date":"2020-03-25T14:48:23.000Z","updated":"2020-03-27T09:29:06.000Z","comments":true,"path":"2020/03/25/batchnorm-sycl-mkldnn-ut/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/25/batchnorm-sycl-mkldnn-ut/","excerpt":"之前遇到的DNNl library加速库的UT无论是CPU还是GPU都是以benchdnn的方式进行结果正确性测试，在深度学习框架调用dnnl相应API。但是有些结果的数值正确性却无法使用benchdnn完全脱离框架复现，因此这里以batch norm op的forward和backward为例，以纯c++的方式完成UT的书写。","text":"之前遇到的DNNl library加速库的UT无论是CPU还是GPU都是以benchdnn的方式进行结果正确性测试，在深度学习框架调用dnnl相应API。但是有些结果的数值正确性却无法使用benchdnn完全脱离框架复现，因此这里以batch norm op的forward和backward为例，以纯c++的方式完成UT的书写。 需要环境： SYCL backend (gen9 machine) computecpp (version 1.0.4) dnnl (dev-v2 branch) code123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215#include &quot;mkldnn.hpp&quot;#include &lt;CL/sycl.hpp&gt;// namespace sycl = cl::sycl;using namespace mkldnn;void BN_TEST() &#123; float epsilon = 0.001; int32_t N = 2; int32_t C = 2; int32_t H = 2; int32_t W = 2; int32_t all_element = N * C * H * W; auto data_t = memory::data_type::bf16; auto dnnl_format = memory::format_tag::nchw; memory::dims input_dims = &#123;N,C,H,W&#125;; memory::dims weight_dims = &#123;C&#125;; memory::dims bias_dims = &#123;C&#125;; memory::dims output_dims = &#123;N,C,H,W&#125;; auto input_md = memory::desc(&#123;input_dims&#125;, data_t, dnnl_format); auto propagation = prop_kind::forward_training; normalization_flags flags = normalization_flags::use_scale_shift; auto bnorm_fwd_d = batch_normalization_forward::desc(propagation, input_md, epsilon, flags); auto engine = mkldnn::engine(mkldnn::engine::kind::gpu, 0); auto bnorm_fwd_pd = batch_normalization_forward::primitive_desc(bnorm_fwd_d, engine); auto input_usr_memory = memory(&#123;&#123;&#123;input_dims&#125;, data_t, dnnl_format&#125;, engine&#125;); auto output_usr_memory = memory(&#123;&#123;&#123;output_dims&#125;, data_t, dnnl_format&#125;, engine&#125;); auto weight_bias_memory = memory(bnorm_fwd_pd.weights_desc(), engine); auto mean_memory = memory(bnorm_fwd_pd.mean_desc(), engine); auto var_memory = memory(bnorm_fwd_pd.variance_desc(), engine); // sycl_set_mkldnn_buffer cl::sycl::buffer&lt;unsigned short&gt; buff_input(cl::sycl::range&lt;1&gt;(16)); &#123; auto ba = buff_input.get_access&lt;cl::sycl::access::mode::write&gt;(); // Convert float to unsigned short (bf16) for (size_t i = 0; i &lt; 16; i++) &#123; float src = 1; uint32_t res = 0; std::memcpy(&amp;res, &amp;src, sizeof(res)); ba[i] = res &gt;&gt; 16; &#125; &#125; input_usr_memory.template set_sycl_buffer&lt;unsigned short, 1&gt;(buff_input); cl::sycl::buffer&lt;unsigned short&gt; buff_output(cl::sycl::range&lt;1&gt;(16)); output_usr_memory.template set_sycl_buffer&lt;unsigned short, 1&gt;(buff_output); cl::sycl::buffer&lt;float&gt; buff_weight(cl::sycl::range&lt;1&gt;(2 * C)); &#123; auto ba = buff_weight.get_access&lt;cl::sycl::access::mode::write&gt;(); ba[0] = 1; ba[1] = 1; ba[2] = 0; ba[3] = 0; // for (size_t i = 0; i &lt; 4; i++) &#123; // ba[i] = i; // &#125; &#125; weight_bias_memory.template set_sycl_buffer&lt;float, 1&gt;(buff_weight); cl::sycl::buffer&lt;float&gt; buff_mean(cl::sycl::range&lt;1&gt;(2)); mean_memory.template set_sycl_buffer&lt;float, 1&gt;(buff_mean); cl::sycl::buffer&lt;float&gt; buff_var(cl::sycl::range&lt;1&gt;(2)); var_memory.template set_sycl_buffer&lt;float, 1&gt;(buff_var); std::shared_ptr&lt;mkldnn::primitive&gt; bn_fwd; auto strm = mkldnn::stream(engine); bn_fwd.reset(new batch_normalization_forward(bnorm_fwd_pd)); std::unordered_map&lt;int, mkldnn::memory&gt; args = &#123; &#123;MKLDNN_ARG_SRC, input_usr_memory&#125;, &#123;MKLDNN_ARG_DST, output_usr_memory&#125;, &#123;MKLDNN_ARG_SCALE_SHIFT, weight_bias_memory&#125;, &#123;MKLDNN_ARG_MEAN, mean_memory&#125;, &#123;MKLDNN_ARG_VARIANCE, var_memory&#125;, &#125;; bn_fwd-&gt;execute(strm, args); // TEST Forward auto input_acc = buff_input.get_access&lt;cl::sycl::access::mode::read&gt;(); printf(&quot;in ( &quot;); for (int i = 0; i &lt; 16; i++) &#123; float res = 0; uint32_t tmp = input_acc[i]; tmp &lt;&lt;= 16; std::memcpy(&amp;res, &amp;tmp, sizeof(tmp)); printf(&quot;%f &quot;, res); // printf(&quot;%f &quot;, (float)input_acc[i]); &#125; printf(&quot;)\\n&quot;); auto weight_acc = buff_weight.get_access&lt;cl::sycl::access::mode::read&gt;(); printf(&quot;weight ( &quot;); for (int i = 0; i &lt; 2 * 2; i++) &#123; printf(&quot;%f &quot;, weight_acc[i]); &#125; printf(&quot;)\\n&quot;); auto mean_acc = buff_mean.get_access&lt;cl::sycl::access::mode::read&gt;(); printf(&quot;mean ( &quot;); for (int i = 0; i &lt; 2; i++) &#123; printf(&quot;%f &quot;, mean_acc[i]); &#125; printf(&quot;)\\n&quot;); auto var_acc = buff_var.get_access&lt;cl::sycl::access::mode::read&gt;(); printf(&quot;var ( &quot;); for (int i = 0; i &lt; 2; i++) &#123; printf(&quot;%f &quot;, var_acc[i]); &#125; printf(&quot;)\\n&quot;); auto out_acc = buff_output.get_access&lt;cl::sycl::access::mode::read&gt;(); printf(&quot;out ( &quot;); for (int i = 0; i &lt; 16; i++) &#123; // Convert unsigned short (bf16) to float float res = 0; uint32_t tmp = out_acc[i]; tmp &lt;&lt;= 16; std::memcpy(&amp;res, &amp;tmp, sizeof(tmp)); printf(&quot;%f &quot;, res); // printf(&quot;%d &quot;, out_acc[i]); &#125; printf(&quot;)\\n&quot;); auto pk_bwd = prop_kind::backward; auto grad_output_md = memory::desc(&#123;input_dims&#125;, data_t, dnnl_format); auto bwd_desc = batch_normalization_backward::desc(pk_bwd, grad_output_md, input_md, epsilon, flags); auto bn_bwd_pd = batch_normalization_backward::primitive_desc( bwd_desc, engine, bnorm_fwd_pd); auto grad_input_memory = memory(&#123;&#123;&#123;input_dims&#125;, data_t, dnnl_format&#125;, engine&#125;); auto grad_output_memory = memory(&#123;&#123;&#123;output_dims&#125;, data_t, dnnl_format&#125;, engine&#125;); auto grad_weight_bias_memory = memory(bn_bwd_pd.diff_weights_desc(), engine); // sycl_set_mkldnn_buffer grad_input_memory cl::sycl::buffer&lt;unsigned short&gt; buff_grad_input(cl::sycl::range&lt;1&gt;(16)); grad_input_memory.template set_sycl_buffer&lt;unsigned short, 1&gt;(buff_grad_input); // sycl_set_mkldnn_buffer grad_output_memory cl::sycl::buffer&lt;unsigned short&gt; buff_grad_output(cl::sycl::range&lt;1&gt;(16)); &#123; auto ba = buff_grad_output.get_access&lt;cl::sycl::access::mode::write&gt;(); for (size_t i = 0; i &lt; 16; i++) &#123; float src = 1; uint32_t res = 0; std::memcpy(&amp;res, &amp;src, sizeof(res)); ba[i] = res &gt;&gt; 16; // ba[i] = 1; &#125; &#125; grad_output_memory.template set_sycl_buffer&lt;unsigned short, 1&gt;(buff_grad_output); // sycl_set_mkldnn_buffer grad_weight_bias_memory cl::sycl::buffer&lt;float&gt; buff_grad_weight_bias(cl::sycl::range&lt;1&gt;(2 * 2)); grad_weight_bias_memory.template set_sycl_buffer&lt;float, 1&gt;(buff_grad_weight_bias); std::shared_ptr&lt;mkldnn::primitive&gt; bn_bwd; bn_bwd.reset(new batch_normalization_backward(bn_bwd_pd)); std::unordered_map&lt;int, memory&gt; bwd_args = &#123; &#123;MKLDNN_ARG_SRC, input_usr_memory&#125;, &#123;MKLDNN_ARG_SCALE_SHIFT, weight_bias_memory&#125;, &#123;MKLDNN_ARG_MEAN, mean_memory&#125;, &#123;MKLDNN_ARG_VARIANCE, var_memory&#125;, &#123;MKLDNN_ARG_DIFF_DST, grad_output_memory&#125;, &#123;MKLDNN_ARG_DIFF_SRC, grad_input_memory&#125;, &#123;MKLDNN_ARG_DIFF_SCALE_SHIFT, grad_weight_bias_memory&#125;, &#125;; bn_bwd-&gt;execute(strm, bwd_args); auto grad_input_acc = buff_grad_input.get_access&lt;cl::sycl::access::mode::read&gt;(); printf(&quot;grad input ( &quot;); for (int i = 0; i &lt; 16; i++) &#123; float res = 0; uint32_t tmp = grad_input_acc[i]; tmp &lt;&lt;= 16; std::memcpy(&amp;res, &amp;tmp, sizeof(tmp)); printf(&quot;%f &quot;, res); // printf(&quot;%f &quot;, grad_input_acc[i]); &#125; printf(&quot;)\\n&quot;); auto grad_weight_bias_acc = buff_grad_weight_bias.get_access&lt;cl::sycl::access::mode::read&gt;(); printf(&quot;grad weight bais ( &quot;); for (int i = 0; i &lt; 2 * 2; i++) &#123; // float res = 0; // uint32_t tmp = grad_weight_bias_acc[i]; // tmp &lt;&lt;= 16; // std::memcpy(&amp;res, &amp;tmp, sizeof(tmp)); // printf(&quot;%f &quot;, res); printf(&quot;%f &quot;, grad_weight_bias_acc[i]); &#125; printf(&quot;)\\n&quot;);&#125;int main()&#123; BN_TEST(); printf(&quot;BN_TEST DONE!\\n&quot;); return 0;&#125; Build1g++ -g -o bn -std=c++11 -I/home/huang/mkl-dnn/build/include -I/home/huang/mkl-dnn/include -I/usr/local/computecpp/include test_bn.cc -L/usr/local/computecpp/lib/ -L/home/huang/mkl-dnn/build/src/ -ldnnl -ldl -lComputeCpp 2&gt;&amp;1 | tee log Run12345678910$ export MKLDNN_VERBOSE=1(base) huang@mlt:~/lnorm_test/bn$ ./bndnnl_verbose,info,DNNL v1.91.0 (commit 9d45cad761dc4dda9ebb34c601fe546b1a9edc5e)dnnl_verbose,info,cpu,runtime:SYCLdnnl_verbose,info,cpu,isa:Intel AVX2dnnl_verbose,info,gpu,runtime:SYCLdnnl_verbose,info,cpu,engine,0,name:Intel(R) Core(TM) i7-8700K CPU @ 3.70GHz,driver_version:1.2.0dnnl_verbose,info,gpu,engine,0,name:Intel(R) Gen9 HD Graphics NEO,driver_version:19.20.13008dnnl_verbose,exec,gpu,batch_normalization,ocl:ref:any,forward_inference,data_bf16::blocked:abcd:f0 diff_undef::undef::f0,,flags:S,mb2ic2ih2iw2,1.59204dnnl_verbose,exec,gpu,batch_normalization,ocl:ref:any,backward,data_bf16::blocked:abcd:f0 diff_bf16::blocked:abcd:f0,,flags:S,mb2ic2ih2iw2,1.1311 link:https://intel.github.io/mkl-dnn/dev_guide_batch_normalization.htmlhttps://intel.github.io/mkl-dnn/dev_guide_data_types.htmlhttps://github.com/intel/mkl-dnn/tree/master/tests/benchdnn","categories":[{"name":"GPU","slug":"GPU","permalink":"http://huangzhiyuan.github.io/categories/GPU/"}],"tags":[{"name":"DNNL","slug":"DNNL","permalink":"http://huangzhiyuan.github.io/tags/DNNL/"},{"name":"SYCL","slug":"SYCL","permalink":"http://huangzhiyuan.github.io/tags/SYCL/"}]},{"title":"cuda stream and event","slug":"cuda-stream-and-event","date":"2020-03-24T06:31:47.000Z","updated":"2020-03-24T09:12:02.000Z","comments":true,"path":"2020/03/24/cuda-stream-and-event/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/24/cuda-stream-and-event/","excerpt":"一般来说，cuda c并行性表现在下面两个层面上： Kernel level Grid level 到目前为止，我们讨论的一直是kernel level的，也就是一个kernel或者一个task由许多thread并行的执行在GPU上。Stream的概念是相对于后者来说的，Grid level是指多个kernel在一个device上同时执行。","text":"一般来说，cuda c并行性表现在下面两个层面上： Kernel level Grid level 到目前为止，我们讨论的一直是kernel level的，也就是一个kernel或者一个task由许多thread并行的执行在GPU上。Stream的概念是相对于后者来说的，Grid level是指多个kernel在一个device上同时执行。 Stream和event简介Cuda stream是指一堆异步的cuda操作，他们按照host代码调用的顺序执行在device上。Stream维护了这些操作的顺序，并在所有预处理完成后允许这些操作进入工作队列，同时也可以对这些操作进行一些查询操作。这些操作包括host到device的数据传输，launch kernel以及其他的host发起由device执行的动作。这些操作的执行总是异步的，cuda runtime会决定这些操作合适的执行时机。我们则可以使用相应的cuda api来保证所取得结果是在所有操作完成后获得的。同一个stream里的操作有严格的执行顺序，不同的stream则没有此限制。 由于不同stream的操作是异步执行的，就可以利用相互之间的协调来充分发挥资源的利用率。典型的cuda编程模式我们已经熟知了： 将输入数据从host转移到device 在device上执行kernel 将结果从device上转移回host 在许多情况下，花费在执行kernel上的时间要比传输数据多得多，所以很容易想到将cpu和gpu之间的沟通时间隐藏在其他kernel执行过程中，我们可以将数据传输和kernel执行放在不同的stream中来实现此功能。Stream可以用来实现pipeline和双buffer（front-back）渲染。 Cuda API可分为同步和异步两类，同步函数会阻塞host端的线程执行，异步函数会立刻将控制权返还给host从而继续执行之后的动作。异步函数和stream是grid level并行的两个基石。 从软件角度来看，不同stream中的不同操作可以并行执行，但是硬件角度却不一定如此。这依赖于PCIe链接或者每个SM可获得的资源，不同的stream仍然需要等待别的stream来完成执行。下面会简单介绍在不同CC版本下，stream在device上的行为。 Cuda Streams所有的cuda操作（包括kernel执行和数据传输）都显式或隐式的运行在stream中，stream也就两种类型，分别是： 隐式声明stream（NULL stream） 显示声明stream（non-NULL stream）默认情况下是NULL stream，在之前未涉及到stream的博文中，都是该类型。如果显式的声明一个stream就是non-NULL stream了。 异步且基于stream的kernel执行和数据传输能够实现以下几种类型的并行： Host运算操作和device运算操作并行 Host运算操作和host到device的数据传输并行 Host到device的数据传输和device运算操作并行 Device内的运算并行 下面代码是之前常见的使用形式，默认使用NULL stream: 123cudaMemcpy(..., cudaMemcpyHostToDevice);kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(...);cudaMemcpy(..., cudaMemcpyDeviceToHost); 从device角度看，所有者三个操作都是使用的默认stream，并且按照代码从上到下的顺序依次执行，device本身是不知道其他的host操作怎样执行的。从host角度来看，数据传输都是同步的并且会一直等待，直到操作完成。不过不同于数据传输，Kernel的launch是异步的，host差不多立刻就能重新得到控制权，不用管kernel是否执行完毕，从而进行下一步动作。很明显，这种异步行为有助于重叠device和host之间的运算时间。 上文内容在之前博文都有涉及，这里特别说明的是数据传输，它也是可以异步执行的，这就用到了本次讲的stream，我们必须显示的声明一个stream来分派它的执行。下面版本是异步版本的cudaMemcpy： 1cudaError_t cudaMemcpyAsync(void* dst, const void* src, size_t count,cudaMemcpyKind kind, cudaStream_t stream = 0); 注意新增加的最后一个参数。这样，在host issue了这个函数给device执行后，控制权可以立刻返还给host。上面代码使用了默认stream，如果要声明一个新的stream则使用下面的API定义一个： 1cudaError_t cudaStreamCreate(cudaStream_t* pStream); 这样就定义了一个可以使用在cuda异步API函数中stream。使用该函数的一个比较常见的错误，或者说容易引起混乱的地方是，这个函数返回的error code可能是上一次调用异步函数产生的。也就是说，函数返回error并不是调用该函数产生error的必要条件。 当执行一次异步数据传输时，我们必须使用pinned（或者non-pageable）memory。Pinned memory的分配如下，具体请参见前面博文： 12cudaError_t cudaMallocHost(void **ptr, size_t size);cudaError_t cudaHostAlloc(void **pHost, size_t size, unsigned int flags); 通过在将该内存pin到host的虚拟内存上，就可以将该memory的物理位置强制分配到CPU内存中以便使之在整个程序生命周期中保持不变。否则的话，操作系统可能会在任意时刻改变该host端的虚拟内存对应的物理地址。假设异步数据传输函数没有使用pinned host memory的话，操作系统就可能将数据从一块物理空间移动到另一块物理空间（因为是异步的，CPU在执行其他的动作就可能影响这块数据），而此时cuda runtime正在执行数据的传输，这会导致不确定的行为。 在执行kernel时要想设置stream的话，也是很简单的，同样只要加一个stream参数就好： 1234567kernel_name&lt;&lt;&lt;grid, block, sharedMemSize, stream&gt;&gt;&gt;(argument list);// 非默认的stream声明cudaStream_t stream;// 初始化cudaStreamCreate(&amp;stream);// 资源释放cudaError_t cudaStreamDestroy(cudaStream_t stream); 当执行资源释放的时候，如果仍然有stream的工作没干完，那么虽然该函数仍然会立刻返回，但是相关的工作做完后，这些资源才会自动的释放掉。 由于所有stream的执行都是异步的，就需要一些API在必要的时候做同步操作： 12cudaError_t cudaStreamSynchronize(cudaStream_t stream);cudaError_t cudaStreamQuery(cudaStream_t stream); 第一个会强制host阻塞等待，直至stream中所有操作完成为止；第二个会检查stream中的操作是否全部完成，即使有操作没完成也不会阻塞host。如果所有操作都完成了，则返回cudaSuccess，否则返回cudaErrorNotReady。 下面看一下一个代码片段来帮助理解： 12345678910for (int i = 0; i &lt; nStreams; i++) &#123; int offset = i * bytesPerStream; cudaMemcpyAsync(&amp;d_a[offset], &amp;a[offset], bytePerStream, streams[i]); kernel&lt;&lt;grid, block, 0, streams[i]&gt;&gt;(&amp;d_a[offset]); cudaMemcpyAsync(&amp;a[offset], &amp;d_a[offset], bytesPerStream, streams[i]);&#125;for (int i = 0; i &lt; nStreams; i++) &#123; cudaStreamSynchronize(streams[i]);&#125; 上图就跟流水线一样差不多的道理，不多说。需要注意的是，上图中数据传输的操作并不是并行执行的，即使他们是在不同的stream中。按惯例，这种情况肯定就是硬件资源的锅了，硬件资源就那么些，软件层面做的优化无非就是尽量让所有硬件资源一刻不停的被利用起来（万恶的资本主义，嗯……），而这里就是PCIe卡了瓶颈。当然从编程角度来看，这些操作依然是相互独立的，只是他们要共享硬件资源，就不得不是串行的。有两个PCIe就可以重叠这两次数据传输操作，不过也是要保证不同的stream和不同的传输方向。 最大并发kernel数目是依赖于device本身的，Fermi支持16路并行，Kepler是32。并行数是受限于shared memory，寄存器等device资源。 Stream Scheduling概念上来说，所有stream是同时运行的。但是，事实上通常并非如此。 False Dependencies尽管Fermi最高支持16路并行，但是在物理上，所有stream是被塞进硬件上唯一一个工作队列来调度的，当选中一个grid来执行时，runtime会查看task的依赖关系，如果当前task依赖前面的task，该task就会阻塞，由于只有一个队列，后面的都会跟着等待，即使后面的task是别的stream上的任务。就如下图所示： C和P以及R和X是可以并行的，因为他们在不同的stream中，但是ABC，PQR以及XYZ却不行，比如，在B没完成之前，C和P都在等待。 Hyper-Q伪依赖的情况在Kepler系列里得到了解决，采用的一种叫Hyper-Q的技术，简单粗暴的理解就是，既然工作队列不够用，那就增加好了，于是Kepler上出现了32个工作队列。该技术也实现了TPC上可以同时运行compute和graphic的应用。当然，如果超过32个stream被创建了，依然会出现伪依赖的情况。 Stream Priorities对于CC3.5及以上版本，stream可以有优先级的属性： 1cudaError_t cudaStreamCreateWithPriority(cudaStream_t* pStream, unsigned int flags, int priority); 该函数创建一个stream，赋予priority的优先级，高优先级的grid可以抢占低优先级执行。不过优先级属性只对kernel有效，对数据传输无效。此外，如果设置的优先级超出了可设置范围，则会自动设置成最高或者最低。有效可设置范围可用下列函数查询： 1cudaError_t cudaDeviceGetStreamPriorityRange(int *leastPriority, int *greatestPriority); 顾名思义，leastPriority是下限，gretestPriority是上限。老规矩，数值较小则拥有较高优先级。如果device不支持优先级设置，则这两个值都返回0。 Cuda EventsEvent是stream相关的一个重要概念，其用来标记strean执行过程的某个特定的点。其主要用途是： 同步stream执行 操控device运行步调Cuda api提供了相关函数来插入event到stream中和查询该event是否完成（或者叫满足条件？）。只有当该event标记的stream位置的所有操作都被执行完毕，该event才算完成。关联到默认stream上的event则对所有的stream有效。 Creation and Destruction123456// 声明cudaEvent_t event;// 创建cudaError_t cudaEventCreate(cudaEvent_t* event);// 销毁cudaError_t cudaEventDestroy(cudaEvent_t event); 同理streeam的释放，在调用该函数的时候，如果相关操作没完成，则会在操作完成后自动释放资源。 Recording Events and Mesuring Elapsed TimeEvents标记了stream执行过程中的一个点，我们就可以检查正在执行的stream中的操作是否到达该点，我们可以把event当成一个操作插入到stream中的众多操作中，当执行到该操作时，所做工作就是设置CPU的一个flag来标记表示完成。下面函数将event关联到指定stream。 1cudaError_t cudaEventRecord(cudaEvent_t event, cudaStream_t stream = 0); 等待event会阻塞调用host线程，同步操作调用下面的函数： 1cudaError_t cudaEventSynchronize(cudaEvent_t event); 该函数类似于cudaStreamSynchronize，只不过是等待一个event而不是整个stream执行完毕。我们同时可以使用下面的API来测试event是否完成，该函数不会阻塞host： 1cudaError_t cudaEventQuery(cudaEvent_t event); 该函数类似cudaStreamQuery。此外，还有专门的API可以度量两个event之间的时间间隔： 1cudaError_t cudaEventElapsedTime(float* ms, cudaEvent_t start, cudaEvent_t stop); 返回start和stop之间的时间间隔，单位是毫秒。Start和stop不必关联到同一个stream上，但是要注意，如果二者任意一个关联到了non-NULL stream上，时间间隔可能要比期望的大。这是因为cudaEventRecord是异步发生的，我们没办法保证度量出来的时间恰好就是两个event之间，所以只是想要gpu工作的时间间隔，则stop和strat都关联到默认stream就好了。 下面代码简单展示了如何使用event来度量时间： 123456789101112131415161718// create two eventscudaEvent_t start, stop;cudaEventCreate(&amp;start);cudaEventCreate(&amp;stop);// record start event on the default streamcudaEventRecord(start);// execute kernelkernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(arguments);// record stop event on the default streamcudaEventRecord(stop);// wait until the stop event completescudaEventSynchronize(stop);// calculate the elapsed time between two eventsfloat time;cudaEventElapsedTime(&amp;time, start, stop);// clean up the two eventscudaEventDestroy(start);cudaEventDestroy(stop); Stream Synchronization由于所有non-default stream的操作对于host来说都是非阻塞的，就需要相应的同步操作。 从host的角度来看，cuda操作可以被分为两类： Memory相关的操作 Kernel launchKernel launch对于host来说都是异步的，许多memory操作则是同步的，比如cudaMemcpy，但是，cuda runtime也会提供异步函数来执行memory操作。 我们已经知道Stream可以被分为同步（NULL stream）和异步（non-NULL stream）两种，同步异步是针对host来讲的，异步stream不会阻塞host的执行，而大多数同步stream则会阻塞host，不过kernel launch例外，不会阻塞host。 此外，异步stream又可以被分为阻塞和非阻塞两种，阻塞非阻塞是异步stream针对同步stream来讲的。异步stream如果是阻塞stream，那么同步stream会阻塞该异步stream中的操作。如果异步stream是非阻塞stream，那么该stream不会阻塞同步stream中的操作（有点绕……）。 阻塞和非阻塞stream使用cudaStreamCreate创建的是阻塞stream，也就是说，该stream中执行的操作会被早先执行的同步stream阻塞。通常来说，当issue一个NULL stream时，cuda context会等待之前所有阻塞stream完成后才执行该NULL stream，当然所有阻塞stream也会等待之前的NULL stream完成才开始执行。 例如： 123kernel_1&lt;&lt;&lt;1, 1, 0, stream_1&gt;&gt;&gt;();kernel_2&lt;&lt;&lt;1, 1&gt;&gt;&gt;();kernel_3&lt;&lt;&lt;1, 1, 0, stream_2&gt;&gt;&gt;(); 从device角度来说，这三个kernel是串行依次执行的，当然从host角度来说，却是并行非阻塞的。除了通过cudaStreamCreate生成的阻塞stream外，我们还可以通过下面的API配置生成非阻塞stream： 1234cudaError_t cudaStreamCreateWithFlags(cudaStream_t* pStream, unsigned int flags);// flag为以下两种，默认为第一种，非阻塞便是第二种。cudaStreamDefault: default stream creation flag (blocking)cudaStreamNonBlocking: asynchronous stream creation flag (non-blocking) 如果之前的kernel_1和kernel_3的stream被定义成第二种，就不会被阻塞。 Implicit SynchronizationCuda有两种类型的host和device之间同步：显式和隐式。我们之前已经了解到显式同步API有： cudaDeviceSynchronize cudaStreamSynchronize cudaEventSynchronize这三个函数由host显式的调用，在device上执行。 隐式同步我们也了解过，比如cudaMemcpy就会隐式的同步device和host，因为该函数同步作用只是数据传输的副作用，所以称为隐式。了解这些隐式同步是很中要的，因为不经意的调用这样一个函数可能会导致性能急剧降低。 隐式同步是cuda编程中比较特殊情况，因为隐式同步行为可能会导致意外的阻塞行为，通常发生在device端。许多memory相关的操作都会影响当前device的操作，比如： A page-locked host memory allocation A device memory allocation A device memset A memory copy between two addresses on the same device A modification to the L1/shared memory confi guration Explicit Synchronization从grid level来看显式同步方式，有如下几种： Synchronizing the device Synchronizing a stream Synchronizing an event in a stream Synchronizing across streams using an event我们可以使用之前提到过的cudaDeviceSynchronize来同步该device上的所有操作。该函数会导致host等待所有device上的运算或者数据传输操作完成。显而易见，该函数是个heavyweight的函数，我们应该尽量减少这类函数的使用。 通过使用cudaStreamSynchronize可以使host等待特定stream中的操作全部完成或者使用非阻塞版本的cudaStreamQuery来测试是否完成。 Cuda event可以用来实现更细粒度的阻塞和同步，相关函数为cudaEventSynchronize和cudaEventSynchronize，用法类似stream相关的函数。此外，cudaStreamWaitEvent提供了一种灵活的方式来引入stream之间的依赖关系： 1cudaError_t cudaStreamWaitEvent(cudaStream_t stream, cudaEvent_t event); 该函数会指定该stream等待特定的event，该event可以关联到相同或者不同的stream，对于不同stream的情况，如下图所示： Stream2会等待stream1中的event完成后继续执行。 Configurable Events Event的配置可用下面函数： 12345cudaError_t cudaEventCreateWithFlags(cudaEvent_t* event, unsigned int flags);cudaEventDefaultcudaEventBlockingSynccudaEventDisableTimingcudaEventInterprocess cudaEventBlockingSync说明该event会阻塞host。cudaEventSynchronize默认行为是使用CPU时钟来固定的查询event状态。使用cudaEventBlockingSync，调用线程会进入休眠，将控制权交给其他线程或者进程，直到event完成为止。但是这样会导致少量的CPU时钟浪费，也会增加event完成和唤醒线程的之间的时间消耗。 cudaEventDisableTiming指定event只能用来同步，并且不需要记录计时数据。这样扔掉记录时间戳的消耗可以提高cuudaStreamWaitEvent和cudaEventQuery的调用性能。 cudaEventInterprocess指定event可以被用来作为inter-process event。 NVIDIA CUDA板块：https://developer.nvidia.com/cuda-zoneCUDA在线文档：http://docs.nvidia.com/cuda/index.html转载原文注明：http://www.cnblogs.com/1024incn/p/5891051.html","categories":[{"name":"CUDA","slug":"CUDA","permalink":"http://huangzhiyuan.github.io/categories/CUDA/"}],"tags":[{"name":"stream/event","slug":"stream-event","permalink":"http://huangzhiyuan.github.io/tags/stream-event/"}]},{"title":"CPU THREADING AND TORCHSCRIPT INFERENCE","slug":"cpu-threading-and-torchscript-inference","date":"2020-03-23T12:40:53.000Z","updated":"2020-03-23T13:09:50.000Z","comments":true,"path":"2020/03/23/cpu-threading-and-torchscript-inference/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/23/cpu-threading-and-torchscript-inference/","excerpt":"PyTorch allows using multiple CPU threads during TorchScript model inference. The following figure shows different levels of parallelism one would find in a typical application:","text":"PyTorch allows using multiple CPU threads during TorchScript model inference. The following figure shows different levels of parallelism one would find in a typical application: One or more inference threads execute a model’s forward pass on the given inputs. Each inference thread invokes a JIT interpreter that executes the ops of a model inline, one by one. A model can utilize a fork TorchScript primitive to launch an asynchronous task. Forking several operations at once results in a task that is executed in parallel. The fork operator returns a future object which can be used to synchronize on later, for example: 12345678910111213@torch.jit.scriptdef compute_z(x): return torch.mm(x, self.w_z)@torch.jit.scriptdef forward(x): # launch compute_z asynchronously: fut = torch.jit._fork(compute_z, x) # execute the next operation in parallel to compute_z: y = torch.mm(x, self.w_y) # wait for the result of compute_z: z = torch.jit._wait(fut) return y + z PyTorch uses a single thread pool for the inter-op parallelism, this thread pool is shared by all inference tasks that are forked within the application process. In addition to the inter-op parallelism, PyTorch can also utilize multiple threads within the ops (intra-op parallelism). This can be useful in many cases, including element-wise ops on large tensors, convolutions, GEMMs, embedding lookups and others. Build optionsPyTorch uses an internal ATen library to implement ops. In addition to that, PyTorch can also be built with support of external libraries, such as MKL and MKL-DNN, to speed up computations on CPU. ATen, MKL and MKL-DNN support intra-op parallelism and depend on the following parallelization libraries to implement it: OpenMP - a standard (and a library, usually shipped with a compiler), widely used in external libraries; TBB - a newer parallelization library optimized for task-based parallelism and concurrent environments. OpenMP historically has been used by a large number of libraries. It is known for a relative ease of use and support for loop-based parallelism and other primitives. At the same time OpenMP is not known for a good interoperability with other threading libraries used by the application. In particular, OpenMP does not guarantee that a single per-process intra-op thread pool is going to be used in the application. On the contrary, two different inter-op threads will likely use different OpenMP thread pools for intra-op work. This might result in a large number of threads used by the application. TBB is used to a lesser extent in external libraries, but, at the same time, is optimized for the concurrent environments. PyTorch’s TBB backend guarantees that there’s a separate, single, per-process intra-op thread pool used by all of the ops running in the application. Depending of the use case, one might find one or another parallelization library a better choice in their application. PyTorch allows selecting of the parallelization backend used by ATen and other libraries at the build time with the following build options: library Build Option Values Notes ATen ATEN_THREADING OMP (default), TBB MKL MKL_THREADING (same) To enable MKL use BLAS=MKL MKL-DNN MKLDNN_THREADING (same) To enable MKL-DNN use USE_MKLDNN=1 It is strongly recommended not to mix OpenMP and TBB within one build. Any of the TBB values above require USE_TBB=1 build setting (default: OFF). A separate setting USE_OPENMP=1 (default: ON) is required for OpenMP parallelism. Runtime APIThe following API is used to control thread settings: Type of parallelism Settings Inter-op parallelism at::set_num_interop_threads, at::get_num_interop_threads (C++) set_num_interop_threads, get_num_interop_threads (Python, torch module) Intra-op parallelism at::set_num_threads, at::get_num_threads (C++) set_num_threads, get_num_threads (Python, torch module)Environment variables: OMP_NUM_THREADS and MKL_NUM_THREADS Notes:at::set_num_interop_threads, at::get_num_interop_threads (C++)set_num_interop_threads, get_num_interop_threads (Python, torch module) For the intra-op parallelism settings, at::set_num_threads, torch.set_num_threads always take precedence over environment variables, MKL_NUM_THREADS variable takes precedence over OMP_NUM_THREADS. parallel_info utility prints information about thread settings and can be used for debugging. Similar output can be also obtained in Python with torch.__config__.parallel_info() call. source link","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://huangzhiyuan.github.io/tags/pytorch/"}]},{"title":"how to use tensoriterator","slug":"how-to-use-tensoriterator","date":"2020-03-23T04:42:16.000Z","updated":"2020-03-23T12:44:30.000Z","comments":true,"path":"2020/03/23/how-to-use-tensoriterator/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/23/how-to-use-tensoriterator/","excerpt":"This example using existing Linear Interpolation (aka lerp) operator, but same guidelines apply for other operators (new and existing ones). As all changes going to impact performance significantly, we can use the simplest benchmark to measure operator speed before updates and establish the baseline.","text":"This example using existing Linear Interpolation (aka lerp) operator, but same guidelines apply for other operators (new and existing ones). As all changes going to impact performance significantly, we can use the simplest benchmark to measure operator speed before updates and establish the baseline. 12345y = torch.randn(1000000)x = torch.randn(1000000)timeit torch.lerp(x, y, 0.5)2.4 ms ± 25.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) Simple implementationAs the first step, we are naively introducing TensorIterator to replace CPU_tensor_apply3 (it is considered the lousy pattern to use cpu/Loops.h anywhere but in cpu/ subfolder, but we will return to it later in this doc). Update: binary_kernel recently was replaced by more universal cpu_kernel, they have exact same API. code: https://github.com/pytorch/pytorch/pull/21025/commits/c5593192e1f21dd5eb1062dbacfdf7431ab1d47f In compare to TH_APPLY_* and CPU_tensor_apply* and solutions, TensorIterator usage is separated by two steps. Defining iterator configuration with the TensorIterator::Builder. Under the hood, builder calculates tensors shapes and types to find the most performant way to traverse them (https://github.com/pytorch/pytorch/blob/dee11a92c1f1c423020b965837432924289e0417/aten/src/ATen/native/TensorIterator.h#L285) Loop implementation. There are multiple different kernels in Loops.h depending on the number of inputs, they dispatch automatically by cpu_kernel, the ability to do parallel calculations, vectorized version availability (cpu_kernel_vec), type of operation (vectorized_inner_reduction). In our case, we have one output and two inputs, in this type of scenario we can use cpu_kernel. TensorIterator automatically picks the best way to traverse tensors (such as taking into account contiguous layout) as well as using parallelization for bigger tensors. As a result, we have a 24x performance gain. 12345In [*2*]: x = torch.randn(1000000)In [*3*]: y = torch.randn(1000000)In [*4*]: timeit torch.lerp(x,y,0.5)106 µs ± 3.4 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each) Most of the gain comes from the parallelization, so it is worth checking with OMP_NUM_THREADS=1. Here we can see 2x improvement in comparison to the CPU_tensor_apply3 version of the code. 12345In [*2*]: x = torch.randn(1000000)In [*3*]: y = torch.randn(1000000)In [*4*]: timeit torch.lerp(x,y,0.5)1.15 ms ± 65 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) Using native/cpuHowever, as noted above, it is sub-optimal (and NOT recommended solution) as we are not using vectorization and various compile-time optimizations. We can fix it by moving code into the native/cpu folder. // Content of the aten/src/ATen/native/cpu/LerpKernel.cpp1 12345678910111213141516171819202122232425262728293031323334353637#include &lt;ATen/ATen.h&gt;#include &lt;ATen/Dispatch.h&gt;#include &lt;ATen/native/Lerp.h&gt;#include &lt;ATen/native/TensorIterator.h&gt;#include &lt;ATen/native/cpu/Loops.h&gt;namespace at &#123;namespace native &#123;namespace &#123;static void lerp_kernel( Tensor&amp; ret, const Tensor&amp; self, const Tensor&amp; end, Scalar weight) &#123; auto builder = at::TensorIterator::Builder(); builder.add_output(ret); builder.add_input(self); builder.add_input(end); auto iter = builder.build(); AT_DISPATCH_FLOATING_TYPES(ret.scalar_type(), &quot;lerp_kernel&quot;, [&amp;] &#123; scalar_t weight_val = weight.to&lt;scalar_t&gt;(); at::native::binary_kernel(*iter, [=](scalar_t self_val, scalar_t end_val) &#123; return (weight_val &lt; 0.5) ? self_val + weight_val * (end_val - self_val) : end_val - (end_val - self_val) * (1 - weight_val); &#125;); &#125;);&#125;&#125; // anonymous namespaceREGISTER_DISPATCH(lerp_stub, &amp;lerp_kernel);&#125; // namespace native&#125; // namespace at code: https://github.com/pytorch/pytorch/pull/21025/commits/bcf9b1f30331fc897fcc94661e84d47e91bf7290 This code using the same type dispatch pattern AT_DISPATCH_FLOATING_TYPES, but moving everything into the kernel code for better optimization. Such code organization gives us 2x performance boost in comparison to the previous naive implementation. This optimization is archived by compiling AVX2 kernel versions (only applies to native/cpu folder) and dispatching supported code based on cpuinfo. Multithreaded 12345In [*2*]: x = torch.randn(1000000)In [*3*]: y = torch.randn(1000000)In [*4*]: timeit torch.lerp(x,y,0.5)51 µs ± 934 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each) Single thread 12345In [*2*]: x = torch.randn(1000000)In [*3*]: y = torch.randn(1000000)In [*4*]: timeit torch.lerp(x,y,0.5)440 µs ± 2.13 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) Vectorization with the cpu_kernel_vecIn many cases, we can also benefit from the explicit vectorization (provided by Vec256 library). TensorIterator provides the easy way to do it by using _vec loops. code: https://github.com/pytorch/pytorch/pull/21025/commits/83a23e745e839e8db81cf58ee00a5755d7332a43 We are doing so by replacing the cpu_kernel with the cpu_kernel_vec. At this particular case, weight_val check was omitted (to simplify example code), and performance benchmark show no significant gain. Source link: How to use TensorIterator","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://huangzhiyuan.github.io/tags/pytorch/"},{"name":"tensoriterator","slug":"tensoriterator","permalink":"http://huangzhiyuan.github.io/tags/tensoriterator/"}]},{"title":"如何评价清华大学发布的自研深度学习框架-计图(Jittor)？","slug":"jittor-by-Tsinghua","date":"2020-03-22T12:34:22.000Z","updated":"2020-03-23T04:42:54.000Z","comments":true,"path":"2020/03/22/jittor-by-Tsinghua/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/22/jittor-by-Tsinghua/","excerpt":"2020年3月20日，清华自研的深度学习框架，正式对外开源。清华大学计算机系的图形实验室出品，取名Jittor，中文名计图。计图（Jittor）：一个完全基于动态编译（Just-in-time）,内部使用创新的元算子和统一计算图的深度学习框架， 元算子和Numpy一样易于使用，并且超越Numpy能够实现更复杂更高效的操作。而统一计算图则是融合了静态计算图和动态计算图的诸多优点，在易于使用的同时，提供高性能的优化。基于元算子开发的深度学习模型，可以被计图实时的自动优化并且运行在指定的硬件上，如CPU，GPU。官网链接： https://cg.cs.tsinghua.edu.cn/jittor/github地址： https://github.com/Jittor/jittor","text":"2020年3月20日，清华自研的深度学习框架，正式对外开源。清华大学计算机系的图形实验室出品，取名Jittor，中文名计图。计图（Jittor）：一个完全基于动态编译（Just-in-time）,内部使用创新的元算子和统一计算图的深度学习框架， 元算子和Numpy一样易于使用，并且超越Numpy能够实现更复杂更高效的操作。而统一计算图则是融合了静态计算图和动态计算图的诸多优点，在易于使用的同时，提供高性能的优化。基于元算子开发的深度学习模型，可以被计图实时的自动优化并且运行在指定的硬件上，如CPU，GPU。官网链接： https://cg.cs.tsinghua.edu.cn/jittor/github地址： https://github.com/Jittor/jittor 简单看了一下代码，非常有意思的一个项目。因为看得不深入，所以如果有观察错误的话，我想对作者先致以歉意。总的来说：项目更加关注在如何进行计算图优化以及just in time compilation上面（所以叫jittor），并不是关注完整的端到端的框架设计（比如说建模前端等等），我觉得定位是比较清楚的，自动代码生成是现在大家都很关注的方向，在校的同学能够着手把这一套都做一次，值得点赞。一些能看到的工程点： 实现了一个比较经典的DAG graph，以及在图上来做fusion和各种pass。从op的实现上，选择了细粒度的op，例如bcast，reduce，等等，然后通过这种方式来形成meta op，比如说convolution：https://github.com/Jittor/jittor/blob/master/notebook/meta_op.src.md 值得关注的一点是，在XLA的早期，也有过对于op粒度的探索，目前大家的一些结论是，常见的op，比如说convolution，gemm，如果用细粒度op来实现，然后这些细粒度op是在一个op graph当中来做jit的，对性能会是一个很大的挑战（除了在代码里面embed constant value，loop reordering等等）之外，很多关于计算的细节信息都丢失了，会对后面的fusion pass有很大的挑战。 现在一般的自动编译框架选择的方式其实是选择两层IR，一层做计算图DAG，一层做数学表达（比如说bcast，reduce，最典型的是Halide）。可能值得看一看。编译是通过route到binary call，比如说nvcc和gcc，然后读取编译器的输出来做的（https://github.com/Jittor/jittor/blob/85d9ccc004b83ad2d83769ffd1803fb038a17264/src/jit_compiler.cc#L20）。这个让我想起当年的theano的设计，背后的思想一脉相承。相比较于对接libllvm等runtime library，是一个比较短平快可以来实现从前端到编译的办法。 一个小tip，c++其实有demangle的一些utility（https://github.com/pytorch/pytorch/blob/master/c10/util/Type.h#L13），可以不需要把symbol的magic string放在代码里面（https://github.com/Jittor/jittor/blob/85d9ccc004b83ad2d83769ffd1803fb038a17264/src/jit_compiler.cc#L59） 因为编译的产出是直接的binary，不需要一个通用的framework runtime，所以编译以后的产出binary size会很小，这个也许会在端上设备、microcontroller等场景比较有意思？ 很赞的一些工程细节： 有log和flag的设计（应该是学习了google glog和gflags？），并且代码当中做了比较多的logging，这个是个工程实现当中很重要的环节。https://github.com/Jittor/jittor/tree/master/src/utils 因为compilation时间比较久，所以考虑了做cache：https://github.com/Jittor/jittor/blob/master/src/utils/cache_compile.h – 贾扬清1","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习框架","slug":"深度学习框架","permalink":"http://huangzhiyuan.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/"}]},{"title":"Understanding Conda and Pip","slug":"conda-and-pip","date":"2020-03-18T02:26:53.000Z","updated":"2020-03-18T02:34:18.000Z","comments":true,"path":"2020/03/18/conda-and-pip/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/18/conda-and-pip/","excerpt":"Conda and pip are often considered as being nearly identical. Although some of the functionality of these two tools overlap, they were designed and should be used for different purposes. Pip is the Python Packaging Authority’s recommended tool for installing packages from the Python Package Index, PyPI. Pip installs Python software packaged as wheels or source distributions. The latter may require that the system have compatible compilers, and possibly libraries, installed before invoking pip to succeed.","text":"Conda and pip are often considered as being nearly identical. Although some of the functionality of these two tools overlap, they were designed and should be used for different purposes. Pip is the Python Packaging Authority’s recommended tool for installing packages from the Python Package Index, PyPI. Pip installs Python software packaged as wheels or source distributions. The latter may require that the system have compatible compilers, and possibly libraries, installed before invoking pip to succeed. Conda is a cross platform package and environment manager that installs and manages conda packages from the Anaconda repository as well as from the Anaconda Cloud. Conda packages are binaries. There is never a need to have compilers available to install them. Additionally conda packages are not limited to Python software. They may also contain C or C++ libraries, R packages or any other software. This highlights a key difference between conda and pip. Pip installs Python packages whereas conda installs packages which may contain software written in any language. For example, before using pip, a Python interpreter must be installed via a system package manager or by downloading and running an installer. Conda on the other hand can install Python packages as well as the Python interpreter directly. Another key difference between the two tools is that conda has the ability to create isolated environments that can contain different versions of Python and/or the packages installed in them. This can be extremely useful when working with data science tools as different tools may contain conflicting requirements which could prevent them all being installed into a single environment. Pip has no built in support for environments but rather depends on other tools like virtualenv or venv to create isolated environments. Tools such as pipenv, poetry, and hatch wrap pip and virtualenv to provide a unified method for working with these environments. Pip and conda also differ in how dependency relationships within an environment are fulfilled. When installing packages, pip installs dependencies in a recursive, serial loop. No effort is made to ensure that the dependencies of all packages are fulfilled simultaneously. This can lead to environments that are broken in subtle ways, if packages installed earlier in the order have incompatible dependency versions relative to packages installed later in the order. In contrast, conda uses a satisfiability (SAT) solver to verify that all requirements of all packages installed in an environment are met. This check can take extra time but helps prevent the creation of broken environments. As long as package metadata about dependencies is correct, conda will predictably produce working environments. Given the similarities between conda and pip, it is not surprising that some try to combine these tools to create data science environments. A major reason for combining pip with conda is when one or more packages are only available to install via pip. Over 1,500 packages are available in the Anaconda repository, including the most popular data science, machine learning, and AI frameworks. These, along with thousands of additional packages available on Anaconda cloud from channeling including conda-forge and bioconda, can be installed using conda. Despite this large collection of packages, it is still small compared to the over 150,000 packages available on PyPI. Occasionally a package is needed which is not available as a conda package but is available on PyPI and can be installed with pip. In these cases, it makes sense to try to use both conda and pip. Comparison of conda and pip conda pip manages binaries wheel or source can require compilers no yes package types any Python-only create environment yes, built-in no, requires virtualenv or venv dependency checks yes no package sources Anaconda repo and cloud PyPI","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"conda/pip","slug":"conda-pip","permalink":"http://huangzhiyuan.github.io/tags/conda-pip/"}]},{"title":"AI+的应用场景","slug":"the-application-area-of-AI","date":"2020-03-17T13:17:45.000Z","updated":"2020-03-17T13:53:04.000Z","comments":true,"path":"2020/03/17/the-application-area-of-AI/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/17/the-application-area-of-AI/","excerpt":"要说现在什么最火，那都不用说，肯定是AI。AI已经渗入到了我们生活的方方面面。除了大家熟知的自动驾驶汽车、图像美颜，聊天机器人等，还有许多方面都应用到了AI，今天我就和大家聊下AI当前在各大领域的应用。","text":"要说现在什么最火，那都不用说，肯定是AI。AI已经渗入到了我们生活的方方面面。除了大家熟知的自动驾驶汽车、图像美颜，聊天机器人等，还有许多方面都应用到了AI，今天我就和大家聊下AI当前在各大领域的应用。 AI+交通既然要说AI+交通，那就不得不提自动驾驶了，自动驾驶作为AI与制造业的一大产物，目前各大主机厂和IT公司都想在自动驾驶领域分一杯羹。另外值得注意的是习近平总书记在2019年1月21日的中央党校的一个研讨班的开班仪式上强调要加快“自动驾驶的立法”，这说明什么，应该不需要我多言。自动驾驶在未来绝对是最热门的领域之一，当然自动驾驶发展的怎么样，必须要看AI能发展到什么程度。 上图是2018年央视春晚珠海分会场，由百度Apollo无人车领衔的百余辆无人车精彩亮相的画面。是不是很震撼。 当前自动驾驶技术的关键还是在环境感知、决策规划和控制这三个方面。而要想环境感知有更好的发展，以深度学习为代表的 AI 技术如计算机视觉等非常重要。 AI+医疗根据美国著名市场调研机构Tractica的数据显示，预计到2025年，医疗机构将为人工智能支付超过340亿美元，2018年为21亿美元。这说明医疗卫生行业对AI的迫切需要！ 医疗卫生行业对AI的迫切需求主要在以下几个方面：1、医学数据分析需求AI能通过患者数据训练的深度学习诊疗模型，快速筛选出适合患者的药物和治疗方案，追踪和预测个体疾病的进程。2、远程医疗人工智能的影响不仅在医院内部的临床诊疗工作，还将提升院外服务的可及性。一旦患者离开医院，医生可能难以确保患者遵守规定的治疗计划或监测慢性健康状况，AI能够弥合时间和空间距离，实现信息的高效传输和共享，这将帮助医生及时了解患者的健康状况。3、促进医疗行为规范化引入医疗AI工具，在诊疗流程中能督促医生按照临床诊疗规范执行医疗方法，这将大大减少不规范行为给患者带来的安全威胁。4、新药开发新药研发的目标是找到可调控机体生物学功能的实体物质，如小分子、大分子或生物活体等；然而优质靶点的寻找很难，而且一旦出现一个获得临床验证的新靶点，叠罗汉式的实验前仆后继并不鲜见，研发成本会是疯狂增加。为了解决这个问题，现在我们可以将人工智能的相关技术应用于疾病的靶点预测、高通量数据的分析以及系统生物学的建模过程中。 AI+金融当前人工智能的应用已广泛渗透到金融行业中，且日渐成熟，并推动银行、保险、资本市场三大金融行业的深刻变革。AI在金融方面的应用主要集中在以下几个方面： 信用评分以往的多数信用评分模型，大多数是使用金融机构的交易与支付数据，并运用回归、决策树、统计分析等工具来生成信用评分。现在，银行等金融机构日益使用新型的非结构化与半结构化的数据来源(如社交媒体、手机和短信)来捕捉借款人的信用，并用人工智能来评估消费者行为和支付意愿等定性因素，使筛选借款人的速度更快、成本更低。 金融AI机器人金融AI机器人是用于帮助客户处理问题的虚拟助手，其使用NLP自然语言与客户交互，并使用机器学习算法来优化。当前许多金融服务公司在其移动应用程序或公司里面都引入了AI机器人。下图就是日本瑞穗银行的机器人顾问。 当前在金融业AI应用还有很多，如优化金融资本、模型风险管理与压力测试、投资组合管理等方面。可以说AI促进金融业更加智能化，更好的为公众服务。 AI+教育在人工智能风潮的引领下，AI+教育的浪潮如火如荼，国内的各大教育机构如新东方、英语流利说都迅速在AI领域跑马圈地，试图用人工智能变革传统的教育体系。在AI+教育做的最多的应该就是英语流利说了，他们有个口号就是“用AI替代真人老师”。下图是英语流利说的英语水平测试： 据介绍其可以精准定位用户英语水平，然后量身定制系统课程。有兴趣的可以测一下水平，这个是免费的。在AI+教育领域还有个比较好的应用——魔镜系统。 魔镜系统是好未来自主研发的人工智能教学辅助系统。该系统可以借助摄像头捕捉学生上课时的举手、练习、听课、发言等课堂状态和面部情绪变化数据，生成专属于每一个学生的学习报告。其提供的多维度量化数据，能够真实反映出学生学习过程中的每个阶段、每种状态，帮助老师基于科学数据而调整、优化教学方案，帮助学生更好地提升专注度，实现更高效的学习，同时也可以使每一个孩子都得到充分的关注；另外它能利用表情动作识别，可以产出孩子整堂课专注度曲线以及学习状态小视频，父母即使不去陪读也可以轻松了解孩子在课堂的学习状态。 AI+农业要提到农业与AI，大家可能有点不可思议，AI这么高科技的东西能应用到农业吗？哈哈，当然能，而且AI与农业的结合可以说效果显著。现在靠天吃饭的农业正在发生巨大转变，人工智能可以使农场品更安全、更营养、更值钱。 在2018年6月7日的云栖大会·上海峰会上，阿里云正式发布ET农业大脑。其目标是希望将人工智能与农业深入结合，目前已应用于生猪养殖、苹果及甜瓜种植，已具备数字档案生成、全生命周期管理、智能农事分析、全链路溯源等功能。 AI+新零售大家应该都听过新零售这个词，按照阿里的解释，新零售就是以消费者体验为中心的数据驱动的泛零售形态。说简单点实际上就是AI与我们平时见到的零售相结合。 说到新零售就不得不提阿里旗下的盒马鲜生了，其是阿里巴巴对线下超市完全重构的新零售业态。阿里巴巴能够为盒马鲜生的消费者提供会员服务，用户可以使用淘宝或支付宝账户注册，以便消费者从最近的商店查看和购买商品。盒马未来可以跟踪消费者购买行为，借助大数据做出个性化的建议。除了盒马鲜生，阿里的淘咖啡也是一个新零售的典型。用户首次进店后需通过手机淘宝扫码获得电子入场码，通过认证闸机后可以开始购物。最后离开时，用户必须经过两道门，第一道门感应到用户的离店需求时自动开启，经过几秒结算时间后开启第二道门，顾客可离店。淘咖啡在技术上主要采用生物特征自主感知和学习系统、结算意图识别和交易系统、目标检测与追踪系统来追踪消费者在店内的行为及运动轨迹。 AI+家居智能家居可以说是近年来最热门的话题之一。随着科技的发展，从感应采光调节室内光线的智能灯到输入指令就可自动扫地的机器人，从记录食物新鲜程度的冰箱到能识别指纹和人脸的门锁，曾经只能在科幻大片中见到的场景正渐渐成为现实。 上图是电影《钢铁侠》中无所不能的智能管家Jarvis，而扎克伯格受此启发设计出了类似的AI系统来控制智能家居，也命令为Jarvis，在Jarvis的帮助下，扎克伯克可以用手机和电脑来调节空调温度、室内灯光明暗，还能烤面包，从网上搜索歌曲自动播放。另外Jarvis 系统识别访客的功能也非常好，扎克伯格在自己家门口安装了多个摄像头，从不同角度拍摄家门口的画面。当访客靠近时，Jarvis 系统会马上识别到门口有人，然后激活程序，对访客的面部细节进行探测，接着会在 Facebook人脸数据库中找到对应的目标，并根据扎克伯格的日程表及访客名单来判断对方是否为不速之客，确认后才会打开门，并告知扎克伯格客人已经到了。哈哈你是不是也想要有一个这样的智能家居！ AI+体育在体育领域，人们开始对AI格外关注应该是在2016年李世石和谷歌围棋AI“AlphaGo”的比赛，根据媒体报道，比赛的第一天吸引了全球共计1亿人次观看，这应该是AI在体育领域第一次大规模的爆发。 现在体育行业有许多方面都涉及了AI技术。下面和大家介绍一些体育方面的AI，如下： 比赛精彩视频引入AI技术在2018年俄罗斯世界杯开赛前夕，IBM和美国福克斯体育（Fox Sports）合作制作一个AI平台。IBM的解决方案包括分析视觉、音频、文本数据，福克斯体育将利用IBM沃森媒体的功能来进行自动高级元数据标记以及快速筛选新的和存档视频片段，以识别比赛时刻并生成实时的精彩视频。 现在在腾讯体育的NBA直播比赛中同样也有这样的AI技术，他们利用IBM AI Vision视觉大脑来快速完成对精彩瞬间的剪辑。 AI裁判国际体操联合会当前和日本通信技术公司富士通（Fujitsu）合作，计划把AI技术引进东京奥运会的打分系统，利用3D传感器接收的数据并结合AI分析鞍马和自由体操等体操项目，让AI分担一部分裁判工作。AI技术的引入，在一定程度上，能够使评分系统更加公正。 智能运动鞋在2019年的1月16日，耐克发布了新款运动鞋Nike Adapt BB。这双篮球鞋可不一般，是一双智能篮球鞋，它可以自系鞋带，还能与智能手机应用程序配合使用以调整合脚度。 AI在体育方面的应用还有很多，相信在未来，大数据和人工智能将是推动体育产业发展的最重要的角色之一。 AI+电竞最近几年随着英雄联盟、王者荣耀、绝地求生等游戏大火，电子竞技也变的火爆起来。现在似乎每个小孩都会玩王者荣耀。哈哈，你曾经被“小学生”坑过不！这就是现在电子游戏的火爆程度。现在随着AI技术的发展，许多游戏也都引入了AI，下面将介绍下AI在游戏中的应用。 王者荣耀AI“绝悟”“绝悟”的首次露面是在2018KPL秋季赛的总决赛上（12月22日），当时“绝悟”与前KPL职业选手和职业解说组成的人类战队（平均水平超过99%玩家）进行5V5的水平测试，并取得胜利。 星际争霸2AI“AlphaStar”AlphaStar是谷歌的子公司Deepmind在游戏领域推出的一款对战型游戏AI作品。在2019年1月24日，AlphaStar与《星际争霸2》的两位职业选手TLO和MaNa对战，都取得了5:0的骄人战绩。据了解，目前的AlphaStar已经十分成熟了，它相当于拥有200年《星际争霸 2》游戏经验的玩家。 AI+直播你曾想过AI与直播这两个新的东西碰到一起会产生怎样的火花吗？下面请看AI在直播中的应用。 AI+直播内容审核直播平台利用深度学习算法，通过模拟人脑神经网络，构建具有高层次表现力的模型，能够对高复杂度数据形成良好的解读。这样在审核违法内容时能有效节省人工复审的工作，大大的提高了工作效率。 AI+直播个性化推荐现在许多直播平台把AI技术实际运用到了直播之中，利用其进行内容分析，并作出智能优化，为观众提供更加优质的内容。 就比如拿目前最受欢迎的游戏之一绝地求生来说，虎牙直播运用AI技术建立了全新的观看模式，自动分析直播内容，让玩家更加直观的找到想看的内容。比如：决赛圈、单排、双排等功能。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://huangzhiyuan.github.io/tags/AI/"}]},{"title":"全球前10的计算机深度学习科学家","slug":"top10-cv-scientist","date":"2020-03-16T12:45:37.000Z","updated":"2020-03-16T13:10:04.000Z","comments":true,"path":"2020/03/16/top10-cv-scientist/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/16/top10-cv-scientist/","excerpt":"本篇文章来总结下业界有名的计算机深度学习方向的大佬们。引用最权威的资料来，学术界公认的h-index排名。所谓H-index，就是high citations，简单来说就是论文被引用的频次。","text":"本篇文章来总结下业界有名的计算机深度学习方向的大佬们。引用最权威的资料来，学术界公认的h-index排名。所谓H-index，就是high citations，简单来说就是论文被引用的频次。 H-index排名前十的计算机科学家下图是2018年计算机科学领域的H-index排名前十，相信从中就是小白们也能看到不少熟悉的名字。完整名单 H-index排名越高说明论文被人引用的越频繁，在学术界来说这就意味着影响力。下面我们来了解一下排名前十的大佬们都是谁，做过什么。 Yoshua Bengio加拿大计算机科学家，深度学习三巨头之一，LeNet5作者之一，花书《Deep learning》作者之一，一直呆在学术界。 代表性文章： 1 LÉcun, Yann, et al. “Gradient-Based Learning Applied to Document Recognition.” Proceedings of the IEEE, vol. 86, no. 11, 1998, pp. 2278–2324. [2] Bengio Y, Courville A C, Vincent P, et al. Representation Learning: A Review and New Perspectives[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35(8): 1798-1828. Geoffrey Hinton加拿大认知心理学家和计算机科学家，深度学习三巨头之一，反向传播算法提出者之一，2006年在science期刊发表深层网络逐层初始化训练方法，揭开深度学习世纪新序幕，其弟子Alex Krizhevsky提出AlexNet网络。 代表性文章： 1 Rumelhart D E , Hinton G E , Williams R J . Learning internal representations by error propagation[M]// Neurocomputing: foundations of research. MIT Press, 1988. [2] Hinton G E, Salakhutdinov R. Reducing the dimensionality of data with neural networks.[J]. Science, 2006, 313(5786): 504-507. [3] Krizhevsky A , Sutskever I , Hinton G . ImageNet Classification with Deep Convolutional Neural Networks[C]// NIPS. Curran Associates Inc. 2012. Yann LeCun法国计算机科学家，深度学习三巨头之一，Facebook首席人工智能科学家，LeNet5网络第一作者，深度学习综述《Deep learning》作者之一。 至此三巨头都出现了，不愧是三巨头，它们之间也有着千丝万缕的合作，从上面同时出现在LeNet5和深度学习花书的Yoshua Bengio和Yann LeCun就可以看出，两人年纪也相当，而Hinton其实已经是两者的老师级别。 Andrew Zisserman英国计算机科学家，牛津大学教授，计算机视觉研究员，经典书《Multiple View Geometryin Computer Vision》作者，VGG网络作者之一，Pascal Visual Object Classes (VOC) Challenge发起者之一，Deep Mind研究员。 代表性文章： 1 Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition[J]. international conference on learning representations, 2015. [2] Everingham M, Van Gool L, Williams C K, et al. The Pascal Visual Object Classes (VOC) Challenge[J]. International Journal of Computer Vision, 2010, 88(2): 303-338. [3] Jaderberg M, Simonyan K, Zisserman A, et al. Spatial transformer networks[J]. neural information processing systems, 2015: 2017-2025. David Haussler美国生物信息学家，霍华德休斯医学研究所研究员、生物分子工程教授等，人类基因组计划竞赛中组装了第一个人类基因组序列。 代表性文章： 1 Lander E S, Linton L, Birren B, et al. Initial sequencing and analysis of the human genome.[J]. Nature, 2001, 409(6822): 860-921. Trevor Darrell加州大学伯克利分教授，伯克利人工智能研究（BAIR）实验室的联合主任，Caffe，RCNN作者之一。 代表性文章： 1 Jia Y, Shelhamer E, Donahue J, et al. Caffe: Convolutional Architecture for Fast Feature Embedding[J]. acm multimedia, 2014: 675-678. [2] Girshick R B, Donahue J, Darrell T, et al. Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation[J]. computer vision and pattern recognition, 2014: 580-587. StephenP.Boyd三星工程教授，斯坦福大学信息系统实验室电气工程教授，凸优化书籍《Convex optimization》作者。 代表性文章： 1 Stephen Boyd L V, Stephen Boyd L V. Convex optimization[J]. IEEE Transactions on Automatic Control, 2006, 51(11):1859-1859. [2] Candes E J, Wakin M B, Boyd S P. Enhancing Sparsity by Reweighted l(1) Minimization[J]. Journal of Fourier Analysis &amp; Applications, 2007, 14(5):877-905. Michael I. Jordan 美国科学家、加州大学伯克利分校教授。机器学习领域的领军人物之一，2016年《科学》杂志评定的世界上最具影响力的计算机科学家。Latent Dirichlet Allocation模型作者。 代表性文章： 1 Blei D M, Ng A Y, Jordan M I. Latent dirichlet allocation[J]. Journal of Machine Learning Research, 2012, 3:993-1022. Christopher Manning斯坦福大学人工智能实验室主任，语言学和计算机科学家。书籍《Introduction to information retrieval》，《Foundations of Statistical Natural Language Processing》作者。 代表性文章： 1 Manning C D. Foundations of statistical natural language processing[M]// Foundations of Statistical Natural Language Processing. 1999. [2] Larson R R. Introduction to Information Retrieval[J]. Journal of the Association for Information Science and Technology, 2010, 61(4): 852-853. Herbert A Simon诺贝尔经济学奖，图灵奖等获得者，书籍《The Sciences of the Artificial》，《Human Problem Solving》作者，也是唯一一个已经不在世近二十年的科学家，却还能在过去一年的论文引用前十中占据一席，可见影响力之大。 代表论文： 除了上面的10位，计算机科学领域还有很多世界级的研究人员值得我们去关注的，比如花书作者之一和生成对抗网络的提出者Ian Goodfellow等，不再过多介绍。 深度学习领域的优秀青年华人如果说世界级科学家离我们太遥远，那么身边优秀的华人是不是需要好好关注？下面介绍几个优秀的80后青年华人，都是非常有代表性的人物，对深度学习有突破性的学术贡献或开源框架作者。 何恺明本科就读于清华大学，博士毕业于香港中文大学多媒体实验室，曾在微软亚洲研究院担任实习生，目前在Facebook人工智能实验室（FAIR）担任研究科学家。他是Resnet、Mask R-CNN第一作者，也是首位获计算机视觉领域三大国际会议之一CVPR“最佳论文奖”的中国学者。另外他也获得了CVPR 2016和ICCV 2017（Marr Prize）的最佳论文奖，并获得了ICCV 2017最佳学生论文奖，CVPR 2018的PAMI年轻学者奖，这就是别人隔壁家的小明和学霸。 代表性文章： 1 He K , Zhang X , Ren S , et al. Deep Residual Learning for Image Recognition[J]. 2015. [2] He K, Gkioxari G, Dollar P, et al. Mask R-CNN[J]. IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, 2017, PP(99):1-1. 贾扬青深度学习框架Caffe之父。本科和硕士研究生就读于清华大学，博士毕业于加州大学伯克利分校，曾在新加坡国立大学、微软亚洲研究院、NEC美国实验室、Google Brain工作，任Facebook研究科学家，负责前沿AI平台的开发以及前沿的深度学习研究。现已入职阿里巴巴。 代表性文章： 1 Jia Y , Shelhamer E , Donahue J , et al. Caffe: Convolutional Architecture for Fast Feature Embedding[J]. 2014. [2] Decaf: A deep convolutional activation feature for generic visual recognition如果说何凯明是学术界的青年扛把子，那么贾扬清就是工业界的青年扛把子了，他还有知乎账号，冒过几个泡。 李沐2008年本科毕业于上海交通大学计算机系，CMU博士毕业，深度学习开源框架MXNet作者之一，曾在微软亚洲研究院担任实习生，在亚马逊就职。沐神有一本在线书籍《动手学深度学习》，另外现在有很多的群，算是做深度学习的普及工作贡献了。 代表性文章： 1 Li M , Liu Z , Smola A J , et al. DiFacto - Distributed Factorization Machines[C]// Acm International Conference on Web Search &amp; Data Mining. ACM, 2016. 陈天奇本科毕业于上海交通大学ACM班，华盛顿大学计算机系博士生。深度学习编译器TVM，SVDFeature，XGBoost，cxxnet等作者，MxNet，DMLC发起人之一。 代表性文章： 1 MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed SystemsTianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, Zheng ZhangLearningSys at Neural Information Processing Systems 2015 [2] TVM: An Automated End-to-End Optimizing Compiler for Deep LearningTianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Meghan Cowan, Haichen Shen, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, Arvind Krishnamurthy 韩松本科毕业于清华大学后，博士毕业于斯坦福大学，深鉴科技联合创始人之一，2016年ICLR最佳论文deep compression论文一作。就放深鉴科技四个创始人的照片吧，都是青年才俊。 代表性文章： 1 Han S , Kang J , Mao H , et al. ESE: Efficient Speech Recognition Engine with Sparse LSTM on FPGA[J]. 2016. [2] Han S, Mao H, Dally W J, et al. Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding[J]. international conference on learning representations, 2016.","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"大牛","slug":"大牛","permalink":"http://huangzhiyuan.github.io/tags/%E5%A4%A7%E7%89%9B/"}]},{"title":"深入理解计算机系统-3-程序的机器级表示","slug":"computer-system-chapter-3-data-format-transmission","date":"2020-03-15T03:40:13.000Z","updated":"2020-03-16T13:24:42.000Z","comments":true,"path":"2020/03/15/computer-system-chapter-3-data-format-transmission/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/15/computer-system-chapter-3-data-format-transmission/","excerpt":"该系列文章摘抄于深入理解计算机系统第三章程序的机器级表示部分。书中部分章节介绍过于详细或基础，故不总结于此。只摘抄之前认识不深刻或者不理解的知识盲区。","text":"该系列文章摘抄于深入理解计算机系统第三章程序的机器级表示部分。书中部分章节介绍过于详细或基础，故不总结于此。只摘抄之前认识不深刻或者不理解的知识盲区。 数据格式由于是从16位体系结构扩展成32位，Intel用术语“字（word）”，表示16位数据类型。因此称32位位“双字（double words）”，称64位位“四字（quad words）”。C语言基本数据类型对应的x86-64表示如下表： C声明 Intel数据类型 汇编代码后缀 大小(字节) char 字节 b 1 short 字 w 2 int 双字 l 4 long 四字 q 8 char* 四字 q 8 float 单精度 s 4 double 双精度 l 8 如图所示，大多数GCC生成的汇编代码指令都有一个字符的后缀，报名操作数的大小。例如，数据传输指令有四个变种： movb 传送字节 movw 传送字 movl 传送双字 movq 传送四字 访问信息一个x86-64的中央处理单元CPU包含一组16个存储64位值的通用目的寄存器。这些寄存器用来存储整数数据和指针。下图所示。他们的名字都以%r开头，不过后面还跟着一些不同的命名规则的名字，这是由于指令集历史演化造成的。最初的80886中有8个16位的寄存区，即图中的%ax到%sp。扩展到IA32架构时，这些寄存器也扩展成了32位，标号从%eax到%esp。扩展到x86-64后，原来的8个寄存器扩展成64位，标号从%rax到%rsp。除此之外还增加了8个新的寄存器，标号按照新的命名规则制定，从%r8到%r15。 操作数指示符大多数指令有一个或多个操作数（operand），指示出执行一个操作中要使用的源数据值，以及放置结果的目的位置。x86-64支持多种操作数格式。 各种操作数类型分为三种类型： 立即数（immediate）。用来表示常数值， “$” + 标准C表示法表示的整数。如$-123, $0x1F。 寄存区（register）。表示某个寄存器的内容。用ra表示任意寄存区a，用引用R[ra]表示它的值。 内存引用。它会根据计算出来的地址（有效地址）访问某个内存位置。因为它将内存看成一个很大的字节数组。 数据传送指令最频繁使用的指令是将数据从一个位置复制到另一个位置的指令。下表的指令都执行同样的操作，主要的区别在于操作的数据大小不同，分别是1,2,4,8字节。 指令 效果 描述 MOV S, D D &lt;– S 传送 movb 传送字节 movw 传送字 movl 传送双字 movq 传送四字 movabsq I,R R &lt;– I 传送绝对的四字 下面的MOV指令示例给出了源和目的类型的五种可能的组合，第一个是源操作数，第二个是目的操作数。 1234567891011121314151. movl $0x4050,%easImmediate -- Register, 4 bytes2. movw %bp,%spRegister -- Register, 2 bytes3. movb (%rdi, %rcx), %alMemory -- Register, 1 byte4. movb $-17, (%rsp)Immediate -- Memory, 1 byte5. movq %rax, -12(%rbp)Register -- Memory, 8 bytes 下图记录的是两类数据移动指令，在将较小的源值复制到较大的目的时使用。MOVZ类中的指令把目的中剩余的字节填充为0，而MOVS类中的指令通过符号扩展来填充。 指令 效果 描述 MOVZ S, R R &lt; - -零扩展（S） 以零扩展进行传送 movzbw 将做了零扩展的字节传送到字 movzbl 将做了零扩展的字节传送到双字 movzbq 将做了零扩展的字节传送到四字 movzwl 将做了零扩展的字传送到双字 movzwq 将做了零扩展的字传送到四字 这些指令以寄存区或内存地址作为源，以寄存区作为目的。注意，并没有将做了零扩展的双字传送到四字的指令movzlq。不过这样的数据传送可以用以寄存器为目的的movl实现。这一技术利用的属性是，生成4字节值并以寄存器作为目的指令会把高四字节置为零。 指令 效果 描述 MOVS S, R R &lt; - -符合扩展（S） 以符号扩展进行传送 movsbw 将做了符号扩展的字节传送到字 movsbl 将做了符号扩展的字节传送到双字 movsbq 将做了符号扩展的字节传送到四字 movswl 将做了符号扩展的字传送到双字 movswq 将做了符号扩展的字传送到四字 movslq 将做了符号扩展的双字传送到四字 cltq %rax &lt; – 符号扩展（%eax） 把%eax符号扩展到%rax MOVS这些指令以寄存区或内存地址作为源，以寄存区作为目的。cltq指令只用于寄存器%eax和%rax。 理解数据传输如何改变目的寄存器关于数据传送指令是否以及如何修改目的寄存器的高位字节有两种不同的方法。 123451 movabsq $0x0011223344556677, %rax %rax = 00112233445566772 movb $-1, %al %rax = 00112233445566FF3 movw $-1, %ax %rax = 001122334455FFFF4 movl $-1, %eax %rax = 00000000FFFFFFFF5 movq $-1, %rax %rax = FFFFFFFFFFFFFFFF 接下来的讨论中，都用十六进制表示。这个例子中，第一行的指令把寄存区%rax初始化为位模式0011223344556677。剩下的指令的源操作数是立即数值-1。-1的十六进制表示位FF…F，这里F的数量是表述中字节数量的2倍。2,3,5行结果源出于此。例外是第四行movl，大多数情况下mov指令只会修改目的操作数指定的那些寄存区字节或内存位置。但是movl指令以寄存器作为目的时，它会把寄存器的高位4字节设置为0，造成这个例外的原因是x86-64采用的惯例，即任何为寄存器生成32位值的指令都会把该寄存器的高位部分置零。 字节传送指令比较下面这个例子说明不同的是数据传送指令如何改变或者不改变目的的高位字节。仔细观察就可以发现movb， movsbq, movzbq之间的细微差别。 123451 movabsq $0x0011223344556677, %rax %rax = 00112233445566772 movb $0xAA, %dl %dl = AA3 movb $dl, %al %rax = 00112233445566AA4 movsbq $dl, %rax %rax = FFFFFFFFFFFFFFAA5 movzbq $dl, %rax %rax = 00000000000000AA 前2行分别完成对%rax和%dl寄存区的初始化，后3行分别使用movb/movsbq/movzbq完成从%dl到%rax的低字节copy。movb指令不改变其他字节。movsbq根据符号将其他7个字节设为全1或全0，A的十六进制表示为0x1010，因此符号扩展会将高位字节全部设置为FF。movzbq指令总是将其他7个字节全部设置为0。 数据传送示例假设变量sp和dp被声明为类型 1234src_t *sp;dest_t *dp;*dp = (dest_t)*sp; 假设sp和dp的值分别存储在寄存器%rdi和%rsi中，对于下面表中的每个表项，给出实现指定数据传送的两条指令。其中第一条指令应该从内存中读数，做适当的装换，并设置寄存器%rax的适当部分。然后第二条指令要把%rax的适当部分写到内存，在这两种情况下，寄存器的部分可以是%rax,%eax,%ax和%al，两者互不相同。 src_t dest_t 指令 注释 long long movq(%rdi), %raxmovq %rax, (%rsi) 读8个字节存8个字节 char int movsbl(%rdi), %eaxmovl %eax, (%rsi) 将char转换成int存4个字节 char unsigned movsbl(%rdi), %eaxmovl %eax, (%rsi) 将char转换成int存4个字节 unsigned char long movzbl(%rdi), %eaxmovq %rax, (%rsi) 读一个字节并零扩展存8个字节 int char movl(%rdi), %eaxmovb %al, (%rsi) 读4个字节存低位字节 unsigned unsigned char movl(%rdi), %eaxmovb %al, (%rsi) 读4个字节存低位字节 char short movsbw(%rdi), %axmovw %ax, (%rsi) 读1个字节并符号存2个字节","categories":[{"name":"读书","slug":"读书","permalink":"http://huangzhiyuan.github.io/categories/%E8%AF%BB%E4%B9%A6/"}],"tags":[{"name":"深入理解计算机系统","slug":"深入理解计算机系统","permalink":"http://huangzhiyuan.github.io/tags/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"}]},{"title":"比尔·盖茨退出微软董事会，回顾盖茨与微软的传奇故事","slug":"bill-gates-exit-board-director","date":"2020-03-14T13:55:00.000Z","updated":"2020-03-14T14:03:50.000Z","comments":true,"path":"2020/03/14/bill-gates-exit-board-director/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/14/bill-gates-exit-board-director/","excerpt":"微软于周五宣布，公司联合创始人兼技术顾问比尔·盖茨（Bill Gates）辞去微软董事会职务。对于这一决定，盖茨表示：“伯克希尔公司和微软的领导层从未如此强大过，所以现在是采取这一步骤的时候了。”","text":"微软于周五宣布，公司联合创始人兼技术顾问比尔·盖茨（Bill Gates）辞去微软董事会职务。对于这一决定，盖茨表示：“伯克希尔公司和微软的领导层从未如此强大过，所以现在是采取这一步骤的时候了。” 不过，从董事会辞职也不意味着离开公司, 毕竟微软是他一手创建并苦苦经营才发展到今天这个规模的。他将继续担任首席执行官萨提亚·纳德拉（Satya Nadella）和公司其他领导人的技术顾问。盖茨强调，“微软将永远是我一生工作的重要组成部分，我将继续与萨提亚和技术领导层合作，帮助塑造愿景，实现公司的宏伟目标。” 根据FactSet数据，盖茨持有微软1.36%的股份，身价高达千亿美元。一方面，微软把盖茨送上了“世界首富”的宝座，而另一方面，盖茨令微软公司得以诞生并一路壮大至今，跻身于世界排名靠前的几大科技巨头之一。可以说，盖茨与微软是互相成就、密不可分的。笔者特地盘点了盖茨与微软的充满传奇色彩的故事。 小旅馆中诞生的微软公司1975年，尚在哈佛大学校园读书的比尔·盖茨，和他的高中校友保罗·艾伦（Paul Allen）为MITS公司的Altair编写可运行的程序，售价为3000美元，但相应版税却高达18万美元。 1976年11月26日，盖茨和艾伦注册了“微软”（Microsoft）商标。后来，盖茨和艾伦搬到阿尔伯克基，并在当地一家旅馆房间里创建了微软公司。说是公司，但是老板和员工加起来也就两个人——盖茨与他的伙伴，两人既是老板，也是员工。两人的主要工作就是一起销售BASIC解释器。 1977年，两人决定将微软公司搬到西雅图的贝尔维尤，在那里开发PC的编程软件。1979年，MITS公司倒闭，微软公司只好调整业务方向，转而以修改BASIC程序艰难求生。同年，微软总部迁到了华盛顿州贝莱佛。此后，他们曾试图通过设计MSX家庭计算机标准来进入家用计算机市场，但是取得效益不太理想。 经营惨淡之下迎来转机1980年8月28日，当时的科技巨头IBM公司选中微软公司为旗下产品PC机开发操作系统，事后证明，这是公司发展历程中的一个重大转折点。然而签下协议后，盖茨却不得不面临着时间紧迫、程序复杂的难题。这时由艾伦牵头，微软以五万九千美元的价格从位程序编制者Tim Paterson手中买下了一个操作系统QDOS的使用权，在进行部分改写后提供给IBM，并将其命名为Microsoft DOS（Disk Operating System，磁盘操作系统）。IBM-PC机的普及很快打响了Microsoft DOS的名声，因为其他PC制造商业都希望与IBM兼容。因此盖茨在1982年向50家硬件制造商授予了MS-DOS操作系统的使用权，DOS逐渐成为了PC机的标准操作系统。此后盖茨与IBM签订合同，微软为IBM PC提供BASIC解译器和操作系统。1983年11月10日，微软首次向外界展示了Windows操作系统，该产品是MS-DOS操作系统的演进版。也正是在这一年，盖茨的伙伴艾伦因为罹患霍奇金氏病而离开微软。此后的微软在盖茨等人的带领下取得了令人瞩目的业绩。到1984年，微软公司的销售额超过1亿美元。微软继续为IBM、苹果公司以及无线电器材公司的计算机开发软件。随着公司的不断壮大，Microsoft与IBM渐渐在许多方面成为了竞争对手。由于利益冲突，微软在1991年先后与IBM及苹果中断了长久以来保持的合作关系。 从PC领域的第一个亿万富翁到世界首富1986年３月13日，微软公司在美国纳斯达克上市。盖茨持股45%，这使得他成为1987年PC产业中的第一位亿万富翁。此后盖茨与微软的高层逐渐将目光瞄向了操作系统之外的市场。1992年，微软买进Fox公司，正式宣布进军数据库软件领域。1995年，微软推出在线服务MSN（Microsoft Network，微软网络），这是一款即时信息客户程序，这使得微软成了当时的AOL（American Online,美国在线）的直接竞争对手。1996年，微软与美国的广播业巨头NBC（国家广播公司）联合创立了MSNBC，这是一个综合性的24小时新闻频道以及在线新闻服务供应商。1997年，微软收购了当时最受欢迎的webmail服务商Hotmail，并重新命名为MSN Hotmail，之后盖茨将其发展成了成为.NET Passport（这是一个综合登入服务系统的平台）。可以说，20世纪90年代是微软的发展历程中极为辉煌的一页，相信也是盖茨个人的人生履历中值得铭记的一段经历。正是在这一段时间内，微软从一个替其他公司编写程序的供应商一跃成为独立开发Windows操作系统、横跨多个领域的大型互联网企业。截止1995年，微软的员工数量已经达到了17801人，当年的销售总额为59亿美元。而这一年，39岁的盖茨也因此荣登当年的《福布斯》全球亿万富翁排行榜榜首，成为最年轻的全球首富，个人财富为129亿美元。 离开微软，致力于慈善事业在盖茨成为世界首富之后，他的个人财富与微软公司的利润一直保持快速的增长之势。到1998年，微软的收入增至140亿美元，其中利润高达40亿美元，增长率比1997年提高了23%。目前微软涉及很多行业，从网络旅游服务、网络汽车销售、投资咨询一直到有线电视、游戏娱乐业等，影响无所不在。美国曾有一家杂志发表过一篇文题为“谁是微软下一道菜？”的文章，从中我们不难看出盖茨的野心与壮志。盖茨的野心在2000年按下了暂停键，盖茨对外宣布自己将卸任微软首席执行官一职，转而任命他长期的好友史蒂夫·巴尔默（Steve Ballmer）代替他成为下一任CEO，而自己则担任CFO。目前，盖茨个人持有的股票市值已突破1000亿美元，有人曾戏谑道：假如盖茨掉了1000美元，他也懒得去捡，因为他去捡要花掉4秒钟，这一弯腰他已赚回1000美元。很多人会好奇，盖茨将怎样处理这笔数目庞大的财富，毕竟无论他怎么挥霍，也花不完这么多的钱。巨大财富的背后，盖茨一直专注于慈善事业。早在1994年，盖茨便在父亲的建议下，拿出了9400万美元，创立威廉·盖茨基金会。1999年，盖茨和他的妻子将基金会更名为比尔和梅琳达·盖茨基金会，并宣称该基金会的宗旨是“减少全球存在的不平等现象”。2008年，盖茨宣布将 580亿美元个人财产捐到比尔和梅琳达·盖茨基金会，用于研究艾滋病和疟疾的疫苗，并为世界贫穷国家提供援助。2017年，盖茨捐出了相当于其个人财富5%的微软公司股票，价值46亿美元，成为当年全球数额最大的捐款。即使在今天，盖茨宣布离开微软董事会的时候，也不忘表明他对慈善事业的支持，他说：我将把更多的时间投入到慈善事业的优先事项上，包括全球健康与发展、教育，以及我在应对气候变化方面越来越多的投入。 资料：比尔盖茨退出微软董事会：把更多时间投入慈善事业百度百科https://wiki.mbalib.com/wiki/%E5%BE%AE%E8%BD%AF%E5%85%AC%E5%8F%B8","categories":[{"name":"其他","slug":"其他","permalink":"http://huangzhiyuan.github.io/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"新闻","slug":"新闻","permalink":"http://huangzhiyuan.github.io/tags/%E6%96%B0%E9%97%BB/"}]},{"title":"pytorch内部机制","slug":"pytorch-internals","date":"2020-03-13T08:33:50.000Z","updated":"2020-04-10T01:43:34.000Z","comments":true,"path":"2020/03/13/pytorch-internals/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/13/pytorch-internals/","excerpt":"斯坦福大学博士生与 Facebook 人工智能研究所研究工程师 Edward Z. Yang 是 PyTorch 开源项目的核心开发者之一。他在 5 月 14 日的 PyTorch 纽约聚会上做了一个有关 PyTorch 内部机制的演讲，本文是该演讲的长文章版本。","text":"斯坦福大学博士生与 Facebook 人工智能研究所研究工程师 Edward Z. Yang 是 PyTorch 开源项目的核心开发者之一。他在 5 月 14 日的 PyTorch 纽约聚会上做了一个有关 PyTorch 内部机制的演讲，本文是该演讲的长文章版本。 PyTorch Internals","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://huangzhiyuan.github.io/tags/pytorch/"}]},{"title":"Intel CPU命名规则","slug":"cpu-name-rule","date":"2020-03-12T12:14:54.000Z","updated":"2020-03-12T12:45:32.000Z","comments":true,"path":"2020/03/12/cpu-name-rule/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/12/cpu-name-rule/","excerpt":"对大部分对CPU有一定了解的同学一定知道购买桌面级英特尔的CPU的时候，要看看CPU是i3、i5还是i7的，这大致决定了CPU的性能定位和价格。另外部分同学还会注意一下CPU带不带“K”。在我们的印象里带“K”后缀的CPU是能超频的，性能和价格也一定是比不带“K”的CPU要高出一截来的。这种粗广的判断方式不能算是错误的，但确实是不全面的。随着英特尔对自家新品的命名慢慢进入了一种匪夷所思的境界，这种情况正在变得越来越严重。","text":"对大部分对CPU有一定了解的同学一定知道购买桌面级英特尔的CPU的时候，要看看CPU是i3、i5还是i7的，这大致决定了CPU的性能定位和价格。另外部分同学还会注意一下CPU带不带“K”。在我们的印象里带“K”后缀的CPU是能超频的，性能和价格也一定是比不带“K”的CPU要高出一截来的。这种粗广的判断方式不能算是错误的，但确实是不全面的。随着英特尔对自家新品的命名慢慢进入了一种匪夷所思的境界，这种情况正在变得越来越严重。 系列分类上表是目前英特尔在售的全部8中产品系列，其中就包含好几个我们耳熟能详的名字。例如奔腾系列、酷睿系列、至强系列、赛扬系列等等。我们今天要展开讲的就是玩家接触最多的桌面版酷睿系列处理器。 酷睿第一代英特尔酷睿处理器使用在笔记本上的，比较短命。酷睿品牌从2006年5月上市的二代产品开始正式进入大众视野。这一代酷睿处理器正式成为英特尔处理器的代表产品。而酷睿系列在成为行业无可争议的老大之后，也就出现了Core i3/i5/i7/i9这些不同的等级代号，以核心数和线程数作为区分标准。以目前最新的8代CPU为例，i3是4核4线程，i5是6核6线程，两者都不支持超线程技术。但i7是6核12线程，i9则被定位成旗舰机桌面产品与他的小弟弟们不在一个行列之内了。 迭代还是以酷睿处理器的命名规则为例，在产品系列、等级（Core i3/i5/i7/i9）之后通常会有几位数字和一位字母，而这几位数字的第一位通常就是代表CPU的迭代，比如Core i3-8100，这个8就代表这是第八代酷睿产品。 型号划分迭代号之后的三位数字是CPU的型号代码，一般都是比较稳定不轻易发生变更的。比如Core i7-6700K、Core i7-7700K、 Core i7-8700K，都是700这个型号。Core i5-6500、Core i5-7500、Core i5-8500都是500这个型号。一般来说，数字越大代表频率越高、性能越强，这一点和CPU的等级代号（i3/i5/i7）是类似的。 命名后缀这一部分是CPU命名体系里最复杂最难懂的，在英特尔冗长的产品线中，CPU的后缀也是千变万化。不带后缀的CPU一般就是最普通的桌面级处理器，不管是性能还是价格都比较中庸，比如当前性价比较高的Core i5-8400。由于CPU后缀分类没有一个明显的界限，所以下面的排序按照字母顺序进行排列，有精力的同学可以自行分类。 B这一后缀是为了满足一些例如一体机等紧凑型设备的需要而开发出来的，这类CPU并不靠针脚与主板连接，而是利用FCBGA1440封装直接焊接在主板上。代表作Core i5-8500B。 C这个后缀只在5代CPU上出现过，但因为5代CPU特别短命，了解这一代产品的人不多，所以导致这个后缀也基本没人了解。带有这个后缀的CPU都拥有当时最强核显Iris Pro6200，但由于售价太高导致匆匆退市，整个5代产品也少有人提起。代表作：Core i7-5775C。 G带G后缀的CPU通常被称为Kaby Lake G处理器，这类处理器是Intel和AMD合作的产品。CPU部分由Intel负责，GPU部分则是AMD提供，两者连同超强性能的HBM2显存一同整合在同一块基板上，性能强大。这类产品已经可以脱离独立显卡流畅工作。代表作Core i7-8809G。 H在第八代以前，H的意思是bga焊接封装（不可动手更换），与其相对应的是M（pga封装，一般都可更换同代CPU）。八代开始，H对i7来说不止意味着bga封装，也意味着6核（Hexagon），相对的是Q（4核，quad），H不代表有高于本代平均水准的gpu，也没有自带超线程的意思，比如i3 8100H就是4核4线。 HK这类产品是在H后缀的处理器上加入了超频属性，从此笔记本产品也可以超频了。但是超频之后带来的功耗和散热问题，可能就不是Intel要操心的事情了。代表作Core i9-8950HK。 HQ四代CPU中就出现的CPU后缀，代表作为i7 4700hq。 K这个就不多说了，普通玩家接触最多的后缀，搭配合适的主板可以进行超频。代表作Core i7-8700K。 M这一后缀代表双核移动版处理器，但是从4代CPU以后就再也见不到了。代表作Core i5-4310M。 R和B后缀类似的移动版处理器，代表作Core i7-5775R。 T低压版桌面处理器，功耗和性能都比不带后缀的产品低。代表作Core i7-8700T。 U低压版移动处理器，被用在各种上网本、超极本、超薄本上，代表作Core i7-8550U。 X带有这一后缀的CPU一般都具有王霸之气，基本上就是当代最强桌面级CPU的代号，一般会搭配X99/X299/X399这类平台使用，代表作Core i7-7820X。 XE比X后缀还牛逼的CPU后缀，这类产品都有与生俱来的超王霸之气，旗舰中的旗舰，顶级中的顶级产品。目前市面上只有一款：Core i9-7980XE。 后记虽然Intel对CPU的命名一直都是稳中有乱，但这并不妨碍我们从CPU的命名上大致了解CPU的定位、性能、价格、用途。在了解了这部分知识之后，选购CPU或是其他PC产品的时候也就能少走不少弯路。 原文链接","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"cpu","slug":"cpu","permalink":"http://huangzhiyuan.github.io/tags/cpu/"}]},{"title":"莱斯大学&英特尔新算法证明CPU加速深度学习优于GPU!","slug":"cpu-is-better-thatn-gpu","date":"2020-03-11T14:44:30.000Z","updated":"2020-03-11T14:52:54.000Z","comments":true,"path":"2020/03/11/cpu-is-better-thatn-gpu/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/11/cpu-is-better-thatn-gpu/","excerpt":"实验室一块GPU都没有怎么做深度学习？如果让莱斯大学和英特尔的研究人员来回答，答案大概是：用CPU啊。莱斯大学和英特尔的最新研究证明，无需专门的加速硬件（如GPU），也可以加速深度学习。算法名为SLIDE。研究人员称，SLIDE是第一个基于CPU的深度学习智能算法，并且，在具有大型全连接架构的行业级推荐数据集上，SLIDE训练深度神经网络的速度甚至超过了GPU。代码已开源。","text":"实验室一块GPU都没有怎么做深度学习？如果让莱斯大学和英特尔的研究人员来回答，答案大概是：用CPU啊。莱斯大学和英特尔的最新研究证明，无需专门的加速硬件（如GPU），也可以加速深度学习。算法名为SLIDE。研究人员称，SLIDE是第一个基于CPU的深度学习智能算法，并且，在具有大型全连接架构的行业级推荐数据集上，SLIDE训练深度神经网络的速度甚至超过了GPU。代码已开源。 基于局部敏感哈希摆脱GPU的核心思想，是利用局部敏感哈希来摆脱矩阵乘法。 代码采用C++编写。 论文一作Beidi Chen介绍： 基于TensorFlow和PyTorch来实现SLIDE算法是没有意义的，因为那必须把问题转换成矩阵乘法问题，而这一点恰恰是我们想要摆脱的。 在架构上，SLIDE的中心模块是神经网络。网络的每个层模块由神经元和一些哈希表组成，神经元ID被哈希到其中。 每个神经元模块都包含： 一个二进制数组，提示该神经元是否对于batch中的每一个输入都有效 batch中的每一个输入的activation batch中每个输入的累积梯度 与上一层的连接权重 最后一个数组的长度等于上一层中神经元的数量。 每层中的LSH哈希表构造都是一次性操作，可以与该层中不同神经元上的多个线程并行。 论文作者之一、莱斯大学助理教授Anshumali Shrivastava表示，SLIDE相对于反向传播的最大优势就在于数据并行。 举个例子，数据并行的情况下，要训练两个数据实例，一个是猫的图像，另一个是公共汽车的图像，它们可能会激活不同的神经元，而SLIDE可以分别独立地更新、训练它们。 如此，就能更好地利用CPU的并行性。 不过，与GPU相比，该方法对内存要求较高。Shrivastava也提到，在与英特尔的合作中，他们针对SLIDE，对CPU进行了优化，比如支持Kernel Hugepages以减少缓存丢失。这些优化使得SLIDE的性能提高了约30%。 实验结果所以，与依赖GPU的深度学习相比，SLIDE到底表现如何？ 研究人员在Delicious-200K和Amazon-670K这两个大型数据集上进行了验证。 实验配置，是2个22核/44线程处理器（Intel Xeon E5-2699A v4 2.40GHz），和英伟达TeslaV100 Volta 32GB GPU。 结果表明，在任意精度上，CPU上的SLIDE总是比V100上基于TensorFlow的GPU算法快。 在Delicious-200K数据集上，SLIDE比TF-GPU快1.8倍；而在算力要求更高的Amazon-670K数据集上，SLIDE的速度更是TF-GPU的2.7倍。 其大部分计算优势，来自于对输出层中一小部分激活神经元的采样。 而在相同的CPU上，SLIDE要比基于TensorFlow的算法快10倍以上。 网友：英特尔的广告？在CPU上跑深度学习能快过GPU，这样的结论立刻吸引住了网友们的目光。 有网友分析说： 该方法不仅使用了哈希表，其速度之快还得归功于OpenMP的硬件多核优化。（OpenMP是一套支持跨平台共享内存方式的多线程并发的编程API）看起来在小型DNN中是非常有前途的替代方案。不过，问题在于，该方法是否可以推广到其他CPU架构中？这种方法中还是存在巨大的冲突和牺牲准确性的风险。 还有人表示，在与作者的交流中，他认为该方法主要适用于宽网络，否则哈希表的开销会大于其收益。那么至少，在架构探索中，该方法提供了探索更宽网络的可能性。 不过，也有网友提出了更尖锐的质疑：怕不是来给英特尔打广告的。 1、预处理步骤看上去开销高昂。2、采用了特殊优化的架构，那么性能增益有多少是归功于方法本身的？3、缺少分别在CPU和GPU上运行SLIDE的比较。 传送门论文地址：https://arxiv.org/abs/1903.03129githu: https://github.com/keroro824/HashingDeepLearning","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"cpu","slug":"cpu","permalink":"http://huangzhiyuan.github.io/tags/cpu/"}]},{"title":"一不小心错过几个亿！解密微信红包算法","slug":"qiang-hong-bao","date":"2020-03-10T07:15:30.000Z","updated":"2020-03-10T08:16:08.000Z","comments":true,"path":"2020/03/10/qiang-hong-bao/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/10/qiang-hong-bao/","excerpt":"还记得2017年，微信红包收发总量达到460亿个，2019年，除夕到初五，8.23亿人收发微信红包。一觉醒来，微信群里各种红包，顿时觉得错过了几个亿，破解了红包的规律，是不是就可以发财了呢？","text":"还记得2017年，微信红包收发总量达到460亿个，2019年，除夕到初五，8.23亿人收发微信红包。一觉醒来，微信群里各种红包，顿时觉得错过了几个亿，破解了红包的规律，是不是就可以发财了呢？ 抢红包流程红包生成，数据库中创建红包信息，把红包的ID、数量放入缓存用户抢红包，分为抢和拆两个动作，抢动作只是决定用户是否得到红包资格，如果抢到了，进入拆动作，此时实时计算红包的金额、记录红包流水记录、入账操作 算法满足要求 每个人都要能够领取到红包； 每个人领取到的红包金额总和=总金额； 每个人领取到的红包金额不等，但也不能差的太离谱，不然就没趣味； 算法一定要简单 微信红包算法剩余红包金额为 M，剩余人数为 N，那么有如下公式： 每次抢到的金额 = 随机区间 （0.01， M / N X 2） 这个公式，保证了每次随机金额的平均值是相等的，不会因为抢红包的先后顺序而造成不公平。 举个栗子： 假设有 10个人，红包总额 100元。 100/10X2 = 20, 所以第一个人的随机范围是（0.01，20 )，平均可以抢到 10元。 假设第一个人随机到 10元，那么剩余金额是 100-10 = 90 元。 90/9X2 = 20, 所以第二个人的随机范围同样是（0.01，20 )，平均可以抢到 10元。 假设第二个人随机到 10元，那么剩余金额是 90-10 = 80 元。 80/8X2 = 20, 所以第三个人的随机范围同样是（0.01，20 )，平均可以抢到 10元。 以此类推，每一次随机范围的均值是相等的。 架构设计一个抢红包的功能当然不会那么简单，会涉及并发、缓存等等、这个也是面试的考题，下面补充几点关注点： 微信的金额什么时候算？金额是拆的时候实时算的，不是预先分配，采用纯内存计算，不需要计算空间存储。采用实时计算的原因：预算需要占内存且效率低，实时效率高 红包的设计：微信从财付通拉取金额数据，生成个数/红包类型/金额放到redis集群里，app端红包id的请求放入请求队列中，如果发现超过红包个数，直接返回。根据红包的处理成功得到令牌请求，由财付通进行一次性调用，通过像比特币一样，两边保存交易记录，交易后交给第三方服务审计，如果交易过程中出现不一致则强制回归。 并发处理：红包如何计算被抢完？Cache会抵抗无效请求，将无效的请求过滤，实际进入后台的量不大。Cache记录红包个数，原子操作进行个数递减，到0表示被抢光。财付通按照20万笔每秒入账准备，但实际还不到8万每秒 财付通如何保持8万每秒写入？多主sharding，水平拓展机器 数据容量多少？一个红包占一条记录，有效期只有几天，因此不需要太多空间 查询红包分配，压力大不大？抢到红包的人数和红包都在一条cache记录上，没有太大的查询压力 一个红包一个队列？没有队列，一个红包一条记录，数据上有一个计数器字段 会不会出现两个最佳？会出现金额一样的，但是最佳只有一个，先抢到的最佳。 每领一个红包就更新数据吗？每抢到一个红包，cache更新剩余金额和红包个数 红包如何入库入账？数据库会累加已经领取的个数与金额，插入一条领取记录。入账则是后台异步操作。 入账错了怎么办，比如红包个数没了，红包还有？最后会有一个take all操作，另外还有一个对账来保障。 python实现12345678910111213141516171819202122232425import random# M: the all mouont the money, N: the num of personM = 100N = 10remain_m = Mremain_n = Npackege = 0for i in range(1, N): if remain_n == 0: print(&quot;no red envelope!&quot;) else: # [0.01, remain_m / remain_n * 2] max = remain_m / remain_n * 2 package = random.random() * max if package &lt; 0.01: package = 0.01 remain_m -= package remain_n -= 1 print(&quot;%.2f&quot; % (package))print(&quot;%.2f&quot; % (remain_m)) 运行20次每人平均抢到红包金额如下： 结论 先抢后抢，期望大致相等 对于第一个人来说，抽取红包大小服从均匀分布；对于之后的人来说，不服从均匀分布。 越到后面，越有可能抽到一个很小的红包，也越有可能抽到一个很大的红包 理论上来说，如果前面的运气都很差，最后一个人可以拿到接近总额的红包结论就是： 风险规避的人，应该尽可能往前抽取风险偏爱的人，应该尽可能往后抽取，而且越往后，增速标准差越大，极有可能抽到一个很大的红包；同理，小红包的概率也越大。 参考链接：https://cloud.tencent.com/developer/article/1575704https://cloud.tencent.com/developer/article/1082036https://www.zhihu.com/question/22625187","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"抢红包","slug":"抢红包","permalink":"http://huangzhiyuan.github.io/tags/%E6%8A%A2%E7%BA%A2%E5%8C%85/"}]},{"title":"horovod如何实现大规模分布式深度学习","slug":"horovod-distributed-training","date":"2020-03-09T13:17:40.000Z","updated":"2020-03-09T13:42:40.000Z","comments":true,"path":"2020/03/09/horovod-distributed-training/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/09/horovod-distributed-training/","excerpt":"Horovod是Uber（优步）开源的又一个深度学习工，Horovod在2017年10月，Uber以Apache 2.0授权许可开源发布。Horovod是优步跨多台机器的分布式训练框架，现已加入开源计划LF Deep Learning Foundation。 Uber利用Horovod来支持自动驾驶汽车，欺诈检测和出行预测。该项目的贡献者包括亚马逊，IBM，英特尔和Nvidia。除了优步，阿里巴巴，亚马逊和Nvidia也在使用Horovod。Horovod项目可以与TensorFlow，Keras和PyTorch等流行框架一起使用。优步于上个月加入了Linux基金会，并加入了其他科技公司，如AT＆T和诺基亚，他们出面支持LF Deep Learning Foundation开源项目。LF深度学习基金会成立于3月，旨在支持深度学习和机器学习的开源项目，并且是Linux基金会的一部分。在推出Acumos（用于训练和部署AI模型）和Acumos Marketplace（AI模型的开放式交易所）推出一个月后，Horovod正式推出。自该基金会启动以来，开展的其他项目包括机器学习平台Angel and Elastic Deep Learning，该项目旨在帮助云服务提供商利用TensorFlow等框架制作云集群服务。百度和腾讯分别于八月份加入这些项目，它们也是LF深度学习基金会的创始成员。","text":"Horovod是Uber（优步）开源的又一个深度学习工，Horovod在2017年10月，Uber以Apache 2.0授权许可开源发布。Horovod是优步跨多台机器的分布式训练框架，现已加入开源计划LF Deep Learning Foundation。 Uber利用Horovod来支持自动驾驶汽车，欺诈检测和出行预测。该项目的贡献者包括亚马逊，IBM，英特尔和Nvidia。除了优步，阿里巴巴，亚马逊和Nvidia也在使用Horovod。Horovod项目可以与TensorFlow，Keras和PyTorch等流行框架一起使用。优步于上个月加入了Linux基金会，并加入了其他科技公司，如AT＆T和诺基亚，他们出面支持LF Deep Learning Foundation开源项目。LF深度学习基金会成立于3月，旨在支持深度学习和机器学习的开源项目，并且是Linux基金会的一部分。在推出Acumos（用于训练和部署AI模型）和Acumos Marketplace（AI模型的开放式交易所）推出一个月后，Horovod正式推出。自该基金会启动以来，开展的其他项目包括机器学习平台Angel and Elastic Deep Learning，该项目旨在帮助云服务提供商利用TensorFlow等框架制作云集群服务。百度和腾讯分别于八月份加入这些项目，它们也是LF深度学习基金会的创始成员。 深度学习到底是如何训练数据的？深度学习训练的算法叫做反向传播。即通过神经网络得到预测结果，把预测结果跟标注Label进行比对，发现误差；然后得到神经网络里每个神经元权重导数；接着通过算法得到每个神经元导数，再更新神经元的权重以得到更好的神经元网络，周而复始迭代训练，使得误差减少。 神经网络推理能力随着规模、复杂度增加，能力会极大的增强。但从计算能力角度来说又出现了新问题：很多时候大规模神经网络很难在单个/单点计算单元里面运行，这会导致计算很慢，以至无法运行大规模数据。所以人们提出两种深度学习的基本方法以解决这个问题。 深度学习的两种基本方法第一种是模型并行。即把复杂的神经网络拆分，分布在计算单元或者GPU里面进行学习，让每个GPU同步进行计算。这个方法通常用在模型比较复杂的情况下。 另一种是数据并行。即让每个机器里都有一个完整模型，然后把数据切分成n块，把n块分发给每个计算单元，每个计算单元独自计算出自己的梯度。同时每个计算单元的梯度会进行平均、同步，同步后的梯度可以在每个节点独立去让它修正模型，整个过程结束后每个节点会得到同样的模型。这个方法可以让能够处理的数据量增加，变成了原来的n倍。 白话版走心解读：深度学习的这两种基本方法之前提及过。简单说就是一个拆模型，一个拆数据，但目标相同：让运算更高效。这里可以让上面提及的“梯度”先来混个脸熟。 首先“梯度下降”是深度学习优化的一种方法。很好理解，就是说它可以更好地训练模型。那“梯度下降”为什么可以实现优化呢？ 我们可以把神经网络理解成一个非常复杂的函数，包含数百万个参数。这些参数代表的是一个问题的数学解答。实质上，训练神经网络=最小化一个损失函数。而“梯度下降”可以帮助我们找到那个最小化的损失函数。好了，先到这里…… 实现“数据并行”的两种工程方法参数服务器（Parameter Server）。在计算单元以外加设新的服务器叫做参数服务器。每次训练的时候每个计算单元把梯度发送给参数服务器，服务器把他们进行汇总计算平均值，把平均值返回到每个计算单元，这样每个计算单元就同步了。 Ring-AllReduce它是从高性能计算集合通信找到的想法。做法是把每个计算单元构建成一个环，要做梯度平均的时候每个计算单元先把自己梯度切分成N块，然后发送到相邻下一个模块。现在有N个节点，那么N-1次发送后就能实现所有节点掌握所有其他节点的数据。这个方法被证明是一个带宽最优算法。 白话版走心解读：“工程做法”其实就是实现方法。（另外再敲个黑板！使用多个GPU卡训练同一个深度学习任务就是分布式计算）再过几天就十一了，我们不妨借着祖国70年华诞举个栗子。这里讲的数据并行的两种实现办法，可以这样理解。 学校舞蹈团想排一支集体舞庆祝华诞。由于报名火爆，入选人员超出预期。于是领队由1人增加为2人，便于管理庞大的团员，这个便是“参数服务器”方法，以增加“领队”的方式提升管理数据的效率；还有一种是，取消领队，大家彼此监督，发挥集体各成员的最大脑力，这个就是“Ring-All Reduce”，以取消“领队”的方式降低沟通成本并且激发成员最大潜力。这样来看，两者优劣势就清晰了。 参数服务器的做法理论容错性比较强，因为每个节点相互之间没有牵制，互相没有关联，它只是需要跟参数服务器本身进行通信，就可以运作了。缺点是有额外的网络开销，扩展效率会受到影响。 Ring-AllReduce优点非常明显，性能非常好，如果在大规模分布式训练时候资源利用率相当高，网络占用是最优的。它的缺点是在工程上的缺点，容错性较差，很多实现都是用MPI实现（MP本身并不是为容错设计的，它更偏向于照顾高性能的计算）。 Horovod是什么Horovod是基于Ring-AllReduce方法的深度分布式学习插件，以支持多种流行架构包括TensorFlow、Keras、PyTorch等。这样平台开发者只需要为Horovod进行配置，而不是对每个架构有不同的配置方法。白话版走心解读：Horovod的名字来源于俄罗斯的民族舞蹈，跳舞者手牵手围成一个圈。给它命名的人是位俄罗斯工程师，因为他觉得这个架构看起来很像在“转圈”。 很形象，我们可以看出它是基于Ring-AllReduce方法进行开发的；而另一方面我们也可以窥见，看似只是生活在枯燥艰涩的代码世界的程序员们，自有着独特的浪漫。 Horovod 如何使用首先认识一下Keras单机训练脚本。 第一步用Keras定义它的model；第二步X-train、Y-train，定义训练样本和测试样本；第三步optimizer，即选定优化器；第四步model.compile；最后是model. fit。这样就可以开始训练了，这是Keras训练脚本骨干。 接下来就是怎样把单机训练脚本跟Horovod结合变成多机训练脚本、分布式训练脚本。 第一，在代码里引入Horovod库，做初始化；第二，定义优化器的时候，优化器重要参数是学习率；第三，用一个对象把原来优化器包起来，这就是抽象了Horovod需要进行的梯度平均的逻辑，会比较容易使用；第四，让Horovod把每次训练的初始状态广播到每个节点，这样保证每个节点从同一个地方开始；第五，训练的长度，由于把训练分布在N个机器上了，所以可以极大减小，除以N就可以了。 黄色是添加的代码或者要修改的地方 总结一下，把单个计算单元训练变成多机分布式训练用Horovod是非常简单的，只需要做三步。第一步程序引入Horovod稍微做修改，调整学习率、时间；第二步处理训练的数据，进行分布化；第三步用Horovodrun程序进行启动，就可以进行分布式训练了。 Horovod 使用案例美国橡树岭国家实验室在全球最快的超级计算机Summit上，用Horovod进行像素级高清气象云图分析，去年获得了ACN Gordon Bell Prize。 这是世界上首次突破Exaops，即每秒一百亿亿次的深度学习应用，峰值计算达1.13 EF/S，持续计算999.0 PF/S。什么概念呢？2017年得奖是中国在太湖之光做的地震仿真的项目，这个项目持续计算是18.9个Exaops，上面有4560节点，每个节点上有6个GPU，有27360 Volta GPU，用到了Horovod高级功能分级化AllReduce、Tensor融合、16位浮点数，90.7%扩展效率。ORNL一份声明称，Summit超级计算机在一秒钟时间内完成的计算任务，能让全世界所有人不吃不喝，不休不眠地以每秒1次的计算速度连续算上306天。如果只交给1个人来计算的话，那么就算太阳燃料耗尽，然后超新星爆炸，地球被毁灭，这个人都不会算完，因为这个计算过程需要63.4亿年，然而我们太阳系却只有50亿年寿命。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Horovod","slug":"Horovod","permalink":"http://huangzhiyuan.github.io/tags/Horovod/"}]},{"title":"知识图谱的技术应用","slug":"technology-application-of-knowledge-graph","date":"2020-03-09T02:00:41.000Z","updated":"2020-03-09T14:21:50.000Z","comments":true,"path":"2020/03/09/technology-application-of-knowledge-graph/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/09/technology-application-of-knowledge-graph/","excerpt":"从一开始的Google搜索，到现在的聊天机器人、大数据风控、证券投资、智能医疗、自适应教育、推荐系统，无一不跟知识图谱相关。它在技术领域的热度也在逐年上升。本文以通俗易懂的方式来讲解知识图谱相关的知识、尤其对从零开始搭建知识图谱过程当中需要经历的步骤以及每个阶段需要考虑的问题都给予了比较详细的解释。对于读者，我们不要求有任何AI相关的背景知识。","text":"从一开始的Google搜索，到现在的聊天机器人、大数据风控、证券投资、智能医疗、自适应教育、推荐系统，无一不跟知识图谱相关。它在技术领域的热度也在逐年上升。本文以通俗易懂的方式来讲解知识图谱相关的知识、尤其对从零开始搭建知识图谱过程当中需要经历的步骤以及每个阶段需要考虑的问题都给予了比较详细的解释。对于读者，我们不要求有任何AI相关的背景知识。 2012 年 5 月 17 日，Google 正式提出了知识图谱（Knowledge Graph）的概念，其初衷是为了优化搜索引擎返回的结果，增强用户搜索质量及体验。 概论随着移动互联网的发展，万物互联成为了可能，这种互联所产生的数据也在爆发式地增长，而且这些数据恰好可以作为分析关系的有效原料。如果说以往的智能分析专注在每一个个体上，在移动互联网时代则除了个体，这种个体之间的关系也必然成为我们需要深入分析的很重要一部分。在一项任务中，只要有关系分析的需求，知识图谱就“有可能”派的上用场。 什么是知识图谱知识图谱是由Google公司在2012年提出来的一个新的概念。从学术的角度，我们可以对知识图谱给一个这样的定义：“知识图谱本质上是语义网络（Semantic Network）的知识库”。但这有点抽象，所以换个角度，从实际应用的角度出发其实可以简单地把知识图谱理解成多关系图（Multi-relational Graph）。 那什么叫多关系图呢？学过数据结构的都应该知道什么是图（Graph）。图是由节点（Vertex）和边（Edge）来构成，但这些图通常只包含一种类型的节点和边。但相反，多关系图一般包含多种类型的节点和多种类型的边。比如左下图表示一个经典的图结构，右边的图则表示多关系图，因为图里包含了多种类型的节点和边。这些类型由不同的颜色来标记。在知识图谱里，我们通常用“实体（Entity）”来表达图里的节点、用“关系（Relation）”来表达图里的“边”。实体指的是现实世界中的事物比如人、地名、概念、药物、公司等，关系则用来表达不同实体之间的某种联系，比如人-“居住在”-北京、张三和李四是“朋友”、逻辑回归是深度学习的“先导知识”等等。 现实世界中的很多场景非常适合用知识图谱来表达。比如一个社交网络图谱里，我们既可以有“人”的实体，也可以包含“公司”实体。人和人之间的关系可以是“朋友”，也可以是“同事”关系。人和公司之间的关系可以是“现任职”或者“曾任职”的关系。类似的，一个风控知识图谱可以包含“电话”、“公司”的实体，电话和电话之间的关系可以是“通话”关系，而且每个公司它也会有固定的电话。 知识图谱的表示知识图谱应用的前提是已经构建好了知识图谱，也可以把它认为是一个知识库。这也是为什么它可以用来回答一些搜索相关问题的原因，比如在Google搜索引擎里输入“Who is the wife of Bill Gates?”，我们直接可以得到答案-“Melinda Gates”。这是因为我们在系统层面上已经创建好了一个包含“Bill Gates”和“Melinda Gates”的实体以及他俩之间关系的知识库。所以，当我们执行搜索的时候，就可以通过关键词提取（”Bill Gates”, “Melinda Gates”, “wife”）以及知识库上的匹配可以直接获得最终的答案。这种搜索方式跟传统的搜索引擎是不一样的，一个传统的搜索引擎它返回的是网页、而不是最终的答案，所以就多了一层用户自己筛选并过滤信息的过程。 在现实世界中，实体和关系也会拥有各自的属性，比如人可以有“姓名”和“年龄”。当一个知识图谱拥有属性时，我们可以用属性图（Property Graph）来表示。下面的图表示一个简单的属性图。李明和李飞是父子关系，并且李明拥有一个138开头的电话号，这个电话号开通时间是2018年，其中2018年就可以作为关系的属性。类似的，李明本人也带有一些属性值比如年龄为25岁、职位是总经理等。 这种属性图的表达很贴近现实生活中的场景，也可以很好地描述业务中所包含的逻辑。除了属性图，知识图谱也可以用RDF来表示，它是由很多的三元组（Triples）来组成。RDF在设计上的主要特点是易于发布和分享数据，但不支持实体或关系拥有属性，如果非要加上属性，则在设计上需要做一些修改。目前来看，RDF主要还是用于学术的场景，在工业界我们更多的还是采用图数据库（比如用来存储属性图）的方式。感兴趣的读者可以参考RDF的相关文献，在文本里不多做解释。 知识抽取知识图谱的构建是后续应用的基础，而且构建的前提是需要把数据从不同的数据源中抽取出来。对于垂直领域的知识图谱来说，它们的数据源主要来自两种渠道：一种是业务本身的数据，这部分数据通常包含在公司内的数据库表并以结构化的方式存储；另一种是网络上公开、抓取的数据，这些数据通常是以网页的形式存在所以是非结构化的数据。 前者一般只需要简单预处理即可以作为后续AI系统的输入，但后者一般需要借助于自然语言处理等技术来提取出结构化信息。比如在上面的搜索例子里，Bill Gates和Malinda Gate的关系就可以从非结构化数据中提炼出来，比如维基百科等数据源。信息抽取的难点在于处理非结构化数据。在下面的图中，我们给出了一个实例。左边是一段非结构化的英文文本，右边是从这些文本中抽取出来的实体和关系。在构建类似的图谱过程当中，主要涉及以下几个方面的自然语言处理技术： 实体命名识别（Name Entity Recognition） 关系抽取（Relation Extraction） 实体统一（Entity Resolution） 指代消解（Coreference Resolution） 下面针对每一项技术解决的问题做简单的描述，以至于这些是具体怎么实现的，不在这里一一展开，感兴趣的读者可以查阅相关资料，或者学习我的课程。 首先是实体命名识别，就是从文本里提取出实体并对每个实体做分类/打标签：比如从上述文本里，我们可以提取出实体-“NYC”，并标记实体类型为 “Location”；我们也可以从中提取出“Virgil’s BBQ”，并标记实体类型为“Restarant”。这种过程称之为实体命名识别，这是一项相对比较成熟的技术，有一些现成的工具可以用来做这件事情。其次，我们可以通过关系抽取技术，把实体间的关系从文本中提取出来，比如实体“hotel”和“Hilton property”之间的关系为“in”；“hotel”和“Time Square”的关系为“near”等等。 另外，在实体命名识别和关系抽取过程中，有两个比较棘手的问题：一个是实体统一，也就是说有些实体写法上不一样，但其实是指向同一个实体。比如“NYC”和“New York”表面上是不同的字符串，但其实指的都是纽约这个城市，需要合并。实体统一不仅可以减少实体的种类，也可以降低图谱的稀疏性（Sparsity）；另一个问题是指代消解，也是文本中出现的“it”, “he”, “she”这些词到底指向哪个实体，比如在本文里两个被标记出来的“it”都指向“hotel”这个实体。 实体统一和指代消解问题相对于前两个问题更具有挑战性。 知识图谱的存储知识图谱主要有两种存储方式：一种是基于RDF的存储；另一种是基于图数据库的存储。它们之间的区别如下图所示。RDF一个重要的设计原则是数据的易发布以及共享，图数据库则把重点放在了高效的图查询和搜索上。其次，RDF以三元组的方式来存储数据而且不包含属性信息，但图数据库一般以属性图为基本的表示形式，所以实体和关系可以包含属性，这就意味着更容易表达现实的业务场景。 根据最新的统计（2018年上半年），图数据库仍然是增长最快的存储系统。相反，关系型数据库的增长基本保持在一个稳定的水平。同时，我们也列出了常用的图数据库系统以及他们最新使用情况的排名。其中Neo4j系统目前仍是使用率最高的图数据库，它拥有活跃的社区，而且系统本身的查询效率高，但唯一的不足就是不支持准分布式。相反，OrientDB和JanusGraph（原Titan）支持分布式，但这些系统相对较新，社区不如Neo4j活跃，这也就意味着使用过程当中不可避免地会遇到一些刺手的问题。如果选择使用RDF的存储系统，Jena或许一个比较不错的选择。 金融知识图谱的搭建接下来我们看一个实际的具体案例，讲解怎么一步步搭建可落地的金融风控领域的知识图谱系统。首先需要说明的一点是，有可能不少人认为搭建一个知识图谱系统的重点在于算法和开发。但事实并不是想象中的那样，其实最重要的核心在于对业务的理解以及对知识图谱本身的设计，这就类似于对于一个业务系统，数据库表的设计尤其关键，而且这种设计绝对离不开对业务的深入理解以及对未来业务场景变化的预估。当然，在这里我们先不讨论数据的重要性。 一个完整的知识图谱的构建包含以下几个步骤：1. 定义具体的业务问题 2. 数据的收集 &amp; 预处理 3. 知识图谱的设计 4. 把数据存入知识图谱 5. 上层应用的开发，以及系统的评估。下面我们就按照这个流程来讲一下每个步骤所需要做的事情以及需要思考的问题。 定义具体的业务问题在P2P网贷环境下，最核心的问题是风控，也就是怎么去评估一个借款人的风险。在线上的环境下，欺诈风险尤其为严重，而且很多这种风险隐藏在复杂的关系网络之中，而且知识图谱正好是为这类问题所设计的，所以我们“有可能”期待它能在欺诈，这个问题上带来一些价值。在进入下一个话题的讨论之前，要明确的一点是，对于自身的业务问题到底需不需要知识图谱系统的支持。因为在很多的实际场景，即使对关系的分析有一定的需求，实际上也可以利用传统数据库来完成分析的。所以为了避免使用知识图谱而选择知识图谱，以及更好的技术选型，以下给出了几点总结，供参考。 数据收集 &amp; 预处理下一步就是要确定数据源以及做必要的数据预处理。针对于数据源，我们需要考虑以下几点：1. 我们已经有哪些数据？2. 虽然现在没有，但有可能拿到哪些数据？3. 其中哪部分数据可以用来降低风险？4. 哪部分数据可以用来构建知识图谱？在这里需要说明的一点是，并不是所有跟反欺诈相关的数据都必须要进入知识图谱，对于这部分的一些决策原则在接下来的部分会有比较详细的介绍。 对于反欺诈，有几个数据源是我们很容易想得到的，包括用户的基本信息、行为数据、运营商数据、网络上的公开信息等等。假设我们已经有了一个数据源的列表清单，则下一步就要看哪些数据需要进一步的处理，比如对于非结构化数据我们或多或少都需要用到跟自然语言处理相关的技术。用户填写的基本信息基本上会存储在业务表里，除了个别字段需要进一步处理，很多字段则直接可以用于建模或者添加到知识图谱系统里。对于行为数据来说，我们则需要通过一些简单的处理，并从中提取有效的信息比如“用户在某个页面停留时长”等等。对于网络上公开的网页数据，则需要一些信息抽取相关的技术。 举个例子，对于用户的基本信息，我们很可能需要如下的操作。一方面，用户信息比如姓名、年龄、学历等字段可以直接从结构化数据库中提取并使用。但另一方面，对于填写的公司名来说，我们有可能需要做进一步的处理。比如部分用户填写“北京贪心科技有限公司”，另外一部分用户填写“北京望京贪心科技有限公司”，其实指向的都是同一家公司。所以，这时候我们需要做公司名的对齐，用到的技术细节可以参考前面讲到的实体对齐技术。 知识图谱的设计图谱的设计是一门艺术，不仅要对业务有很深的理解、也需要对未来业务可能的变化有一定预估，从而设计出最贴近现状并且性能高效的系统。在知识图谱设计的问题上，我们肯定会面临以下几个常见的问题：1. 需要哪些实体、关系和属性？2. 哪些属性可以做为实体，哪些实体可以作为属性？3. 哪些信息不需要放在知识图谱中？ 基于这些常见的问题，我们从以往的设计经验中抽象出了一系列的设计原则。这些设计原则就类似于传统数据库设计中的范式，来引导相关人员设计出更合理的知识图谱系统，同时保证系统的高效性。接下来，我们举几个简单的例子来说明其中的一些原则。首先是，业务原则（Business Principle），它的含义是 “一切要从业务逻辑出发，并且通过观察知识图谱的设计也很容易推测其背后业务的逻辑，而且设计时也要想好未来业务可能的变化”。 举个例子，可以观察一下下面这个图谱，并试问自己背后的业务逻辑是什么。通过一番观察，其实也很难看出到底业务流程是什么样的。做个简单的解释，这里的实体-“申请”意思就是application，如果对这个领域有所了解，其实就是进件实体。在下面的图中，申请和电话实体之间的“has_phone”，“parent phone”是什么意思呢？ 接下来再看一下下面的图，跟之前的区别在于我们把申请人从原有的属性中抽取出来并设置成了一个单独的实体。在这种情况下，整个业务逻辑就变得很清晰，我们很容易看出张三申请了两个贷款，而且张三拥有两个手机号，在申请其中一个贷款的时候他填写了父母的电话号。总而言之，一个好的设计很容易让人看到业务本身的逻辑。 接下来再看一个原则叫做效率原则（Efficiency Principle）。 效率原则让知识图谱尽量轻量化、并决定哪些数据放在知识图谱，哪些数据不需要放在知识图谱。在这里举一个简单的类比，在经典的计算机存储系统中，我们经常会谈论到内存和硬盘，内存作为高效的访问载体，作为所有程序运行的关键。这种存储上的层次结构设计源于数据的局部性-“locality”，也就是说经常被访问到的数据集中在某一个区块上，所以这部分数据可以放到内存中来提升访问的效率。类似的逻辑也可以应用到知识图谱的设计上：我们把常用的信息存放在知识图谱中，把那些访问频率不高，对关系分析无关紧要的信息放在传统的关系型数据库当中。 效率原则的核心在于把知识图谱设计成小而轻的存储载体比如在下面的知识图谱中，我们完全可以把一些信息比如“年龄”，“家乡”放到传统的关系型数据库当中，因为这些数据对于：a. 分析关系来说没有太多作用 b. 访问频率低，放在知识图谱上反而影响效率另外，从分析原则（Analytics Principle）的角度，我们不需要把跟关系分析无关的实体放在图谱当中；从冗余原则（Redundancy Principle）的角度，有些重复性信息、高频信息可以放到传统数据库当中。 把数据存入知识图谱存储上我们要面临存储系统的选择，但由于我们设计的知识图谱带有属性，图数据库可以作为首选。但至于选择哪个图数据库也要看业务量以及对效率的要求。如果数据量特别庞大，则Neo4j很可能满足不了业务的需求，这时候不得不去选择支持准分布式的系统比如OrientDB, JanusGraph等，或者通过效率、冗余原则把信息存放在传统数据库中，从而减少知识图谱所承载的信息量。通常来讲，对于10亿节点以下规模的图谱来说Neo4j已经足够了。 上层应用的开发等我们构建好知识图谱之后，接下来就要使用它来解决具体的问题。对于风控知识图谱来说，首要任务就是挖掘关系网络中隐藏的欺诈风险。从算法的角度来讲，有两种不同的场景：一种是基于规则的；另一种是基于概率的。鉴于目前AI技术的现状，基于规则的方法论还是在垂直领域的应用中占据主导地位，但随着数据量的增加以及方法论的提升，基于概率的模型也将会逐步带来更大的价值。基于规则的方法论 首先，我们来看几个基于规则的应用，分别是不一致性验证、基于规则的特征提取、基于模式的判断。 不一致性验证 为了判断关系网络中存在的风险，一种简单的方法就是做不一致性验证，也就是通过一些规则去找出潜在的矛盾点。这些规则是以人为的方式提前定义好的，所以在设计规则这个事情上需要一些业务的知识。比如在下面的这个图中，李明和李飞两个人都注明了同样的公司电话，但实际上从数据库中判断这俩人其实在不同的公司上班，这就是一个矛盾点。类似的规则其实可以有很多，不在这里一一列出。 基于规则提取特征 我们也可以基于规则从知识图谱中提取一些特征，而且这些特征一般基于深度的搜索比如2度，3度甚至更高维度。比如我们可以问一个这样的问题：“申请人二度关系里有多少个实体触碰了黑名单？”，从图中我们很容观察到二度关系中有两个实体触碰了黑名单（黑名单由红色来标记）。等这些特征被提取之后，一般可以作为风险模型的输入。在此还是想说明一点，如果特征并不涉及深度的关系，其实传统的关系型数据库则足以满足需求。 基于模式的判断 这种方法比较适用于找出团体欺诈，它的核心在于通过一些模式来找到有可能存在风险的团体或者子图（sub-graph），然后对这部分子图做进一步的分析。这种模式有很多种，在这里举几个简单的例子。比如在下图中，三个实体共享了很多其他的信息，我们可以看做是一个团体，并对其做进一步的分析。 再比如，我们也可以从知识图谱中找出强连通图，并把它标记出来，然后做进一步风险分析。强连通图意味着每一个节点都可以通过某种路径达到其他的点，也就说明这些节点之间有很强的关系。 基于概率的方法 除了基于规则的方法，也可以使用概率统计的方法。比如社区挖掘、标签传播、聚类等技术都属于这个范畴。对于这类技术，在本文里不做详细的讲解，感兴趣的读者可以参考相关文献。 社区挖掘算法的目的在于从图中找出一些社区。对于社区，我们可以有多种定义，但直观上可以理解为社区内节点之间关系的密度要明显大于社区之间的关系密度。下面的图表示社区发现之后的结果，图中总共标记了三个不同的社区。一旦我们得到这些社区之后，就可以做进一步的风险分析。 由于社区挖掘是基于概率的方法论，好处在于不需要人为地去定义规则，特别是对于一个庞大的关系网络来说，定义规则这事情本身是一件很复杂的事情。 标签传播算法的核心思想在于节点之间信息的传递。这就类似于，跟优秀的人在一起自己也会逐渐地变优秀是一个道理。因为通过这种关系会不断地吸取高质量的信息，最后使得自己也会不知不觉中变得更加优秀。具体细节不在这里做更多解释。 相比规则的方法论，基于概率的方法的缺点在于：需要足够多的数据。如果数据量很少，而且整个图谱比较稀疏（Sparse），基于规则的方法可以成为我们的首选。尤其是对于金融领域来说，数据标签会比较少，这也是为什么基于规则的方法论还是更普遍地应用在金融领域中的主要原因。 基于动态网络的分析 以上所有的分析都是基于静态的关系图谱。所谓的静态关系图谱，意味着我们不考虑图谱结构本身随时间的变化，只是聚焦在当前知识图谱结构上。然而，我们也知道图谱的结构是随时间变化的，而且这些变化本身也可以跟风险有所关联。 在下面的图中，我们给出了一个知识图谱T时刻和T+1时刻的结构，我们很容易看出在这两个时刻中间，图谱结构（或者部分结构）发生了很明显的变化，这其实暗示着潜在的风险。那怎么去判断这些结构上的变化呢？感兴趣的读者可以查阅跟“dynamic network mining”相关的文献。 知识图谱在其他行业中的应用除了金融领域，知识图谱的应用可以涉及到很多其他的行业，包括医疗、教育、证券投资、推荐等等。其实，只要有关系存在，则有知识图谱可发挥价值的地方。在这里简单举几个垂直行业中的应用。 比如对于教育行业，我们经常谈论个性化教育、因材施教的理念。其核心在于理解学生当前的知识体系，而且这种知识体系依赖于我们所获取到的数据比如交互数据、评测数据、互动数据等等。为了分析学习路径以及知识结构，我们则需要针对于一个领域的概念知识图谱，简单来讲就是概念拓扑结构。在下面的图中，我们给出了一个非常简单的概念图谱：比如为了学习逻辑回归则需要先理解线性回归；为了学习CNN，得对神经网络有所理解等等。所有对学生的评测、互动分析都离不开概念图谱这个底层的数据。 在证券领域，我们经常会关心比如“一个事件发生了，对哪些公司产生什么样的影响？” 比如有一个负面消息是关于公司1的高管，而且我们知道公司1和公司2有种很密切的合作关系，公司2有个主营产品是由公司3提供的原料基础上做出来的。其实有了这样的一个知识图谱，我们很容易回答哪些公司有可能会被这次的负面事件所影响。当然，仅仅是“有可能”，具体会不会有强相关性必须由数据来验证。所以在这里，知识图谱的好处就是把我们所需要关注的范围很快给我们圈定。接下来的问题会更复杂一些，比如既然我们知道公司3有可能被这次事件所影响，那具体影响程度有多大？对于这个问题，光靠知识图谱是很难回答的，必须要有一个影响模型、以及需要一些历史数据才能在知识图谱中做进一步推理以及计算。 实践上的几点建议首先，知识图谱是一个比较新的工具，它的主要作用还是在于分析关系，尤其是深度的关系。所以在业务上，首先要确保它的必要性，其实很多问题可以用非知识图谱的方式来解决。 知识图谱领域一个最重要的话题是知识的推理。而且知识的推理是走向强人工智能的必经之路。但很遗憾的，目前很多语义网络的角度讨论的推理技术（比如基于深度学习，概率统计）很难在实际的垂直应用中落地。其实目前最有效的方式还是基于一些规则的方法论，除非我们有非常庞大的数据集。 最后，还是要强调一点，知识图谱工程本身还是业务为重心，以数据为中心。不要低估业务和数据的重要性。 结语知识图谱是一个既充满挑战而且非常有趣的领域。只要有正确的应用场景，对于知识图谱所能发挥的价值还是可以期待的。我相信在未来不到2，3年时间里，知识图谱技术会普及到各个领域当中。 很多细节性的内容很难在一篇文章里面面俱到、如果想对知识图谱领域有更全面的了解，并且快速开发出一款可落地的知识图谱产品，可以参考我近期推出的《搭建企业级知识图谱系统》课程。在课程里，我会详细地给大家介绍怎么从零开始一步步搭建完整的知识图谱系统，并把每一个细节中遇到的问题以及坑给大家讲解。 课程汇集了多年在知识图谱一线的实践经验，可以帮助学员快速地对知识图谱入门。本课程是100%实战类知识图谱课程，目的是帮助学员搭建一个完整的可应用于工业界产品的知识图谱系统。项目涉及到的模块包括：通过NLP技术来处理非结构化数据、知识图谱的设计、大规模数据的离线/实时导入、基于Spark GraphX的图算法实现、对外服务接口的编写以及Neo4j/Cypher的使用。虽然课程以风控为案例，但所涉及到的思路、架构均适合其他的应用场景。 作者 | 李文哲，人工智能、知识图谱领域专家文章选自贪心科技AI","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"知识图谱","slug":"知识图谱","permalink":"http://huangzhiyuan.github.io/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"}]},{"title":"解读神经网络编译器TVM","slug":"tvm-guide","date":"2020-03-08T12:52:13.000Z","updated":"2020-03-08T13:16:04.000Z","comments":true,"path":"2020/03/08/tvm-guide/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/08/tvm-guide/","excerpt":"TVM可以称为许多工具集的集合，其中这些工具可以组合起来使用，来实现我们的一些神经网络的加速和部署功能。这也是为什么叫做TVM Stack了。TVM的使用途径很广，几乎可以支持市面上大部分的神经网络权重框架(ONNX、TF、Caffe2等)，也几乎可以部署在任何的平台，例如Windows、Linux、Mac、ARM等等。 以下面一张图来形容一下，这张图来源于(https://tvm.ai/about)：","text":"TVM可以称为许多工具集的集合，其中这些工具可以组合起来使用，来实现我们的一些神经网络的加速和部署功能。这也是为什么叫做TVM Stack了。TVM的使用途径很广，几乎可以支持市面上大部分的神经网络权重框架(ONNX、TF、Caffe2等)，也几乎可以部署在任何的平台，例如Windows、Linux、Mac、ARM等等。 以下面一张图来形容一下，这张图来源于(https://tvm.ai/about)： 乍看这么多感觉非常地复杂，但我们只需要知道TVM的核心功能就可以：TVM可以优化的训练好的模型，并将你的模型打包好，然后你可以将这个优化好的模型放在任何平台去运行，可以说是与落地应用息息相关。 TVM包含的东西和知识概念都有很多，不仅有神经网络优化量化op融合等一系列步骤，还有其他更多细节技术的支持(Halide、LLVM)，从而使TVM拥有很强大的功能…好了废话不说了，再说就憋不出来了，如果想多了解TVM的可以在知乎上直接搜索TVM关键字，那些大佬有很多关于TVM的介绍文章，大家可以去看看。 其实做模型优化这一步骤的库已经出现很多了，不论是Nvidia自家的TensorRT还是Pytorch自家的torch.jit模块，都在做一些模型优化的工作，这里就不多说了，感兴趣的可以看看以下文章： 利用Pytorch的C++前端(libtorch)读取预训练权重并进行预测利用TensorRT实现神经网络提速(读取ONNX模型并运行)利用TensorRT对深度学习进行加速 开始使用说到这里了，感觉有必要说下：我们为什么要使用TVM？ 如果你想将你的训练模型移植到Window端、ARM端(树莓派、其他一系列使用该内核的板卡)或者其他的一些平台，利用其中的CPU或者GPU来运行，并且希望可以通过优化模型来使模型在该平台运算的速度更快(这里与模型本身的算法设计无关)，实现落地应用研究，那么TVM就是你的不二之选。另外TVM源码是由C++和Pythoh共同搭建，阅读相关源码也有利于我们程序编写方面的提升。 安装安装其实没什么多说的，官方的例子说明的很详细。大家移步到那里按照官方的步骤一步一步来即可。 不过有两点需要注意下： 建议安装LLVM，虽然LLVM对于TVM是可选项，但是如果我们想要部署到CPU端，那么llvm几乎是必须的 因为TVM是python和C++一起的工程，python可以说是C++的前端，安装官方教程编译好C++端后，这里建议选择官方中的Method 1来进行python端的设置，这样我们就可以随意修改源代码，再重新编译，而Python端就不需要进行任何修改就可以直接使用了。 利用Pytorch导出Onnx模型说了这么多，演示一个例子才能更好地理解TVM到底是做什么的，所以我们这里以一个简单的例子来演示一下TVM是怎么使用的。 首先我们要做的是，得到一个已经训练好的模型，这里我选择这个github仓库中的mobilenet-v2，model代码和在ImageNet上训练好的权重都已经提供。好，我们将github中的模型代码移植到本地，然后调用并加载已经训练好的权重： 123456789101112131415import torchimport timefrom models.MobileNetv2 import mobilenetv2model = mobilenetv2(pretrained=True)example = torch.rand(1, 3, 224, 224) # 假想输入with torch.no_grad(): model.eval() since = time.time() for i in range(10000): model(example) time_elapsed = time.time() - since print(&#x27;Time elapsed is &#123;:.0f&#125;m &#123;:.0f&#125;s&#x27;. format(time_elapsed // 60, time_elapsed % 60)) # 打印出来时间 这里我们加载训练好的模型权重，并设定了输入，在python端连续运行了10000次，这里我们所花的时间为：6m2s。 然后我们将Pytorch模型导出为ONNX模型： 123456789101112import torchfrom models.MobileNetv2 import mobilenetv2model = mobilenetv2(pretrained=True)example = torch.rand(1, 3, 224, 224) # 假想输入torch_out = torch.onnx.export(model, example, &quot;mobilenetv2.onnx&quot;, verbose=True, export_params=True # 带参数输出 ) 这样我们就得到了mobilenetv2.onnx这个onnx格式的模型权重。注意这里我们要带参数输出，因为我们之后要直接读取ONNX模型进行预测。导出来之后，建议使用Netron来查看我们模型的结构，可以看到这个模型由Pytorch-1.0.1导出，共有152个op，以及输入id和输入格式等等信息，我们可以拖动鼠标查看到更详细的信息: 好了，至此我们的mobilenet-v2模型已经顺利导出了。 利用TVM读取并预测ONNX模型在我们成功编译并且可以在Python端正常引用TVM后，我们首先导入我们的onnx格式的模型。这里我们准备了一张飞机的图像： 这个图像在ImageNet分类中属于404: &#39;airliner&#39;，也就是航空客机。下面我们将利用TVM部署onnx模型并对这张图像进行预测。 12345678910111213141516171819202122import onnximport timeimport tvmimport numpy as npimport tvm.relay as relayfrom PIL import Imageonnx_model = onnx.load(&#x27;mobilenetv2.onnx&#x27;) # 导入模型mean = [123., 117., 104.] # 在ImageNet上训练数据集的mean和stdstd = [58.395, 57.12, 57.375]def transform_image(image): # 定义转化函数，将PIL格式的图像转化为格式维度的numpy格式数组 image = image - np.array(mean) image /= np.array(std) image = np.array(image).transpose((2, 0, 1)) image = image[np.newaxis, :].astype(&#x27;float32&#x27;) return imageimg = Image.open(&#x27;../datasets/images/plane.jpg&#x27;).resize((224, 224)) # 这里我们将图像resize为特定大小x = transform_image(img) 这样我们得到的x为[1,3,224,224]维度的ndarray。这个符合NCHW格式标准，也是我们通用的张量格式。接下来我们设置目标端口llvm，也就是部署到CPU端，而这里我们使用的是TVM中的Relay IR，这个IR简单来说就是可以读取我们的模型并按照模型的顺序搭建出一个可以执行的计算图出来，当然，我们可以对这个计算图进行一系列优化。(现在TVM主推Relay而不是NNVM，Relay可以称为二代NNVM)。 123456target = &#x27;llvm&#x27;input_name = &#x27;0&#x27; # 注意这里为之前导出onnx模型中的模型的输入id，这里为0shape_dict = &#123;input_name: x.shape&#125;# 利用Relay中的onnx前端读取我们导出的onnx模型sym, params = relay.frontend.from_onnx(onnx_model, shape_dict) 上述代码中导出的sym和params是我们接下来要使用的核心的东西，其中params就是导出模型中的权重信息，在python中用dic表示： 而sym就是表示计算图结构的功能函数，这个函数中包含了计算图的流动过程，以及一些计算中需要的各种参数信息，Relay IR之后对网络进行优化就是主要对这个sym进行优化的过程： 123456789101112131415161718192021222324252627282930fn (%v0: Tensor[(1, 3, 224, 224), float32], %v1: Tensor[(32, 3, 3, 3), float32], %v2: Tensor[(32,), float32], %v3: Tensor[(32,), float32], %v4: Tensor[(32,), float32], %v5: Tensor[(32,), float32], ... %v307: Tensor[(1280, 320, 1, 1), float32], %v308: Tensor[(1280,), float32], %v309: Tensor[(1280,), float32], %v310: Tensor[(1280,), float32], %v311: Tensor[(1280,), float32], %v313: Tensor[(1000, 1280), float32], %v314: Tensor[(1000,), float32]) &#123; %0 = nn.conv2d(%v0, %v1, strides=[2, 2], padding=[1, 1], kernel_size=[3, 3]) %1 = nn.batch_norm(%0, %v2, %v3, %v4, %v5, epsilon=1e-05) %2 = %1.0 %3 = clip(%2, a_min=0, a_max=6) %4 = nn.conv2d(%3, %v7, padding=[1, 1], groups=32, kernel_size=[3, 3]) ... %200 = clip(%199, a_min=0, a_max=6) %201 = mean(%200, axis=[3]) %202 = mean(%201, axis=[2]) %203 = nn.batch_flatten(%202) %204 = multiply(1f, %203) %205 = nn.dense(%204, %v313, units=1000) %206 = multiply(1f, %v314) %207 = nn.bias_add(%205, %206) %207&#125; 好了，接下来我们需要对这个计算图模型进行优化，这里我们选择优化的等级为3： 12345with relay.build_config(opt_level=3): intrp = relay.build_module.create_executor(&#x27;graph&#x27;, sym, tvm.cpu(0), target)dtype = &#x27;float32&#x27;func = intrp.evaluate(sym) 最后我们得到可以直接运行的func。 其中优化的等级分这几种： 123456789OPT_PASS_LEVEL = &#123; &quot;SimplifyInference&quot;: 0, &quot;OpFusion&quot;: 1, &quot;FoldConstant&quot;: 2, &quot;CombineParallelConv2D&quot;: 3, &quot;FoldScaleAxis&quot;: 3, &quot;AlterOpLayout&quot;: 3, &quot;CanonicalizeOps&quot;: 3,&#125; 最后，我们将之前已经转化格式后的图像x数组和模型的参数输入到这个func中，并且返回这个输出数组中的最大值 12output = func(tvm.nd.array(x.astype(dtype)), **params).asnumpy()print(output.argmax()) 这里我们得到的输出为404，与前文描述图像在ImageNet中的分类标记一致，说明我们的TVM正确读取onnx模型并将其应用于预测阶段。 我们另外单独测试一下模型优化后运行的速度和之前直接利用pytorch运行速度之间比较一下，可以发现最后的运行时间为：3m20s，相较之前的6m2s快了将近一倍。 123456since = time.time()for i in range(10000): output = func(tvm.nd.array(x.astype(dtype)), **params).asnumpy()time_elapsed = time.time() - sinceprint(&#x27;Time elapsed is &#123;:.0f&#125;m &#123;:.0f&#125;s&#x27;. format(time_elapsed // 60, time_elapsed % 60)) # 打印出来时间 当然，这个比较并不是很规范，不过我们可以大概分析出TVM的一些可用之处了。 原文链接","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"TVM","slug":"TVM","permalink":"http://huangzhiyuan.github.io/tags/TVM/"}]},{"title":"mAP","slug":"mAP-in-cv","date":"2020-03-07T12:42:13.000Z","updated":"2020-03-07T13:44:26.000Z","comments":true,"path":"2020/03/07/mAP-in-cv/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/07/mAP-in-cv/","excerpt":"mAP是目标检测领域的一个常用指标，但却少有教程能真正说清楚，说明白这个东西。自己也是看了无数次，每次看了忘，忘了又去看。这次写下来，希望能记得牢固一些。 一句话概括AP：AP表示Recall从0~1的平均精度值。比如说Recall=0.1时，Precision=y1；Recall=0.2时，Precision=y2；…… ；Recall=1.0时，Precision=yn。那么可以以Recall为横轴，Precision为纵轴画一条曲线，曲线下的面积就是AP。 一句话概括mAP：mAP表示多个物体类别上的平均AP。注意，mAP有两次平均，一次在Recall取不同值时平均，一次在类别上平均。","text":"mAP是目标检测领域的一个常用指标，但却少有教程能真正说清楚，说明白这个东西。自己也是看了无数次，每次看了忘，忘了又去看。这次写下来，希望能记得牢固一些。 一句话概括AP：AP表示Recall从0~1的平均精度值。比如说Recall=0.1时，Precision=y1；Recall=0.2时，Precision=y2；…… ；Recall=1.0时，Precision=yn。那么可以以Recall为横轴，Precision为纵轴画一条曲线，曲线下的面积就是AP。 一句话概括mAP：mAP表示多个物体类别上的平均AP。注意，mAP有两次平均，一次在Recall取不同值时平均，一次在类别上平均。 AP相关的基础概念要讲清楚AP，我们先看一下Accuracy，Precision，Recall，IOU，Recall@TopK，Precision@TopK。介绍完这几个基础概念之后，我们会介绍AP的基本计算思想，即PR曲线下的面积，之后会介绍不同数据集PASCAL VOC，COCO，ImageNet计算AP的方式。 Accuracy：准确率这个计算比较简单，准确率=预测正确的样本数/所有样本数，即预测正确的样本比例（包括预测正确的正样本和预测正确的负样本，不过在目标检测领域，没有预测正确的负样本这一说法，所以目标检测里面没有用Accuracy的）。计算公式如下： Precision：查准率Precision的计算则很容易和Accuracy弄混，Precision表示某一类样本预测有多准，比如说，模型预测出有100个病患，可是实际上这里面只有80个是病患，那么Precision就是80%。顾名思义，即查准的几率为80%。注意：Precision针对的是某一类样本，如果没有说明类别，那么Precision是毫无意义的（有些地方不说明类别，直接说Precision，是因为二分类问题通常说的Precision都是正样本的Precision）。Accuracy则针对的是所有类别的样本，这一点很多人并没有去区分。 Recall：召回率Recall和Precision一样，脱离类别是没有意义的。说到Recall，一定指的是某个类别的Recall。Recall表示某一类样本，预测正确的与所有Ground Truth的比例。比如说，某批数据有100个人为患者，可是模型只预测出80个人为患者，那么被召回医院就诊的比例就是80%，即Recall是80%。仔细和Precision的计算方法对比就可以发现，Recall和Precision计算中的分子都是一样的，但是Recall计算的时候，分母是Ground Truth中某一类样本的数量，而Precision计算的时候，是预测出来的某一类样本数。自然地，可以得到如下有关Recall的计算公式： IOU目标检测里面我们如何判定一个目标检测正确与否呢？第一，我们需要看CNN给出的标签是不是对的，第二，我们需要看预测的框和真实框的重叠比例有多少。这个重叠比例就是IOU： 如果重叠比例IOU&gt;Threshold，且类别正确，那么就认为检测正确，否则错误。有了这个规则，我们对于目标检测问题，就可以计算Precision，Recall了，进一步也可以计算mAP。 Recall@TopK，Precision@TopK说到Recall的时候，有时候会提及TopK的Recall和Precision。下面是一个小例子，假设有一个目标检测数据集，这个数据集中7张图片。下面是检测结果：上面七张图每张图片的检测结果均已标出，绿色是Ground Truth，红色是检测到的对象（总共24个，A~Y）。对上面的所有检测结果按照confidence排名统计出一个表如下，（ [公式] 则被判定为正样本）： 第三列表示预测的置信度(Confidence)排名，第四列表示当前目标框预测正确与否，倒数第一，第二列实际上是TopK的Precision和Recall。前文说过，Precision表示预测出来的正样本正确的比例有多少。那么Top1时，预测了一个正样本，且预测正确，所以Precision=1.0。同理可以算出Recall=0.0666，即总共15个正样本，却只预测出来1个，召回比例就是0.0666。同理TopK的Precision和Recall均可以计算。 AP显然，上述例子中，随着K的增大，Recall和Precision会一增一减。那么以Recall为横轴，Precision为纵轴，就可以画出一条PR曲线如下（实际计算时，当Precision下降到一定程度时，后面就直接默认为0，不算了。）：上图PR曲线下的面积就定义为AP，即： 上面就是AP的基本思想，实际计算过程中，PASCAL VOC，COCO比赛在上述基础上都有不同的调整策略。 Interpolated AP（PASCAL VOC 2008的评测指标）在PASCAL VOC 2008中，在计算AP之前会对上述曲线进行平滑，平滑方法为，对每一个Precision值，使用其右边最大的Precision值替代。具体示意图如下：通过平滑策略，上面蓝色的PR曲线就变成了红色的虚线了。平滑的好处在于，平滑后的曲线单调递减，不会出现摇摆的情况。这样的话，随着Recall的增大，Precision逐渐降低，这才是符合逻辑的。实际计算时，对平滑后的Precision曲线进行均匀采样出11个点（每个点间隔0.1），然后计算这11个点的平均Precision。具体如下： 在本例子中， 。这种计算方法也叫插值AP（Interpolated AP）。对于PASCAL VOC有20个类别，那么mAP就是对20个类别的AP进行平均。 AP (Area under curve AUC，PASCAL VOC2010–2012评测指标)上述11点插值的办法由于插值点数过少，容易导致结果不准。一个解决办法就是内插所有点。所谓内插所有点，其实就是对上述平滑之后的曲线算曲线下面积。 这样计算之所以会更准确一点，可以这么看！原先11点采样其实算的是曲线下面积的近似，具体近似办法是：取10个宽为0.1，高为Precision的小矩形的面积平均。现在这个则不然，现在这个算了无数个点的面积平均，所以结果要准确一些。示意图如下：在本例中， COCO mAPCOCO mAP使用101个点的内插mAP（Interpolated AP），此外，COCO还使用了不同IOU阈值，不同尺度下的AP平均来作为评测结果，比如AP @ [.5 : .95]对应于IoU的平均AP，从0.5到0.95，步长为0.05。下面是具体评价指标介绍： 值得注意的是，COCO的AP就是指mAP，没有刻意区分二者。 ImageNet目标检测评测指标在ImageNet目标检测数据集里面则采用上面介绍的AUC方法来计算mAP。一般来说，不同的数据集mAP介绍方法会有一些细微差异。 参考资料： https://github.com/rafaelpadilla/Object-Detection-Metrics 原文链接","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"mAP","slug":"mAP","permalink":"http://huangzhiyuan.github.io/tags/mAP/"}]},{"title":"app快速定位算法的实现-geohash算法研究","slug":"how-app-located-geohash","date":"2020-03-07T12:41:20.000Z","updated":"2020-03-07T13:47:26.000Z","comments":true,"path":"2020/03/07/how-app-located-geohash/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/07/how-app-located-geohash/","excerpt":"什么是geohash？它的原理是什么？它帮助我们解决了哪些痛点，本文为你娓娓道来。（本文包含以下内容，阅读完需要约10分钟） 我们日常生活中遇到哪些定位的场景 简单复习一下经纬度 geohash原理解析 geohash存在的边界问题 如何解决边界问题 计算两点距离的计算","text":"什么是geohash？它的原理是什么？它帮助我们解决了哪些痛点，本文为你娓娓道来。（本文包含以下内容，阅读完需要约10分钟） 我们日常生活中遇到哪些定位的场景 简单复习一下经纬度 geohash原理解析 geohash存在的边界问题 如何解决边界问题 计算两点距离的计算 我们日常生活中遇到哪些定位的场景我们上下班经常会用APP打车和共享单车，下面2张图，应该都很熟悉，打开定位，查找我附近的车，那么，这个是怎么实现的呢？ 我脑海中第一个实现方式是：实时上报经纬度。在数据库里，把经纬度都标记为索引，通过查找对比经纬度的值，来找到附近1km的车子，但是这种做法第一是索引比较多，数值比较大，二是需要循环遍历经纬度，查询会很慢，效率很低。 那么，这些APP是怎么做到，既能精准定位，又能快速查找呢？答案就是 geohash geohash通过算法将1个定位的经度和纬度2个数值，转换成1个hash字符串。如果2个地方距离越近，那么他们的hash值的前缀越相同。然后通过数据库中like操作符 “ like wtw366%” 快速查找到附近的车。 比如上海腾讯大厦的经纬度是： （31.1688749, 121.3975184），那么转换成geohash就是 wtw366ngz5qt，我们想找附近的车子，可以用： 12select * from cart where geohash like &#x27;wtw366%&#x27; ;select * from cart where LEFT(geohash, 6) = &#x27;wtw366&#x27;; 简单复习一下经纬度在大致了解什么是geohash之后，我们先来复习一下什么是经纬度（高中学的，可能已经忘记光了（逃）），这对于理解geohash有很大的帮助。 我们将地球铺平开来，会得到下面这个平面图。 以赤道和本初子午线为界，将地球分为经度和纬度。赤道是在0度，本初子午线也在0度。以赤道作为经度X横坐标，以本初子午线作为纬度 Y 竖坐标。 经度（longitude）和纬度（latitude）简称 lng 和 lat。 其中，从本初子午线向东划分180度称为东经，用”E”表示：（0, 180]；向西划分180度为西经,用“W”表示：[-180, 0） 以赤道为0度,向南北各分出90度，南北极的读数均是90度，北纬用“N”表示 :（0, 90] ，南纬用“S”表示: [-90, 0） 12纬线和纬线是角度数值，并不是米。[ 表示等于， （表示小于 所以，我们常用十字坐标法来表示经纬度坐标图： 我们一般读“经纬度”，其实，表示一个定位的书面经纬度是 “（纬度，经度）”。比如上海腾讯大厦的定位就是： (31.1688749, 121.3975184）表示的是：纬度=31.1688749，经度=121.3975184 geohash原理解析在了解什么是经纬度之后，现在我们就可以开始来说下geohash的原理了，geohash通过以下步骤，实现了将一个经纬度数子串，转换成1个hash字符串。 指定一个位置的经纬度坐标值。 根据十字坐标图和二分法，将纬度和经度划分成1和0的二进制数字串。 按照“偶数位放经度，奇数位放纬度”算法，合并经度和纬度这2个二进制数字串。 合并后的二进制数字串，按照从前往后，每隔5位，换算成十进制数字，最后不足5位的用0补齐。 十进制数字，对应base32字符串算法的所在位置，一一匹配，得到了最后的字符串结果。 按照进度划分截取，得到最终的geohash值。 我们按照这个顺序，结合实际的例子，依次计算操作一下。 找出一个位置的经纬度我们可以用各种地图和定位工具，比如依靠Google地图，通过定位或者搜索一个地点，就容易找出经纬度。 这样，我们就找出了上海腾讯大厦的经纬度是 (31.1688749, 121.3975184) 将经纬度按照二分算法变成01二进制 由于31.1688749属于(0, 90)，所以取编码为1。 然后再将(0, 90)分成 (0, 45), (45, 90)两个区间，而31.1688749位于(0, 45)，所以编码为0。 然后再将(0, 45)分成 (0, 22.5), (22.5, 45)两个区间，而31.1688749位于(22.5, 45)，所以编码为1。….….依次类推可得上海腾讯大厦纬度编码为：1101011000101010000111101101101 经度也用同样的算法，对(-180, 180)依次细分，(-180，0)、(0,180) ，得出编码为：1110101100101001110111110011010 偶数位放经度，奇数位放纬度通过二分算法，我们得到了腾讯大厦的纬度和经度的二级制串为： 12string(30) &quot;101011000101010000111101101101&quot;string(30) &quot;110101100101001110111110011010&quot; 现在需要按照”偶数位放经度，奇数位放纬度”，将这2个数字串，合二为一。那么这个到底怎么理解呢？我刚开始不理解到底怎么操作，后来经过一系列的思考，可以如下操作： 由于无法用文字表述，我截了个操作图，如图上的箭头操作顺序所示，就是把纬度往右移动一个位置，然后依次串起来。 这样，合并之后，我们得了一个60个字符长度的的二进制数字串：1string(60) &quot;111001100111100000110011000110101000111111111001011011011001&quot; 二进制转换成十进制我们把这个60位的二进制，按照从左往右，每5位划分成1个组，最后一组如果不足5位就用0补齐到5位。划分后如下所示： 111100 11001 11100 00011 00110 00110 10100 01111 11111 00101 10110 11001 然后，把分好的二进制，转换成十进制： 128 25 28 3 6 6 20 15 31 5 22 25 匹配对应base32表算法的所在位置base32表是用0-9、b-z（去掉a, i, l, o）这32个字母进行组合的编码集合，base-32如下所示：0123456789bcdefghjkmnpqrstuvwxyz为了更好理解和一一对应，我们把base32各个字符的位置信息和它的字符串用表对应起来： 所以， 28 25 28 3 6 6 20 15 31 5 22 25 对应上面的表的位置就得到了，是： 1wtw366ngz5qt 这样，最后我们得到了，上海腾讯大厦的经纬度（31.1688749, 121.3975184） 对应的 geohash 为 wtw366ngz5qt。 geohash 的精度问题geohash其实表示的是一个矩形的块状区间，它总共分成最大12个字符串，也就是表示从 1-12 级。字符数越大，块区间就越小，那么定位就越精准。 我们刚才计算上海腾讯大厦的geohash采用的是12级，基本计算出来的位置就是毫秒级别了，可以说是非常的精准了。 上面是geohash字符串长度对应的区间精度，我们可以看到，当geohash为12位时，表示是37毫米范围的区间，已经是非常的精准了。当geohash为6位时，表示为1.2k米范围内的矩形位置。 所以，当2个定位的geohash 前7位是一样的时，表示他们在附近1.2km的范围内。 那我们还是用腾讯大厦的geohash值，分别截取经度为前7，6，5位看看，在地图上是怎么样的： 所以，根据上面的图，随着字符越来越少，精度越来越小，这个矩形也越来越大，这一整块区间都共用一个geohash来表示。在实际应用中，我们就可以动态的调整精度，实现更大或者更小范围内的搜索，既能精准定位，又可以隐藏住一个地点的具区位信息。 geohash存在的边界问题由于geohash表示的是一个区块信息，在同一个区块里的2个位置，它会认为是最近的，然而，其实更近的位置可能刚好在另一个区间，这样就造成了不匹配的问题。这就存在一个边界问题。 我们用实际例子来看。我们想找腾大附近1.5km范围内的便利店，我们选取geohash精度为6。园区有2家 A 和 B。B距离我们更近一点，但是，由于A 和腾大在一个hash区块内，所以，就得出了A是最佳的选择。这就是边界的问题。 如何解决边界问题那么如何解决这个边界问题，给出最近最优的算法方案呢？答案就是：把定位附近的8个方向的geohash都算出来。最后分别计算这些点和自己的距离（由于范围很小，点的数量就也很少，计算量就很少）过滤掉不满足条件的点就ok了。 计算两点距离的计算通过余弦定理以及弧度计算方法，最终推导出来的算式A为： 1$s = acos(cos($radLat1) * cos($radLat2) * cos($radLng1 - $radLng2) + sin($radLat1) * sin($radLat2)) * $R 目前大多使用的是Google公开的距离计算公司，推导算式B为： 1$s = 2*asin(sqrt(pow(sin(($radLat1-$radLat2)/2),2)+cos($radLat1)*cos($radLat2)*pow(sin(($radLng1-$radLng2)/2),2)))*$R; 其中 : $radLat1、$radLng1，$radLat2，$``radLng2 为弧度$R为地球半径","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"geohash","slug":"geohash","permalink":"http://huangzhiyuan.github.io/tags/geohash/"}]},{"title":"Git内部原理揭秘","slug":"this-is-real-git","date":"2020-03-06T12:27:40.000Z","updated":"2020-03-06T12:55:26.000Z","comments":true,"path":"2020/03/06/this-is-real-git/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/06/this-is-real-git/","excerpt":"本文以一个具体例子结合动图介绍了Git的内部原理，包括Git是什么储存我们的代码和变更历史的、更改一个文件时，Git内部是怎么变化的、Git这样实现的有什么好处等等。通过例子解释清楚上面这张动图，让大家了解Git的内部原理。如果你已经能够看懂这张图了，下面的内容可能对你来说会比较基础。","text":"本文以一个具体例子结合动图介绍了Git的内部原理，包括Git是什么储存我们的代码和变更历史的、更改一个文件时，Git内部是怎么变化的、Git这样实现的有什么好处等等。通过例子解释清楚上面这张动图，让大家了解Git的内部原理。如果你已经能够看懂这张图了，下面的内容可能对你来说会比较基础。 视频链接：https://www.bilibili.com/video/av77252063PPT链接：https://www.lzane.com/slide/git-under-the-hood/index.html#/6 前言近几年技术发展十分迅猛，让部分同学养成了一种学习知识停留在表面，只会调用一些指令的习惯。我们时常有一种“我会用这个技术、这个框架”的错觉，等到真正遇到问题，才发现事情没有那么简单。而Git也是一个大部分人都知道如何去使用它，知道有哪些命令，却只有少部分人知道具体原理的东西。了解一些底层的东西，可以更好的帮你理清思路，知道你真正在操作什么，不会迷失在Git大量的指令和参数上面。 首先我们先创建两个文件 1234$ git init$ echo &#x27;111&#x27; &gt; a.txt$ echo &#x27;222&#x27; &gt; b.txt$ git add *.txt Git会将整个数据库储存在.git/目录下，如果你此时去查看.git/objects目录，你会发现仓库里面多了两个object。$ tree .git/objects 1234567.git/objects├── 58│ └── c9bdf9d017fcd178dc8c073cbfcbb7ff240d6c├── c2│ └── 00906efd24ec5e783bee7f23b5d7c941b0c12c├── info└── pack 好奇的我们来看一下里面存的是什么东西 12$ cat .git/objects/58/c9bdf9d017fcd178dc8c073cbfcbb7ff240d6cxKOR0a044K% 怎么是一串乱码？这是因为Git将信息压缩成二进制文件。但是不用担心，因为Git也提供了一个能够帮助你探索它的api git cat-file [-t] [-p]， -t可以查看object的类型，-p可以查看object储存的具体内容。 1234$ git cat-file -t 58c9blob$ git cat-file -p 58c9111 可以发现这个object是一个blob类型的节点，他的内容是111，也就是说这个object储存着a.txt文件的内容。这里我们遇到第一种Git object，blob类型，它只储存的是一个文件的内容，不包括文件名等其他信息。然后将这些信息经过SHA1哈希算法得到对应的哈希值58c9bdf9d017fcd178dc8c073cbfcbb7ff240d6c，作为这个object在Git仓库中的唯一身份证。也就是说，我们此时的Git仓库是这样子的： 我们继续探索，我们创建一个commit。 12345678$ git commit -am &#x27;[+] init&#x27;$ tree .git/objects.git/objects├── 0c│ └── 96bfc59d0f02317d002ebbf8318f46c7e47ab2├── 4c│ └── aaa1a9ae0b274fba9e3675f9ef071616e5b209... 我们会发现当我们commit完成之后，Git仓库里面多出来两个object。同样使用cat-file命令，我们看看它们分别是什么类型以及具体的内容是什么。 12345$ git cat-file -t 4caaa1tree$ git cat-file -p 4caaa1100644 blob 58c9bdf9d017fcd178dc8c0... a.txt100644 blob c200906efd24ec5e783bee7... b.txt 这里我们遇到了第二种Git object类型——tree，它将当前的目录结构打了一个快照。从它储存的内容来看可以发现它储存了一个目录结构（类似于文件夹），以及每一个文件（或者子文件夹）的权限、类型、对应的身份证（SHA1值）、以及文件名。此时的Git仓库是这样的： 1234567$ git cat-file -t 0c96bfcommit$ git cat-file -p 0c96bftree 4caaa1a9ae0b274fba9e3675f9ef071616e5b209author lzane 李泽帆 1573302343 +0800committer lzane 李泽帆 1573302343 +0800[+] init 接着我们发现了第三种Git object类型——commit，它储存的是一个提交的信息，包括对应目录结构的快照tree的哈希值，上一个提交的哈希值（这里由于是第一个提交，所以没有父节点。在一个merge提交中还会出现多个父节点），提交的作者以及提交的具体时间，最后是该提交的信息。 此时我们去看Git仓库是这样的： 到这里我们就知道Git是怎么储存一个提交的信息的了，那有同学就会问，我们平常接触的分支信息储存在哪里呢？ 12345$ cat .git/HEADref: refs/heads/master$ cat .git/refs/heads/master0c96bfc59d0f02317d002ebbf8318f46c7e47ab2 在Git仓库里面，HEAD、分支、普通的Tag可以简单的理解成是一个指针，指向对应commit的SHA1值。 其实还有第四种Git object，类型是tag，在添加含附注的tag（git tag -a）的时候会新建，这里不详细介绍，有兴趣的朋友按照上文中的方法可以深入探究。至此我们知道了Git是什么储存一个文件的内容、目录结构、commit信息和分支的。其本质上是一个key-value的数据库加上默克尔树形成的有向无环图（DAG）。这里可以蹭一下区块链的热度，区块链的数据结构也使用了默克尔树。 Git的三个分区接下来我们来看一下Git的三个分区（工作目录、Index 索引区域、Git仓库），以及Git变更记录是怎么形成的。了解这三个分区和Git链的内部原理之后可以对Git的众多指令有一个“可视化”的理解，不会再经常搞混。接着上面的例子，目前的仓库状态如下 这里有三个区域，他们所储存的信息分别是： 工作目录 （ working directory ）：操作系统上的文件，所有代码开发编辑都在这上面完成。 索引（ index or staging area ）：可以理解为一个暂存区域，这里面的代码会在下一次commit被提交到Git仓库。 Git仓库（ git repository ）：由Git object记录着每一次提交的快照，以及链式结构记录的提交变更历史。 我们来看一下更新一个文件的内容这个过程会发生什么事。 运行echo “333” &gt; a.txt将a.txt的内容从111修改成333，此时如上图可以看到，此时索引区域和git仓库没有任何变化。 运行git add a.txt将a.txt加入到索引区域，此时如上图所示，git在仓库里面新建了一个blob object，储存了新的文件内容。并且更新了索引将a.txt指向了新建的blob object。 运行git commit -m &#39;update&#39;提交这次修改。如上图所示 Git首先根据当前的索引生产一个tree object，充当新提交的一个快照。 创建一个新的commit object，将这次commit的信息储存起来，并且parent指向上一个commit，组成一条链记录变更历史。 将master分支的指针移到新的commit结点。 至此我们知道了Git的三个分区分别是什么以及他们的作用，以及历史链是怎么被建立起来的。基本上Git的大部分指令就是在操作这三个分区以及这条链。可以尝试的思考一下git的各种命令，试一下你能不能够在上图将它们“可视化”出来，这个很重要，建议尝试一下。 如果不能很好的将日常使用的指令“可视化”出来，推荐阅读 图解Git 一些有趣的问题有兴趣的同学可以继续阅读，这部分不是文章的主要内容 问题1：为什么要把文件的权限和文件名储存在tree object里面而不是blob object呢？想象一下修改一个文件的命名。如果将文件名保存在blob里面，那么Git只能多复制一份原始内容形成一个新的blob object。而Git的实现方法只需要创建一个新的tree object将对应的文件名更改成新的即可，原本的blob object可以复用，节约了空间。 问题2：每次commit，Git储存的是全新的文件快照还是储存文件的变更部分？由上面的例子我们可以看到，Git储存的是全新的文件快照，而不是文件的变更记录。也就是说，就算你只是在文件中添加一行，Git也会新建一个全新的blob object。那这样子是不是很浪费空间呢?这其实是Git在空间和时间上的一个取舍，思考一下你要checkout一个commit，或对比两个commit之间的差异。如果Git储存的是问卷的变更部分，那么为了拿到一个commit的内容，Git都只能从第一个commit开始，然后一直计算变更，直到目标commit，这会花费很长时间。而相反，Git采用的储存全新文件快照的方法能使这个操作变得很快，直接从快照里面拿取内容就行了。 当然，在涉及网络传输或者Git仓库真的体积很大的时候，Git会有垃圾回收机制gc，不仅会清除无用的object，还会把已有的相似object打包压缩。 问题3：Git怎么保证历史记录不可篡改？通过SHA1哈希算法和哈系树来保证。假设你偷偷修改了历史变更记录上一个文件的内容，那么这个问卷的blob object的SHA1哈希值就变了，与之相关的tree object的SHA1也需要改变，commit的SHA1也要变，这个commit之后的所有commit SHA1值也要跟着改变。又由于Git是分布式系统，即所有人都有一份完整历史的Git仓库，所以所有人都能很轻松的发现存在问题。希望大家读完有所收获，下一篇文章会写一些我日常工作中觉得比较实用的Git技巧、经常被问到的问题、以及发生一些事故时的处理方法。 作者：lzaneli，腾讯 TEG 前端开发工程师","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"git","slug":"git","permalink":"http://huangzhiyuan.github.io/tags/git/"}]},{"title":"服务端高并发分布式架构演进之路","slug":"evolution-of-high-concurrency-distributed-architecture-of-server","date":"2020-03-05T08:20:27.000Z","updated":"2020-03-05T14:04:50.000Z","comments":true,"path":"2020/03/05/evolution-of-high-concurrency-distributed-architecture-of-server/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/05/evolution-of-high-concurrency-distributed-architecture-of-server/","excerpt":"本文以淘宝作为例子，介绍从一百个到千万级并发情况下服务端的架构的演进过程，同时列举出每个演进阶段会遇到的相关技术，让大家对架构的演进有一个整体的认知，文章最后汇总了一些架构设计的原则。特别说明：本文以淘宝为例仅仅是为了便于说明演进过程可能遇到的问题，并非是淘宝真正的技术演进路径","text":"本文以淘宝作为例子，介绍从一百个到千万级并发情况下服务端的架构的演进过程，同时列举出每个演进阶段会遇到的相关技术，让大家对架构的演进有一个整体的认知，文章最后汇总了一些架构设计的原则。特别说明：本文以淘宝为例仅仅是为了便于说明演进过程可能遇到的问题，并非是淘宝真正的技术演进路径 基本概念在介绍架构之前，为了避免部分读者对架构设计中的一些概念不了解，下面对几个最基础的概念进行介绍： 分布式系统中的多个模块在不同服务器上部署，即可称为分布式系统，如Tomcat和数据库分别部署在不同的服务器上，或两个相同功能的Tomcat分别部署在不同服务器上 高可用系统中部分节点失效时，其他节点能够接替它继续提供服务，则可认为系统具有高可用性 集群一个特定领域的软件部署在多台服务器上并作为一个整体提供一类服务，这个整体称为集群。如Zookeeper中的Master和Slave分别部署在多台服务器上，共同组成一个整体提供集中配置服务。在常见的集群中，客户端往往能够连接任意一个节点获得服务，并且当集群中一个节点掉线时，其他节点往往能够自动的接替它继续提供服务，这时候说明集群具有高可用性 负载均衡请求发送到系统时，通过某些方式把请求均匀分发到多个节点上，使系统中每个节点能够均匀的处理请求负载，则可认为系统是负载均衡的 正向代理和反向代理系统内部要访问外部网络时，统一通过一个代理服务器把请求转发出去，在外部网络看来就是代理服务器发起的访问，此时代理服务器实现的是正向代理；当外部请求进入系统时，代理服务器把该请求转发到系统中的某台服务器上，对外部请求来说，与之交互的只有代理服务器，此时代理服务器实现的是反向代理。简单来说，正向代理是代理服务器代替系统内部来访问外部网络的过程，反向代理是外部请求访问系统时通过代理服务器转发到内部服务器的过程。 架构演进单机架构 以淘宝作为例子。在网站最初时，应用数量与用户数都较少，可以把Tomcat和数据库部署在同一台服务器上。浏览器往www.taobao.com发起请求时，首先经过DNS服务器（域名系统）把域名转换为实际IP地址10.102.4.1，浏览器转而访问该IP对应的Tomcat。 随着用户数的增长，Tomcat和数据库之间竞争资源，单机性能不足以支撑业务 第一次演进：Tomcat与数据库分开部署 Tomcat和数据库分别独占服务器资源，显著提高两者各自性能。随着用户数的增长，并发读写数据库成为瓶颈 第二次演进：引入本地缓存和分布式缓存在Tomcat同服务器上或同JVM中增加本地缓存，并在外部增加分布式缓存，缓存热门商品信息或热门商品的html页面等。通过缓存能把绝大多数请求在读写数据库前拦截掉，大大降低数据库压力。其中涉及的技术包括：使用memcached作为本地缓存，使用Redis作为分布式缓存，还会涉及缓存一致性、缓存穿透/击穿、缓存雪崩、热点数据集中失效等问题。 缓存抗住了大部分的访问请求，随着用户数的增长，并发压力主要落在单机的Tomcat上，响应逐渐变慢 第三次演进：引入反向代理实现负载均衡在多台服务器上分别部署Tomcat，使用反向代理软件（Nginx）把请求均匀分发到每个Tomcat中。此处假设Tomcat最多支持100个并发，Nginx最多支持50000个并发，那么理论上Nginx把请求分发到500个Tomcat上，就能抗住50000个并发。其中涉及的技术包括：Nginx、HAProxy，两者都是工作在网络第七层的反向代理软件，主要支持http协议，还会涉及session共享、文件上传下载的问题。 反向代理使应用服务器可支持的并发量大大增加，但并发量的增长也意味着更多请求穿透到数据库，单机的数据库最终成为瓶颈 第四次演进：数据库读写分离 把数据库划分为读库和写库，读库可以有多个，通过同步机制把写库的数据同步到读库，对于需要查询最新写入数据场景，可通过在缓存中多写一份，通过缓存获得最新数据。其中涉及的技术包括：Mycat，它是数据库中间件，可通过它来组织数据库的分离读写和分库分表，客户端通过它来访问下层数据库，还会涉及数据同步，数据一致性的问题。 业务逐渐变多，不同业务之间的访问量差距较大，不同业务直接竞争数据库，相互影响性能 第五次演进：数据库按业务分库 把不同业务的数据保存到不同的数据库中，使业务之间的资源竞争降低，对于访问量大的业务，可以部署更多的服务器来支撑。这样同时导致跨业务的表无法直接做关联分析，需要通过其他途径来解决，但这不是本文讨论的重点，有兴趣的可以自行搜索解决方案。 随着用户数的增长，单机的写库会逐渐会达到性能瓶颈 第六次演进：把大表拆分为小表 比如针对评论数据，可按照商品ID进行hash，路由到对应的表中存储；针对支付记录，可按照小时创建表，每个小时表继续拆分为小表，使用用户ID或记录编号来路由数据。只要实时操作的表数据量足够小，请求能够足够均匀的分发到多台服务器上的小表，那数据库就能通过水平扩展的方式来提高性能。其中前面提到的Mycat也支持在大表拆分为小表情况下的访问控制。 这种做法显著的增加了数据库运维的难度，对DBA的要求较高。数据库设计到这种结构时，已经可以称为分布式数据库，但是这只是一个逻辑的数据库整体，数据库里不同的组成部分是由不同的组件单独来实现的，如分库分表的管理和请求分发，由Mycat实现，SQL的解析由单机的数据库实现，读写分离可能由网关和消息队列来实现，查询结果的汇总可能由数据库接口层来实现等等，这种架构其实是MPP（大规模并行处理）架构的一类实现。 目前开源和商用都已经有不少MPP数据库，开源中比较流行的有Greenplum、TiDB、Postgresql XC、HAWQ等，商用的如南大通用的GBase、睿帆科技的雪球DB、华为的LibrA等等，不同的MPP数据库的侧重点也不一样，如TiDB更侧重于分布式OLTP场景，Greenplum更侧重于分布式OLAP场景，这些MPP数据库基本都提供了类似Postgresql、Oracle、MySQL那样的SQL标准支持能力，能把一个查询解析为分布式的执行计划分发到每台机器上并行执行，最终由数据库本身汇总数据进行返回，也提供了诸如权限管理、分库分表、事务、数据副本等能力，并且大多能够支持100个节点以上的集群，大大降低了数据库运维的成本，并且使数据库也能够实现水平扩展。 数据库和Tomcat都能够水平扩展，可支撑的并发大幅提高，随着用户数的增长，最终单机的Nginx会成为瓶颈 第七次演进：使用LVS或F5来使多个Nginx负载均衡 由于瓶颈在Nginx，因此无法通过两层的Nginx来实现多个Nginx的负载均衡。图中的LVS和F5是工作在网络第四层的负载均衡解决方案，其中LVS是软件，运行在操作系统内核态，可对TCP请求或更高层级的网络协议进行转发，因此支持的协议更丰富，并且性能也远高于Nginx，可假设单机的LVS可支持几十万个并发的请求转发；F5是一种负载均衡硬件，与LVS提供的能力类似，性能比LVS更高，但价格昂贵。由于LVS是单机版的软件，若LVS所在服务器宕机则会导致整个后端系统都无法访问，因此需要有备用节点。可使用keepalived软件模拟出虚拟IP，然后把虚拟IP绑定到多台LVS服务器上，浏览器访问虚拟IP时，会被路由器重定向到真实的LVS服务器，当主LVS服务器宕机时，keepalived软件会自动更新路由器中的路由表，把虚拟IP重定向到另外一台正常的LVS服务器，从而达到LVS服务器高可用的效果。 此处需要注意的是，上图中从Nginx层到Tomcat层这样画并不代表全部Nginx都转发请求到全部的Tomcat，在实际使用时，可能会是几个Nginx下面接一部分的Tomcat，这些Nginx之间通过keepalived实现高可用，其他的Nginx接另外的Tomcat，这样可接入的Tomcat数量就能成倍的增加。 由于LVS也是单机的，随着并发数增长到几十万时，LVS服务器最终会达到瓶颈，此时用户数达到千万甚至上亿级别，用户分布在不同的地区，与服务器机房距离不同，导致了访问的延迟会明显不同 第八次演进：通过DNS轮询实现机房间的负载 在DNS服务器中可配置一个域名对应多个IP地址，每个IP地址对应到不同的机房里的虚拟IP。当用户访问www.taobao.com时，DNS服务器会使用轮询策略或其他策略，来选择某个IP供用户访问。此方式能实现机房间的负载均衡，至此，系统可做到机房级别的水平扩展，千万级到亿级的并发量都可通过增加机房来解决，系统入口处的请求并发量不再是问题。 随着数据的丰富程度和业务的发展，检索、分析等需求越来越丰富，单单依靠数据库无法解决如此丰富的需求 第九次演进：引入NoSQL数据库和搜索引擎等技术 当数据库中的数据多到一定规模时，数据库就不适用于复杂的查询了，往往只能满足普通查询的场景。对于统计报表场景，在数据量大时不一定能跑出结果，而且在跑复杂查询时会导致其他查询变慢，对于全文检索、可变数据结构等场景，数据库天生不适用。因此需要针对特定的场景，引入合适的解决方案。如对于海量文件存储，可通过分布式文件系统HDFS解决，对于key value类型的数据，可通过HBase和Redis等方案解决，对于全文检索场景，可通过搜索引擎如ElasticSearch解决，对于多维分析场景，可通过Kylin或Druid等方案解决。 当然，引入更多组件同时会提高系统的复杂度，不同的组件保存的数据需要同步，需要考虑一致性的问题，需要有更多的运维手段来管理这些组件等。 引入更多组件解决了丰富的需求，业务维度能够极大扩充，随之而来的是一个应用中包含了太多的业务代码，业务的升级迭代变得困难 第十次演进：大应用拆分为小应用 按照业务板块来划分应用代码，使单个应用的职责更清晰，相互之间可以做到独立升级迭代。这时候应用之间可能会涉及到一些公共配置，可以通过分布式配置中心Zookeeper来解决。 不同应用之间存在共用的模块，由应用单独管理会导致相同代码存在多份，导致公共功能升级时全部应用代码都要跟着升级 第十一次演进：复用的功能抽离成微服务如用户管理、订单、支付、鉴权等功能在多个应用中都存在，那么可以把这些功能的代码单独抽取出来形成一个单独的服务来管理，这样的服务就是所谓的微服务，应用和服务之间通过HTTP、TCP或RPC请求等多种方式来访问公共服务，每个单独的服务都可以由单独的团队来管理。此外，可以通过Dubbo、SpringCloud等框架实现服务治理、限流、熔断、降级等功能，提高服务的稳定性和可用性。 不同服务的接口访问方式不同，应用代码需要适配多种访问方式才能使用服务，此外，应用访问服务，服务之间也可能相互访问，调用链将会变得非常复杂，逻辑变得混乱 第十二次演进：引入企业服务总线ESB屏蔽服务接口的访问差异 通过ESB统一进行访问协议转换，应用统一通过ESB来访问后端服务，服务与服务之间也通过ESB来相互调用，以此降低系统的耦合程度。这种单个应用拆分为多个应用，公共服务单独抽取出来来管理，并使用企业消息总线来解除服务之间耦合问题的架构，就是所谓的SOA（面向服务）架构，这种架构与微服务架构容易混淆，因为表现形式十分相似。个人理解，微服务架构更多是指把系统里的公共服务抽取出来单独运维管理的思想，而SOA架构则是指一种拆分服务并使服务接口访问变得统一的架构思想，SOA架构中包含了微服务的思想。 业务不断发展，应用和服务都会不断变多，应用和服务的部署变得复杂，同一台服务器上部署多个服务还要解决运行环境冲突的问题，此外，对于如大促这类需要动态扩缩容的场景，需要水平扩展服务的性能，就需要在新增的服务上准备运行环境，部署服务等，运维将变得十分困难 第十三次演进：引入容器化技术实现运行环境隔离与动态服务管理 目前最流行的容器化技术是Docker，最流行的容器管理服务是Kubernetes(K8S)，应用/服务可以打包为Docker镜像，通过K8S来动态分发和部署镜像。Docker镜像可理解为一个能运行你的应用/服务的最小的操作系统，里面放着应用/服务的运行代码，运行环境根据实际的需要设置好。把整个“操作系统”打包为一个镜像后，就可以分发到需要部署相关服务的机器上，直接启动Docker镜像就可以把服务起起来，使服务的部署和运维变得简单。 在大促的之前，可以在现有的机器集群上划分出服务器来启动Docker镜像，增强服务的性能，大促过后就可以关闭镜像，对机器上的其他服务不造成影响（在3.14节之前，服务运行在新增机器上需要修改系统配置来适配服务，这会导致机器上其他服务需要的运行环境被破坏）使用容器化技术后服务动态扩缩容问题得以解决，但是机器还是需要公司自身来管理，在非大促的时候，还是需要闲置着大量的机器资源来应对大促，机器自身成本和运维成本都极高，资源利用率低 第十四次演进：以云平台承载系统 系统可部署到公有云上，利用公有云的海量机器资源，解决动态硬件资源的问题，在大促的时间段里，在云平台中临时申请更多的资源，结合Docker和K8S来快速部署服务，在大促结束后释放资源，真正做到按需付费，资源利用率大大提高，同时大大降低了运维成本。 所谓的云平台，就是把海量机器资源，通过统一的资源管理，抽象为一个资源整体，在之上可按需动态申请硬件资源（如CPU、内存、网络等），并且之上提供通用的操作系统，提供常用的技术组件（如Hadoop技术栈，MPP数据库等）供用户使用，甚至提供开发好的应用，用户不需要关系应用内部使用了什么技术，就能够解决需求（如音视频转码服务、邮件服务、个人博客等）。在云平台中会涉及如下几个概念： IaaS：基础设施即服务。对应于上面所说的机器资源统一为资源整体，可动态申请硬件资源的层面； PaaS：平台即服务。对应于上面所说的提供常用的技术组件方便系统的开发和维护； SaaS：软件即服务。对应于上面所说的提供开发好的应用或服务，按功能或性能要求付费。 至此，以上所提到的从高并发访问问题，到服务的架构和系统实施的层面都有了各自的解决方案，但同时也应该意识到，在上面的介绍中，其实是有意忽略了诸如跨机房数据同步、分布式事务实现等等的实际问题，这些问题以后有机会再拿出来单独讨论 架构设计总结 架构的调整是否必须按照上述演变路径进行？不是的，以上所说的架构演变顺序只是针对某个侧面进行单独的改进，在实际场景中，可能同一时间会有几个问题需要解决，或者可能先达到瓶颈的是另外的方面，这时候就应该按照实际问题实际解决。如在政府类的并发量可能不大，但业务可能很丰富的场景，高并发就不是重点解决的问题，此时优先需要的可能会是丰富需求的解决方案。 对于将要实施的系统，架构应该设计到什么程度？**对于单次实施并且性能指标明确的系统，架构设计到能够支持系统的性能指标要求就足够了，但要留有扩展架构的接口以便不备之需。对于不断发展的系统，如电商平台，应设计到能满足下一阶段用户量和性能指标要求的程度，并根据业务的增长不断的迭代升级架构，以支持更高的并发和更丰富的业务。 服务端架构和大数据架构有什么区别？所谓的“大数据”其实是海量数据采集清洗转换、数据存储、数据分析、数据服务等场景解决方案的一个统称，在每一个场景都包含了多种可选的技术，如数据采集有Flume、Sqoop、Kettle等，数据存储有分布式文件系统HDFS、FastDFS，NoSQL数据库HBase、MongoDB等，数据分析有Spark技术栈、机器学习算法等。总的来说大数据架构就是根据业务的需求，整合各种大数据组件组合而成的架构，一般会提供分布式存储、分布式计算、多维分析、数据仓库、机器学习算法等能力。而服务端架构更多指的是应用组织层面的架构，底层能力往往是由大数据架构来提供。 有没有一些架构设计的原则？ 1.N+1设计。系统中的每个组件都应做到没有单点故障；2.回滚设计。确保系统可以向前兼容，在系统升级时应能有办法回滚版本；3.禁用设计。应该提供控制具体功能是否可用的配置，在系统出现故障时能够快速下线功能；4.监控设计。在设计阶段就要考虑监控的手段；5.多活数据中心设计。若系统需要极高的高可用，应考虑在多地实施数据中心进行多活，至少在一个机房断电的情况下系统依然可用；6.采用成熟的技术。刚开发的或开源的技术往往存在很多隐藏的bug，出了问题没有商业支持可能会是一个灾难；7.资源隔离设计。应避免单一业务占用全部资源；8.架构应能水平扩展。系统只有做到能水平扩展，才能有效避免瓶颈问题；9.非核心则购买。非核心功能若需要占用大量的研发资源才能解决，则考虑购买成熟的产品；10.使用商用硬件。商用硬件能有效降低硬件故障的机率；11.快速迭代。系统应该快速开发小功能模块，尽快上线进行验证，早日发现问题大大降低系统交付的风险；12.无状态设计。服务接口应该做成无状态的，当前接口的访问不依赖于接口上次访问的状态（全文完） 原文链接","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://huangzhiyuan.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"高并发","slug":"高并发","permalink":"http://huangzhiyuan.github.io/tags/%E9%AB%98%E5%B9%B6%E5%8F%91/"}]},{"title":"什么是hash","slug":"what-is-hash","date":"2020-03-04T13:53:46.000Z","updated":"2020-03-07T12:39:52.000Z","comments":true,"path":"2020/03/04/what-is-hash/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/04/what-is-hash/","excerpt":"提到hash，相信大多数同学都不会陌生，之前很火现在也依旧很火的技术区块链背后的底层原理之一就是hash，下面就从hash算法的原理和实际应用等几个角度，对hash算法进行一个讲解。","text":"提到hash，相信大多数同学都不会陌生，之前很火现在也依旧很火的技术区块链背后的底层原理之一就是hash，下面就从hash算法的原理和实际应用等几个角度，对hash算法进行一个讲解。 什么是HashHash也称散列、哈希，对应的英文都是Hash。基本原理就是把任意长度的输入，通过Hash算法变成固定长度的输出。这个映射的规则就是对应的Hash算法，而原始数据映射后的二进制串就是哈希值。活动开发中经常使用的MD5和SHA都是历史悠久的Hash算法。 12echo md5(&quot;这是一个测试文案&quot;);// 输出结果：2124968af757ed51e71e6abeac04f98d 在这个例子里，这是一个测试文案是原始值，2124968af757ed51e71e6abeac04f98d 就是经过hash算法得到的Hash值。整个Hash算法的过程就是把原始任意长度的值空间，映射成固定长度的值空间的过程。 Hash的特点一个优秀的hash算法，需要什么样的要求呢？ a)、从hash值不可以反向推导出原始的数据这个从上面MD5的例子里可以明确看到，经过映射后的数据和原始数据没有对应关系 b)、输入数据的微小变化会得到完全不同的hash值，相同的数据会得到相同的值1234echo md5(&quot;这是一个测试文案&quot;);// 输出结果：2124968af757ed51e71e6abeac04f98decho md5(&quot;这是二个测试文案&quot;);// 输出结果：bcc2a4bb4373076d494b2223aef9f702 可以看到我们只改了一个文字，但是整个得到的hash值产生了非常大的变化。 c)、哈希算法的执行效率要高效，长的文本也能快速地计算出哈希值 d)、hash算法的冲突概率要小由于hash的原理是将输入空间的值映射成hash空间内，而hash值的空间远小于输入的空间。 根据抽屉原理，一定会存在不同的输入被映射成相同输出的情况。那么作为一个好的hash算法，就需要这种冲突的概率尽可能小。 桌上有十个苹果，要把这十个苹果放到九个抽屉里，无论怎样放，我们会发现至少会有一个抽屉里面放不少于两个苹果。这一现象就是我们所说的“抽屉原理”。抽屉原理的一般含义为：“如果每个抽屉代表一个集合，每一个苹果就可以代表一个元素，假如有n+1个元素放到n个集合中去，其中必定有一个集合里至少有两个元素。” 抽屉原理有时也被称为鸽巢原理。它是组合数学中一个重要的原理 Hash碰撞的解决方案前面提到了hash算法是一定会有冲突的，那么如果我们如果遇到了hash冲突需要解决的时候应该怎么处理呢？比较常用的算法是链地址法和开放地址法。 链地址法链表地址法是使用一个链表数组，来存储相应数据，当hash遇到冲突的时候依次添加到链表的后面进行处理。 链地址在处理的流程如下：添加一个元素的时候，首先计算元素key的hash值，确定插入数组中的位置。如果当前位置下没有重复数据，则直接添加到当前位置。当遇到冲突的时候，添加到同一个hash值的元素后面，行成一个链表。这个链表的特点是同一个链表上的Hash值相同。java的数据结构HashMap使用的就是这种方法来处理冲突，JDK1.8中，针对链表上的数据超过8条的时候，使用了红黑树进行优化。由于篇幅原因，这里不深入讨论相关数据结构，有兴趣的同学可以参考这篇文章：《Java集合之一—HashMap》 开放地址法开放地址法是指大小为 M 的数组保存 N 个键值对，其中 M &gt; N。我们需要依靠数组中的空位解决碰撞冲突。基于这种策略的所有方法被统称为“开放地址”哈希表。线性探测法，就是比较常用的一种“开放地址”哈希表的一种实现方式。线性探测法的核心思想是当冲突发生时，顺序查看表中下一单元，直到找出一个空单元或查遍全表。简单来说就是：一旦发生冲突，就去寻找下 一个空的散列表地址，只要散列表足够大，空的散列地址总能找到。 线性探测法的数学描述是：h(k, i) = (h(k, 0) + i) mod m，i表示当前进行的是第几轮探查。i=1时，即是探查h(k, 0)的下一个；i=2，即是再下一个。这个方法是简单地向下探查。mod m表示：到达了表的底下之后，回到顶端从头开始。 对于开放寻址冲突解决方法，除了线性探测方法之外，还有另外两种比较经典的探测方法，二次探测（Quadratic probing）和双重散列（Double hashing）。但是不管采用哪种探测方法，当散列表中空闲位置不多的时候，散列冲突的概率就会大大提高。为了尽可能保证散列表的操作效率，一般情况下，我们会尽可能保证散列表中有一定比例的空闲槽位。我们用装载因子（load factor）来表示空位的多少。 散列表的装载因子=填入表中的元素个数/散列表的长度。装载因子越大，说明冲突越多，性能越差。 两种方案的demo示例假设散列长为8，散列函数H(K)=K mod 7，给定的关键字序列为{32,14,23,2, 20}当使用链表法时，相应的数据结构如下图所示：当使用线性探测法时，相应的数据结果如下图所示：这里的两种算法的区别是2这个元素，在链表法中还是在节点2的位置上，但是在线性探测法遇到冲突时会将冲突数据放到下一个空的位置下面。 hash算法在日常活动中的应用在日常运营活动中，我们活动开发经常遇到的应用场景是信息加密、数据校验、负载均衡。下面分别对这三种应用场景进行讲解。 信息加密首先我们看一下信息加密的应用。2011年CSDN脱库事件，导致超过600W的用户的密码泄露，让人失望的是，CSDN是明文存储用户的注册邮箱和密码的。作为用户的非常隐私的信息，最简单的保护措施就是对密码进行hash加密。在客户端对用户输入的密码进行hash运算，然后在服务端的数据库中保存用户密码的hash值。由于服务器端也没有存储密码的明文，所以目前很多网站也就不再有找回密码的功能了。这里也友情提示一下大家：如果在使用中发现某网站还有提供找回密码的功能，就要好好担心下这个网站的安全性了。看到这里有些同学会觉得那么我们是不是对用户输入的密码进行一次MD5加密就可以了呢，这样就算恶意用户知道了hash值，也没有办法拿到用户的真实密码。假设用户的密码是123456789，经过一次md5以后得到的值是: 125f9e794323b453885f5181f1b624d0b 那么是不是使用了这个加密后的字符串来存密码就万无一失了呢，理想总是很丰满，而现实总是很骨感的。大家可以看一下这个网站：www.cmd5.com/这里是该网站的相关介绍： 本站针对md5、sha1等全球通用公开的加密算法进行反向查询，通过穷举字符组合的方式，创建了明文密文对应查询数据库，创建的记录约90万亿条，占用硬盘超过500TB，查询成功率95%以上，很多复杂密文只有本站才可查询。已稳定运行十余年，国内外享有盛誉 那么一般针对这种问题，我们的解决之道就是引入salt(加盐)，即利用特殊字符（盐）和用户的输入合在一起组成新的字符串进行加密。通过这样的方式，增加了反向查询的复杂度。但是这样的方式也不是万无一失，如果发生了盐被泄露的问题，就需要所有用到的地方来重置密码。 针对salt泄露的问题，其实还有一种解决办法，即使用HMAC进行加密（Hash-based Message Authentication Code）。这种算法的核心思路是加密使用的key是从服务器端获取的，每一个用户的是不一样的。如果发生了泄露，那么也就是这一个用户的会被泄露，不会影响到全局。 这里也留给大家一个思考点，如果恶意用户直接抓取了你的活动参与链接，也就是拿到了你计算后的hash值，那从技术的角度上说，我们还有没有其他可以提升恶意用户的违法成本呢？ 数据校验git commit id 使用过git的同学都应该清楚，每次git提交后都有一个commit id，比如:19d02d2cc358e59b3d04f82677dbf3808ae4fc40就是一次git commit的结果，那么这个id是如何生成出来的呢？查阅了相关资料，使用如下代码可以进行查看： 1printf &quot;commit %s\\0&quot; $(git cat-file commit HEAD | wc -c); git cat-file commit HEAD git的commit id主要包括了以下几部分内容：Tree 哈希，parent哈希、作者信息和本次提交的备注。 针对这些信息进行SHA-1 算法后得到值就是本次提交的commit id。简单来讲，就是对于单次提交的头信息的一个校验和。 Linux kernel开创者和Git的开发者——Linus说，Git使用了sha1并非是为了安全性，而是为了数据的完整性；它可以保证，在很多年后，你重新checkout某个commit时，一定是它多年前的当时的状态，完全一摸一样，完全值得信任。 但最新研究表明，理论上对其进行哈希碰撞（hash collision，不同的两块数据有相同的hash值）的攻击可以在2^51（2的51次方）左右的次数内实现。不过由于commit id 是针对单个仓库里的，所以实际应用中我们可以认为如果两个文件的SHA-1值是相同的，那么它们确是完全相同的内容。 注：对于git里tree、parent等结构感兴趣的同学，可以参考下这篇文章《Git 内部原理 - Git 对象》，这里由于篇幅原因就不进行深入分析了。 版权校验在数据校验方面的另一个应用场景就是版权的保护或者违禁信息的打击，比如某个小视频，第一个用户上传的时候，我们认为是版权所有者，计算一个hash值存下来。当第二个用户上传的时候，同样计算hash值，如果hash值一样的话，就算同一个文件。这种方案其实也给用户传播违禁文件提高了一些门槛，不是简单的换一个名字或者改一下后缀名就可以躲避掉打击了。（当然这种方式也是可以绕过的，图片的你随便改一下颜色，视频去掉一帧就又是完全不同的hash值了。注意：我没有教你变坏，我只是和你在讨论这个技术。。。）另外我们在社区里，也会遇到玩家重复上传同一张图片或者视频的情况，使用这种校验的方式，可以有效减少cos服务的存储空间。 大文件分块校验使用过bt的同学都有经验，在p2p网络中会把一个大文件拆分成很多小的数据各自传输。这样的好处是如果某个小的数据块在传输过程中损坏了，只要重新下载这个块就好。为了确保每一个小的数据块都是发布者自己传输的，我们可以对每一个小的数据块都进行一个hash的计算，维护一个hash List，在收到所有数据以后，我们对于这个hash List里的每一块进行遍历比对。这里有一个优化点是如果文件分块特别多的时候，如果遍历对比就会效率比较低。可以把所有分块的hash值组合成一个大的字符串，对于这个字符串再做一次Hash运算，得到最终的hash（Root hash）。在实际的校验中，我们只需要拿到了正确的Root hash，即可校验Hash List，也就可以校验每一个数据块了。 负载均衡活动开发同学在应对高星级业务大用户量参与时，都会使用分库分表，针对用户的openid进行hashtime33取模，就可以得到对应的用户分库分表的节点了。 如上图所示，这里其实是分了10张表，openid计算后的hash值取模10，得到对应的分表，在进行后续处理就好。 对于一般的活动或者系统，我们一般设置10张表或者100张表就好。下面我们来看一点复杂的问题，假设我们活动初始分表了10张，运营一段时间以后发现需要10张不够，需要改到100张。这个时候我们如果直接扩容的话，那么所有的数据都需要重新计算Hash值，大量的数据都需要进行迁移。如果更新的是缓存的逻辑，则会导致大量缓存失效，发生雪崩效应，导致数据库异常。造成这种问题的原因是hash算法本身的缘故，只要是取模算法进行处理，则无法避免这种情况。针对这种问题，我们就需要利用一致性hash进行相应的处理了。 一致性hash的基本原理是将输入的值hash后，对结果的hash值进行2^32取模，这里和普通的hash取模算法不一样的点是在一致性hash算法里将取模的结果映射到一个环上。将缓存服务器与被缓存对象都映射到hash环上以后，从被缓存对象的位置出发，沿顺时针方向遇到的第一个服务器，就是当前对象将要缓存于的服务器，由于被缓存对象与服务器hash后的值是固定的，所以，在服务器不变的情况下，一个openid必定会被缓存到固定的服务器上，那么，当下次想要访问这个用户的数据时，只要再次使用相同的算法进行计算，即可算出这个用户的数据被缓存在哪个服务器上，直接去对应的服务器查找对应的数据即可。这里的逻辑其实和直接取模的是一样的。如下图所示： 初始情况如下：用户1的数据在服务器A里，用户2、3的数据存在服务器C里，用户4的数据存储在服务器B里 下面我们来看一下当服务器数量发生变化的时候，相应影响的数据情况： 服务器缩容 服务器B发生了故障，进行剔除后，只有用户4的数据发生了异常。这个时候我们需要继续按照顺时针的方案，把缓存的数据放在用户A上面 服务器扩容 同样的，我们进行了服务器扩容以后，新增了一台服务器D，位置落在用户2和3之间。按照顺时针原则，用户2依然访问的是服务器C的数据，而用户3顺时针查询后，发现最近的服务器是D，后续数据就会存储到d上面。 虚拟节点 当然这只是一种理想情况，实际使用中，由于服务器节点数量有限，有可能出现分布不均匀的情况。这个时候会出现大量数据都被映射到某一台服务器的情况，如下图左侧所示。为了解决这个问题，我们采用了虚拟节点的方案。虚拟节点是实际节点（实际的物理服务器）在hash环上的复制品，一个实际节点可以对应多个虚拟节点。虚拟节点越多，hash环上的节点就越多，数据被均匀分布的概率就越大。 如右图所示，B、C、D 是原始节点复制出来的虚拟节点，原本都要访问机器D的用户1、4，分别被映射到了B,D。通过这样的方式，起到了一个服务器均匀分布的作用。 几种hash算法的扩展应用下面介绍几种大家可能不经常遇到的应用，由于篇幅原因，不做深入介绍，只抛砖引玉。 SimHashsimHash是google用于海量文本去重的一种方法，它是一种局部敏感hash。那什么叫局部敏感呢，假定两个字符串具有一定的相似性，在hash之后，仍然能保持这种相似性，就称之为局部敏感hash。普通的hash是不具有这种属性的。simhash被Google用来在海量文本中去重。 simHash算法的思路大致如下： 将Doc进行关键词抽取(其中包括分词和计算权重)，抽取出n个(关键词，权重)对， 即图中的多个(feature, weight)。记为 feature_weight_pairs = [fw1, fw2 … fwn]，其中 fwn = (feature_n,weight_n)。 对每个feature_weight_pairs中的feature进行hash。然后对hash_weight_pairs进行位的纵向累加，如果该位是1，则+weight,如果是0，则-weight，最后生成bits_count个数字，大于0标记1，小于0标记0 最后转换成一个64位的字节，判断重复只需要判断他们的特征字的距离是不是&lt;n (n根据经验一般取3)，就可以判断两个文档是否相似。 如下图所示，当两个文本只有一个字变化时，如果使用普通Hash则会导致两次的结果发生较大改变，而SimHash的局部敏感特性，会导致只有部分数据发生变化。 GeoHashGeoHash将地球作为为一个二维平面进行递归分解。每个分解后的子块在一定经纬度范围内拥有相同的编码。以下图为例，这个矩形区域内所有的点（经纬度坐标）都共享相同的GeoHash字符串，这样既可以保护隐私（只表示大概区域位置而不是具体的点），又比较容易做缓存。 下面以一个例子来理解下这个算法，我们对纬度39.3817进行逼近编码 ： 地球纬度区间是[-90,90]，对于这个区间进行二分划分左区间[-90,0), 右区间[0,90]。39.3817属于右区间，标记为1 将右区间[0,90]继续进行划分，左区间[0,45) ,右区间[45,90]。39.3817属于左区间，标记为0 递归上面的过程，随着每次迭代，区间[a，b]会不断接近39.3817。递归的次数决定了生成的序列长度。 对于经度做同样的处理。得到的字符串，偶数位放经度，奇数位放纬度，把2串编码组合生成新串。对于新串转成对应10进制查出实际的base32编码就是类似WX4ER的hash值。 整体递归过程如下表所示： 这里有一篇文章详细介绍了GeoHash，有兴趣的同学可以移步这里： 腾讯技术工程：app 是如何快速定位我们位置的？深入了解 geohash 算法及其实现 布隆过滤器布隆过滤器被广泛用于黑名单过滤、垃圾邮件过滤、爬虫判重系统以及缓存穿透问题。对于数量小，内存足够大的情况，我们可以直接用hashMap或者hashSet就可以满足这个活动需求了。但是如果数据量非常大，比如5TB的硬盘上放满了用户的参与数据，需要一个算法对这些数据进行去重，取得活动的去重参与用户数。这种时候，布隆过滤器就是一种比较好的解决方案了。 布隆过滤器其实是基于bitmap的一种应用，在1970年由布隆提出的。它实际上是一个很长的二进制向量和一系列随机映射函数，用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难，主要用于大数据去重、垃圾邮件过滤和爬虫url记录中。核心思路是使用一个bit来存储多个元素，通过这样的方式来减少内存的消耗。通过多个hash函数，将每个数据都算出多个值，存放在bitmap中对应的位置上。 上图所示的例子中，数据a、b、c经过三次hash映射后，对应的bit位都是1，表示这三个数据已经存在了。而d这份数据经过映射后有一个结果是0，则表明d这个数据一定没有出现过。布隆过滤器存在假阳率（判定存在的元素可能不存在）的问题，但是没有假阴率（判断不存在的原因可能存在）的问题。即对于数据e，三次映射的结果都是1，但是这份数据也可能没有出现过。 总结Hash算法作为一种活动开发经常遇到的算法，我们在使用中不仅仅要知道这种算法背后真正的原理，才可以在使用上做到有的放矢。Hash的相关知识还有很多，有兴趣的同学可以继续深入研究。","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"hash","slug":"hash","permalink":"http://huangzhiyuan.github.io/tags/hash/"}]},{"title":"cpu cache总结","slug":"cpu-cache","date":"2020-03-03T12:58:52.000Z","updated":"2020-03-03T14:22:58.000Z","comments":true,"path":"2020/03/03/cpu-cache/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/03/cpu-cache/","excerpt":"早想写篇关于计算机CPU cache技术总结的文章，今天偶然看到这篇陈晧大神的总结，看完之后一个感觉，算了吧，有这篇足够了。我只是大自然的搬运工。向大神学习！这篇文章主要分成这么几个部分：基础知识、缓存的命中、缓存的一致性、相关的代码示例和延伸阅读。其中会讲述一些多核 CPU 的系统架构以及其原理，包括对程序性能上的影响，以及在进行并发编程的时候需要注意到的一些问题。这篇文章我会尽量地写简单和通俗易懂一些，主要是讲清楚相关的原理和问题，而对于一些细节和延伸阅读我会在文章最后会给出相关的资源。","text":"早想写篇关于计算机CPU cache技术总结的文章，今天偶然看到这篇陈晧大神的总结，看完之后一个感觉，算了吧，有这篇足够了。我只是大自然的搬运工。向大神学习！这篇文章主要分成这么几个部分：基础知识、缓存的命中、缓存的一致性、相关的代码示例和延伸阅读。其中会讲述一些多核 CPU 的系统架构以及其原理，包括对程序性能上的影响，以及在进行并发编程的时候需要注意到的一些问题。这篇文章我会尽量地写简单和通俗易懂一些，主要是讲清楚相关的原理和问题，而对于一些细节和延伸阅读我会在文章最后会给出相关的资源。 因为无论你写什么样的代码都会交给CPU来执行，所以，如果你想写出性能比较高的代码，这篇文章中提到的技术还是值得认真学习的。另外，千万别觉得这些东西没用，这些东西非常有用，十多年前就是这些知识在性能调优上帮了我的很多大忙，从而跟很多人拉开了差距…… 基础知识首先，我们都知道现在的CPU多核技术，都会有几级缓存，老的CPU会有两级内存（L1和L2），新的CPU会有三级内存（L1，L2，L3 ），如下图所示： 其中： L1缓存分成两种，一种是指令缓存，一种是数据缓存。L2缓存和L3缓存不分指令和数据。 L1和L2缓存在每一个CPU核中，L3则是所有CPU核心共享的内存。 L1、L2、L3的越离CPU近就越小，速度也越快，越离CPU远，速度也越慢。 再往后面就是内存，内存的后面就是硬盘。我们来看一些他们的速度： L1 的存取速度：4 个CPU时钟周期 L2 的存取速度： 11 个CPU时钟周期 L3 的存取速度：39 个CPU时钟周期 RAM内存的存取速度：107 个CPU时钟周期 我们可以看到，L1的速度是RAM的27倍，但是L1/L2的大小基本上也就是KB级别的，L3会是MB级别的。例如：Intel Core i7-8700K ，是一个6核的CPU，每核上的L1是64KB（数据和指令各32KB），L2 是 256K，L3有12MB（我的苹果电脑是 Intel Core i9-8950HK，和Core i7-8700K的Cache大小一样）。 我们的数据就从内存向上，先到L3，再到L2，再到L1，最后到寄存器进行CPU计算。为什么会设计成三层？这里有下面几个方面的考虑： 一个方面是物理速度，如果要更大的容量就需要更多的晶体管，除了芯片的体积会变大，更重要的是大量的晶体管会导致速度下降，因为访问速度和要访问的晶体管所在的位置成反比，也就是当信号路径变长时，通信速度会变慢。这部分是物理问题。 另外一个问题是，多核技术中，数据的状态需要在多个CPU中进行同步，并且，我们可以看到，cache和RAM的速度差距太大，所以，多级不同尺寸的缓存有利于提高整体的性能。 这个世界永远是平衡的，一面变得有多光鲜，另一面也会变得有多黑暗。建立这么多级的缓存，一定就会引入其它的问题，这里有两个比较重要的问题， 一个是比较简单的缓存的命中率的问题。 另一个是比较复杂的缓存更新的一致性问题。 尤其是第二个问题，在多核技术下，这就很像分布式的系统了，要对多个地方进行更新。 缓存的命中在说明这两个问题之前。我们需要要解一个术语 Cache Line。缓存基本上来说就是把后面的数据加载到离自己近的地方，对于CPU来说，它是不会一个字节一个字节的加载的，因为这非常没有效率，一般来说都是要一块一块的加载的，对于这样的一块一块的数据单位，术语叫“Cache Line”，一般来说，一个主流的CPU的Cache Line 是 64 Bytes（也有的CPU用32Bytes和128Bytes），64Bytes也就是16个32位的整型，这就是CPU从内存中捞数据上来的最小数据单位。 比如：Cache Line是最小单位（64Bytes），所以先把Cache分布多个Cache Line，比如：L1有32KB，那么，32KB/64B = 512 个 Cache Line。 一方面，缓存需要把内存里的数据放到cache里面，英文叫 CPU Associativity。Cache的数据放置的策略决定了内存中的数据块会拷贝到CPU Cache中的哪个位置上，因为Cache的大小远远小于内存，所以，需要有一种地址关联的算法，能够让内存中的数据可以被映射到Cache中来。这个有点像内存地址从逻辑地址向物理地址映射的方法，但不完全一样。 基本上来说，我们会有如下的一些方法。 一种方法是，任何一个内存地址的数据可以被缓存在任何一个Cache Line里，这种方法是最灵活的，但是，如果我们要知道一个内存是否存在于Cache中，我们就需要进行O(n)复杂度的Cache遍历，这是很没有效率的。 另一种方法，为了降低缓存搜索算法，我们需要使用像Hash Table这样的数据结构，最简单的hash table就是做“求模运算”，比如：我们的L1 Cache有512个Cache Line，那么，公式：（内存地址 mod 512）* 64 就可以直接找到所在的Cache地址的偏移了。但是，这样的方式需要我们的程序对内存地址的访问要非常地平均，不然冲突就会非常严重。这成了一种非常理想的情况了。 为了避免上述的两种方案的问题，于是就要容忍一定的hash冲突，也就出现了 N-Way 关联。也就是把连续的N个Cache Line绑成一组，然后，先把找到相关的组，然后再在这个组内找到相关的Cache Line。这叫 Set Associativity。如下图所示。 对于 N-Way 组关联，可能有点不好理解，这里有个例子，并多说一些细节（不然后面的代码你会不能理解），Intel 大多数处理器的L1 Cache都是32KB，8-Way 组相联，Cache Line 是64 Bytes。这意味着， 32KB的可以分成，32KB / 64 = 512 条 Cache Line。 因为有8 Way，于是会每一Way 有 512 / 8 = 64 条 Cache Line。 于是每一路就有 64 x 64 = 4096 Byts 的内存。 为了方便索引内存地址， Tag：每条 Cache Line 前都会有一个独立分配的 24 bits来存的 tag，其就是内存地址的前24bits Index：内存地址后续的6个bits则是在这一Way的是Cache Line 索引，2^6 = 64 刚好可以索引64条Cache Line Offset：再往后的6bits用于表示在Cache Line 里的偏移量 如下图所示：（图片来自《Cache: a place for concealment and safekeeping》） 当拿到一个内存地址的时候，先拿出中间的 6bits 来，找到是哪组。 然后，在这一个8组的cache line中，再进行O(n) n=8 的遍历，主是要匹配前24bits的tag。如果匹配中了，就算命中，如果没有匹配到，那就是cache miss，如果是读操作，就需要进向后面的缓存进行访问了。L2/L3同样是这样的算法。而淘汰算法有两种，一种是随机一种是LRU。现在一般都是以LRU的算法（通过增加一个访问计数器来实现） 这也意味着： -L1 Cache 可映射 36bits 的内存地址，一共 2^36 = 64GB的内存-当CPU要访问一个内存的时候，通过这个内存中间的6bits 定位是哪个set，通过前 24bits 定位相应的Cache Line。-就像一个hash Table的数据结构一样，先是O(1)的索引，然后进入冲突搜索。-因为中间的 6bits 决定了一个同一个set，所以，对于一段连续的内存来说，每隔4096的内存会被放在同一个组内，导致缓存冲突。 此外，当有数据没有命中缓存的时候，CPU就会以最小为Cache Line的单元向内存更新数据。当然，CPU并不一定只是更新64Bytes，因为访问主存实在是太慢了，所以，一般都会多更新一些。好的CPU会有一些预测的技术，如果找到一种pattern的话，就会预先加载更多的内存，包括指令也可以预加载。这叫 Prefetching 技术 （参看，Wikipedia 的 Cache Prefetching 和 纽约州立大学的 Memory Prefetching）。比如，你在for-loop访问一个连续的数组，你的步长是一个固定的数，内存就可以做到prefetching。（注：指令也是以预加载的方式执行，参看本站的《代码执行的效率》中的第三个示例） 了解这些细节，会有利于我们知道在什么情况下有可以导致缓存的失效。 缓存的一致性对于主流的CPU来说，缓存的写操作基本上是两种策略（参看本站《缓存更新的套路》）， 一种是Write Back，写操作只要在cache上，然后再flush到内存上。 一种是Write Through，写操作同时写到cache和内存上。 为了提高写的性能，一般来说，主流的CPU（如：Intel Core i7/i9）采用的是Write Back的策略，因为直接写内存实在是太慢了。 好了，现在问题来了，如果有一个数据 x 在 CPU 第0核的缓存上被更新了，那么其它CPU核上对于这个数据 x 的值也要被更新，这就是缓存一致性的问题。（当然，对于我们上层的程序我们不用关心CPU多个核的缓存是怎么同步的，这对上层的代码来说都是透明的） 一般来说，在CPU硬件上，会有两种方法来解决这个问题 Directory 协议。这种方法的典型实现是要设计一个集中式控制器，它是主存储器控制器的一部分。其中有一个目录存储在主存储器中，其中包含有关各种本地缓存内容的全局状态信息。当单个CPU Cache 发出读写请求时，这个集中式控制器会检查并发出必要的命令，以在主存和CPU Cache之间或在CPU Cache自身之间进行数据同步和传输。 Snoopy 协议。这种协议更像是一种数据通知的总线型的技术。CPU Cache通过这个协议可以识别其它Cache上的数据状态。如果有数据共享的话，可以通过广播机制将共享数据的状态通知给其它CPU Cache。这个协议要求每个CPU Cache 都可以“窥探”数据事件的通知并做出相应的反应。如下图所示，有一个Snoopy Bus的总线。 因为Directory协议是一个中心式的，会有性能瓶颈，而且会增加整体设计的复杂度。而Snoopy协议更像是微服务+消息通讯，所以，现在基本都是使用Snoopy的总线的设计。 这里，我想多写一些细节，因为这种微观的东西，不自然就会跟分布式系统相关联，在分布式系统中我们一般用Paxos/Raft这样的分布式一致性的算法。而在CPU的微观世界里，则不必使用这样的算法，原因是因为CPU的多个核的硬件不必考虑网络会断会延迟的问题。所以，CPU的多核心缓存间的同步的核心就是要管理好数据的状态就好了。 这里介绍几个状态协议，先从最简单的开始，MESI协议，这个协议跟那个著名的足球运动员梅西没什么关系，其主要表示缓存数据有四个状态：Modified（已修改）, Exclusive（独占的）,Shared（共享的），Invalid（无效的）。 这些状态的状态机如下所示（有点复杂，你可以先不看，这个图就是想告诉你状态控制有多复杂）： 下面是个示例（如果你想看一下动画演示的话，这里有一个网页（MESI Interactive Animations），你可以进行交互操作，这个动画演示中使用的Write Through算法）： MESI 这种协议在数据更新后，会标记其它共享的CPU缓存的数据拷贝为Invalid状态，然后当其它CPU再次read的时候，就会出现 cache miss 的问题，此时再从内存中更新数据。从内存中更新数据意味着20倍速度的降低。我们能不能直接从我隔壁的CPU缓存中更新？是的，这就可以增加很多速度了，但是状态控制也就变麻烦了。还需要多来一个状态：Owner(宿主)，用于标记，我是更新数据的源。于是，现了 MOESI 协议 MOESI协议的状态机和演示示例我就不贴了，我们只需要理解MOESI协议允许 CPU Cache 间同步数据，于是也降低了对内存的操作，性能是非常大的提升，但是控制逻辑也非常复杂。 顺便说一下，与 MOESI 协议类似的一个协议是 MESIF，其中的 F 是 Forward，同样是把更新过的数据转发给别的 CPU Cache 但是，MOESI 中的 Owner 状态 和MESIF 中的 Forward 状态有一个非常大的不一样—— Owner状态下的数据是dirty的，还没有写回内存，Forward状态下的数据是clean的，可以丢弃而不用另行通知。 需要说明的是，AMD用MOESI，Intel用MESIF。所以，F 状态主要是针对 CPU L3 Cache 设计的（前面我们说过，L3是所有CPU核心共享的）。（相关的比较可以参看StackOverlow上这个问题的答案） 程序性能了解了我们上面的这些东西后，我们来看一下对于程序的影响。 示例一首先，假设我们有一个64M长的数组，设想一下下面的两个循环： 123456const int LEN = 64*1024*1024;int *arr = new int[LEN];for (int i = 0; i &lt; LEN; i += 2) arr[i] *= i;for (int i = 0; i &lt; LEN; i += 8) arr[i] *= i; 按我们的想法来看，第二个循环要比第一个循环少4倍的计算量，其应该也是要快4倍的。但实际跑下来并不是，在我的机器上，第一个循环需要127毫秒，第二个循环则需要121毫秒，相差无几。这里最主要的原因就是 Cache Line，因为CPU会以一个Cache Line 64Bytes最小时单位加载，也就是16个32bits的整型，所以，无论你步长是2还是8，都差不多。而后面的乘法其实是不耗CPU时间的。 示例二我们再来看一个与缓存命中率有关的代码，我们以一定的步长increment来访问一个连续的数组。 12345for (int i = 0; i &lt; 10000000; i++) &#123; for (int j = 0; j &lt; size; j += increment) &#123; memory[j] += j; &#125;&#125; 我们测试一下，在下表中， 表头是步长，也就是每次跳多少个整数，而纵向是这个数组可以跳几次（你可以理解为要几条Cache Line），于是表中的任何一项代表了这个数组有多少，而且步长是多少。比如：横轴是 512，纵轴是4，意思是，这个数组有 4*512 = 2048 个长度，访问时按512步长访问，也就是访问其中的这几项：[0, 512, 1024, 1536] 这四项。 表中同的项是，是循环1000万次的时间，单位是“微秒”（除以1000后是毫秒） 1234567891011121314151617181920| count | 1 | 16 | 512 | 1024 |------------------------------------------| 1 | 17539 | 16726 | 15143 | 14477 || 2 | 15420 | 14648 | 13552 | 13343 || 3 | 14716 | 14463 | 15086 | 17509 || 4 | 18976 | 18829 | 18961 | 21645 || 5 | 23693 | 23436 | 74349 | 29796 || 6 | 23264 | 23707 | 27005 | 44103 || 7 | 28574 | 28979 | 33169 | 58759 || 8 | 33155 | 34405 | 39339 | 65182 || 9 | 37088 | 37788 | 49863 |156745 || 10 | 41543 | 42103 | 58533 |215278 || 11 | 47638 | 50329 | 66620 |335603 || 12 | 49759 | 51228 | 75087 |305075 || 13 | 53938 | 53924 | 77790 |366879 || 14 | 58422 | 59565 | 90501 |466368 || 15 | 62161 | 64129 | 90814 |525780 || 16 | 67061 | 66663 | 98734 |440558 || 17 | 71132 | 69753 |171203 |506631 || 18 | 74102 | 73130 |293947 |550920 | 我们可以看到，从 [9，1024] 以后，时间显著上升。包括 [17，512] 和 [18,512] 也显著上升。这是因为，我机器的 L1 Cache 是 32KB, 8 Way 的，前面说过，8 Way的有64组，每组8个Cache Line，当for-loop步长超过1024个整型，也就是正好 4096 Bytes时，也就是导致内存地址的变化是变化在高位的24bits上，而低位的12bits变化不大，尤其是中间6bits没有变化，导致全部命中同一组set，导致大量的cache 冲突，导致性能上升。而 [16, 512]也是一样的，其中的几步开始导致L1 Cache开始冲突失效。 示例三接下来，我们再来看个示例。下面是一个二维数组的两种遍历方式，一个逐行遍历，一个是逐列遍历，这两种方式在理论上来说，寻址和计算量都是一样的，执行时间应该也是一样的。 12345678910111213141516171819const int row = 1024;const int col = 512int matrix[row][col];//逐行遍历int sum_row=0;for(int _r=0; _r&lt;row; _r++) &#123; for(int _c=0; _c&lt;col; _c++)&#123; sum_row += matrix[_r][_c]; &#125;&#125;//逐列遍历int sum_col=0;for(int _c=0; _c&lt;col; _c++) &#123; for(int _r=0; _r&lt;row; _r++)&#123; sum_col += matrix[_r][_c]; &#125;&#125; 然而，并不是，在我的机器上，得到下面的结果。 逐行遍历：0.081ms 逐列遍历：1.069ms 执行时间有十几倍的差距。其中的原因，就是逐列遍历对于CPU Cache 的运作方式并不友好，所以，付出巨大的代价。 示例四接下来，我们来看一下多核下的性能问题，参看如下的代码。两个线程在操作一个数组的两个不同的元素（无需加锁），线程循环1000万次，做加法操作。在下面的代码中，我高亮了一行，就是p2指针，要么是p1，或是 p[30]，理论上来说，无论访问哪两个数组元素，都应该是一样的执行时间。 123456789101112void fn (int* data) &#123; for(int i = 0; i &lt; 10*1024*1024; ++i) *data += rand();&#125;int p[32];int *p1 = &amp;p[0];int *p2 = &amp;p[1]; // int *p2 = &amp;p[30]; // height codethread t1(fn, p1);thread t2(fn, p2); 然而，并不是，在我的机器上执行下来的结果是： 对于 p[0] 和 p1 ：560ms 对于 p[0] 和 p[30]：104ms 这是因为 p[0] 和 p1 在同一条 Cache Line 上，而 p[0] 和 p[30] 则不可能在同一条Cache Line 上 ，CPU的缓冲最小的更新单位是Cache Line，所以，这导致虽然两个线程在写不同的数据，但是因为这两个数据在同一条Cache Line上，就会导致缓存需要不断进在两个CPU的L1/L2中进行同步，从而导致了5倍的时间差异。 示例五接下来，我们再来看一下另外一段代码：我们想统计一下一个数组中的奇数个数，但是这个数组太大了，我们希望可以用多线程来完成，这个统计。下面的代码中，我们为每一个线程传入一个 id ，然后通过这个 id 来完成对应数组段的统计任务。这样可以加快整个处理速度。 123456789101112131415int total_size = 16 * 1024 * 1024; //数组长度int* test_data = new test_data[total_size]; //数组int nthread = 6; //线程数（因为我的机器是6核的）int result[nthread]; //收集结果的数组void thread_func (int id) &#123; result[id] = 0; int chunk_size = total_size / nthread + 1; int start = id * chunk_size; int end = min(start + chunk_size, total_size); for ( int i = start; i &lt; end; ++i ) &#123; if (test_data[i] % 2 != 0 ) ++result[id]; &#125;&#125; 然而，在执行过程中，你会发现，6个线程居然跑不过1个线程。因为根据上面的例子你知道 result[] 这个数组中的数据在一个Cache Line中，所以，所有的线程都会对这个 Cache Line 进行写操作，导致所有的线程都在不断地重新同步 result[] 所在的 Cache Line，所以，导致 6 个线程还跑不过一个线程的结果。这叫 False Sharing。 优化也很简单，使用一个线程内的变量。 123456789101112void thread_func (int id) &#123; result[id] = 0; int chunk_size = total_size / nthread + 1; int start = id * chunk_size; int end = min(start + chunk_size, total_size); int c = 0; //使用临时变量，没有cache line的同步了 for ( int i = start; i &lt; end; ++i ) &#123; if (test_data[i] % 2 != 0 ) ++c; &#125; result[id] = c;&#125; 我们把两个程序分别在 1 到 32 个线程上跑一下，得出的结果画一张图如下所示： 上图中，我们可以看到，灰色的曲线就是第一种方法，橙色的就是第二种（用局部变量的）方法。当只有一个线程的时候，两个方法相当，基本没有什么差别，但是在线程数增加的时候的时候，你会发现，第二种方法的性能提高的非常快。直到到达6个线程的时候，开始变得稳定（前面说过，我的CPU是6核的）。而第一种方法无论加多少线程也没有办法超过第二种方法。因为第一种方法不是CPU Cache 友好的。也就是说，第二种方法，只要我的CPU核数足够多，就可以做到线性的性能扩展，让每一个CPU核都跑起来，而第一种则不能。 篇幅问题，示例就写到这里，相关的代码参看我的Github相关仓库。 延伸阅读 Wikipedia : CPU Cache 经典文章：Gallery of Processor Cache Effects （这篇文章中的测试已经有点过时了，但是这篇文章中所说的那些东西还是非常适用的） Effective C++作者 Scott Meyers 的演讲 CPU Caches and Why You Care （Youtube，PPT） 美国私立大学Swarthmore的教材 Cache Architecture and Design 经典文章：What Every Programmer Should Know About Memory （这篇文章非常经典，但是开篇太晦涩了，居然告诉你晶体管内的构造，第三章和第六章是重点） Nonblocking Algorithms and Scalable Multicore Programming （英文版，中文版） Github上的一个代码库 hardware-effects 里面有受CPU影响的程序的演示 Optimizing for instruction caches （Part 1，Part 2， Part 3） 经典数据：Latency Numbers Every Programmer Should Know 关于Java的可以看一下这篇Optimizing Memory Access With CPU Cache 或是 Writing cache-friendly code","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"CPU-cache","slug":"CPU-cache","permalink":"http://huangzhiyuan.github.io/tags/CPU-cache/"}]},{"title":"谈谈我的“三观”","slug":"talk-about-three-values","date":"2020-03-03T12:34:03.000Z","updated":"2020-03-03T12:53:28.000Z","comments":true,"path":"2020/03/03/talk-about-three-values/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/03/talk-about-three-values/","excerpt":"原文选自： 谈谈我的“三观” 也许是人到了四十多了，敢写这么大的命题，我也醉了，不过，我还是想把我的想法记录下来，算是对我思考的一个snapshot，给未来的我看看，要么被未来的我打脸，要么打未来我的脸。无论怎么样，我觉得对我自己都很有意义。注意，这篇文章是长篇大论。","text":"原文选自： 谈谈我的“三观” 也许是人到了四十多了，敢写这么大的命题，我也醉了，不过，我还是想把我的想法记录下来，算是对我思考的一个snapshot，给未来的我看看，要么被未来的我打脸，要么打未来我的脸。无论怎么样，我觉得对我自己都很有意义。注意，这篇文章是长篇大论。 三观是世界观、人生观和价值观， 世界观代表你是怎么看这个世界的。 是左还是右，是激进还是保守，是理想还是现实，是乐观还是悲观…… 人生观代表你要想成为什么样的人。 是成为有钱人，还是成为人生的体验者，是成为老师，还是成为行业专家，是成为有思想的人，还是成为有创造力的人…… 价值观则是你觉得什么对你来说更重要。 是名是利，是过程还是结果，是付出还是索取，是国家还是自己，是家庭还是职业…… 人的三观其实是会变的，回顾一下我的过去，我感觉我的三观至少有这么几比较明显的变化，学生时代、刚走上社会的年轻时代，三十岁后的时代，还有现在。估计人都差不多吧…… 学生时代的三观更多的是学校给的，用各种标准答案给的，是又红又专的 刚走上社会后发现完全不是这么一回事，但学生时代的三观根深蒂固，三观开始分裂，内心开始挣扎 三十岁后，不如意的事越来越多，对社会越来越了解，有些人屈从现实，有些人不服输继续奋斗，而有些人展露才能开始影响社会，而分裂的三观开始收敛，我属于还在继续奋斗的人。 四十岁时，经历过的事太多，发现留给自己的时间不多，世界太复杂，而还有好多事没做，从而变得与世无争，也变得更为地自我。 面对世界年轻的时候，抵制过日货，虽然没上过街，但是也激动过，一次是1999南斯拉夫大使馆被炸，一次是2005反日示威，以前，我也是一个爱国愤青。但是后来，有过各种机会出国长时间生活工作，加拿大、英国、美国、日本……随着自己的经历和眼界的开阔，自己的三观自己也随着有了很多的变化，发现有些事并不是自己一开始所认识的那样，而且还是截然相反的。我深深感觉到，要有一个好的世界观，你需要亲身去经历和体会这个世界，而不是听别人说。所以，当我看到身边的人情绪激动地要抵制这个国家，搞死那个民族的时候，我都会建议他去趟那个国家最好在在那个国家呆上一段时间，亲自感受一下。 再后来发现，要抵制的越来越多，小时候的美英帝国主义，然后是日本，再后面是法国、韩国、菲利宾、印度、德国、瑞典、加拿大……从小时候的台独到现在的港独、藏独、疆独……发现再这样下去，基本上来说，自己的人生也不用干别的事了……另外，随着自己的成长，越来越明白，抵制这个抵制那个只不过是幼稚和狭隘的爱国主义，真想强国，想别让他人看得起，就应该把时间和精力放在努力学习放在精益求精上，做出比他们更好的东西来。另外，感觉用对内的爱国主义解决对外的外交问题也有点驴唇不对马嘴，无非也就是转移一下内部的注意力罢了，另外还发现爱国主义还可以成为消费营销手段……不是我不爱国，是我觉得世道变复杂了，我只是一个普通的老百姓，能力有限，请不要赋予我那么大的使命，我只想在我的专业上精进，能力所能及地帮助身边的人，过一个简单纯粹安静友善的生活…… 另外，为什么国与国之间硬要比个你高我低，硬要分个高下，硬要争出个输赢，我也不是太理解，世界都已经发展到全球化的阶段了，很多产品早就是你中有我，我中有你的情况了。举个例子，一部手机中的元件，可能来自全世界数十个国家，我们已经说不清楚一部手机是究竟是哪个国家生产的了。即然，整个世界都在以一种合作共赢全球化的姿态下运作，认准自己的位置，拥抱世界，持续向先进国家学习，互惠互利，不好吗？你可能会说，不是我们不想这样，是别人不容我们发展……老实说，大的层面我也感受不到，但就我在的互联网计算机行业方面，我觉得整个世界的开放性越来越好，开源项目空前地繁荣，世界上互联网文化也空前的开放，在计算机和互联网行业，我们享受了太多的开源和开放的红利，人家不开放，我们可能在很多领域还落后数十年。然而现在很多资源我们都访问不了，用个VPN也非法，你说是谁阻碍了发展？我只想能够流畅地访问互联网，让我的工作能够更有效率，然而，我在自己的家里却像做贼一样去学习新知识新技术，随时都有可能被抓进监狱…… 随着自己的经历越多，发现这个世界越复杂，也发现自己越渺小，很多国家大事并不是我不关心，是我觉得那根本不是我这个平头老百姓可以操心的事，这个世界有这个世界运作的规律和方法，而还有很多事情超出了我能理解的范围，也超出了我能控制的范围，我关心不关心都一个样，这些大事都不会由我的意志所决定的。而所谓的关心，无非就是喊喊口号，跟人争论一下，试图改变其它老百姓的想法，然而，对事情的本身的帮助却没有多大意义。过上几天，生活照旧，人家该搞你还不是继续搞你，而你自己并不因为做这些事而过得更好。 我对国与国之间的关系的态度是，有礼有节，不卑不亢，对待外国人，有礼貌但也要有节气，既不卑躬屈膝，也不趾高气昂，整体上，我并不觉得我们比国外有多差，但我也不觉得我们比国外有多好，我们还在成长，还需要帮助和协作，四海之内皆兄弟，无论在哪个国家，在老百姓的世界里，哪有那么多矛盾。有机会多出去走走，多结交几个其它民族的朋友，你会觉得，在友善和包容的环境下，你的心情和生活可以更好。 我现在更多关心的是和我生活相关的东西，比如：上网、教育、医疗、食品、治安、税务、旅游、收入、物价、个人权益、个人隐私……这些东西对我的影响会更大一些，也更值得关注，可以看到过去的几十年，我们国家已经有了长足的进步，这点也让我让感到很开心和自豪的，在一些地方也不输别人。但是，依然有好些事的仍然没有达到我的预期，而且还很糟糕，这个也要承认。而对，未来的变数谁也不好说，我在这个国度里的安全感似乎还不足够，所以，我还是要继续努力，以便我可以有更多的选项。有选项总比没得选要好。==所以，我想尽一切办法，努力让选项多起来，无法改变无法影响，那就只能提高自己有可选择的可能性。== 面对社会另外，在网上与别人对一些事或观点的争论，我觉得越来越无聊，以前被怼了，一定要怼回去，现在不会了，视而不见，不是怕了，是因为，网络上的争论在我看来大多数都是些没有章法，逻辑混乱的争论。 很多讨论不是说事，直接就是怼人骂人。随意就给人扣个帽子。 非黑即白的划分，你说这个不是黑的，他们就把你划到白的那边。 飘移观点，复杂化问题。东拉西扯，牵强附会，还扯出其它不相关的事来混淆。 杠精很多，不关心你的整体观点，抓住一个小辫子大作文章。 ==很明显，与其花时间教育这些人，不如花时间提升自己，让自己变得更优秀，这样就有更高的可能性去接触更聪明更成功更高层次的人。== 因为，一方面，你改变不了他们，另外，改变他们对你自己也没什么意义，改变自己，提升自己，让自己成长才有意义。时间是宝贵的，那些人根本不值得花时间，应该花时间去结交更有素质更聪明的人，做更有价值的事。 把时间多放在一些想法上，对自己对社会都是有意义的，把时间放在八卦别人，说长到短，你也不可能改善自己的生活，你批评这个批评那个，看不上这个看不起那个，不会让你有成长，也不会提升你的影响力，你的影响力不是你对别人说长道短的能力，而是别人信赖你并希望得到你的帮助的现象。多交一些有想法的朋友，多把自己的想法付诸实践，那怕没有成功，你的人生也会比别人过得有意义。 如果你看过我以前的文章，你会看到一些吐槽性质的文章，而后面就再也没有了。另外，我也不再没有针对具体的某个人做出评价，因为人太复杂的了，经历的越多，你就会发现你很难评价人，与其花时间在评论人和事上，不如把时间花在做一些力所能及的事来改善自己或身边的环境。所以，我建议大家少一些对人的指责和批评，通过对一件事来引发你的思考，想一想有什么可以改善，有什么方法可以做得更好，有哪些是自己可以添砖加瓦的？你会发现，只要你坚持这么做，你个人的提升和对社会的价值会越来越大，而你的影响力也会越来越大。 面对人生现在的我，即不是左派也不是右派，我不喜欢爱国主义，我也不喜欢崇洋媚外，我更多的时候是一个自由派，哪边我都不站，我站我自己。因为，生活在这样的一个时代，能让自己过好都是一些比较奢望的事了。 《教父》里有这样的人生观：第一步要努力实现自我价值，第二步要全力照顾好家人，第三步要尽可能帮助善良的人，第四步为族群发声，第五步为国家争荣誉。事实上作为男人，前两步成功，人生已算得上圆满，做到第三步堪称伟大，而随意颠倒次序的那些人，一般不值得信任。 这也是古人的“修身齐家治国平天下”！所以，在你我准备要开始要“平天下”的时候，也得先想想，自己的生活有没有过好了，家人照顾好了么，身边有哪些力所能及的事是可以去改善的…… 穷则独善其身，达则兼济天下。提升自己，实现自我，照顾好自己的家人，帮助身边的人。这已经很不错了！ 什么样的人干什么样的事，什么样的阶段做什么样的选择，有人的说，选择比努力更重要的，我深以为然，而且，我觉得选择和决定，比努力更难，努力是认准了一个事后不停地发力，而决定要去认准哪个事是自己该坚持努力的，则是令人彷徨和焦虑的（半途而废的人也很多）。面对人生，你每天都在作一个一个的决定，在做一个又一个的选择，有的决定大，有的决定小，你的人生的轨迹就是被这一个一个的决定和选择所走走出来的。 我在24岁放弃了一房子离开银行到小公司的时候，我就知道，人生的选择就是一个翘翘板，你要一头就没有另一头，选择是有代价的，你不选择的代价更大；选择是要冒险的，你不敢冒险的风险更大；选择是需要放弃的，因为无论怎么选你都会要放弃。想想你老了以后，回头一看，好多事情在年轻的时候都不敢做，而你再也没有机会，你就知道不敢选择不敢冒险的代价有多大了。 选择就是一种 trade-off，这世上根本不会有什么完美，只要你想做事，你有雄心壮志，你的人生就是一个坑接着一个坑，你所能做的就是找到你喜欢的方向跳坑。 所以， 你要想清楚你要什么，不要什么，而且还不能要得太多，这样你才好做选择。否则，你影响你的因子太多，决定不好做，也做不好。 就像最前面说的一样，你是激进派还是保守派，你是喜欢领导还是喜欢跟从，你是注重长期还是注重短期，你是注重过程还是注重结果……等等，你对这些东西的坚持和守护，成为了你的“三观”，而你的三观则影响着你的选择，而你的选择影响着你的人生。 价值取向下面是一些大家经常在说，可能也是大多数人关心的问题，就这些问题，我也谈谈我的价值取向。 ==挣钱。== 挣钱是一个大家都想做的事，但你得解决一个很核心的问题，那就是为什么别人愿意给你钱？对于挣钱的价值观从我大学毕业到现我就没怎么变过，那就是我更多关注的是怎么提高自己的能力，让自己值那个价钱，让别人愿意付钱。另外一方面，我发现，越是有能力的人，就越不计较一些短期得失，越计较短期得失的人往往都是很平庸的人。有能力的人不会关心自己的年终奖得拿多少，会不会晋升，他们更多的关心自己真正的实力有没有超过更多的人，更多的关注的是自己长远的成长，而不是一时的利益。聪明的人从来不关心眼前的得失，不会关心表面上的东西，他们更多关心的是长期利益，关心长期利益的人一定不是投机者，一定是投资者，投资会把自己的时间精力金钱投资在能让自己成长和提升的地方，那些让自己可以操更大的盘的地方，他们培养自己的领导力和影响力。 而投机者在职场上会通过溜须拍马讨好领导，在学习上追求速成，在投资上使用跟随策略，在创业上甚至会不择手段，当风险来临时，投机者是几乎完全没有抗风险能力的，他们所谓的能力只不过因为形势好。 ==技术。== 对于计算机技术来说，要学的东西实在是太多，我并不害怕要学的东西很多，因为学习能力是一个好的工程师必需具备的事，我不惧怕困难和挑战。我觉得在语言和技术争论谁好谁坏是一种幼稚的表现， 没有完美的技术，Engineering 玩的是 Tradeoff。所以，我对没有完美的技术并不担心，但是我反而担心的是，当我们进入到一些公司后，这些公司会有一些技术上的沉淀也就是针对公司自己的专用技术，比如一些中间件，一些编程框架，lib库什么的。老实说，我比较害怕公司的专用技术，因为一旦失业，我建立在这些专用技术上的技能也会随之瓦解，有时候，我甚至害怕把我的技术建立在某一个平台上，小众的不用说了，大众的我也比较担扰，比如Windows或Unix/Linux上，因为一旦这个平台不流行或是被取代，那么我也会随之淘汰（过去的这20年已经发生过太多这样的事了）。为了应对这样的焦虑，我更愿意花时间在技术的原理和技术的本质上，这导致我需要了解各种各样的技术的设计方法，以及内在原理。 所以，当国内的绝大多数程序员们更多的关注架构性能的今天，我则花更多的时间去了解编程范式，代码重构，软件设计，计算机系统原理，领域设计，工程方法……因为只有原理、本质和设计思想才可能让我不会被绑在某个专用技术或平台上，除非，我们人类的计算机这条路没走对。 ==职业。== 在过去20多年的职业生涯中，我从基层工程师做到管理，很多做技术的人都会转管理，但我却还是扎根技术，就算是在今天，还是会抠很多技术细节，包括写代码。因为我心里觉得，不写代码的人一定是做不好技术管理的，因为做技术管理有人要做技术决定，从不上手技术的人是做不好技术决定的，另一方面，我觉得管理是支持性的工作，不是产出性的工作，大多数的管理者无非是因为组织大了，所以需要管人管事，所以，必然要花大量的时间和精力处理各种问题，甚至办公室政治，然而，如果有一天失业了，大环境变得不好了，一个管理者和一个程序员要出去找工作，程序员会比管理者更能自食其力。所以，我并不觉得管理者这个职业有意思，我还是觉得程序员这个有创造性的职业更有趣。通常来说，管理者的技能力需要到公司和组织里才能展现，而有创造力的技能的人是可以自己独立的能力，所以，我觉得程序员的技能比管理者的技能能让我更稳定更自地活着。所以，我更喜欢“电影工作组”那样的团队和组织形式。 ==打工。== 对于打工，也就是加入一家公司工作，无论是在一家小公司还是一家大公司工作，都会有好的和不好的，任何公司都有其不完美的地方，这个需要承认。首先第一的肯定是完成公司交给你的任务（但我也不会是傻傻地完成工作，对于一些有问题的任务我也会提出我的看法），然后我会尽我所能在工作找到可以提高效率的地方进行改善。在推动公司/部门/团队在一技术和工程方面进步并不是一件很容易的事，因为进步是需要成本的，有时候，这种成本并不一定是公司和团队愿意接受的，而另外，从客观规律上来说，一件事的进步一定是会有和现状有一些摩擦的。有的人害怕有摩擦而忍了，而我则不是，我觉得与别人的摩擦并不可怕，因为大家的目标都是基本一致的，只是做事的标准和方式不一样，这是可能沟通的，始终是会相互理解的。而如果你没有去推动一个事，我觉得对于公司对于我个人来说，都是一种对人生的浪费，敬业也好，激情也好，其就是体现在你是否愿意冒险去推动一件于公于私都有利的事，而不是成为一个“听话”、“随大流”、“懒政”的人，即耽误了公司也耽误了自己。所以，我更信仰的是《做正确的事情，等着被开除》，这些东西，可参看《我看绩效考核》。 ==创业。== 前两天，有个小伙来跟我说，说他要离开BAT要去创业公司了，说在那些更自由一些，没有大公司的种种问题。我毫不犹豫地教育了他一下，我说，你选择这个创业公司的动机不对啊，你无非就是在逃避一些东西罢了，你把创业公司当做是一个避风港，这是不对的，创业公司的问题可能会更多，去创业公司的更好的心态是，这个创业公司在干的事业是不是你的事业？说白了，如果你是为了你的事业，为了解决个什么，为了改进个什么，那么，创业是适合你的，也只有在做自己事业的时候，你才能不惧困难，才会勇敢地面对一切。那种想找一个安稳的避风港呆着的心态是不会让你平静地，你要知道世界本来就是不平静的，找了自己的归宿和目标才可能让你真正的平静。所以，在我现的创业团队，我不要求大家加班，我也不鸡汤洗脑，对于想要加入的人，我会跟他讲我现在遇到的各种问题以及各种机遇，并一直在让他自己思考，我们在做的事是不是自己的事业诉求？还可不可以更好？每个人都应该为自己的事业为自己的理想去活一次，追逐自己的事业和理想并不容易，需要有很大的付出，而也只有你心底里的那个理想值得这么大的付出…… ==客户。== 基于上述的价值观，在我现在创业的时候，我在面对客户的时候，也是一样的，我并不会完全的迁就于客户，我的一些银行客户和互联网客户应该体会到我的做的方式了，我并不觉得迁就用户，用户要什么我就应该给什么，用户想听什么，我就说什么，虽然这样可以省着精力，更圆滑，但这都不是我喜欢的，我更愿意鲜明地表达我的观点，并拉着用户跟我一起成长，因为我并不觉得完成客户的项目有成就感，我的成就感来自客户的成长。所以，面对客户有些做得不对有问题有隐患的地方，或是有什么做错的事，我基本上都是直言不讳地说出来，因为我觉得把真实的相法说出来是对客户和对自己最基本的尊重，不管客户最终的选择是什么，我都要把利弊跟客户讲清楚。我并不是在这里装，因为，我也想做一些更高级更有技术含量的事，所以，对于一些还达到的客户，我如果不把他们拉上来，我也对不起自己。 在我“不惑之年”形成了这些价值观体系，也许未来还会变，也许还不成熟，总之，我不愿跟大多数人一样，因为大多数人都是随遇而安随大流的，因为这样风险最小，而我想走一条属于自己的路，做真正的自己，就像我24岁从银行里出来时想的那样，我选择对了一个正确的专业（计算机科学），呆在了一个正确的年代（信息化革命），这样的“狗屎运”几百年不遇，如果我还患得患失，那我岂不辜负活在这样一个刺激的时代？！我所要做的就是在这个时代中做有价值的事就好了！这个时代真的是太好了！","categories":[{"name":"其他","slug":"其他","permalink":"http://huangzhiyuan.github.io/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"价值观","slug":"价值观","permalink":"http://huangzhiyuan.github.io/tags/%E4%BB%B7%E5%80%BC%E8%A7%82/"}]},{"title":"万物皆可embedding","slug":"everything-is-embedding","date":"2020-03-02T14:05:14.000Z","updated":"2020-03-03T12:53:26.000Z","comments":true,"path":"2020/03/02/everything-is-embedding/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/02/everything-is-embedding/","excerpt":"不知道大家有没有这种感受，在学习推荐系统算法模型时，少不了embedding的应用，有的推荐算法模型甚至可以说就是在做embedding的过程，可见embedding在推荐系统中的重要性。这篇文章就专门把embedding单独提出来，梳理一下embedding在推荐系统中的应用。以下内容主要从深度学习方法和传统的协同过滤方法两个方面加深和理解在推荐系统领域对embedding的认识，感受下“embedding”这一重要思想。","text":"不知道大家有没有这种感受，在学习推荐系统算法模型时，少不了embedding的应用，有的推荐算法模型甚至可以说就是在做embedding的过程，可见embedding在推荐系统中的重要性。这篇文章就专门把embedding单独提出来，梳理一下embedding在推荐系统中的应用。以下内容主要从深度学习方法和传统的协同过滤方法两个方面加深和理解在推荐系统领域对embedding的认识，感受下“embedding”这一重要思想。 深度学习方法先拿一篇推荐系统领域中最为经典的论文之一“Deep Neural Networks for YouTubeRecommendations”来讲，Youtube的这篇视频推荐模型框架基本上奠定了后面推荐系统的主要步骤：召回和排序，如下图所示： 其中召回阶段（candidate generation）就是要从推荐的百万级别的视频库中筛选出用户可能感兴趣的视频，把推荐的视频库量级从百万级降到几百个。但是到底怎样才能快速高效的完成筛选呢？要知道youtube是非常活跃的视频网站，每秒同时访问的人数成千上万，要同时实现每秒对每个用户都个性化的从百万视频候选集中挑出几百个用户感兴趣的视频，想想都不容易，因此每次做用户召回都跑一遍模型是不可能的，其解决方法就和接下来要介绍的embedding应用相关。如下图 1 所示为youtube召回阶段的模型： 在离线的模型训练阶段，采用的是简单粗暴的softmax来对视频库中的所有视频进行分类，label为用户Next watched video的那个视频，因此整个召回阶段构建模型的思想就是预测某一时刻某一用户在对百万级以上的视频库哪个视频感兴趣，它最想点击和观看的视频是哪一个。 这里有一个很重要的问题就是百万级别以上的视频做softmax是很费计算资源和时间的，因此在召回模型的离线训练阶段采用了word2vec中的Negative Sample思想。可是即便是这样，面对百万级别的视频库，以及每秒成千上万的召回请求，在线上还是无法满足需求，不能直接使用模型去对视频做softmax，按照概率大小选取TopN来做召回，因此就有了embedding做召回的方法。 如上图 1 所示主要思想是把softmax的前一层的输出作为user的embedding，而softmax层中的权重矩阵的每一行向量作为video的embedding，这样在模型的部署过程中，就没有必要部署整个神经网络来完成从原始特征向量到最终输出的预测过程，只需要将User Embedding和video Embedding存储到线上内存数据库，通过内积运算再排序的方法就可以得到video的排名，这大大加快了召回效率。 如上图 1 所示主要思想是把softmax的前一层的输出作为user的embedding，而softmax层中的权重矩阵的每一行向量作为video的embedding，这样在模型的部署过程中，就没有必要部署整个神经网络来完成从原始特征向量到最终输出的预测过程，只需要将User Embedding和video Embedding存储到线上内存数据库，通过内积运算再排序的方法就可以得到video的排名，这大大加快了召回效率。 这里再用图示的方法具体介绍一下哪个是user embedding，哪些是video embedding，为什么可以向量内积召回，为什么内积得到的值大小就可以反映出用户对视频的兴趣程度。如上图所示，把图1 中的倒数第二层和最后一层softmax单独拿出来分析，实际上图 1 中最后一个softmax是一个加了softmax函数的全连接层，该全连接层（图中softmax）的神经元个数和需要分类的视频个数相同，都为T，softmax函数对该全连接层（图中softmax）的输出再进行归一化得到每个视频对应的概率值。 这个全连接层（图中softmax）的每一个神经元都对应有上一层全连接层（图中relu）输出的权重向量，为上图中W[T×N]权重矩阵的每一行，而这每一行权重向量就是video embedding，个数和神经元的个数相同都是视频库中视频的个数T，上一层全连接层（图中relu）的输出为user embedding，是上图中的X[N×1]。因此通过内积user embedding和video embedding其实得到的正是未经过softmax函数归一化的每个视频的预测值，从一定程度上反映了预测概率的大小。 因此user embedding和video embedding内积值越大，则反应该用户对该视频感兴趣的概率值大，所以可以提前将User Embedding和video Embedding存储到线上内存数据库，通过内积运算再排序的方法得到video的排名，以此来提高召回速度和效率。 总结一句话就是，为了提高召回速度和效率，相当于把召回模型的inference转换成了通过embedding向量内积方法快速得到用户对视频得分的方法。从这个例子可以看出embedding的应用之一：==通过计算用户和物品的Embedding相似度，Embedding可以直接作为推荐系统的召回层或者召回方法之一。== 其实像embedding的这种应用非常多，像微软的一篇论文Item2Vec: Neural Item Embeddingfor Collaborative Filtering，专门训练item的embedding，然后通过计算item之间的embedding相似度来做基于物品的协同过滤，实际上通过这种方式也可以缩小推荐候选物品的范围，用来直接做物品的相似推荐。 而专门学习item embedding的方法还有很多，比如通过graph embedding方式的deepwalk，LINE，Node2vec和SDNE等等。因此再结合这些embedding的应用例子，再对embedding在召回方面的应用浓缩总结一下就是：通过计算用户和物品或物品和物品的Embedding相似度，来缩小推荐候选库的范围。 除此之外，通过总结目前主流的ctr预估模型比如wide&amp;deep，deepFM，PNN和DCN等等可以发现，embedding还有一个非常普遍的应用就是实现高维稀疏特征向量向低维稠密特征向量的转换，通俗来讲就是把离散特征经过独热编码后的稀疏向量表达转化成稠密的特征向量表达。 或者从另一个角度看，embedding本身就是对事物的多维度特征表示，因此在ctr预估模型中，训练好的embedding可以当作输入深度学习模型的特征，比如FNN利用FM预训练好的embedding来当作模型的输入，然后通过特征交叉操作比如多层感知机得到这些embedding的交叉特征。具体的交叉特征分析以及有关推荐系统中对于特征的处理可以查看公众号中的特征处理系列文章。 有关深度学习方法中的embedding应用就介绍到这，这里再着重介绍一下最近学习的有关协同过滤的传统推荐方法中embedding思想的体现。 传统方法虽然目前有关推荐系统的模型中深度学习越来越占据重要地位，但是embedding的重要思想却贯穿始终，在传统的推荐方法中依然可以看到embedding的影子。例如基于矩阵分解的协同过滤模型，如下图所示： 通过将用户对物品的打分矩阵Rating Matrix分解成User Matrix和Item Matrix两个矩阵相乘，我们可以把User Matrix和Item Matrix相乘分别看作是A，B，C，D四个user embedding和W，X，Y，Z四个item embedding相乘。 可以看到打分矩阵比较稀疏，说明有的user没有给item打分，因此矩阵分解的目的就是通过分解的user embedding和item embedding相乘来填充user没有打分的item，从而可以进行推荐。而模型通过user embedding和item embedding相乘拟合user已给item的打分来学习embedding参数。 如上图所示，分解的user embedding B（[1.4,0.9]）和item embedding W（[1.5,1.7]）相乘得到3.63要尽量接近Rating Matrix中的B行W列也就是4.0，根据这种拟合学习得到embedding，最终再根据模型学习的user embedding和item embedding相乘得到user没有给item的打分，比如Rating Matrix中的A行W列的得分为user embedding A（[1.2,0.8]）和item embedding W（[1.5,1.7]）相乘得到3.16. 虽然模型比较简单，但是可以发现embedding的思想其实贯穿在整个模型当中。 除了基于矩阵分解的协同过滤，还有基于自编码器的协同过滤，自编码器做协同过滤的思想主要是把用户对所有物品的打分组成一个固定维度的向量（没有打分的填充为0）然后通过自编码器对该打分向量进行编码解码然后得到和该打分向量维度相同的向量，该自编码器模型的输出向量就是为了拟合输入向量。如图所示为自编码器模型： 先对输入向量通过两个全连接层进行编码，然后再通过两个全连接层进行解码最后得到拟合输入向量的输出向量。该模型同样也是通过拟合已经有的打分来学习整个模型的参数，模型学习好之后，之前填充为0的物品打分就可以通过模型输出向量的对应位置得到。 虽然这个模型没有很明显的embedding思想的体现，但是以我个人对embedding的理解，在模型最终输出的用户对所有物品的打分稠密向量是否可以用来当作表示用户兴趣的user embedding向量呢？ 总结通过这篇文章对embedding的分析，总结embedding有以下三个作用： 通过计算用户和物品或物品和物品的Embedding相似度，来缩小推荐候选库的范围。 实现高维稀疏特征向量向低维稠密特征向量的转换。 训练好的embedding可以当作输入深度学习模型的特征。 无论embedding在模型中最终起到哪些重要的作用，在对embedding的本质理解上，它自始至终都是用一个多维稠密向量来对事物从多维度进行的特征刻画。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"embedding","slug":"embedding","permalink":"http://huangzhiyuan.github.io/tags/embedding/"}]},{"title":"代码优化概要","slug":"code-optimze-guide","date":"2020-03-01T11:27:38.000Z","updated":"2020-03-01T12:32:32.000Z","comments":true,"path":"2020/03/01/code-optimze-guide/","link":"","permalink":"http://huangzhiyuan.github.io/2020/03/01/code-optimze-guide/","excerpt":"本文译自Dr. Dobb’s Blogger的Walter Bright写的《Overlooked Essentials For Optimizing Code》。 我编写程序至今有35年了，我做了很多关于程序执行速度方面优化的工(一个示例)，我也看过其它人做的优化。我发现有两个最基本的优化技术总是被人所忽略。 注意，这两个技术并不是避免时机不成熟的优化。并不是把冒泡排序变成快速排序（算法优化）。也不是语言或是编译器的优化。也不是把 i*4写成i&lt;&lt;2 的优化。 这两个技术是： 使用 一个profiler。 查看程序执行时的汇编码。 使用这两个技术的人将会成功地写出运行快的代码，不会使用这两个技术的人则不行。下面让我为你细细道来。","text":"本文译自Dr. Dobb’s Blogger的Walter Bright写的《Overlooked Essentials For Optimizing Code》。 我编写程序至今有35年了，我做了很多关于程序执行速度方面优化的工(一个示例)，我也看过其它人做的优化。我发现有两个最基本的优化技术总是被人所忽略。 注意，这两个技术并不是避免时机不成熟的优化。并不是把冒泡排序变成快速排序（算法优化）。也不是语言或是编译器的优化。也不是把 i*4写成i&lt;&lt;2 的优化。 这两个技术是： 使用 一个profiler。 查看程序执行时的汇编码。 使用这两个技术的人将会成功地写出运行快的代码，不会使用这两个技术的人则不行。下面让我为你细细道来。 使用一个 Profiler我们知道，程序运行时的90%的时间是用在了10%的代码上。我发现这并不准确。一次又一次地，我发现，几乎所有的程序会在1%的代码上花了99%的运行时间。但是，是哪个1%？一个好的Profiler可以告诉你这个答案。就算我们需要使用100个小时在这1%的代码上进行优化，也比使用100个小时在其它99%的代码上优化产生的效益要高得多得多。 问题是什么？人们不用profiler？不是。我工作过的一个地方使用了一个华丽而奢侈的Profiler，但是自从购买这个Profiler后，它的包装3年来还是那么的暂新。为什么人们不用？我真的不知道。有一次，我和我的同事去了一个负载过大的交易所，我同事坚持说他知道哪里是瓶颈，毕竟，他是一个很有经验的专家。最终，我把我的Profiler在他的项目上运行了一下，我们发现那个瓶颈完全在一个意想不到的地方。 就像是赛车一样。团队是赢在传感器和日志上，这些东西提供了所有的一切。你可以调整一下赛车手的裤子以让其在比赛过程中更舒服，但是这不会让你赢得比赛，也不会让你更有竞争力。如果你不知道你的速度上不去是因为引擎、排气装置、空体动力学、轮胎气压，或是赛车手，那么你将无法获胜。编程为什么会不同呢？只要没有测量，你就永远无法进步。这个世界上有太多可以使用的Profiler了。随便找一个你就可以看到你的函数的调用层次，调用的次数，以前每条代码的时间分解表（甚至可以到汇编级）。我看过太多的程序员回避使用Profiler，而是把时间花在那些无用的，错误的方向上的“优化”，而被其竞争对手所羞辱。（译者注：使用Profiler时，重点需要关注：1）花时间多的函数以优化其算法，2）调用次数巨多的函数——如果一个函数每秒被调用300K次，你只需要优化出0.001毫秒，那也是相当大的优化。这就是作者所谓的1%的代码占用了99%的CPU时间） 查看汇编代码几年前，我有一个同事，Mary Bailey，她在华盛顿大学教矫正代数（remedial algebra），有一次，她在黑板上写下： x + 3 = 5 然后问他的学生“求解x”，然后学生们不知道答案。于是她写下： __ + 3 = 5 然后，再问学生“填空”，所有的学生都可以回答了。未知数x就像是一个有魔法的字母让大家都在想“x意味着代数，而我没有学过代数，所以我就不知道这个怎么做”。 汇编程序就是编程世界的代数。如果某人问我“inline函数是否被编译器展开了？”或是问我“如果我写下i*4，编译器会把其优化为左移位操作吗？”。这个时候，我都会建议他们看看编译器的汇编码。这样的回答是不是很粗暴和无用？通常，在我这样回答了提问者后，提问都通常都会说，对不起，我不知道什么是汇编！甚至C++的专家都会这么回答。 汇编语言是最简单的编程语言了（就算是和C++相比也是这样的），如： 1ADD ESI,x 就是（C风格的代码） 1ESI += x; 而： 1CALL foo 则是： 1foo(); 细节因为CPU的种类而不同，但这就是其如何工作的。有时候，我们甚至都不需要细节，只需要看看汇编码的长啥样，然后和源代码比一比，你就可以知道汇编代码很多很多了。那么，这又如何帮助代码优化？举个例子，我几年前认识一个程序员认为他应该去发现一个新的更快的算法。他有一个benchmark来证明这个算法，并且其写了一篇非常漂亮的文章关于他的这个算法。但是，有人看了一下其原来算法以及新算法的汇编，发现了他的改进版本的算法允许其编译器把两个除法操作变成了一个。这和算法真的没有什么关系。我们知道除法操作是一个很昂贵的操作，并且在其算法中，这俩个除法操作还在一个内嵌循环中，所以，他的改进版的算法当然要快一些。但，只需要在原来的算法上做一点点小的改动——使用一个除法操作，那么其原来的算法将会和新的一样快。而他的新发现什么也不是。下一个例子，一个D用户张贴了一个 benchmark 来显示 dmd (Digital Mars D 编译器)在整型算法上的很糟糕，而ldc (LLVM D 编译器) 就好很多了。对于这样的结果，其相当的有意见。我迅速地看了一下汇编，发现两个编译器编译出来相当的一致，并没有什么明显的东西要对2：1这么大的不同而负责。但是我们看到有一个对long型整数的除法，这个除法调用了运行库。而这个库成为消耗时间的杀手，其它所有的加减法都没有速度上的影响。出乎意料地，benchmark 和算法代码生成一点关系也没有，完全就是long型整数的除法的问题。这暴露了在dmd的运行库中的long型除法的实现很差。修正后就可以提高速度。所以，这和编译器没有什么关系，但是如果不看汇编，你将无法发现这一切。查看汇编代码经常会给你一些意想不到的东西让你知道为什么程序的性能是那样。一些意想不到的函数调用，预料不到的自傲，以及不应该存在的东西，等等其实所有的一切。但也不需要成为一个汇编代码的黑客才能干的事。 结论如果你觉得需要程序有更好的执行速度，那么，最基本的方法就是使用一个profiler和愿意去查看一下其汇编代码以找到程序的瓶颈。只有找到了程序的瓶颈，此时才是真正在思考如何去改进的时候，比如思考一个更好的算法，使用更快的语言优化，等等。 常规的做法是制胜法宝是挑选一个最佳的算法而不是进行微优化。虽然这种做法是无可异议的，但是有两件事情是学校没有教给你而需要你重点注意的。第一个也是最重要的，如果你优化的算法没没有参与到你程序性能中的算法，那么你优化他只是在浪费时间和精力，并且还转移了你的注意力让你错过了应该要去优化的部分。第二点，算法的性能总和处理的数据密切相关的，就算是冒泡排序有那么多的笑柄，但是如果其处理的数据基本是排好序的，只有其中几个数据是未排序的，那么冒泡排序也是所有排序算法里性能最好的。所以，担心没有使用好的算法而不去测量，只会浪费时间，无论是你的还是计算机的。 就好像赛车零件的订购速底是不会让你更靠进冠军（就算是你正确安装零件也不会），没有Profiler，你不会知道问题在哪里，不去看汇编，你可能知道问题所在，但你往往不知道为什么。 (全文完)","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"Code Optimization","slug":"Code-Optimization","permalink":"http://huangzhiyuan.github.io/tags/Code-Optimization/"}]},{"title":"VIM攻略","slug":"vim-brief-usage","date":"2020-02-29T13:20:07.000Z","updated":"2020-02-29T14:00:10.000Z","comments":true,"path":"2020/02/29/vim-brief-usage/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/29/vim-brief-usage/","excerpt":"学习 vim 并且其会成为你最后一个使用的文本编辑器。没有比这个更好的文本编辑器了，非常地难学，但是却不可思议地好用。学习建议： 学习vim在开始时是痛苦的。 需要时间 需要不断地练习，就像你学习一个乐器一样。 不要期望你能在3天内把vim练得比别的编辑器更有效率。 事实上，你需要2周时间的苦练，而不是3天。","text":"学习 vim 并且其会成为你最后一个使用的文本编辑器。没有比这个更好的文本编辑器了，非常地难学，但是却不可思议地好用。学习建议： 学习vim在开始时是痛苦的。 需要时间 需要不断地练习，就像你学习一个乐器一样。 不要期望你能在3天内把vim练得比别的编辑器更有效率。 事实上，你需要2周时间的苦练，而不是3天。 学习阶段： 存活 感觉良好 觉得更好，更强，更快 使用VIM的超能力 存活当你安装好一个编辑器后，你一定会想在其中输入点什么东西，然后看看这个编辑器是什么样子。但vim不是这样的，请按照下面的命令操作： 启动Vim后，vim在 Normal 模式下。 让我们进入 Insert 模式，请按下键 i 。(你会看到vim左下角有一个–insert–字样，表示，你可以以插入的方式输入了） 此时，你可以输入文本了，就像你用“记事本”一样。 如果你想返回 Normal 模式，请按 ESC 键。 现在，你知道如何在 Insert 和 Normal 模式下切换了。下面是一些命令，可以让你在 Normal 模式下幸存下来： 123456789i → Insert 模式，按 ESC 回到 Normal 模式.x → 删当前光标所在的一个字符。:wq → 存盘 + 退出 (:w 存盘, :q 退出) （陈皓注：:w 后可以跟文件名）dd → 删除当前行，并把删除的行存到剪贴板里p → 粘贴剪贴板 你能在vim幸存下来只需要上述的那5个命令，你就可以编辑文本了，你一定要把这些命令练成一种下意识的状态。于是你就可以开始进阶到第二级了。 感觉良好上面的那些命令只能让你存活下来，现在是时候学习一些更多的命令了，下面是我的建议： 各种插入模式1234567a → 在光标后插入o → 在当前行后插入一个新行O → 在当前行前插入一个新行cw → 替换从光标所在位置后到一个单词结尾的字符 简单的移动光标1234567890 → 数字零，到行头^ → 到本行第一个不是blank字符的位置（所谓blank字符就是空格，tab，换行，回车等）$ → 到本行行尾g_ → 到本行最后一个不是blank字符的位置。/pattern → 搜索 pattern 的字符串（陈皓注：如果搜索出多个匹配，可按n键到下一个） Copy/Paste123P → 粘贴yy → 拷贝当前行当行于 ddP Undo/Redo123u → undo&lt;C-r&gt; → redo 打开/保存/退出/改变文件(Buffer)1234567891011:e &lt;path/to/file&gt; → 打开一个文件:w → 存盘:saveas &lt;path/to/file&gt; → 另存为 &lt;path/to/file&gt;:x， ZZ 或 :wq → 保存并退出 (:x 表示仅在需要时保存，ZZ不需要输入冒号并回车):q! → 退出不保存 :qa! 强行退出所有的正在编辑的文件，就算别的文件有更改。:bn 和 :bp → 你可以同时打开很多文件，使用这两个命令来切换下一个或上一个文件。 花点时间熟悉一下上面的命令，一旦你掌握他们了，你就几乎可以干其它编辑器都能干的事了。但是到现在为止，你还是觉得使用vim还是有点笨拙，不过没关系，你可以进阶到第三级了。 更好，更强，更快先恭喜你！你干的很不错。我们可以开始一些更为有趣的事了。在第三级，我们只谈那些和vi可以兼容的命令。 更好下面，让我们看一下vim是怎么重复自己的： . → (小数点) 可以重复上一次的命令 N → 重复某个命令N次 下面是一个示例，找开一个文件你可以试试下面的命令： 123456782dd → 删除2行3p → 粘贴文本3次100idesu [ESC] → 会写下 “desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu “. → 重复上一个命令—— 100 “desu “.3. → 重复 3 次 “desu” (注意：不是 300，你看，VIM多聪明啊). 更强你要让你的光标移动更有效率，你一定要了解下面的这些命令，千万别跳过。 NG → 到第 N 行 （陈皓注：注意命令中的G是大写的，另我一般使用 : N 到第N行，如 :137 到第137行） gg → 到第一行。（陈皓注：相当于1G，或 :1） G → 到最后一行。 按单词移动： 123w → 到下一个单词的开头。e → 到下一个单词的结尾。 下面，让我来说说最强的光标移动 123% : 匹配括号移动，包括 (, &#123;, [. （陈皓注：你需要把光标先移到括号上）* 和 #: 匹配光标当前所在的单词，移动光标到下一个（或上一个）匹配单词（*是下一个，#是上一个） 更快你一定要记住光标的移动，因为很多命令都可以和这些移动光标的命令连动。很多命令都可以如下来干： &lt;start position&gt;&lt;command&gt;&lt;end position&gt; 例如 0y$ 命令意味着： 0 → 先到行头 y → 从这里开始拷贝 $ → 拷贝到本行最后一个字符 你可可以输入 ye，从当前位置拷贝到本单词的最后一个字符。 你也可以输入 y2/foo 来拷贝2个 “foo” 之间的字符串。 还有很多时间并不一定你就一定要按y才会拷贝，下面的命令也会被拷贝： d (删除 ) v (可视化的选择) gU (变大写) gu (变小写) 等等（注：可视化选择是一个很有意思的命令，你可以先按v，然后移动光标，你就会看到文本被选择，然后，你可能d，也可y，也可以变大写等） Vim 超能力你只需要掌握前面的命令，你就可以很舒服的使用VIM了。但是，现在，我们向你介绍的是VIM杀手级的功能。下面这些功能是我只用vim的原因。 在当前行上移动光标: 0 ^ $ f F t T , ; 1234567891011121314150 → 到行头^ → 到本行的第一个非blank字符$ → 到行尾g_ → 到本行最后一个不是blank字符的位置。fa → 到下一个为a的字符处，你也可以fs到下一个为s的字符。t, → 到逗号前的第一个字符。逗号可以变成其它字符。3fa → 在当前行查找第三个出现的a。F 和 T → 和 f 和 t 一样，只不过是相反方向。 块操作: &lt;C-v&gt;块操作，典型的操作： 0 &lt;C-v&gt; &lt;C-d&gt; I-- [ESC] ^ → 到行头 &lt;C-v&gt; → 开始块操作 &lt;C-d&gt; → 向下移动 (你也可以使用hjkl来移动光标，或是使用%，或是别的) I– [ESC] → I是插入，插入“–”，按ESC键来为每一行生效。 自动提示： &lt;C-n&gt; 和 &lt;C-p&gt;在 Insert 模式下，你可以输入一个词的开头，然后按 或是，自动补齐功能就出现了…… 可视化选择： v,V,&lt;C-v&gt;前面，我们看到了 的示例 （在Windows下应该是），我们可以使用 v 和 V。一但被选好了，你可以做下面的事： J → 把所有的行连接起来（变成一行） &lt; 或 &gt; → 左右缩进 = → 自动给缩进 .在所有被选择的行后加上点东西： &lt;C-v&gt; 选中相关的行 (可使用 j 或 或是 /pattern 或是 % 等……) $ 到行最后 A, 输入字符串，按 ESC。 分屏: :split 和 vsplit.下面是主要的命令，你可以使用VIM的帮助 :help split. 你可以参考本站以前的一篇文章VIM分屏。 1234:split → 创建分屏 (:vsplit创建垂直分屏)&lt;C-w&gt;&lt;dir&gt; : dir就是方向，可以是 hjkl 或是 ←↓↑→ 中的一个，其用来切换分屏。&lt;C-w&gt;_ (或 &lt;C-w&gt;|) : 最大化尺寸 (&lt;C-w&gt;| 垂直分屏)&lt;C-w&gt;+ (或 &lt;C-w&gt;-) : 增加尺寸","categories":[{"name":"Linux","slug":"Linux","permalink":"http://huangzhiyuan.github.io/categories/Linux/"}],"tags":[{"name":"VIM","slug":"VIM","permalink":"http://huangzhiyuan.github.io/tags/VIM/"}]},{"title":"AWK使用教程","slug":"AWK-brief-usage","date":"2020-02-28T13:02:20.000Z","updated":"2020-02-28T13:48:12.000Z","comments":true,"path":"2020/02/28/AWK-brief-usage/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/28/AWK-brief-usage/","excerpt":"AWK和sed作为Linux文本解析神器，早就该进行总结，今天看到coolshell博客，总结的很好，特地做此记录。AWK来源于1977年的贝尔实验室。之所以叫AWK是因为其取了三位创始人 Alfred Aho，Peter Weinberger, 和 Brian Kernighan 的Family Name的首字符。要学AWK，就得提一提AWK的一本相当经典的书《The AWK Programming Language》.","text":"AWK和sed作为Linux文本解析神器，早就该进行总结，今天看到coolshell博客，总结的很好，特地做此记录。AWK来源于1977年的贝尔实验室。之所以叫AWK是因为其取了三位创始人 Alfred Aho，Peter Weinberger, 和 Brian Kernighan 的Family Name的首字符。要学AWK，就得提一提AWK的一本相当经典的书《The AWK Programming Language》. 起步上台首先从一个文件为例进行后续的awk操作，从netstat命令中提取了如下信息填充到文件： 123456789101112131415161718192021$ cat netstat.txtProto Recv-Q Send-Q Local-Address Foreign-Address Statetcp 0 0 0.0.0.0:3306 0.0.0.0:* LISTENtcp 0 0 0.0.0.0:80 0.0.0.0:* LISTENtcp 0 0 127.0.0.1:9000 0.0.0.0:* LISTENtcp 0 0 coolshell.cn:80 124.205.5.146:18245 TIME_WAITtcp 0 0 coolshell.cn:80 61.140.101.185:37538 FIN_WAIT2tcp 0 0 coolshell.cn:80 110.194.134.189:1032 ESTABLISHEDtcp 0 0 coolshell.cn:80 123.169.124.111:49809 ESTABLISHEDtcp 0 0 coolshell.cn:80 116.234.127.77:11502 FIN_WAIT2tcp 0 0 coolshell.cn:80 123.169.124.111:49829 ESTABLISHEDtcp 0 0 coolshell.cn:80 183.60.215.36:36970 TIME_WAITtcp 0 4166 coolshell.cn:80 61.148.242.38:30901 ESTABLISHEDtcp 0 1 coolshell.cn:80 124.152.181.209:26825 FIN_WAIT1tcp 0 0 coolshell.cn:80 110.194.134.189:4796 ESTABLISHEDtcp 0 0 coolshell.cn:80 183.60.212.163:51082 TIME_WAITtcp 0 1 coolshell.cn:80 208.115.113.92:50601 LAST_ACKtcp 0 0 coolshell.cn:80 123.169.124.111:49840 ESTABLISHEDtcp 0 0 coolshell.cn:80 117.136.20.85:50025 FIN_WAIT2tcp 0 0 :::22 :::* LISTEN 下面是最简单最常用的awk示例，其输出第1列和第4例 其中单引号中的被大括号括着的就是awk的语句，注意，其只能被单引号包含。 其中的$1..$n表示第几例。注：$0表示整个行。1234567891011121314151617181920$ awk &#x27;&#123;print $1, $4&#125;&#x27; netstat.txtProto Local-Addresstcp 0.0.0.0:3306tcp 0.0.0.0:80tcp 127.0.0.1:9000tcp coolshell.cn:80tcp coolshell.cn:80tcp coolshell.cn:80tcp coolshell.cn:80tcp coolshell.cn:80tcp coolshell.cn:80tcp coolshell.cn:80tcp coolshell.cn:80tcp coolshell.cn:80tcp coolshell.cn:80tcp coolshell.cn:80tcp coolshell.cn:80tcp coolshell.cn:80tcp coolshell.cn:80tcp :::22 格式化输出 1234567891011121314151617181920$ awk &#x27;&#123;printf &quot;%-8s %-8s %-8s %-18s %-22s %-15s\\n&quot;,$1,$2,$3,$4,$5,$6&#125;&#x27; netstat.txtProto Recv-Q Send-Q Local-Address Foreign-Address Statetcp 0 0 0.0.0.0:3306 0.0.0.0:* LISTENtcp 0 0 0.0.0.0:80 0.0.0.0:* LISTENtcp 0 0 127.0.0.1:9000 0.0.0.0:* LISTENtcp 0 0 coolshell.cn:80 124.205.5.146:18245 TIME_WAITtcp 0 0 coolshell.cn:80 61.140.101.185:37538 FIN_WAIT2tcp 0 0 coolshell.cn:80 110.194.134.189:1032 ESTABLISHEDtcp 0 0 coolshell.cn:80 123.169.124.111:49809 ESTABLISHEDtcp 0 0 coolshell.cn:80 116.234.127.77:11502 FIN_WAIT2tcp 0 0 coolshell.cn:80 123.169.124.111:49829 ESTABLISHEDtcp 0 0 coolshell.cn:80 183.60.215.36:36970 TIME_WAITtcp 0 4166 coolshell.cn:80 61.148.242.38:30901 ESTABLISHEDtcp 0 1 coolshell.cn:80 124.152.181.209:26825 FIN_WAIT1tcp 0 0 coolshell.cn:80 110.194.134.189:4796 ESTABLISHEDtcp 0 0 coolshell.cn:80 183.60.212.163:51082 TIME_WAITtcp 0 1 coolshell.cn:80 208.115.113.92:50601 LAST_ACKtcp 0 0 coolshell.cn:80 123.169.124.111:49840 ESTABLISHEDtcp 0 0 coolshell.cn:80 117.136.20.85:50025 FIN_WAIT2tcp 0 0 :::22 :::* LISTEN 过滤记录下面过滤条件为：第三列的值为0 &amp;&amp; 第6列的值为LISTEN其中的“==”为比较运算符。其他比较运算符：!=, &gt;, &lt;, &gt;=, &lt;= 12345$ awk &#x27;$3==0 &amp;&amp; $6==&quot;LISTEN&quot; &#x27; netstat.txttcp 0 0 0.0.0.0:3306 0.0.0.0:* LISTENtcp 0 0 0.0.0.0:80 0.0.0.0:* LISTENtcp 0 0 127.0.0.1:9000 0.0.0.0:* LISTENtcp 0 0 :::22 下面过滤条件为：第三列的值大于0，则打印该条记录 12345$ awk &#x27; $3&gt;0 &#123;print $0&#125;&#x27; netstat.txtProto Recv-Q Send-Q Local-Address Foreign-Address Statetcp 0 4166 coolshell.cn:80 61.148.242.38:30901 ESTABLISHEDtcp 0 1 coolshell.cn:80 124.152.181.209:26825 FIN_WAIT1tcp 0 1 coolshell.cn:80 208.115.113.92:50601 LAST_ACK 如果我们需要表头的话，我们可以引入内建变量NR： 123456$ awk &#x27;$3==0 &amp;&amp; $6==&quot;LISTEN&quot; || NR==1 &#x27; netstat.txtProto Recv-Q Send-Q Local-Address Foreign-Address Statetcp 0 0 0.0.0.0:3306 0.0.0.0:* LISTENtcp 0 0 0.0.0.0:80 0.0.0.0:* LISTENtcp 0 0 127.0.0.1:9000 0.0.0.0:* LISTENtcp 0 0 :::22 :::* LISTEN 再加上格式化输出: 123456$ awk &#x27;$3==0 &amp;&amp; $6==&quot;LISTEN&quot; || NR==1 &#123;printf &quot;%-20s %-20s %s\\n&quot;,$4,$5,$6&#125;&#x27; netstat.txtLocal-Address Foreign-Address State0.0.0.0:3306 0.0.0.0:* LISTEN0.0.0.0:80 0.0.0.0:* LISTEN127.0.0.1:9000 0.0.0.0:* LISTEN:::22 :::* LISTEN 内建变量 变量 意义 $0 当前记录（这个变量中存放着整个行的内容） $1~$n 当前记录的第n个字段，字段间由FS分隔 FS 输入字段分隔符 默认是空格或Tab NF 当前记录中的字段个数，就是有多少列 NR 已经读出的记录数，就是行号，从1开始，如果有多个文件话，这个值也是不断累加中。 FNR 当前记录数，与NR不同的是，这个值会是各个文件自己的行号 RS 输入的记录分隔符， 默认为换行符 OFS 输出字段分隔符， 默认也是空格 ORS 输出的记录分隔符，默认为换行 FILENAME 当前输入文件的名字 怎么使用呢，比如：我们如果要==输出行号==： 1234567$ awk &#x27;$3==0 &amp;&amp; $6==&quot;ESTABLISHED&quot; || NR==1 &#123;printf &quot;%02s %s %-20s %-20s %s\\n&quot;,NR, FNR, $4,$5,$6&#125;&#x27; netstat.txt01 1 Local-Address Foreign-Address State07 7 coolshell.cn:80 110.194.134.189:1032 ESTABLISHED08 8 coolshell.cn:80 123.169.124.111:49809 ESTABLISHED10 10 coolshell.cn:80 123.169.124.111:49829 ESTABLISHED14 14 coolshell.cn:80 110.194.134.189:4796 ESTABLISHED17 17 coolshell.cn:80 123.169.124.111:49840 ESTABLISHED ==指定分隔符== 123456789$ awk &#x27;BEGIN&#123;FS=&quot;:&quot;&#125; &#123;print $1,$3,$6&#125;&#x27; /etc/passwdroot 0 /rootbin 1 /bindaemon 2 /sbinadm 3 /var/admlp 4 /var/spool/lpdsync 5 /sbinshutdown 6 /sbinhalt 7 /sbin 上面的命令也等价于：（-F的意思就是指定分隔符） 1$ awk -F: &#x27;&#123;print $1,$3,$6&#125;&#x27; /etc/passwd 注：如果你要指定多个分隔符，你可以这样来 1awk -F &#x27;[;:]&#x27; 再来看一个以\\t作为分隔符输出的例子（下面使用了/etc/passwd文件，这个文件是以:分隔的）： 1234567$ awk -F: &#x27;&#123;print $1,$3,$6&#125;&#x27; OFS=&quot;\\t&quot; /etc/passwdroot 0 /rootbin 1 /bindaemon 2 /sbinadm 3 /var/admlp 4 /var/spool/lpdsync 5 /sbin 字符串匹配再来看几个字符串匹配的示例： 123456$ awk &#x27;$6 ~ /FIN/ || NR==1 &#123;print NR,$4,$5,$6&#125;&#x27; OFS=&quot;\\t&quot; netstat.txt1 Local-Address Foreign-Address State6 coolshell.cn:80 61.140.101.185:37538 FIN_WAIT29 coolshell.cn:80 116.234.127.77:11502 FIN_WAIT213 coolshell.cn:80 124.152.181.209:26825 FIN_WAIT118 coolshell.cn:80 117.136.20.85:50025 FIN_WAIT2 123456789$ $ awk &#x27;$6 ~ /WAIT/ || NR==1 &#123;print NR,$4,$5,$6&#125;&#x27; OFS=&quot;\\t&quot; netstat.txt1 Local-Address Foreign-Address State5 coolshell.cn:80 124.205.5.146:18245 TIME_WAIT6 coolshell.cn:80 61.140.101.185:37538 FIN_WAIT29 coolshell.cn:80 116.234.127.77:11502 FIN_WAIT211 coolshell.cn:80 183.60.215.36:36970 TIME_WAIT13 coolshell.cn:80 124.152.181.209:26825 FIN_WAIT115 coolshell.cn:80 183.60.212.163:51082 TIME_WAIT18 coolshell.cn:80 117.136.20.85:50025 FIN_WAIT2 上面的第一个示例匹配FIN状态， 第二个示例匹配WAIT字样的状态。其实 ~ 表示模式开始。/ /中是模式。这就是一个正则表达式的匹配。 其实awk可以像grep一样的去匹配第一行，就像这样： 12345$ awk &#x27;/LISTEN/&#x27; netstat.txttcp 0 0 0.0.0.0:3306 0.0.0.0:* LISTENtcp 0 0 0.0.0.0:80 0.0.0.0:* LISTENtcp 0 0 127.0.0.1:9000 0.0.0.0:* LISTENtcp 0 0 :::22 :::* LISTEN 可以使用 “/FIN|TIME/” 来匹配 FIN 或者 TIME : 123456789$ awk &#x27;$6 ~ /FIN|TIME/ || NR==1 &#123;print NR,$4,$5,$6&#125;&#x27; OFS=&quot;\\t&quot; netstat.txt1 Local-Address Foreign-Address State5 coolshell.cn:80 124.205.5.146:18245 TIME_WAIT6 coolshell.cn:80 61.140.101.185:37538 FIN_WAIT29 coolshell.cn:80 116.234.127.77:11502 FIN_WAIT211 coolshell.cn:80 183.60.215.36:36970 TIME_WAIT13 coolshell.cn:80 124.152.181.209:26825 FIN_WAIT115 coolshell.cn:80 183.60.212.163:51082 TIME_WAIT18 coolshell.cn:80 117.136.20.85:50025 FIN_WAIT2 再来看看模式取反的例子： 12345678910111213$ awk &#x27;$6 !~ /WAIT/ || NR==1 &#123;print NR,$4,$5,$6&#125;&#x27; OFS=&quot;\\t&quot; netstat.txt1 Local-Address Foreign-Address State2 0.0.0.0:3306 0.0.0.0:* LISTEN3 0.0.0.0:80 0.0.0.0:* LISTEN4 127.0.0.1:9000 0.0.0.0:* LISTEN7 coolshell.cn:80 110.194.134.189:1032 ESTABLISHED8 coolshell.cn:80 123.169.124.111:49809 ESTABLISHED10 coolshell.cn:80 123.169.124.111:49829 ESTABLISHED12 coolshell.cn:80 61.148.242.38:30901 ESTABLISHED14 coolshell.cn:80 110.194.134.189:4796 ESTABLISHED16 coolshell.cn:80 208.115.113.92:50601 LAST_ACK17 coolshell.cn:80 123.169.124.111:49840 ESTABLISHED19 :::22 :::* LISTEN 或者 1awk &#x27;!/WAIT/&#x27; netstat.txt 折分文件awk拆分文件很简单，使用重定向就好了。下面这个例子，是按第6例分隔文件，相当的简单（其中的NR!=1表示不处理表头）。 12345678910111213141516171819202122232425262728293031323334$ awk &#x27;NR!=1&#123;print &gt; $6&#125;&#x27; netstat.txt$ lsESTABLISHED FIN_WAIT1 FIN_WAIT2 LAST_ACK LISTEN netstat.txt TIME_WAIT$ cat ESTABLISHEDtcp 0 0 coolshell.cn:80 110.194.134.189:1032 ESTABLISHEDtcp 0 0 coolshell.cn:80 123.169.124.111:49809 ESTABLISHEDtcp 0 0 coolshell.cn:80 123.169.124.111:49829 ESTABLISHEDtcp 0 4166 coolshell.cn:80 61.148.242.38:30901 ESTABLISHEDtcp 0 0 coolshell.cn:80 110.194.134.189:4796 ESTABLISHEDtcp 0 0 coolshell.cn:80 123.169.124.111:49840 ESTABLISHED$ cat FIN_WAIT1tcp 0 1 coolshell.cn:80 124.152.181.209:26825 FIN_WAIT1$ cat FIN_WAIT2tcp 0 0 coolshell.cn:80 61.140.101.185:37538 FIN_WAIT2tcp 0 0 coolshell.cn:80 116.234.127.77:11502 FIN_WAIT2tcp 0 0 coolshell.cn:80 117.136.20.85:50025 FIN_WAIT2$ cat LAST_ACKtcp 0 1 coolshell.cn:80 208.115.113.92:50601 LAST_ACK$ cat LISTENtcp 0 0 0.0.0.0:3306 0.0.0.0:* LISTENtcp 0 0 0.0.0.0:80 0.0.0.0:* LISTENtcp 0 0 127.0.0.1:9000 0.0.0.0:* LISTENtcp 0 0 :::22 :::* LISTEN$ cat TIME_WAITtcp 0 0 coolshell.cn:80 124.205.5.146:18245 TIME_WAITtcp 0 0 coolshell.cn:80 183.60.215.36:36970 TIME_WAITtcp 0 0 coolshell.cn:80 183.60.212.163:51082 TIME_WAIT 统计下面的命令计算所有的C文件，CPP文件和H文件的文件大小总和。 1234$ ls -l *.cpp *.c *.h | awk &#x27;&#123;sum+=$5&#125; END &#123;print sum&#125;&#x27;2511401or$ ls -l *.cpp *.c *.h | wc -l 我们再来看一个统计各个connection状态的用法：（我们可以看到一些编程的影子了，大家都是程序员我就不解释了。注意其中的数组的用法） 1234567$ awk &#x27;NR!=1&#123;a[$6]++;&#125; END &#123;for (i in a) print i &quot;, &quot; a[i];&#125;&#x27; netstat.txtTIME_WAIT, 3FIN_WAIT1, 1ESTABLISHED, 6FIN_WAIT2, 3LAST_ACK, 1LISTEN, 4 再来看看统计每个用户的进程的占了多少内存（注：sum的RSS那一列） 123456$ ps aux | awk &#x27;NR!=1&#123;a[$1]+=$6;&#125; END &#123; for(i in a) print i &quot;, &quot; a[i]&quot;KB&quot;;&#125;&#x27;dbus, 540KBmysql, 99928KBwww, 3264924KBroot, 63644KBhchen, 6020KB awk脚本在上面我们可以看到一个END关键字。END的意思是“处理完所有的行的标识”，即然说到了END就有必要介绍一下BEGIN，这两个关键字意味着执行前和执行后的意思，语法如下： BEGIN{ 这里面放的是执行前的语句 } END {这里面放的是处理完所有的行后要执行的语句 } {这里面放的是处理每一行时要执行的语句} 为了说清楚这个事，我们来看看下面的示例：假设有这么一个文件（学生成绩表）： 123456$ cat score.txtMarry 2143 78 84 77Jack 2321 66 78 45Tom 2122 48 77 71Mike 2537 87 97 95Bob 2415 40 57 62 awk脚本如下（我没有写有命令行上是因为命令行上不易读，另外也在介绍另一种用法）： 123456789101112131415161718192021222324$ cat cal.awk#!/bin/awk -f#运行前BEGIN &#123; math = 0 english = 0 computer = 0 printf &quot;NAME NO. MATH ENGLISH COMPUTER TOTAL\\n&quot; printf &quot;---------------------------------------------\\n&quot;&#125;#运行中&#123; math+=$3 english+=$4 computer+=$5 printf &quot;%-6s %-6s %4d %8d %8d %8d\\n&quot;, $1, $2, $3,$4,$5, $3+$4+$5&#125;#运行后END &#123; printf &quot;---------------------------------------------\\n&quot; printf &quot; TOTAL:%10d %8d %8d \\n&quot;, math, english, computer printf &quot;AVERAGE:%10.2f %8.2f %8.2f\\n&quot;, math/NR, english/NR, computer/NR&#125; 我们来看一下执行结果：（也可以这样运行 ./cal.awk score.txt） 1234567891011$ awk -f cal.awk score.txtNAME NO. MATH ENGLISH COMPUTER TOTAL---------------------------------------------Marry 2143 78 84 77 239Jack 2321 66 78 45 189Tom 2122 48 77 71 196Mike 2537 87 97 95 279Bob 2415 40 57 62 159--------------------------------------------- TOTAL: 319 393 350AVERAGE: 63.80 78.60 70.00 环境变量即然说到了脚本，我们来看看怎么和环境变量交互：（使用-v参数和ENVIRON，使用ENVIRON的环境变量需要export） 1234567891011121314$ x=5$ y=10$ export y$ echo $x $y5 10$ awk -v val=$x &#x27;&#123;print $1, $2, $3, $4+val, $5+ENVIRON[&quot;y&quot;]&#125;&#x27; OFS=&quot;\\t&quot; score.txtMarry 2143 78 89 87Jack 2321 66 83 55Tom 2122 48 82 81Mike 2537 87 102 105Bob 2415 40 62 72 几个花活最后，我们再来看几个小例子： 12345678#从file文件中找出长度大于80的行awk &#x27;length&gt;80&#x27; file#按连接数查看客户端IPnetstat -ntu | awk &#x27;&#123;print $5&#125;&#x27; | cut -d: -f1 | sort | uniq -c | sort -nr#打印99乘法表seq 9 | sed &#x27;H;g&#x27; | awk -v RS=&#x27;&#x27; &#x27;&#123;for(i=1;i&lt;=NF;i++)printf(&quot;%dx%d=%d%s&quot;, i, NR, i*NR, i==NR?&quot;\\n&quot;:&quot;\\t&quot;)&#125;&#x27; 结语关于其中的一些知识点可以参看gawk的手册： 内建变量，参看：http://www.gnu.org/software/gawk/manual/gawk.html#Built_002din-Variables 流控方面，参看：http://www.gnu.org/software/gawk/manual/gawk.html#Statements 内建函数，参看：http://www.gnu.org/software/gawk/manual/gawk.html#Built_002din 正则表达式，参看：http://www.gnu.org/software/gawk/manual/gawk.html#Regexp","categories":[{"name":"Linux","slug":"Linux","permalink":"http://huangzhiyuan.github.io/categories/Linux/"}],"tags":[{"name":"AWK","slug":"AWK","permalink":"http://huangzhiyuan.github.io/tags/AWK/"}]},{"title":"CS 179 GPU programming","slug":"GPU-programming","date":"2020-02-27T13:06:21.000Z","updated":"2020-03-01T12:12:28.000Z","comments":true,"path":"2020/02/27/GPU-programming/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/27/GPU-programming/","excerpt":"The use of Graphics Processing Units for rendering is well known, but their power for general parallel computation has only recently been explored. Parallel algorithms running on GPUs can often achieve up to 100x speedup over similar CPU algorithms, with many existing applications for physics simulations, signal processing, financial modeling, neural networks, and countless other fields. This course will cover programming techniques for the GPU. The course will introduce NVIDIA’s parallel computing language, CUDA. Beyond covering the CUDA programming model and syntax, the course will also discuss GPU architecture, high performance computing on GPUs, parallel algorithms, CUDA libraries, and applications of GPU computing. Problem sets will cover performance optimization and specific GPU applications such as numerical mathematics, medical imaging, finance, and other fields. This quarter we will also cover uses of the GPU in Machine Learning. Labwork will require significant programming. A working knowledge of the C programming language will be necessary. Although CS 24 is not a prerequisite, it (or equivalent systems programming experience) is strongly recommended.","text":"The use of Graphics Processing Units for rendering is well known, but their power for general parallel computation has only recently been explored. Parallel algorithms running on GPUs can often achieve up to 100x speedup over similar CPU algorithms, with many existing applications for physics simulations, signal processing, financial modeling, neural networks, and countless other fields. This course will cover programming techniques for the GPU. The course will introduce NVIDIA’s parallel computing language, CUDA. Beyond covering the CUDA programming model and syntax, the course will also discuss GPU architecture, high performance computing on GPUs, parallel algorithms, CUDA libraries, and applications of GPU computing. Problem sets will cover performance optimization and specific GPU applications such as numerical mathematics, medical imaging, finance, and other fields. This quarter we will also cover uses of the GPU in Machine Learning. Labwork will require significant programming. A working knowledge of the C programming language will be necessary. Although CS 24 is not a prerequisite, it (or equivalent systems programming experience) is strongly recommended. Week 1 (Introduction)Lecture 1 (Mon. 04/01): PPT PDFLecture 2 (Wed. 04/03): PPT PDFLecture 3 (Fri. 04/05): PPT PDFWeek 2 (Shared Memory)Lecture 4 (Mon. 04/08): PPT PDFLecture 5 (Wed. 04/10): PPT PDFLecture 6 (Fri. 04/12): PPT PDFWeek 3 (Reductions, FFT)Lecture 7 (Mon. 04/15): PPT PDFLecture 8 (Wed. 04/17): PPT PDFLecture 9 (Fri. 04/19): PPT PDFWeek 4 (cuBLAS and Graphics)Lecture 10 (Mon. 04/22): PPT PDF [Google Doc][103]Lecture 11 (Wed. 04/24): cuBLAS exampleLecture 12 (Fri. 04/26): PPT PDFWeek 5 (Machine Learning and cuDNN I)Lecture 13 (Mon. 04/29): PPT PDFLecture 14 (Wed. 05/01): PPT PDFLecture 15 (Fri. 05/03): PPT PDFWeek 6 (Machine Learning and cuDNN II)Lecture 16 (Mon. 05/06): PPT PDFLecture 17 (Wed. 05/08): PPT PDF","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"GPU","slug":"GPU","permalink":"http://huangzhiyuan.github.io/tags/GPU/"}]},{"title":"compute bound or memory bound","slug":"compute-memory-bound","date":"2020-02-27T09:56:32.000Z","updated":"2020-03-01T12:40:14.000Z","comments":true,"path":"2020/02/27/compute-memory-bound/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/27/compute-memory-bound/","excerpt":"之前对这两个名词有误解，没有多么深刻的认知 Compute bound: computationally intensive, the time involved in the computation is much longer than I/O operation, e.g. linear and conv. Memory bound: the time involved in I/O operation is bigger.直到今天问了mingfei大神~~","text":"之前对这两个名词有误解，没有多么深刻的认知 Compute bound: computationally intensive, the time involved in the computation is much longer than I/O operation, e.g. linear and conv. Memory bound: the time involved in I/O operation is bigger.直到今天问了mingfei大神~~ 首先关于这个IO的概念是不对的， IO指的是从硬盘drive到内存memory，IO的带宽是有PCI-e决定的，PCIe 4.0的带宽是上行下行各8GB/sMemory BW是从内存到cache，这个就是你内存条带宽 * port数量，一般CPU是100+GB/s; GPU有HBM(high bandwidth memory) v100好像是一千多一般L1的带宽应该是两千多，多以利用好你的L1而不要让code等在Memory的BW上。。。 比如你的CPU 每个cycle可以进行1次FMA的运算，但是每个cycle只能load 1个float (4个Byte)那么。。。 1a = b + c 这个算式是被什么bound呢，这里只有一个add能不能在1个cycle内完成呢？答案是否定的，因为要读b和c(8 byte), 写一次a(写实际是一次读一次写，就是8byte)。。。 这样你就应该很容易理解 我们的CPU peak ALU吞吐量在9Tflops/sec左右，mem带算宽180GB/s，这个ratio就是： 1Ratio = 9 * 1024 Gflops / 180 Gbytes = 50 那么一个byte对应50次fma，高于这个ratio的是compute bound，低于这个ratio的是mem BW bound。这个叫static analysis实际情况要复杂很多。 做性能预测的时候大概就是这个算法，比如一个gemm，不一定是compute bound，MNK如果又一边很小，就是mem BW bound…一般硬件在设计的时候都会让o(n3)的运算compute bound。我们可以对一个kernel进行详细的分析： 多少次计算 多少load 有没有重用（cache影响，有的话是L1/L2/LLC那一个位置） 有了上述条件你就可以算出理论性能上限。。。 当然大多数情况下我们根本达不到理论上限， 为什么呢？？ 一般写的比较好的kernel在vtune里面查CPI（cycles per instruction）应该在0.5以下，如果出现十几这种情况一般是: 有频繁函数调用，访存不连续，还有就是慢速指令（比如整数的除法或者取模，非常慢） 再举个很简单的例子比如fuse 一个kernel： 12a = a * alphaa = a + b 假设a和b都是n长度的tensor. 那么地球人知道要fuse起来比较快，为什么呢？这里我们不考虑fma的情况，就假定这是两次运算 单独计算的时候要读a两次，读b一次，写a两次 Fuse起来a读写各一次，b读一次 如果有复用的情况还要考虑cache（这就是大师说的对于L1的优化，L1速度比reg慢一下但差不多）这时候就要做Blocking，如果做得好还可以规避掉leading dimension的问题。。。这个问题很常见，因为AI里面很多都是256, 512, 1024这种数，很可能下次循环就把上次循环cache的一条数据给flush了。。。 详情请关注大佬即将推出的LayerNorm和FusedLSTMKernel…","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"memory","slug":"memory","permalink":"http://huangzhiyuan.github.io/tags/memory/"}]},{"title":"如何优雅地使用Git","slug":"how-to-use-git-graceful","date":"2020-02-25T11:52:42.000Z","updated":"2020-03-01T12:12:56.000Z","comments":true,"path":"2020/02/25/how-to-use-git-graceful/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/25/how-to-use-git-graceful/","excerpt":"论如何优雅地使用Git。","text":"论如何优雅地使用Git。","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"git","slug":"git","permalink":"http://huangzhiyuan.github.io/tags/git/"}]},{"title":"VSCode插件开发指南","slug":"how-to-develop-a-vscode-plugin","date":"2020-02-24T13:29:42.000Z","updated":"2020-03-01T12:40:20.000Z","comments":true,"path":"2020/02/24/how-to-develop-a-vscode-plugin/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/24/how-to-develop-a-vscode-plugin/","excerpt":"只是一篇干货满满的介绍vscode插件开发的文章，先收藏再观看效果更佳。vscode 提供了哪些开放能力？从 vscode 的官网中我们可以看到，vscode 主要提供了六类开放能力：通用能力、主题、声明类语言特性（我把它称为基础支持）、程序类语言特性（高级支持）、工作区UI扩展、调试。我们开发 vscode 插件，实际上就是在通过这些能力对 vscode 进行扩展。这六类能力具体包括的功能和使用场景可以看下图。","text":"只是一篇干货满满的介绍vscode插件开发的文章，先收藏再观看效果更佳。vscode 提供了哪些开放能力？从 vscode 的官网中我们可以看到，vscode 主要提供了六类开放能力：通用能力、主题、声明类语言特性（我把它称为基础支持）、程序类语言特性（高级支持）、工作区UI扩展、调试。我们开发 vscode 插件，实际上就是在通过这些能力对 vscode 进行扩展。这六类能力具体包括的功能和使用场景可以看下图。 如何编写一个 vscode 插件呢？vscode 插件的形态和一个 npm 包非常相似，需要在项目的根目录添加 package.json，并且在其中增加一些 vscode 独家的设置。其中最主要的设置是 Activation Events(插件的激活时机) 和 contribution points (插件的能力)。接下来我们主要看看这两个配置具体是什么意思。 声明插件的激活时机 Activation Events我将 vscode 的生命周期简单描述为下图。下面会做进一步解释。 activate() 函数 &amp; deactivate() 函数可以看到生命周期中最终要的两个节点就是activate函数和deactivate函数。这两个函数需要在插件 npm 模块的入口文件 export 出去给 vscode 主动调用。其中，activate 会在 vscode 认为合适的时机调用，并且在插件的运行周期内只调用一次。因此在 activate 函数中开始启动插件的逻辑，是一个非常合适的时机。deactivate 函数会在插件卸载之前调用，如果你的卸载逻辑中存在异步操作，那么只需要在deactivate 函数中 retuen 一个 promise 对象，vscode 会在 promise resolve 时才正式将插件卸载掉。 onXxxx Activation Events可以看到在activate函数之前，还有onLanguage等事件的描述，实际上这些就是声明在插件 package.json 文件中的 Activation Events。声明这些 Activation Events 后，vscode 就会在适当的时机回调插件中的 activate函数。vscode之所以这么设计，是为了节省资源开销，只在必要的时候才激活你的插件。当然，如果你的插件非常重要，不希望在某个事件之后才被激活，你可以声明Activation Events为*这样 vscode 就会在启动的时候就开始回调 activate 函数。 插件的具体逻辑插件中的具体逻辑 vscode 没有做任何限制，你可以通过调用vscdoe提供的各种 api 对其进行扩充。不过需要注意的是，出于性能和移植性考虑，vscode不允许开发者直接操作dom。关于 vscode 的 api 可以参考 https://code.visualstudio.com/api/references/vscode-api 这是微软根据 vscode 的 d.ts 文件生成的文档。 举个例子接下来我们来看几个插件的 Activation Events 声明 超越鼓励师 申明了 onCommand:ycy.showReminderView 和 * ，其实我们都知道只声明后一个就足够了 vuter 申明了 onLanguage:vue 所以他会在用户打开 vue 语言文件时被激活 vscode-icons 是一个纯主题插件，声明的是 *GitLens 需要覆盖所有的文件，并且在vscode启动时就需要激活，他的声明是 * 关于 Activation Events 的说明可以参考官方文档 https://code.visualstudio.com/api/references/activation-events 声明插件的贡献点 contribution points需要在 package.json 中声明的另一个重要字段就是 contribution points。 contribution points描述了当前插件支持哪些能力，以及对应能力的配置。 由于 vscode 禁止直接操作dom，往 UI 中插入功能的正确方式是声明贡献点。下图列出了 vscode 支持的所有贡献点。 举个例子接下来我们来看几个插件的 contribution points 声明 超越鼓励师 支持通过 commands 触发杨超越的提醒，同时可以配置提醒出现的时机，因此包括 commands / configuration vuter 主要为 vue 文件提供语言支持，可以看到他提供的 contribution points 比较广，包括 commands / breakpoints / languages / grammars / configuration vscode-icons 已支持主题为主，他提供了 iconThemes / commands / configurationGitLens 是对vscode git 功能的增强，所以他的插入点集中在 UI 上的能力 configuration / commands / menus /resourceLabelFormatters / viewsContainers / views 关于 contribution points 的更多说明可以参考 https://code.visualstudio.com/api/references/contribution-points 编程语言支持那么，要怎么给 vscode 增加一门新的编程语言支持呢？就像之前说的，vscode 主要支持两类编程语言支持： 声明类语言特性主要描述了代码高亮、代码片段等轻量级需要实时给出响应的语言特性支持；而程序类语言特性只要提供更加高级的跳到定义、查找引用、hover提示等对实时性要求不高，而且需要大量计算的语言能力。因此前者更加适合在 IDE 的主线程进行处理，而后者可以考虑拆分到其他线程甚至服务中进行计算。 声明类语言特性（基本支持） 下面主要以语法高亮为例子介绍声明式语言支持。 在最初，微软的工程师们为web开发中常见的开发语言都手写了 paser。这类 paser 执行效率很高，但对开发者的能力要求也比较高，不太适合未来的插件扩展。从 vscode 1.8 版本开始，微软引入了 TextMate 的高亮语法，并逐步将原有的手写 paser 切换到这种语法上。 TextMate 本身是 mac 下的一个文本编辑器，vscode 借用了他对语言高亮文本的定义方式。TextMate语法的本质是用一个 json 文件来描述语言中的 token 和结构，当然为了方便，也可以改用 YAML 并编译成json。 顺便一提，而 TextMate 语法使用的是 oniguruma 库来解析正则表达式，oniguruma 中支持一些 js 引擎目前还不支持的正则特性，因此在 vscode 中使用了一个 oniguruma 的 c++ 模块来加速正则表达式解析速度。 另外，为了方便开发者编写语法高亮插件，vscode还提供了一个 yomen 模板用于生成插件基本目录结构，以及一个名为 inspectTMScopes 的调试器查看词法分析的结果。 除了语法高亮外，vscode还支持这些特性：注释切换、括号定义、自动闭合、Auto surrounding、代码折叠、word Pattern、缩进规则等，详见 https://code.visualstudio.com/api/language-extensions/language-configuration-guide 程序类语言特性（高级能力）对于高级的语言能力支持，vscode 提供了两种方式：方案一：注册 vscode 提供的回调钩子方案二：使用 language server 这两种方式提供的能力是完全相同的，而微软主推方案二，因此下面主要对方案二展开介绍。 language server protocol（LSP） 首先 language server 是一种跨编辑器的语言支持实现规范。它由微软提出，目前 vscode 、vim、atom 都已经支持了这个规范。 在过去，每个IDE遇到一门全新的语言，往往都需要重新实现一次基本功能，对于流行的语言来说还好，因为 IDE 厂商都有动力提供支持。然后对于一门全新的语言，往往需要语言的发明人自己实现各种 IDE 的语言支持。由于各个IDE的接口不同，需要将语言支持在各个IDE中重新移植一遍。 有了 LSP 规范后，语言支持插件开发者只需要编写一次，就可以很快地在 IDE 之间移植代码。 实现一个LSP，只需要在后台开启一个接受LSP请求的 server，并实现 LSP 规范中的接口（往往是通过 JSON RPC进行调用的）即可。关于 LSP 可以从这两篇文档中找到更加详细的介绍 LSP的官网：https://microsoft.github.io/language-server-protocol/ vscode中关于LSP插件的文档: https://code.visualstudio.com/api/language-extensions/language-server-extension-guide 至此，我们对 vscode 插件中的主要知识点进行了学习，相信看到这里的小伙伴已经收获满满了吧 还不过瘾？出门左转vscode插件开发官方文档吧~ https://code.visualstudio.com/api","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"vscode","slug":"vscode","permalink":"http://huangzhiyuan.github.io/tags/vscode/"}]},{"title":"剑指offer","slug":"sword-for-offer","date":"2020-02-22T03:10:23.000Z","updated":"2020-03-01T12:20:02.000Z","comments":true,"path":"2020/02/22/sword-for-offer/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/22/sword-for-offer/","excerpt":"这一章使用python3实现《剑指offer》中的题目。有些题目答案是原创，有些则搬运LeetCode中大神的优秀解法。&lt;未完待续。。。&gt;","text":"这一章使用python3实现《剑指offer》中的题目。有些题目答案是原创，有些则搬运LeetCode中大神的优秀解法。&lt;未完待续。。。&gt; 实现单例模式12345678910# 使用__new__控制实例创建过程class Singleton: _instance = None def __new__(cls, *args, **kw): if not cls._instance: cls._instance = super().__new__(cls) return cls._instanceclass MyClass(Singleton): pass 数组中重复的数字题目描述：在一个长度为n的数组里的所有数字都在0到n-1的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。 例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是第一个重复的数字2。思考题：如果只能使用 O(1) 的额外空间，该怎么做呢？AcWing传送门 Solution 123456789101112131415161718# -*- coding:utf-8 -*-class Solution: # 这里要特别注意~找到任意重复的一个值并赋值到duplication[0] # 函数返回True/False def duplicate(self, numbers, duplication): # write code here if not numbers: return False length = len(numbers) assist = [0] * length for i in numbers: if assist[numbers[i]] == 0: assist[numbers[i]] += 1 else: duplication[0] = numbers[i] return True return False 二维数组中的查找题目描述：编写一个高效的算法来搜索 m x n 矩阵 matrix 中的一个目标值 target。该矩阵具有以下特性： 每行的元素从左到右升序排列。每列的元素从上到下升序排列。示例: 1234567891011现有矩阵 matrix 如下：[ [1, 4, 7, 11, 15], [2, 5, 8, 12, 19], [3, 6, 9, 16, 22], [10, 13, 14, 17, 24], [18, 21, 23, 26, 30]]给定 target = 5，返回 true。给定 target = 20，返回 false。 Solution 123456789101112131415161718class Solution(object): def searchMatrix(self, matrix, target): column = len(matrix) if column == 0: return False row = len(matrix[0]) if row == 0: return False i = 0 j = row - 1 while ((i &lt;= column-1) and (j &gt;= 0)): if(matrix[i][j] &gt; target): j-=1 elif(matrix[i][j] &lt; target): i+=1 else: return True return False 替换空格题目描述请实现一个函数，将一个字符串中的每个空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。 Solution 123456# -*- coding:utf-8 -*-class Solution: # s 源字符串 def replaceSpace(self, s): # write code here return &#x27;&#x27;.join(c if c!=&#x27; &#x27; else &#x27;%20&#x27; for c in s) 从头到尾打印链表题目描述输入一个链表的头结点，按照 从尾到头 的顺序返回节点的值。返回的结果用数组存储。 123样例输入：[2, 3, 5]返回：[5, 3, 2] Solution 1234567891011121314151617# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def printListReversingly(self, head): &quot;&quot;&quot; :type head: ListNode :rtype: List[int] &quot;&quot;&quot; stack, h = [], head while h: stack.append(h.val) h = h.next return stack[::-1] 重建二叉树题目描述根据一棵树的前序遍历与中序遍历构造二叉树。 注意:你可以假设树中没有重复的元素。 例如，给出 123456789前序遍历 preorder = [3,9,20,15,7]中序遍历 inorder = [9,3,15,20,7]返回如下的二叉树： 3 / \\ 9 20 / \\ 15 7 Solution 12345678910111213141516171819202122# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): def buildTree(self, preorder, inorder): &quot;&quot;&quot; :type preorder: List[int] :type inorder: List[int] :rtype: TreeNode &quot;&quot;&quot; if preorder == []: return None root_val = preorder[0] root = TreeNode(root_val) cut = inorder.index(root_val) root.left = self.buildTree(preorder[1:cut+1], inorder[:cut]) root.right = self.buildTree(preorder[cut+1:], inorder[cut+1:]) return root 二叉树的下一个节点题目描述题目：给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。 分析（1） 若该节点存在右子树：则下一个节点为右子树最左子节点（2） 若该节点不存在右子树：这时分两种情况： 2.1 该节点为父节点的左子节点，则下一个节点为其父节点 2.2 该节点为父节点的右子节点，则沿着父节点向上遍历，直到找到一个节点，这个节点的left子树是给出节点的father节点。比如当前节点是D，则第一个满足的就是节点F，因为F节点的left节点是D节点的father节点。故D中序遍历的下一个节点就是F节点。中序遍历如下：A - C - B - D - F - H - E - M - G Solution 123456789101112131415161718192021222324# -*- coding:utf-8 -*-# class TreeLinkNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = None# self.next = Noneclass Solution: def GetNext(self, pNode): # write code here if not pNode: return None # 有右子树，右子树中最左节点 if pNode.right: pre = pNode.right while pre.left: pre = pre.left return pre while pNode.next: parent = pNode.next if parent.left == pNode: return parent pNode = parent return None 用两个栈实现队列使用栈实现队列的下列操作： push(x) – 将一个元素放入队列的尾部。pop() – 从队列首部移除元素。peek() – 返回队列首部的元素。empty() – 返回队列是否为空。示例: 1234567MyQueue queue = new MyQueue();queue.push(1);queue.push(2);queue.peek(); // 返回 1queue.pop(); // 返回 1queue.empty(); // 返回 false 说明: 你只能使用标准的栈操作 – 也就是只有 push to top, peek/pop from top, size, 和 is empty 操作是合法的。你所使用的语言也许不支持栈。你可以使用 list 或者 deque（双端队列）来模拟一个栈，只要是标准的栈操作即可。假设所有操作都是有效的 （例如，一个空的队列不会调用 pop 或者 peek 操作）。 Solution 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class MyQueue(object): def __init__(self): &quot;&quot;&quot; Initialize your data structure here. &quot;&quot;&quot; self.stackA = [] self.stackB = [] def push(self, x): &quot;&quot;&quot; Push element x to the back of queue. :type x: int :rtype: None &quot;&quot;&quot; while self.stackA: self.stackB.append(self.stackA.pop()) self.stackA.append(x) while self.stackB: self.stackA.append(self.stackB.pop()) def pop(self): &quot;&quot;&quot; Removes the element from in front of queue and returns that element. :rtype: int &quot;&quot;&quot; return self.stackA.pop() def peek(self): &quot;&quot;&quot; Get the front element. :rtype: int &quot;&quot;&quot; return self.stackA[-1] def empty(self): &quot;&quot;&quot; Returns whether the queue is empty. :rtype: bool &quot;&quot;&quot; return self.stackA == []# Your MyQueue object will be instantiated and called as such:# obj = MyQueue()# obj.push(x)# param_2 = obj.pop()# param_3 = obj.peek()# param_4 = obj.empty() 用两个队列实现栈题目描述使用队列实现栈的下列操作： push(x) – 元素 x 入栈 pop() – 移除栈顶元素 top() – 获取栈顶元素 empty() – 返回栈是否为空注意: 你只能使用队列的基本操作– 也就是 push to back, peek/pop from front, size, 和 is empty 这些操作是合法的。 你所使用的语言也许不支持队列。 你可以使用 list 或者 deque（双端队列）来模拟一个队列 , 只要是标准的队列操作即可。 你可以假设所有操作都是有效的（例如, 对一个空的栈不会调用 pop 或者 top 操作）。 Solution 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class MyStack(object): def __init__(self): &quot;&quot;&quot; Initialize your data structure here. &quot;&quot;&quot; from collections import deque self.q = deque() def push(self, x): &quot;&quot;&quot; Push element x onto stack. :type x: int :rtype: None &quot;&quot;&quot; self.q.append(x) for _ in range(len(self.q) - 1): self.q.append(self.q.popleft()) def pop(self): &quot;&quot;&quot; Removes the element on top of the stack and returns that element. :rtype: int &quot;&quot;&quot; return self.q.popleft() def top(self): &quot;&quot;&quot; Get the top element. :rtype: int &quot;&quot;&quot; return self.q[0] def empty(self): &quot;&quot;&quot; Returns whether the stack is empty. :rtype: bool &quot;&quot;&quot; return not len(self.q)# Your MyStack object will be instantiated and called as such:# obj = MyStack()# obj.push(x)# param_2 = obj.pop()# param_3 = obj.top()# param_4 = obj.empty() 斐波那契数列斐波那契数，通常用 F(n) 表示，形成的序列称为斐波那契数列。该数列由 0 和 1 开始，后面的每一项数字都是前面两项数字的和。也就是： 12345678910111213141516171819F(0) = 0, F(1) = 1F(N) = F(N - 1) + F(N - 2), 其中 N &gt; 1.给定 N，计算 F(N)。示例 1：输入：2输出：1解释：F(2) = F(1) + F(0) = 1 + 0 = 1.示例 2：输入：3输出：2解释：F(3) = F(2) + F(1) = 1 + 1 = 2.示例 3：输入：4输出：3解释：F(4) = F(3) + F(2) = 2 + 1 = 3. Solution 1 123456789101112class Solution(object): def fib(self, N): &quot;&quot;&quot; :type N: int :rtype: int &quot;&quot;&quot; if N == 0: return 0 elif N == 1: return 1 else: return self.fib(N - 1) + self.fib(N - 2) Solution 2 12345def fibonacci(n): a = b = 1 for _ in range(n-1): a, b = b, a+b return b 旋转数组中的最小数字假设按照升序排序的数组在预先未知的某个点上进行了旋转。 ( 例如，数组 [0,1,2,4,5,6,7] 可能变为 [4,5,6,7,0,1,2] )。 请找出其中最小的元素。 你可以假设数组中不存在重复元素。 12345678示例 1:输入: [3,4,5,1,2]输出: 1示例 2:输入: [4,5,6,7,0,1,2]输出: 0 Solution 1234567891011121314151617class Solution(object): def findMin(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; l, r = 0, len(nums) - 1 if nums[l] &lt; nums[r]: return nums[l] while l &lt;= r: mid = (l+r) // 2 if nums[mid] &gt; nums[l]: l = mid elif nums[mid] &lt; nums[r]: r = mid else: return nums[r] 矩阵中的路径机器人的运动范围剪绳子二进制中1的个数描述编写一个函数，输入是一个无符号整数，返回其二进制表达式中数字位数为 ‘1’ 的个数（也被称为汉明重量）。 123456789101112131415示例 1：输入：00000000000000000000000000001011输出：3解释：输入的二进制串 00000000000000000000000000001011 中，共有三位为 &#x27;1&#x27;。示例 2：输入：00000000000000000000000010000000输出：1解释：输入的二进制串 00000000000000000000000010000000 中，共有一位为 &#x27;1&#x27;。示例 3：输入：11111111111111111111111111111101输出：31解释：输入的二进制串 11111111111111111111111111111101 中，共有 31 位为 &#x27;1&#x27;。 Solution 1234567891011class Solution(object): def hammingWeight(self, n): &quot;&quot;&quot; :type n: int :rtype: int &quot;&quot;&quot; res = 0 while n: res += 1 n = n &amp; (n-1) return res 数值的整数次方描述实现 pow(x, n) ，即计算 x 的 n 次幂函数。 12345678910111213示例 1:输入: 2.00000, 10输出: 1024.00000示例 2:输入: 2.10000, 3输出: 9.26100示例 3:输入: 2.00000, -2输出: 0.25000解释: 2-2 = 1/22 = 1/4 = 0.25 说明: -100.0 &lt; x &lt; 100.0 n 是 32 位有符号整数，其数值范围是 [−231, 231 − 1] 。 Solution 123456789101112131415161718192021class Solution(object): def myPow(self, x, n): &quot;&quot;&quot; :type x: float :type n: int :rtype: float &quot;&quot;&quot; if n == -1: return 1.0/x elif n == 0: return 1 elif n == 1: return x else: res = x tmp = Solution() if n % 2 == 1: res = tmp.myPow(x, n//2) ** 2 * x else: res = tmp.myPow(x, n//2) ** 2 return res 打印从1到最大的n位数删除链表中的节点描述请编写一个函数，使其可以删除某个链表中给定的（非末尾）节点，你将只被给定要求被删除的节点。现有一个链表 – head = [4,5,1,9]，它可以表示为: 12345678910示例 1:输入: head = [4,5,1,9], node = 5输出: [4,1,9]解释: 给定你链表中值为 5 的第二个节点，那么在调用了你的函数之后，该链表应变为 4 -&gt; 1 -&gt; 9.示例 2:输入: head = [4,5,1,9], node = 1输出: [4,5,9]解释: 给定你链表中值为 1 的第三个节点，那么在调用了你的函数之后，该链表应变为 4 -&gt; 5 -&gt; 9. 说明： 链表至少包含两个节点。 链表中所有节点的值都是唯一的。 给定的节点为非末尾节点并且一定是链表中的一个有效节点。 不要从你的函数中返回任何结果。 Solution 1234567891011121314# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def deleteNode(self, node): &quot;&quot;&quot; :type node: ListNode :rtype: void Do not return anything, modify node in-place instead. &quot;&quot;&quot; node.val = node.next.val node.next = node.next.next 正则表达式表示数值的字符串调整数组顺序使奇数位于偶数前面链表中倒数第k个节点思路：两个指针，快指针先走k-1步，然后两个一起走，快指针走到尾节点时，慢指针在倒数第k个节点。需考虑k=0时和fast已经走到尾节点的情况。Solution 12345678910def FindKthToTail(self, head, k): fast = slow = head for _ in range(k): if fast: fast = fast.next else: return None while fast: slow, fast = slow.next, fast.next return slow 链表中环的入口节点反转链表合并两个有序链表树的子结构二叉树的镜像对称的二叉树顺时针打印矩阵包含min函数的栈栈的压入、弹出序列从上到下打印二叉树分层从上到下打印二叉树之字型打印二叉树是否是二叉树的后续遍历二叉树和为某一值的路径复杂链表的复制二叉搜索树与双向链表序列化二叉树字符串的排列数组中出现次数超过一半的数字最小的k个数数据流中的中位数连续子数组的最大和1~n整数中1出现的次数把数组排成最小的数字把数字翻译成字符串礼物的最大价值最长不包含舒服字符的子字符串丑数第一个只出现一次的字符数组中的逆序对两个链表的第一个公共节点在排序数组宏查找数字0~n-1中缺失的数字数组中数值和下标相等的元素二叉搜索树的第k大节点二叉树的深度平衡二叉树数组中只出现1次的两个数字和为s的数字翻转字符串滑动窗口的最大值n个骰子的点数扑克牌中的顺子圆圈中最后剩下的数字股票的最大利润求1+2+…+n不用加减乘除做加法构建乘积数组https://darktiantian.github.io/%E5%89%91%E6%8C%87Offer/https://github.com/darkTianTian/sword-for-offer/tree/masterhttps://github.com/leeguandong/Interview-code-practice-python/tree/master/%E5%89%91%E6%8C%87offer","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"剑指offer","slug":"剑指offer","permalink":"http://huangzhiyuan.github.io/tags/%E5%89%91%E6%8C%87offer/"}]},{"title":"面试-C++-02","slug":"mianshi-CplusCplus-02","date":"2020-02-21T13:03:18.000Z","updated":"2020-03-01T12:18:32.000Z","comments":true,"path":"2020/02/21/mianshi-CplusCplus-02/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/21/mianshi-CplusCplus-02/","excerpt":"这一系列文章会从网络各位同学的面试笔记和常考点进行汇总，主要是针对面试的C++后台开发岗位，涵盖了大部分C++相关的可能会被问道的技术点。笔记大都是比较基础的C++知识点的总结。","text":"这一系列文章会从网络各位同学的面试笔记和常考点进行汇总，主要是针对面试的C++后台开发岗位，涵盖了大部分C++相关的可能会被问道的技术点。笔记大都是比较基础的C++知识点的总结。 1.在C++ 程序中调用被C 编译器编译后的函数，为什么要加extern “C”？答：首先，extern是C/C++语言中表明函数和全局变量作用范围的关键字，该关键字告诉编译器，其声明的函数和变量可以在本模块或其它模块中使用。通常，在模块的头文件中对本模块提供给其它模块引用的函数和全局变量以关键字extern声明。extern “C”是连接申明(linkage declaration),被extern “C”修饰的变量和函数是按照C语言方式编译和连接的。作为一种面向对象的语言，C++支持函数重载，而过程式语言C则不支持。函数被C++编译后在符号库中的名字与C语言的不同。例如，假设某个函数的原型为：void foo( int x, int y );该函数被C编译器编译后在符号库中的名字为_foo，而C++编译器则会产生像_foo_int_int之类的名字。这样的名字包含了函数名、函数参数数量及类型信息，C++就是靠这种机制来实现函数重载的。所以，可以用一句话概括extern “C”这个声明的真实目的:解决名字匹配问题，实现C++与C的混合编程。 2.头文件中的ifndef/define/endif有什么作用？答：这是C++预编译头文件保护符，保证即使文件被多次包含，头文件也只定义一次。 3. ＃include&lt;file.h&gt; 与 ＃include “file.h”的区别？答：前者是从标准库路径寻找和引用file.h，而后者是从当前工作路径搜寻并引用file.h。 4.评价一下C/C++各自的特点答：C语言是一种结构化语言，面向过程，基于算法和数据结构，所考虑的是如何通过一个过程或者函数从输入得到输出；C++是面向对象，基于类、对象和继承，所考虑的是如何构造一个对象模型，让这个模型能够契合与之对应的问题，通过获取对象的状态信息得到输出或实现过程控制。 5．const 有什么用途？答：在C/C++中，（1）可以定义const常量，（2）修饰函数的返回值和形参；在C++中，还可以修饰函数的定义体，定义类的const成员函数。被const修饰的东西受到强制保护，可以预防意外的变动，提高了程序的健壮性。 6．const和#define有什么区别？（1）const和#define都可以定义常量，但是const用途更广。（2）const 常量有数据类型，而宏常量没有数据类型。编译器可以对前者进行类型安全检查。而对后者只进行字符替换，没有类型安全检查，并且在字符替换可能会产生意料不到的错误。（3）有些集成化的调试工具可以对const 常量进行调试，但是不能对宏常量进行调试。 7．关于sizeof小结的。sizeof计算的是在栈中分配的内存大小。（1） sizeof不计算static变量占得内存；（2） 32位系统的指针的大小是4个字节，64位系统的指针是8字节，而不用管指针类型；（3） char型占1个字节，int占4个字节，short int占2个字节long int占4个字节，float占4字节，double占8字节，string占4字节一个空类占1个字节，单一继承的空类占1个字节，虚继承涉及到虚指针所以占4个字节（4） 数组的长度：若指定了数组长度，则不看元素个数，总字节数=数组长度*sizeof（元素类型）若没有指定长度，则按实际元素个数类确定Ps：若是字符数组，则应考虑末尾的空字符。（5） 结构体对象的长度在默认情况下，为方便对结构体内元素的访问和管理，当结构体内元素长度小于处理器位数的时候，便以结构体内最长的数据元素的长度为对齐单位，即为其整数倍。若结构体内元素长度大于处理器位数则以处理器位数为单位对齐。（6） unsigned影响的只是最高位的意义，数据长度不会改变，所以sizeof（unsigned int）=4（7） 自定义类型的sizeof取值等于它的类型原型取sizeof（8） 对函数使用sizeof，在编译阶段会被函数的返回值的类型代替（9） sizeof后如果是类型名则必须加括号，如果是变量名可以不加括号，这是因为sizeof是运算符（10） 当使用结构类型或者变量时，sizeof返回实际的大小。当使用静态数组时返回数组的全部大小，sizeof不能返回动态数组或者外部数组的尺寸 8．sizeof与strlen的区别？（1）sizeof的返回值类型为size_t（unsigned int）；（2）sizeof是运算符，而strlen是函数；（3）sizeof可以用类型做参数，其参数可以是任意类型的或者是变量、函数，而strlen只能用char*做参数，且必须是以’\\0’结尾；（4）数组作sizeof的参数时不会退化为指针，而传递给strlen是就退化为指针；（5）sizeo是编译时的常量，而strlen要到运行时才会计算出来，且是字符串中字符的个数而不是内存大小； 9．指针和引用的区别？答：指针和引用都提供了间接操作对象的功能。（1） 指针定义时可以不初始化，而引用在定义时就要初始化，和一个对象绑定，而且一经绑定，只要引用存在，就会一直保持和该对象的绑定；（2） 赋值行为的差异：指针赋值是将指针重新指向另外一个对象，而引用赋值则是修改对象本身；（3） 指针之间存在类型转换，而引用分const引用和非const应用，非const引用只能和同类型的对象绑定，const引用可以绑定到不同但相关类型的对象或者右值 11.空指针和悬垂指针的区别？答：空指针是指被赋值为NULL的指针；delete指向动态分配对象的指针将会产生悬垂指针。（1） 空指针可以被多次delete，而悬垂指针再次删除时程序会变得非常不稳定；（2） 使用空指针和悬垂指针都是非法的，而且有可能造成程序崩溃，如果指针是空指针，尽管同样是崩溃，但和悬垂指针相比是一种可预料的崩溃。 12.C++中有malloc/free，为什么还有new/delete？答：malloc/free是C/C++标准库函数，new/delete是C++运算符。他们都可以用于动态申请和释放内存。对于内置类型数据而言，二者没有多大区别。malloc申请内存的时候要制定分配内存的字节数，而且不会做初始化；new申请的时候有默认的初始化，同时可以指定初始化；对于类类型的对象而言，用malloc/free无法满足要求的。对象在创建的时候要自动执行构造函数，消亡之前要调用析构函数。由于malloc/free是库函数而不是运算符，不在编译器控制之内，不能把执行构造函数和析构函数的任务强加给它，因此，C++还需要new/delete。 13.什么是智能指针？答：当类中有指针成员时，一般有两种方式来管理指针成员：一是采用值型的方式管理，每个类对象都保留一份指针指向的对象的拷贝；另一种更优雅的方式是使用智能指针，从而实现指针指向的对象的共享。智能指针的一种通用实现技术是使用引用计数。智能指针类将一个计数器与类指向的对象相关联，引用计数跟踪该类有多少个对象共享同一指针。 每次创建类的新对象时，初始化指针并将引用计数置为1；当对象作为另一对象的副本而创建时，拷贝构造函数拷贝指针并增加与之相应的引用计数；对一个对象进行赋值时，赋值操作符减少左操作数所指对象的引用计数（如果引用计数为减至0，则删除对象），并增加右操作数所指对象的引用计数；调用析构函数时，构造函数减少引用计数（如果引用计数减至0，则删除基础对象）。 14.面向对象技术的基本概念是什么，三个基本特征是什么？答：基本概念：类、对象、继承； 基本特征：封装、继承、多态。 封装：将低层次的元素组合起来形成新的、更高实体的技术； 继承：广义的继承有三种实现形式：实现继承、可视继承、接口继承。 多态：允许将子类类型的指针赋值给父类类型的指针 15.C++空类默认有哪些成员函数？答：默认构造函数、析构函数、复制构造函数、赋值函数 16.哪一种成员变量可以在一个类的实例之间共享？答：static静态成员变量 17.继承层次中，为什么基类析构函数是虚函数？答：编译器总是根据类型来调用类成员函数。但是一个派生类的指针可以安全地转化为一个基类的指针。这样删除一个基类的指针的时候，C++不管这个指针指向一个基类对象还是一个派生类的对象，调用的都是基类的析构函数而不是派生类的。如果你依赖于派生类的析构函数的代码来释放资源，而没有重载析构函数，那么会有资源泄漏。 18.为什么构造函数不能为虚函数？答：虚函数采用一种虚调用的方法。需调用是一种可以在只有部分信息的情况下工作的机制。如果创建一个对象，则需要知道对象的准确类型，因此构造函数不能为虚函数。 19.如果虚函数是有效的，那为什么不把所有函数设为虚函数？答：不行。首先，虚函数是有代价的，由于每个虚函数的对象都要维护一个虚函数表，因此在使用虚函数的时候都会产生一定的系统开销，这是没有必要的。 20.构造函数可以是内联函数 21.什么是多态？多态有什么作用？答：多态就是将基类类型的指针或者引用指向派生类型的对象。多态通过虚函数机制实现。多态的作用是接口重用。 22.重载和覆盖有什么区别？答：虚函数是基类希望派生类重新定义的函数，派生类重新定义基类虚函数的做法叫做覆盖；重载就在允许在相同作用域中存在多个同名的函数，这些函数的参数表不同。重载的概念不属于面向对象编程，编译器根据函数不同的形参表对同名函数的名称做修饰，然后这些同名函数就成了不同的函数。重载的确定是在编译时确定，是静态的；虚函数则是在运行时动态确定。 23.公有继承、受保护继承、私有继承（1）公有继承时，派生类对象可以访问基类中的公有成员，派生类的成员函数可以访问基类中的公有和受保护成员；（2）私有继承时，基类的成员只能被直接派生类的成员访问，无法再往下继承；（3）受保护继承时，基类的成员也只被直接派生类的成员访问，无法再往下继承。 24.公有继承时基类受保护的成员，可以通过派生类对象访问但不能修改。 25.有哪几种情况只能用构造函数初始化列表而不能用赋值初始化？答：const成员，引用成员 26.什么是虚指针？答：虚指针或虚函数指针是虚函数的实现细节。带有虚函数的每一个对象都有一个虚指针指向该类的虚函数表。 **27.C++如何阻止一个类被实例化？一般在什么时候将构造函数声明为private？（1）将类定义为抽象基类或者将构造函数声明为private；（2）不允许类外部创建类对象，只能在类内部创建对象 28.main函数执行之前会执行什么？执行之后还能执行代码吗？（1）全局对象的构造函数会在main函数之前执行；（2）可以，可以用_onexit 注册一个函数，它会在main 之后执行;如果你需要加入一段在main退出后执行的代码，可以使用atexit()函数，注册一个函数。语法： 12345678910111213#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;int atexit(void (*function&quot;)(void));void fn1( void ), fn2( void ), fn3( void );int main( void )&#123; atexit(fn1); atexit( fn2 ); printf( &quot;This is executed first.\\n&quot;); &#125;void fn1()&#123;printf( &quot; This is\\n&quot; );&#125;void fn2()&#123;printf( &quot; executed next.&quot; );&#125; 结果：This is executed first.This is executed next. 29.请描述进程和线程的区别？（1）进程是程序的一次执行，线程是进程中的执行单元；（2）进程间是独立的，这表现在内存空间、上下文环境上，线程运行在进程中；（3）一般来讲，进程无法突破进程边界存取其他进程内的存储空间；而同一进程所产生的线程共享内存空间；（4）同一进程中的两段代码不能同时执行，除非引入多线程。 30.进程间如何通信？答：信号、信号量、消息队列、共享内存 31.在网络编程中涉及并发服务器，使用多进程与多线程的区别？（1）线程执行开销小，但不利于资源管理和保护；进程则相反，进程可跨越机器迁移。（2）多进程时每个进程都有自己的内存空间，而多线程间共享内存空间；（3）线程产生的速度快，线程间通信快、切换快；（4）线程的资源利用率比较好；（5）线程使用公共变量或者资源时需要同步机制。 32.说一下TCP 3次握手、4次挥手的全过程。 33.TCP和UDP有什么区别。 TCP——传输控制协议,提供的是面向连接、可靠的字节流服务。当客户和服务器彼此交换数据前，必须先在双方之间建立一个TCP连接，之后才能传输数据。TCP提供超时重发，丢弃重复数据，检验数据，流量控制等功能，保证数据能从一端传到另一端。 UDP——用户数据报协议，是一个简单的面向数据报的传输层协议。UDP不提供可靠性，它只是把应用程序传给IP层的数据报发送出去，但是并不能保证它们能到达目的地。由于UDP在传输数据报前不用在客户和服务器之间建立一个连接，且没有超时重发等机制，故而传输速度很快. TCP协议和UDP协议的一些特性区别如下： TCP协议在传送数据段的时候要给段标号；UDP 协议不需要。 TCP协议可靠；UDP协议不可靠。 TCP协议是面向连接；UDP协议采用无连接。 TCP协议负载较高,采用虚电路；UDP协议低负载。 TCP协议的发送方要确认接受方是否收到数据段(3次握手协议)。 TCP协议采用窗口技术和流控制。 34.如何编写套接字？35.调用函数时要进行参数压栈，一般情况下顺序是从最右边参数往左压栈。 36.经常要操作的内存分为那几个类别？（1）栈区：由编译器自动分配和释放，存放函数的参数值、局部变量的值等；（2）堆：一般由程序员分配和释放，存放动态分配的变量；（3）全局区（静态区）：全局变量和静态变量存放在这一块，初始化的和未初始化的分开放；（4）文字常量区：常量字符串就放在这里，程序结束自动释放；（5）程序代码区：参访函数体的二进制代码。 37.请讲述堆和栈的区别。（1）申请方式不同。栈上有系统自动分配和释放；堆上有程序员自己申请并指明大小；（2）栈是向低地址扩展的数据结构，大小很有限；堆是向高地址扩展，是不连续的内存区域，空间相对大且灵活；（3）栈由系统分配和释放速度快；堆由程序员控制，一般较慢，且容易产生碎片； ==38.全局变量放在数据段，内部变量static int count；放在数据段，内部变量char *p=“AAA”，p的位置在堆栈上，指向的空间的位置数据段，内部变量char *p=new char；p的位置堆，指向的空间的位置数据段== 39.字符数组与字符串的比较：最明显的区别是字符串会在末尾自动添加空字符。 40.函数指针相关概念（C++学习笔记） 41.类使用static成员的优点，如何访问？答：优点：（1）static 成员的名字是在类的作用域中，因此可以避免与其他类的成员或全局对象名字冲突；（2）可以实施封装。static 成员可以是私有成员，而全局对象不可以；（3） static 成员是与特定类关联的，可清晰地显示程序员的意图。static 数据成员必须在类定义体的外部定义(正好一次)，static 关键字只能用于类定义体内部的声明中，定义不能标示为static. 不像普通数据成员，static成员不是通过类构造函数进行初始化，也不能在类的声明中初始化，而是应该在定义时进行初始化.保证对象正好定义一次的最好办法，就是将static 数据成员的定义放在包含类非内联成员函数定义的文件中。 静态数据成员初始化的格式为：＜数据类型＞＜类名＞::＜静态数据成员名＞=＜值＞类的静态数据成员有两种访问形式：＜类对象名＞.＜静态数据成员名＞ 或 ＜类类型名＞::＜静态数据成员名＞ 42. static数据成员和static成员函数（1）static数据成员：static数据成员独立于该类的任意对象而存在；每个static数据成员是与类关联的对象，并不与该类的对象相关联。Static数据成员（const static数据成员除外）必须在类定义体的外部定义。不像普通数据成员，static成员不是通过类的构造函数进行初始化，而是应该在定义时进行初始化。（2）static成员函数：Static成员函数没有this形参，它可以直接访问所属类的static成员，不能直接使用非static成员。因为static成员不是任何对象的组成部分，所以static成员不能被声明为const。同时，static成员函数也不能被声明为虚函数。 43.static成员变量定义放在cpp文件中，不能放在初始化列表中。Const static成员可就地初始化。 44.如何引用一个已经定义过的全局变量？答：可以用引用头文件的方式，也可以用extern关键字，如果用引用头文件方式来引用某个在头文件中声明的全局变量，假定你将那个变量写错了，那么在编译期间会报错，如果你用extern方式引用时，假定你犯了同样的错误，那么在编译期间不会报错，而在连接期间报错。 44.static关键字的作用。答：static总是使得变量或对象的存储形式变成静态存储，连接方式变成内部连接，对于局部变量（已经是内部连接了），它仅改变其存储方式；对于全局变量（已经是静态存储了），它仅改变其连接类型。 45.奈奎斯特定理 46.香农定理 47．多态类中的虚函数表是 Compile-Time，还是 Run-Time时建立的?答案：虚拟函数表是在编译期就建立了,各个虚拟函数这时被组织成了一个虚拟函数的入口地址的数组。而对象的隐藏成员–虚拟函数表指针是在运行期–也就是构造函数被调用时进行初始化的，这是实现多态的关键。 48. 一个父类写了一个 virtual 函数，如果子类覆盖它的函数不加 virtual ,也能实现多态?在子类的空间里，有没有父类的这个函数，或者父类的私有变量? (华为笔试题）答案：只要基类在定义成员函数时已经声明了 virtue关键字，在派生类实现的时候覆盖该函数时，virtue关键字可加可不加，不影响多态的实现。子类的空间里有父类的所有变量(static除外)。 49. 完成字符串拷贝可以使用 sprintf、strcpy 及 memcpy 函数，请问这些函数有什么区别，你喜欢使用哪个，为什么？答案：这些函数的区别在于 实现功能以及操作对象不同。（1）strcpy 函数操作的对象是字符串，完成从源字符串到目的字符串的拷贝功能。（2）sprintf 函数操作的对象不限于字符串：虽然目的对象是字符串，但是源对象可以是字符串、也可以是任意基本类型的数据。这个函数主要用来实现（字符串或基本数据类型）向字符串的转换功能。如果源对象是字符串，并且指定 %s 格式符，也可实现字符串拷贝功能。（3）memcpy 函数顾名思义就是内存拷贝，实现将一个内存块的内容复制到另一个内存块这一功能。内存块由其首地址以及长度确定。程序中出现的实体对象，不论是什么类型，其最终表现就是在内存中占据一席之地（一个内存区间或块）。因此，memcpy 的操作对象不局限于某一类数据类型，或者说可适用于任意数据类型，只要能给出对象的起始地址和内存长度信息、并且对象具有可操作性即可。鉴于memcpy 函数等长拷贝的特点以及数据类型代表的物理意义，memcpy 函数通常限于同种类型数据或对象之间的拷贝，其中当然也包括字符串拷贝以及基本数据类型的拷贝。 对于字符串拷贝来说，用上述三个函数都可以实现，但是其实现的效率和使用的方便程度不同：• strcpy 无疑是最合适的选择：效率高且调用方便。• sprintf 要额外指定格式符并且进行格式转化，麻烦且效率不高。• memcpy 虽然高效，但是需要额外提供拷贝的内存长度这一参数，易错且使用不便；并且如果长度指定过大的话（最优长度是源字符串长度 + 1），还会带来性能的下降。其实 strcpy 函数一般是在内部调用 memcpy 函数或者用汇编直接实现的，以达到高效的目的。因此，使用 memcpy 和 strcpy 拷贝字符串在性能上应该没有什么大的差别。对于非字符串类型的数据的复制来说，strcpy 和 snprintf 一般就无能为力了，可是对 memcpy 却没有什么影响。但是，对于基本数据类型来说，尽管可以用 memcpy 进行拷贝，由于有赋值运算符可以方便且高效地进行同种或兼容类型的数据之间的拷贝，所以这种情况下 memcpy 几乎不被使用 。memcpy 的长处是用来实现（通常是内部实现居多）对结构或者数组的拷贝，其目的是或者高效，或者使用方便，甚或两者兼有。 50. 应用程序在运行时的内存包括代码区和数据区，其中数据区又包括哪些部分？答：对于一个进程的内存空间而言，可以在逻辑上分成 3个部份：代码区，静态数据区和动态数据区。动态数据区一般就是“堆栈”。 栈是一种线性结构，堆是一种链式结构。进程的每个线程都有私有的“栈”。全局变量和静态变量分配在静态数据区，本地变量分配在动态数据区，即堆栈中。程序通过堆栈的基地址和偏移量来访问本地变量。 51. C++函数中值的传递方式有哪几种?答：三种传递方式为：值传递、指针传递和引用传递。 52. C++里面是不是所有的动作都是main()引起的？如果不是，请举例.比如全局变量的初始化，就不是由main函数引起的举例： class A{};A a; //a的构造函数限执行int main() {} 53. 下列哪两个是等同的 12345int b;A const int* a = &amp;b;B const* int a = &amp;b;C const int* const a = &amp;b;D int const* const a = &amp;b; 54. 内联函数在编译时是否做参数类型检查？答：内联函数要做参数类型检查, 这是内联函数跟宏相比的优势。 55. 全局变量和局部变量有什么区别？实怎么实现的？操作系统和编译器是怎么知道的？（1）生命周期不同：全局变量随主程序创建和创建，随主程序销毁而销毁局部变量在局部函数内部，甚至局部循环体等内部存在，退出就不存在； 内存中分配在全局数据区（2）使用方式不同：通过声明后全局变量程序的各个部分都可以用到；局部变量只能在局部使用，分配在栈区操作系统和编译器通过内存分配的位置来知道的，全局变量分配在全局数据段并且在程序开始运行的时候被加载。局部变量则分配在堆栈里面 。 56. 有 A 、 B 、 C 、 D 四个人，要在夜里过一座桥。他们通过这座桥分别需要耗时 1 、 2 、 5 、 10 分钟，只有一支手电，并且同时最多只能两个人一起过桥。请问，如何安排，能够在 17 分钟内这四个人都过桥？Solution:关键是时间最长的两个人必须同时过桥 The First Time ： A(1) 和 B(2) 过桥， A(1) 返回 Cost ： 1+2 The Second Time ： C(5) 和 D(10) 过桥， B(2) 返回 Cost ： 10+2 The Third Time A(1) 和 B(2) 过桥 Cost ： 2 Total Time Cost ： (1+2)+(10+2)+2=17 minutes 57. static全局变量与普通的全局变量有什么区别？static局部变量和普通局部变量有什么区别？static函数与普通函数有什么区别？答：static全局变量与普通全局变量区别：static全局变量只初使化一次，防止在其他文件单元中被引用;static局部变量和普通局部变量区别：static局部变量只被初始化一次，下一次依据上一次结果值；static函数与普通函数区别：static函数在内存中只有一份，普通函数在每个被调用中维持一份拷贝。 58. 程序的局部变量存在于（堆栈）中，全局变量存在于（静态区 ）中，动态申请数据存在于（ 堆）中。 59． 对于一个频繁使用的短小函数,在C语言中应用什么实现,在C++中应用什么实现?c用宏定义，c++用inline 60. 有1,2,….一直到n的无序数组,求排序算法,并且要求时间复杂度为O(n),空间复杂度O(1),使用交换,而且一次只能交换两个数。 1234567891011121314151617#include&lt;iostream.h&gt;Using namespace std；int main() &#123; int a[] = &#123;10,6,9,5,2,8,4,7,1,3&#125;; int len = sizeof(a) / sizeof(int); int temp; for(int i = 0; i &lt; len; ) &#123; temp = a[a[i] - 1]; a[a[i] - 1] = a[i]; a[i] = temp; if ( a[i] == i + 1) i++; &#125; for (int j = 0; j &lt; len; j++) cout &lt;&lt; a[j] &lt;&lt; &quot;,&quot;; return 0;&#125;","categories":[{"name":"C++","slug":"C","permalink":"http://huangzhiyuan.github.io/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://huangzhiyuan.github.io/tags/C/"}]},{"title":"pytorch入门","slug":"pytorch-guideline","date":"2020-02-20T13:32:05.000Z","updated":"2020-04-10T01:42:58.000Z","comments":true,"path":"2020/02/20/pytorch-guideline/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/20/pytorch-guideline/","excerpt":"PyTorch是一个基于Python的库，提供了一个具有灵活易用的深度学习框架，是近年来最受欢迎的深度学习框架之一。","text":"PyTorch是一个基于Python的库，提供了一个具有灵活易用的深度学习框架，是近年来最受欢迎的深度学习框架之一。 初识PyTorch张量1.导入pytorch包 1import torch 2.创建一个空的5x3张量 12x = torch.empty(5, 3)print(x) 3.创建一个随机初始化的5x3张量 12x = torch.rand(5, 3)print(x) 4.创建一个5x3的0张量，类型为long 12x = torch.zeros(5, 3, dtype=torch.long)print(x) 5.直接从数组创建张量 12x = torch.tensor([5.5, 3])print(x) 6.创建一个5x3的单位张量，类型为double 12x = torch.ones(5, 3, dtype=torch.double)print(x) 7.从已有的张量创建相同维度的新张量，并且重新定义类型为float 12x = torch.randn_like(x, dtype=torch.float)print(x) 8.打印一个张量的维度 1print(x.size()) 9.将两个张量相加 1234567891011121314y = torch.rand(5, 3)print(x + y)# 方法二# print(torch.add(x, y))# 方法三# result = torch.empty(5, 3)# torch.add(x, y, out=result)# print(result)# 方法四# y.add_(x)# print(y) 10.取张量的第一列 1print(x[:, 1]) 11.将一个4x4的张量resize成一个一维张量 123x = torch.randn(4, 4)y = x.view(16)print(x.size(),y.size()) 12.将一个4x4的张量，resize成一个2x8的张量 123456y = x.view(2, 8)print(x.size(),y.size())# 方法二z = x.view(-1, 8) # 确定一个维度，-1的维度会被自动计算print(x.size(),z.size()) 13.从张量中取出数字x = torch.randn(1) 12print(x)print(x.item()) Numpy的操作14.将张量装换成numpy数组 12345a = torch.ones(5)print(a)b = a.numpy()print(b) 15.将张量+1，并观察上题中numpy数组的变化 123a.add_(1)print(a)print(b) 16.从numpy数组创建张量 12345import numpy as npa = np.ones(5)b = torch.from_numpy(a)print(a)print(b) 17.将numpy数组+1并观察上题中张量的变化 123np.add(a, 1, out=a)print(a)print(b) 自动微分张量的自动微分18.新建一个张量，并设置requires_grad=True 12x = torch.ones(2, 2, requires_grad=True)print(x) 19.对张量进行任意操作（y = x + 2） 123y = x + 2print(y)print(y.grad_fn) # y就多了一个AddBackward 20.再对y进行任意操作 12345z = y * y * 3out = z.mean()print(z) # z多了MulBackwardprint(out) # out多了MeanBackward 梯度21.对out进行反向传播 1out.backward() 22.打印梯度d(out)/dx 1print(x.grad) #out=0.25*Σ3(x+2)^2 23.创建一个结果为矢量的计算过程（y=x*2^n） 1234567x = torch.randn(3, requires_grad=True)y = x * 2while y.data.norm() &lt; 1000: y = y * 2print(y) 24.计算v = [0.1, 1.0, 0.0001]处的梯度 1234v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)y.backward(v)print(x.grad) 25.关闭梯度的功能 1234567891011print(x.requires_grad)print((x ** 2).requires_grad)with torch.no_grad(): print((x ** 2).requires_grad)# 方法二# print(x.requires_grad)# y = x.detach()# print(y.requires_grad)# print(x.eq(y).all()) 神经网络这部分以LeNet5为例， 定义网络12345678910111213141516171819202122232425262728293031323334353637import torchimport torch.nn as nnimport torch.nn.functional as Fclass Net(nn.Module): def __init__(self): super(Net, self).__init__() # 26.定义①的卷积层，输入为32x32的图像，卷积核大小5x5卷积核种类6 self.conv1 = nn.Conv2d(3, 6, 5) # 27.定义③的卷积层，输入为前一层6个特征，卷积核大小5x5，卷积核种类16 self.conv2 = nn.Conv2d(6, 16, 5) # 28.定义⑤的全链接层，输入为16*5*5，输出为120 self.fc1 = nn.Linear(16 * 5 * 5, 120) # 6*6 from image dimension # 29.定义⑥的全连接层，输入为120，输出为84 self.fc2 = nn.Linear(120, 84) # 30.定义⑥的全连接层，输入为84，输出为10 self.fc3 = nn.Linear(84, 10) def forward(self, x): # 31.完成input-S2，先卷积+relu，再2x2下采样 x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # 32.完成S2-S4，先卷积+relu，再2x2下采样 x = F.max_pool2d(F.relu(self.conv2(x)), 2) #卷积核方形时，可以只写一个维度 # 33.将特征向量扁平成行向量 x = x.view(-1, 16 * 5 * 5) # 34.使用fc1+relu x = F.relu(self.fc1(x)) # 35.使用fc2+relu x = F.relu(self.fc2(x)) # 36.使用fc3 x = self.fc3(x) return xnet = Net()print(net) 37.打印网络的参数 123params = list(net.parameters())# print(params)print(len(params)) 38.打印某一层参数的形状 1print(params[0].size()) 39.随机输入一个向量，查看前向传播输出 123input = torch.randn(1, 1, 32, 32)out = net(input)print(out) 40.将梯度初始化net.zero_grad()41.随机一个梯度进行反向传播 1out.backward(torch.randn(1, 10)) 损失函数42.用自带的MSELoss()定义损失函数 1criterion = nn.MSELoss() 43.随机一个真值，并用随机的输入计算损失 1234567target = torch.randn(10) # 随机真值target = target.view(1, -1) # 变成行向量output = net(input) # 用随机输入计算输出loss = criterion(output, target) # 计算损失print(loss) 44.将梯度初始化，计算上一步中loss的反向传播 1234net.zero_grad()print(&#x27;conv1.bias.grad before backward&#x27;)print(net.conv1.bias.grad) 45.计算43中loss的反向传播 1234loss.backward()print(&#x27;conv1.bias.grad after backward&#x27;)print(net.conv1.bias.grad) 更新权重46.定义SGD优化器算法，学习率设置为0.01 12import torch.optim as optimoptimizer = optim.SGD(net.parameters(), lr=0.01) 47.使用优化器更新权重 1234567optimizer.zero_grad()output = net(input)loss = criterion(output, target)loss.backward()# 更新权重optimizer.step() 训练一个分类器读取CIFAR10数据，做标准化48.构造一个transform，将三通道(0,1)区间的数据转换成(-1,1)的数据 123456import torchvisionimport torchvision.transforms as transformstransform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) 读取数据集： 1234567trainset = cifar(root = &#x27;./input/cifar10&#x27;, segmentation=&#x27;train&#x27;, transforms=transform)testset = cifar(root = &#x27;./input/cifar10&#x27;, segmentation=&#x27;test&#x27;, transforms=transform)trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,shuffle=True, num_workers=2)testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,shuffle=False, num_workers=2)classes = (&#x27;plane&#x27;, &#x27;car&#x27;, &#x27;bird&#x27;, &#x27;cat&#x27;, &#x27;deer&#x27;, &#x27;dog&#x27;, &#x27;frog&#x27;, &#x27;horse&#x27;, &#x27;ship&#x27;, &#x27;truck&#x27;) 建立网络使用前面的网络： 1net2 = Net() 定义损失函数和优化器49.定义交叉熵损失函数 1criterion2 = nn.CrossEntropyLoss() 50.定义SGD优化器算法，学习率设置为0.001，momentum=0.9 1optimizer2 = optim.SGD(net2.parameters(), lr=0.001, momentum=0.9) 训练网络1234567891011121314151617181920212223242526272829303132作者：一两赘肉无链接：https://zhuanlan.zhihu.com/p/99318332来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。for epoch in range(2): running_loss = 0.0 for i, data in enumerate(trainloader, 0): # 获取X,y对 inputs, labels = data # 51.初始化梯度 optimizer2.zero_grad() # 52.前馈 outputs = net2(inputs) # 53.计算损失 loss = criterion2(outputs, labels) # 54.计算梯度 loss.backward() # 55.更新权值 optimizer2.step() # 每2000个数据打印平均代价函数值 running_loss += loss.item() if i % 2000 == 1999: # print every 2000 mini-batches print(&#x27;[%d, %5d] loss: %.3f&#x27; % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0print(&#x27;Finished Training&#x27;) 使用模型预测读一些数据 123456dataiter = iter(testloader)images, labels = dataiter.next()# print imagesimshow(torchvision.utils.make_grid(images))print(&#x27;GroundTruth: &#x27;, &#x27; &#x27;.join(&#x27;%5s&#x27; % classes[labels[j]] for j in range(4))) 56.使用模型预测 123456outputs = net2(images)_, predicted = torch.max(outputs, 1)print(&#x27;Predicted: &#x27;, &#x27; &#x27;.join(&#x27;%5s&#x27; % classes[predicted[j]] for j in range(4))) 57.在测试集上进行打分 123456789101112correct = 0total = 0with torch.no_grad(): for data in testloader: images, labels = data outputs = net2(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item()print(&#x27;Accuracy of the network on the 10000 test images: %d %%&#x27; % ( 100 * correct / total)) 存取模型58.保存训练好的模型 12PATH = &#x27;./cifar_net.pth&#x27;torch.save(net.state_dict(), PATH) 59.读取保存的模型 1pretrained_net = torch.load(PATH) 60.加载模型 123net3 = Net()net3.load_state_dict(pretrained_net)","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://huangzhiyuan.github.io/tags/pytorch/"}]},{"title":"健身之瘦腹系列","slug":"keepfit-thin-belly","date":"2020-02-18T03:13:11.000Z","updated":"2020-03-01T12:13:58.000Z","comments":true,"path":"2020/02/18/keepfit-thin-belly/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/18/keepfit-thin-belly/","excerpt":"该方法源自知乎网友，方法简称“328计划”：每天练2组，每组8个动作，每周至少锻炼3次，坚持2周以上，就可以看到效果啦。首先要知道的是没有局部瘦的概念，瘦指的就是瘦全身，当体脂率降到一定程度，腰自然也就瘦了。简单来说就是通过调整饮食，打开热量缺口，先瘦下来，然后辅助一点腹部肌肉训练增加肌肉塑型。减肥的原理就是摄入热量 &lt; 消耗热量。而基础消耗就占了人体的70%，从饮食开始调整，运动会事半功倍。","text":"该方法源自知乎网友，方法简称“328计划”：每天练2组，每组8个动作，每周至少锻炼3次，坚持2周以上，就可以看到效果啦。首先要知道的是没有局部瘦的概念，瘦指的就是瘦全身，当体脂率降到一定程度，腰自然也就瘦了。简单来说就是通过调整饮食，打开热量缺口，先瘦下来，然后辅助一点腹部肌肉训练增加肌肉塑型。减肥的原理就是摄入热量 &lt; 消耗热量。而基础消耗就占了人体的70%，从饮食开始调整，运动会事半功倍。 一般女性体脂率在14%左右，男性体脂率在12%左右，才能看到腹肌 关于饮食==减肥不能吃什么？==经常性的不吃早餐，中午随便吃，晚上回家暴饮暴食，外加夜宵。吃了夜宵就会导致熬夜，因为撑的睡不着，第二天起不来，没有胃口吃早餐，就会陷入一个恶性循环。减肥期一定要拒绝这些： 零食零食零食！ 少油少盐：高热量的菜火锅、油炸、肥肉什么的再见吧。 所有糖：没错，是所有，不是不吃，是少吃，越少越好，保证明天摄入标准就够了。 主食：全麦谷物＜粗粮＜面条＜米饭＜馒头（按这个顺序馒头最不该吃） 少食多餐：早餐吃好、中午吃饱、晚上吃少、上午下午可以加餐 周期性饮食，以7天，14,21天为周期都可以，一段时间的坚持，可以允许自己吃一回好的，奖励自己更好的减肥。 身体到正常范围后，可以恢复饮食，不必过于控制，但是少油少盐少糖是一直适用的。 ==减肥应该吃什么？== 主食：全谷物粗粮：糙米、薏米、玉米、豆类、燕麦等等 肉类：鸡胸肉、瘦牛肉、鱼肉、兔肉等 蔬菜：白菜、木耳、豆芽、西红柿、芹菜等等 水果：香蕉、苹果、葡萄、草莓、山楂、火龙果、菠萝等 饮料：牛奶、豆浆、水果汁、代糖饮料（比如无糖可乐）、蛋白粉冲剂 外餐：选择荤素搭配型；尽量选择多蔬菜的，是蔬菜，不是青椒大葱这些。米饭选择其他的替代，比如白米粥、燕麦粥、面条等等。 塑形训练计划热身：慢跑or跳绳先做15分钟左右的有氧运动，跑步或者跳神都可以。 侧身+转体卷腹侧身和转体交替进行，左右各做10-15个，可以同时锻炼侧边和中间的腹肌。 半身-全身卷体先做一个半身卷体，然后再做一个全身卷体，依次交替进行，做10个。 小提臀+抬腿双腿绷直向上举，利用腰腹力量提起臀部，做10个。 板撑+侧边撑先做平板支撑，接着做侧边支撑，左右都要做，每一种支撑运动各做30-60秒，可以的话，多做几组。 抬臀+卷腹可借助凳子、人、沙发、床、门框等来完成，注意保持臀部始终离开地面，做5-10个。 抬臀接下来四肢着地，做抬臀运动，左右各10-20个。 侧边抬腿这个动作有点像小狗撒尿，左右各做10-20个。 单腿换边这一动作能强化腿部支撑，帮助燃烧臀部和侧腰脂肪，注意腿部交叉时，要尽可能远地伸腿，左右各做10-15个。 提臀 单腿提臀这个动作因为难度系数比较高，所以做5-10个就可以了。 侧身后抬腿这个动作对于大腿后侧，臀部以及后腰等部位都有所锻炼，做10-20个。 文中图片，速度很快，不用刻意追求速度，根据实际情况来，认真去感受你的肌肉发力。 以上计划中都是一些简单实用的练习动作，无需更多的健身设备。 不过在做以上训练时，需要注意以下几点：无氧只是辅助，要想瘦，前提还是需要大量的有氧才能减脂。1、有氧运动不仅可以跳绳、跑步，还可以选择原地跑步等。跑步速度男生控制在7-9千米/小时，女生速度控制在6-8千米/小时。2、练习者可以根据自身情况来选择全套动作训练，或者选择其中8个动作。当然，做得越多，减肚子的效果也会越好。3、在做各个练习动作时，建议尽可能地多做，能做50次，绝不偷懒做30次。做动作过程中，也可以适当休息3-5秒，以保证能够坚持下来。4、运用这套计划每周至少锻炼3次，每天练2次，每次8个动作，坚持“328运动模式”2个星期，练8次你就可以看到效果。","categories":[{"name":"其他","slug":"其他","permalink":"http://huangzhiyuan.github.io/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"健身","slug":"健身","permalink":"http://huangzhiyuan.github.io/tags/%E5%81%A5%E8%BA%AB/"}]},{"title":"副业的故事","slug":"fuye-story","date":"2020-02-17T13:02:19.000Z","updated":"2020-03-01T12:11:34.000Z","comments":true,"path":"2020/02/17/fuye-story/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/17/fuye-story/","excerpt":"研究过上千个副业，基本上可以总结为8个模式。只要思维和和执行力跟得上，任何一个都可以改善生活方式。8个赚钱的模式： 搬砖赚钱 技能赚钱 人脉模式 资源优势 聚焦赚钱 信息赚钱 圈子赚钱 投资赚钱","text":"研究过上千个副业，基本上可以总结为8个模式。只要思维和和执行力跟得上，任何一个都可以改善生活方式。8个赚钱的模式： 搬砖赚钱 技能赚钱 人脉模式 资源优势 聚焦赚钱 信息赚钱 圈子赚钱 投资赚钱 8种赚钱模式的特点：这8种赚钱法都是什么意思呢？ 搬砖赚钱法最原始的赚钱模式，单纯的靠出体力去赚钱。比如去工地打工，去餐厅当服务员、去街头发传单。一天给个100块钱，做了5天，赚到了500块钱。单纯的靠付出劳动和时间精力的赚钱方法，就叫搬砖赚钱。 技能赚钱法这个呢，也比较好理解。比如我现在是一名设计专业的大学生，通过大学所学的知识和技能，找到一份设计方面的工作，通过PS这项技能，持续获得收入 人脉赚钱法比如一家培训机构的市场销售员，为了销售公司的课程，会加入很多行业圈子，认识很多人。有一天和客户聊天，了解到客户需要找一个负责平台代运营方面的团队，希望我能推荐一个。而我正好知道几个靠谱的公司，推荐过去，最后他们达成合作，效果非常好，客户很满意，给我发了1万块钱红包，这个就是靠人脉赚钱的方法。 资源优势什么叫资源优势呢？简单的讲，就是人无我有，人有我优，人优我廉。要么是这个资源只有我有，或者是我比别人的好，比别人的便宜，这些呢都是叫优势。打个比方，一个朋友家是东北的，他们那边能搞到山里的人参这类名贵的药材。这就算是一种地域的优势资源，他在当地认识很多搞这类项目的人，可以以最优惠的价格拿到好货，然后通过各种渠道去加价销售，从而赚到钱，这个就叫优势赚钱法。 聚焦赚钱法这个很好理解，就相当于给自己打造一个专业领域方面的个人品牌。比如开一个公众号，坚持去写关于情感方面的文章，然后时间久了，写的东西只要不差，慢慢关注的人就会越来越多，假如有个10万的粉丝关注了你的公众号。那也就是你吸引到了这10万的一部分注意力，这个东西就可以拿去换钱，比如接一些情感方面产品的广告，或者是通过一些平台本身的流量奖励获得收益。这些都叫聚焦模式赚钱法。聚焦一个领域，专注深耕。当然，这个东西不一定是局限于公众号，现在网上可以选择的模式有很多，比如直播、短视频、音频、图片等等。这个我以后会再详细的来讲。 信息赚钱这个方面是指，你能通过一些渠道，提前获得关于赚钱方面的消息。比如我们常说的，有个亲戚在政府上班，知道最近哪一片区域的政策变化，然后提前去这些地方做一些布局，比如知道要拆迁、要建一个新的商业区、要修地铁等等消息，就提前去买房、买商铺。这样在政策实施以后，周边的房子商铺的价格猛涨，他就赚到钱了。这个呢，只是一个比方，事实上有没有人这么做过，我不知道，有些人肯定知道。这个就是利用一个信息不对称的方式来赚钱，只要你能够比别人提前知道一些东西，或者是你知道一些别人不知道的东西，你提前做好市场准备，那就是你的优势。 圈子赚钱法物以类聚，人与群分。只要你能把同一需求的人聚集到一起，建立一个圈子，那么这个圈子就值钱。比如我喜欢读书，通过在网上发帖，召集到一波喜欢读书的人，一起交流读书心得，监督彼此打卡，在读书的这个过程中呢，为了提高社群的人员水平，所以这个群是付费的。当群人数到达一定规模以后，可以跟一些商家进行合作赞助，在群内定期也会组织一些线下的聚会，分享一些跟书有关的课程、书籍、电子产品的东西，以此来获得收益。 投资赚钱法说到投资赚钱，大多数第一反应那肯定是炒股、基金、期货。这是一个很复杂且难度比较高的事情，要时刻关注新闻、政策、企业运转情况、最近趋势波动，从而得出一个概率性的判断，判断准，你能赚很多钱，判断失误，你也会赔钱。能玩好的人少之又少，对绝大多数人来说，都可能是那绿油油的韭菜。所以，我也不建议大家把大量的时间精力放在这一块，选择一个最稳妥且持续的赚钱模式，完成自己从0-1的积累，才是适合普通人的路。 如何选择赚钱的模式了解到赚钱模式的几个方向之后，新的问题又产生了。这么多方向，我们到底该怎么去做选择呢？因为每一个模式有好处，也有坏处，我们选择哪一种的前提是要了解自己。自己当前是个什么样的情况，经济状况如何，时间是否充裕，是否有人脉和专业技能等等，知道自己有什么和没有什么，再去找到最适合自己的模式。那么我们又该如何深入的了解自己呢？ 举一个例子：技能模式中，我们讲到，小a，通过自己大学所学的知识技能，进入到一家互联网公司，平时负责设计各种海报、页面等等工作。在度过新手期之后，能熟练的应付工作了，原来1天的工作量，现在只需要半天就能完成，那剩下的时间就没啥事情干。他又是一个比较上进的青年，觉得这样下去浪费时间，于是加入很多的同行交流群，看看大家都在聊什么，想跟别人在学习一下。某天，他看到一个群成员说：“我手里有个设计的活，需要外包，请问谁有空可以接的，内容不难，就是比较急，价格好说，感兴趣的可以找我。”小a发现了这样一个赚钱的方式，就主动联系到了对方，提交了自己的工作中的一些作品，争取到了这么一个机会。他的想法呢，也很简单，本来自己现在空余时间还比较多，闲着也是闲着，这个项目一是锻炼一下自己，二是赚点外快也不错。后来没花多少工夫，他就把这个项目搞定了，也赚到几千块钱，他第一次觉得，原来赚钱还可以这么简单，轻轻松松的接几个活就快赶上工资了。于是接下来的时候，他通过这样的方式，在网上找了不少的活做，慢慢的副业收入也超过了主业收入，日子过得还不错。 **单次收入的赚钱模式的优势与劣势上面这个案例，相信就是绝大多数选择副业的最普遍的方式吧。 那么，我想问大家一个问题： 这个赚钱方式有什么弊端？ 最大的弊端就是：收入的天花板很明显。 有的人肯定就要问： “他不是都靠这个方法，成功让收入翻倍了么，还不够嘛？” 是的，远远不够。 我们来计算一下他实际的操作成本有多少：假设他一个订单报价是3000，预估需要3天的时间做而实际操作过程中呢，因为不是面对面沟通，所以有很多细节问题只能是边设计边交流，甚至有的时候甲方也比较忙，不能及时有效的沟通，一般会在初稿之后，在进行修稿。来回的沟通、反复的修改、一些意外的情况发生（找素材、软件BUG等），实际上，搞定一个项目可能就需要一周的时间来操作，而在这个过程中，因为要一直想着这个事情，精力上也会耗费不少。那么他的一个月收入：假设从来都不缺单，一个月最多赚1.2万，这算是最理想化的情况。真要每个月接4-5个单子，耗费大量的时间和精力去做的话，收入确实是可能翻倍的，但是自己的本职工作，也可能受到很大的影响。也就是自己的职场晋升方面，也没多少工夫去维护了，对不对。所以，我们可以估算出，这样的一个模式，收益很不稳定，只有在甲方一直有单子给他的前提下，才能实现副业月入过万。这种模式的项目，掌柜的建议就是，尽量在不影响职业规划的前提下，偶尔参与一下，改善下生活水平。真要靠这个发财，难！ 有这个功夫，还不如想想如何在公司好好表现，创造更多价值，为跳槽涨薪做准备。 赚钱的几种模式中，搬砖赚钱法、技能赚钱法的优点呢，都是门槛低、操作简单。而弊端也很明显，小钱常有，发财很难。 什么样的赚钱模式值得做只要你坚持混各种圈子，你肯定就能看到各种关于赚钱的机会，那么当我们发现机会的时候，我们要如何决定要不要去做呢？田掌柜的建议是先进行一个思考：我去做这件事情的预期是什么？如果说你只是想体验下这种模式赚钱的感觉，或者是想锻炼下自己的能力，那么我觉得这个事情就值得你去做。如果说你想通过一个项目，通过长期且持续的坚持，来彻底改变生活的话，那么这个事情就不建议你去。我们做任何事情，想赚到钱，那么眼光就一定要放长远一点。诚然，短期内的快钱确实有吸引力，但是它不能够持续，稳定，积累起你的资源优势，成为你人生发展方向的一个基础。 当我们再次碰到这样赚钱的机会的时候，要学会判断一下情况： 这个能赚多少钱？ 它是否可持续的操作？ 项目的收入是否足够多？ 能否靠它帮我去改变人生？ 假如投入全部的时间，能赚多少钱？学会用这种模式去判断项目的话，那么你就会发现，机会也有大小之分。 打个比方，做设计师，一个月最多的上限就是几万块，对吧。做供货商，可能一个客户的成交就给你带来几十万甚至百万的单子。前者靠时间和劳动两个维度去赚钱。后者在此基础上还有人脉和资源的优势。 这个后者呢，也是我们接下来要研究的模式：人脉赚钱法人脉赚钱法的优势与劣势与前面那个项目不同，这个项目需要长期的积累，认识到一些关键的人脉。后面再利用这些人脉自带的资源，达成一种合作。你可以通过卖自己的产品或者服务，让他们去帮助你推广。又或者是你利用人脉关系网，撮合一些项目的达成。除了前面销售的那个例子以外，我们还可以延伸。比如还是这个小a，通过混各种同行交流群，发现有的公司需要招CRM系统开发的，而自己又在其他群，认识了这样的朋友，于是给他们牵线搭桥，帮他们促成了这个项目。有时候，客户合作的比较愉快，给了小a一点介绍费，有的呢，介绍过去后，发现彼此的需求和档期不符合，没谈好，就没给钱。你就会发现这个东西，它并不是一个长期存在的商业模式。个人要真做这个方向的东西，那就需要两头都照顾好，一边找需求方，一边找对接方。类似于一个中介，赚一点中介费。它同样是不稳定，不仅是收入上，还有定价上的较大波动。可能只有一些家庭背景比较深的人适合做这类项目。一个朋友，因为在当地混的比较开，对烟草，酒水，汽车，保险这几个行业，认识的人多，所以经常过有人求他帮忙牵线搭桥，他也因此很轻松的赚到了不少钱。但是绝大多数人来说，并不具备这样的优势。所以，选择这方面的模式的朋友，更希望大家通过关系，去为自己服务。 如何从0开始建立关系体系有人肯定要问了：我没关系怎么去操作这种模式呢？其实有赚钱思维的朋友，一般都不会问这样的问题。因为他们知道关系这个东西，从来都是从无到建立起来的。不是说，你有什么关系，所以你想去做什么事情。而是你想做什么事情，所以想办法去简历这些关系。 举个例子，同样是小a小a通过一段时间的积累，认识到很多的人，最近呢，之前和他有过合作的甲方，找到他，问有没有认识的设计师团队，有个大活儿要做，帮忙介绍一下，有偿的。这个时候，如果按照关系赚钱法，小a直接去其他朋友那里问一下，卖一个信息就行。但是这次小a没有这么做，小a才用了下面这个方法搞定了这件事情。 1.建立一个设计师交流接单群拉了几个朋友进群，表明主题。2.设计了一张海报海报里说这是一个高价值设计师交流群，提供免费的素材、商单、广告等服务，可以帮助大家学习、交流、成长、赚钱等等。3.建立入群规则入群免费，要求转发海报及推荐与至朋友圈，保留4个小时，完成后截图即可进群。4.链接大量优质人才这个群就通过各种同行互相链接介绍，满了500个人，群满以后，就将自己那个甲方的要求及价格报到群里，也很快就搞定了那个大的订单，赚到了几万块钱。那么这个呢就是一个从零起步创造关系的思路，这么讲大家能明白吧。 资源赚钱法的思维模式如何理解资源？开始开头那句话：人无我有，人有我优，人优我廉。只要你具备这个条件，别管是实体的东西，还是虚拟的东西，都叫做资源。你有工厂的货源、你会做一些蛋糕甜品、你认识很多玩汽车的人、和很多跳舞的人关系很好，这些统统可以叫资源。通过这种模式赚钱要分析的东西就是：你到底有哪些优质的资源，你的东西又比别人好在哪里，有一个还是多个好的地方呢…….总之你的优势越多，那么你的机会就越大。 一个简单的例子，之前朋友搞电商的时候，都是用一个货源渠道，但是别人的价格总是能比他低一点，于是销量也有很大的差别。他始终想不明白为什么那人能比他便宜，难道不赚钱了吗？其实并不是，是那个人有物流的优势，每一单物流能比他便宜1-2块钱，这就是优势。 聚焦赚钱法思维模式我们的小a经历过几十个项目的磨练，终于在圈内小有名气。于是他做了一个自己公众号，坚持分享一些自己的创作心得，慢慢的很多行业类以及想进入这个行业的人关注到他，他的公众号粉丝越来越多，后面靠接广告都能月入几万。这个是聚焦赚钱法，是你长期坚持在一个领域深耕，形成一定影响力，以此来赚钱。当然，公众号也只是一种变现手段，还比如你会做字幕剪辑、会配音、会做百度竞价、会写文章、会做PPT，会数据分析，会讲课等等。这些都可以成为你聚焦的赚钱方法，本质上来说都一样，只要你能通过你的长期积累，获得大批人的注意力，那么赚钱就是很容易的。至于这个积累的方法，有哪些技巧，我们以后在细说。 圈子赚钱法和投资赚钱法，我就不细说了。圈子赚钱法其实我在上面的案例中已经讲过，小a创造了一个交流圈子，拉拢一批专业性人才，并以此为背景，作为自己的资源去和甲方谈合作，就是这个模式。投资赚钱法呢，首先就是你得有资金，还需要通过大量数据分析以及实操的积累，是一件很复杂的事情，玩不好就容易深陷其中，这个自己把握。 开启赚钱之路的步骤通过前面这些课程的讲解，基本大家都已经了解到赚钱的基本模式。那么接下来，我们就要通过几个步骤，来判断自己去选择什么样的方式赚钱。 1.拿出一张纸和笔2.详细的写下自己身上所有的资源这个可以包括家乡特产、工作、同学同事、日常喜好、人生经历……3.不管实体还是虚拟，只要你觉得有用的东西都写出来4.什么你的资源属于哪一个模式，有什么优劣势，做一个优先级划分5.根据你填写的资源优劣势优先级表，好好去研究下其中的规律6.去找你资源相关的同行资料，对他们也进行深度的分析，观察商业模式7.复制他们的模式，从零开始深耕下去 ==如果我什么都没有怎么办？ 那只能说明你现在不适合赚钱。看我之前的文章，先学会积累资源和人脉，让自己成长起来。== 有一天，你发现自己有所突破，别人讲的东西，你都能听懂，并且能上去有理有据的说上几句的时候，你就已经具备了赚钱的基础条件，这个时候，你再来分析自己的资源优劣势，找到最适合自己的某种赚钱模式。祝好！ 转自知乎：你的副业是什么？有什么故事","categories":[{"name":"读书","slug":"读书","permalink":"http://huangzhiyuan.github.io/categories/%E8%AF%BB%E4%B9%A6/"}],"tags":[{"name":"摘抄","slug":"摘抄","permalink":"http://huangzhiyuan.github.io/tags/%E6%91%98%E6%8A%84/"},{"name":"副业","slug":"副业","permalink":"http://huangzhiyuan.github.io/tags/%E5%89%AF%E4%B8%9A/"}]},{"title":"leetcode-22-generate-parentheses","slug":"leetcode-22-generate-parentheses","date":"2020-02-16T12:45:59.000Z","updated":"2020-03-01T12:16:28.000Z","comments":true,"path":"2020/02/16/leetcode-22-generate-parentheses/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/16/leetcode-22-generate-parentheses/","excerpt":"给出 n 代表生成括号的对数，请你写出一个函数，使其能够生成所有可能的并且有效的括号组合。 例如，给出 n = 3，生成结果为： 1234567[ &quot;((()))&quot;, &quot;(()())&quot;, &quot;(())()&quot;, &quot;()(())&quot;, &quot;()()()&quot;]","text":"给出 n 代表生成括号的对数，请你写出一个函数，使其能够生成所有可能的并且有效的括号组合。 例如，给出 n = 3，生成结果为： 1234567[ &quot;((()))&quot;, &quot;(()())&quot;, &quot;(())()&quot;, &quot;()(())&quot;, &quot;()()()&quot;] 分析这个题目最直接的想法是生成所有2^2n个 ‘(‘ 和 ‘)’ 字符构成的序列。然后，我们将检查每一个是否有效。为了生成所有序列，我们使用递归。长度为 n 的序列就是 ‘(‘ 加上所有长度为 n-1 的序列，以及 ‘)’ 加上所有长度为 n-1 的序列。为了检查序列是否为有效的，我们会跟踪 平衡，也就是左括号的数量减去右括号的数量的净值。如果这个值始终小于零或者不以零结束，该序列就是无效的，否则它是有效的。优化是只有在我们知道序列仍然保持有效时才添加 ‘(‘ or ‘)’，而不是像方法一那样每次添加。我们可以通过跟踪到目前为止放置的左括号和右括号的数目来做到这一点，如果我们还剩一个位置，我们可以开始放一个左括号。 如果它不超过左括号的数量，我们可以放一个右括号。 Solution123456789101112131415161718class Solution(object): def generateParenthesis(self, n): &quot;&quot;&quot; :type n: int :rtype: List[str] &quot;&quot;&quot; ans = [] def backtrack(S = &#x27;&#x27;, left = 0, right = 0): if len(S) == 2 * n: ans.append(S) return if left &lt; n: backtrack(S+&#x27;(&#x27;, left+1, right) if right &lt; left: backtrack(S+&#x27;)&#x27;, left, right+1) backtrack() return ans","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://huangzhiyuan.github.io/tags/leetcode/"}]},{"title":"面试-C++-01","slug":"mianshi-CplusCplus-01","date":"2020-02-16T11:31:26.000Z","updated":"2020-03-01T12:18:24.000Z","comments":true,"path":"2020/02/16/mianshi-CplusCplus-01/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/16/mianshi-CplusCplus-01/","excerpt":"这一系列文章会从网络各位同学的面试笔记和常考点进行汇总，主要是针对面试的C++后台开发岗位，涵盖了大部分C++相关的可能会被问道的技术点。笔记大都是比较基础的C++知识点的总结。","text":"这一系列文章会从网络各位同学的面试笔记和常考点进行汇总，主要是针对面试的C++后台开发岗位，涵盖了大部分C++相关的可能会被问道的技术点。笔记大都是比较基础的C++知识点的总结。 gdb调试命令step和next的区别： next 执行到下一句 step进入到函数 内存查看： 12345678// 打印变量地址(gdb)p &amp;a// 查看内存单元内变量(gdb）x 0xbffff5430xbffff543: 0x12345678//单字节查看4个内存单元变量的值(gdb) x /4xb 0xbffff5430xbffff543: 0x78 0x56 0x34 0x12 多线程调试： 123456(gdb) info threads：查看GDB当前调试的程序的各个线程的相关信息(gdb) thread threadno：切换当前线程到由threadno指定的线程break filename:linenum thread all 在所有线程相应行设置断点，注意如果主线程不会执行到该行，并且启动all-stop模式，主线程执行n或s会切换过去set scheduler-locking off|on\\step 默认off，执行s或c其它线程也同步执行。on，只有当前相称执行。step，只有当前线程执行show scheduler-locking 显示当前模式thread apply all command 每个线程执行同意命令，如bt。或者thread apply 1 3 bt，即线程1，3执行bt。 查看调用堆栈： 123(gdb)bt(gdb)f 1 //帧简略信息(gdb)info f 1 //帧详细信息 断点： 12b test.cpp:11b test.cpp:main gdb attach: 123gdb-&gt;file xxxx-&gt;attach pid-&gt;这时候进程是停止的-&gt;c 继续运行or$ sudo gdb attach PID 带参数调试： 12(gdb)set args -l a -C abc输入参数命令set args 后面加上程序所要用的参数，注意，不再带有程序名，直接加参数。 list命令： 12list linenum //显示程序第linenum行的周围的程序list function //显示程序名为function的函数的源程序 软链接、硬链接： 软链接：ln –s 源文件 目标文件， 会在选定的位置上生成一个文件的镜像，不会占用磁盘空间，类似于Windows的快捷方式。 硬链接：ln 源文件 目标文件，没有参数-s， 会在选定的位置上生成一个和源文件大小相同的文件，无论是软链接还是硬链接，文件都保持同步变化。 函数指针： 123456789函数指针 int (*func)(int, int)函数指针数组 int (*funcArry[10])(int, int)const int* p; 指向const int的指针int const* p; 同上int* const p; const指针 互斥锁： 1234567891011121314151617181920212223242526272829303132pthread_mutex_t m_mutex;pthread_mutex_init(&amp;m_mutex, NULL)等效于pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZERpthread_mutex_lock(&amp;m_mutex);pthread_mutex_unlock(&amp;m_mutex)pthread_mutex_destroy(&amp;m_mutex)int pthread_create(pthread_t *thread, const pthread_attr_t *attr, void *(*start_routine) (void *), void *arg);bool g_flag = false;void* t1(void* arg)&#123; cout &lt;&lt; &quot;create t1 thread success&quot; &lt;&lt; endl; pthread_mutex_lock(&amp;m_mutex); g_flag = true; pthread_mutex_unlock(&amp;m_mutex);&#125;void* t2(void* arg)&#123; cout &lt;&lt; &quot;create t2 thread success&quot; &lt;&lt; endl; pthread_mutex_lock(&amp;m_mutex); g_flag = false; pthread_mutex_unlock(&amp;m_mutex);&#125;int main(int argc, char* argv[])&#123; pthread_t tid1, tid2; pthread_create(&amp;tid1, NULL, t1, NULL); sleep(2); pthread_create(&amp;tid2, NULL, t2, NULL); pthread_join(tid1, NULL); pthread_join(tid2, NULL);&#125; 大小端转换： 1234#define BigLittleSwap32(A) ((((uint32)(A) &amp; 0xff000000) &gt;&gt; 24) | \\ (((uint32)(A) &amp; 0x00ff0000) &gt;&gt; 8) | \\ (((uint32)(A) &amp; 0x0000ff00) &lt;&lt; 8) | \\ (((uint32)(A) &amp; 0x000000ff) &lt;&lt; 24)) TCP套接字： 服务器端1234567891011listenfd = socket(AF_INET , SOCK_STREAM , 0)bind(listenfd , (struct sockaddr*)&amp;servaddr , sizeof(servaddr))listen(listenfd , LISTENQ)connfd = accept(listenfd , (struct sockaddr *)&amp;cliaddr , &amp;clilen))n = read(connfd , buff , MAX_LINE)write(connfd , buff , n) 客户端12345sockfd = socket(AF_INET , SOCK_STREAM , 0)connect(sockfd , (struct sockaddr *)&amp;servaddr , sizeof(servaddr))write(sockfd , sendline , strlen(sendline)) IP分片与重组 123456789101112131415MTU是1500是指的以太网的MTU，可以用 netstat -i 命令查看这个值。如果IP层有数据包要传，而且数据包的长度超过了MTU，那么IP层就要对数据包进行分片（fragmentation）操作，使每一片的长度都小于或等于MTU。我们假设要传输一个UDP数据包，以太网的MTU为1500字节，一般IP首部为20字节，UDP首部为8字节，数据的净荷（payload）部分预留是1500-20-8=1472字节。如果数据部分大于1472字节，就会出现分片现象，偏移量的单位为8Byte以ID标示是不是同一个分片，以偏移量标示在保文里的位置，每个不完整的ID报文有一个等待计时器，到时丢弃IP层不保证能够送达，如果丢了上层自己处理参考rfc 791IP报文长度单位口诀:4字节单位 - 首部长度单位 1字节单位 - 总长度单位 8字节单位 - 片偏移单位 Vector &amp;&amp; List vector1234567vector和数组类似，拥有一段连续的内存空间，并且起始地址不变。因此能高效的进行随机存取，时间复杂度为o(1);但因为内存空间是连续的，所以在进行插入和删除操作时，会造成内存块的拷贝，时间复杂度为o(n)。另外，当数组中内存空间不够时，会重新申请一块内存空间并进行内存拷贝。 list.12345list是由双向链表实现的，因此内存空间是不连续的。只能通过指针访问数据，所以list的随机存取非常没有效率，时间复杂度为o(n);但由于链表的特点，能高效地进行插入和删除。 Vector动态内存分配： 1这个问题其实很简单，在调用push_back时，若当前容量已经不能够放入心得元素（capacity=size），那么vector会重新申请一块内存，把之前的内存里的元素拷贝到新的内存当中，然后把push_back的元素拷贝到新的内存中，最后要析构原有的vector并释放原有的内存。所以说这个过程的效率是极低的，为了避免频繁的分配内存，C++每次申请内存都会成倍的增长，例如之前是4，那么重新申请后就是8，以此类推。当然不一定是成倍增长，比如在我的编译器环境下实测是0.5倍增长，之前是4，重新申请后就是6 new &amp;&amp; malloc: new和malloc最大区别: new会调用类的构造函数,malloc不会; delete和free同理;new/delete是运算符,malloc/free函数。所以new/delete效率应该会高点。 手动实现strcpy: 12345678910char *strcpy(char *strDest, const char *strSrc)&#123; if ( strDest == NULL || strSrc == NULL) return NULL ; if ( strDest == strSrc) return strDest ; char *tempptr = strDest ; while( (*strDest++ = *strSrc++) != ‘/0’) return tempptr ;&#125;","categories":[{"name":"C++","slug":"C","permalink":"http://huangzhiyuan.github.io/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://huangzhiyuan.github.io/tags/C/"}]},{"title":"吴恩达《机器学习》文字整理版","slug":"ai-stat-ml2014","date":"2020-02-15T09:46:08.000Z","updated":"2020-03-01T12:06:42.000Z","comments":true,"path":"2020/02/15/ai-stat-ml2014/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/15/ai-stat-ml2014/","excerpt":"斯坦福大学2014机器学习教程中文笔记目录","text":"斯坦福大学2014机器学习教程中文笔记目录 第一周一、 引言(Introduction) 1.1 欢迎 1.2 机器学习是什么？ 1.3 监督学习 1.4 无监督学习 二、单变量线性回归(Linear Regression with One Variable) 2.1 模型表示 2.2 代价函数 2.3 代价函数的直观理解I 2.4 代价函数的直观理解II 2.5 梯度下降 2.6 梯度下降的直观理解 2.7 梯度下降的线性回归 2.8 接下来的内容 三、线性代数回顾(Linear Algebra Review) 3.1 矩阵和向量 3.2 加法和标量乘法 3.3 矩阵向量乘法 3.4 矩阵乘法 3.5 矩阵乘法的性质 3.6 逆、转置 第二周四、多变量线性回归(Linear Regression with Multiple Variables) 4.1 多维特征 4.2 多变量梯度下降 4.3 梯度下降法实践1-特征缩放 4.4 梯度下降法实践2-学习率 4.5 特征和多项式回归 4.6 正规方程 4.7 正规方程及不可逆性（选修） 五、Octave教程(Octave Tutorial) 5.1 基本操作 5.2 移动数据 5.3 计算数据 5.4 绘图数据 5.5 控制语句：for，while，if语句 5.6 向量化 88 5.7 工作和提交的编程练习 第三周六、逻辑回归(Logistic Regression) 6.1 分类问题 6.2 假说表示 6.3 判定边界 6.4 代价函数 6.5 简化的成本函数和梯度下降 6.6 高级优化 6.7 多类别分类：一对多 七、正则化(Regularization) 7.1 过拟合的问题 7.2 代价函数 7.3 正则化线性回归 7.4 正则化的逻辑回归模型 第四周第八、神经网络：表述(Neural Networks: Representation) 8.1 非线性假设 8.2 神经元和大脑 8.3 模型表示1 8.4 模型表示2 8.5 样本和直观理解1 8.6 样本和直观理解II 8.7 多类分类 第五周九、神经网络的学习(Neural Networks: Learning) 9.1 代价函数 9.2 反向传播算法 9.3 反向传播算法的直观理解 9.4 实现注意：展开参数 9.5 梯度检验 9.6 随机初始化 9.7 综合起来 9.8 自主驾驶 第六周十、应用机器学习的建议(Advice for Applying Machine Learning) 10.1 决定下一步做什么 10.2 评估一个假设 10.3 模型选择和交叉验证集 10.4 诊断偏差和方差 10.5 正则化和偏差/方差 10.6 学习曲线 10.7 决定下一步做什么 十一、机器学习系统的设计(Machine Learning System Design) 11.1 首先要做什么 11.2 误差分析 11.3 类偏斜的误差度量 11.4 查准率和查全率之间的权衡 11.5 机器学习的数据 第7周 十二、支持向量机(Support Vector Machines) 12.1 优化目标 12.2 大边界的直观理解 12.3 数学背后的大边界分类（选修） 12.4 核函数1 12.5 核函数2 12.6 使用支持向量机 第八周十三、聚类(Clustering) 13.1 无监督学习：简介 13.2 K-均值算法 13.3 优化目标 13.4 随机初始化 13.5 选择聚类数 十四、降维(Dimensionality Reduction) 14.1 动机一：数据压缩 14.2 动机二：数据可视化 14.3 主成分分析问题 14.4 主成分分析算法 14.5 选择主成分的数量 14.6 重建的压缩表示 14.7 主成分分析法的应用建议 第九周十五、异常检测(Anomaly Detection) 15.1 问题的动机 15.2 高斯分布 15.3 算法 15.4 开发和评价一个异常检测系统 15.5 异常检测与监督学习对比 15.6 选择特征 15.7 多元高斯分布（选修） 15.8 使用多元高斯分布进行异常检测（选修） 十六、推荐系统(Recommender Systems) 16.1 问题形式化 16.2 基于内容的推荐系统 16.3 协同过滤 16.4 协同过滤算法 16.5 向量化：低秩矩阵分解 16.6 推行工作上的细节：均值归一化 第十周十七、大规模机器学习(Large Scale Machine Learning) 17.1 大型数据集的学习 17.2 随机梯度下降法 17.3 小批量梯度下降 17.4 随机梯度下降收敛 17.5 在线学习 17.6 映射化简和数据并行 十八、应用实例：图片文字识别(Application Example: Photo OCR) 18.1 问题描述和流程图 18.2 滑动窗口 18.3 获取大量数据和人工数据 18.4 上限分析：哪部分管道的接下去做 十九、总结(Conclusion) 19.1 总结和致谢","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"ml2014","slug":"ml2014","permalink":"http://huangzhiyuan.github.io/tags/ml2014/"}]},{"title":"leetcode-6-zigzag-conversion","slug":"leetcode-6-zigzag-conversion","date":"2020-02-14T13:31:43.000Z","updated":"2020-03-01T12:15:34.000Z","comments":true,"path":"2020/02/14/leetcode-6-zigzag-conversion/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/14/leetcode-6-zigzag-conversion/","excerpt":"题目描述：将一个给定字符串根据给定的行数，以从上往下、从左到右进行 Z 字形排列。 比如输入字符串为 &quot;LEETCODEISHIRING&quot; 行数为 3 时，排列如下： 123L C I RE T O E S I I GE D H N 之后，你的输出需要从左往右逐行读取，产生出一个新的字符串，比如：&quot;LCIRETOESIIGEDHN&quot;。 请你实现这个将字符串进行指定行数变换的函数：","text":"题目描述：将一个给定字符串根据给定的行数，以从上往下、从左到右进行 Z 字形排列。 比如输入字符串为 &quot;LEETCODEISHIRING&quot; 行数为 3 时，排列如下： 123L C I RE T O E S I I GE D H N 之后，你的输出需要从左往右逐行读取，产生出一个新的字符串，比如：&quot;LCIRETOESIIGEDHN&quot;。 请你实现这个将字符串进行指定行数变换的函数： 123456789101112131415string convert(string s, int numRows);示例 1:输入: s = &quot;LEETCODEISHIRING&quot;, numRows = 3输出: &quot;LCIRETOESIIGEDHN&quot;示例 2:输入: s = &quot;LEETCODEISHIRING&quot;, numRows = 4输出: &quot;LDREOEIIECIHNTSG&quot;解释:L D RE O E I IE C I H NT S G 分析该题目主要看映射后的字符和之前的对应关系，属于数学规律。可以构造numRows个list，只需要找到每个字符对应在哪个list以及在该list中的index，之后将numRows个list合并为最终的字符串即可。 Solution1234567891011121314151617181920class Solution(object): def convert(self, s, numRows): &quot;&quot;&quot; :type s: str :type numRows: int :rtype: str &quot;&quot;&quot; if numRows == 1: return s elif len(s) == 0: return &quot;&quot; res = [&#x27;&#x27;] * numRows for i in range(len(s)): ans = i // (numRows - 1) cur = i % (numRows - 1) if ans % 2 == 0: res[cur] += s[i] else: res[numRows-cur-1] += s[i] return &#x27;&#x27;.join(res)","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://huangzhiyuan.github.io/tags/leetcode/"}]},{"title":"leetcode-3-longest-substring-without-repeating-characters","slug":"leetcode-3-longest-substring-without-repeating-characters","date":"2020-02-14T10:45:21.000Z","updated":"2020-03-01T12:15:14.000Z","comments":true,"path":"2020/02/14/leetcode-3-longest-substring-without-repeating-characters/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/14/leetcode-3-longest-substring-without-repeating-characters/","excerpt":"题目描述：给定一个字符串，请你找出其中不含有重复字符的最长子串的长度。","text":"题目描述：给定一个字符串，请你找出其中不含有重复字符的最长子串的长度。 12345678910111213141516示例 1:输入: &quot;abcabcbb&quot;输出: 3解释: 因为无重复字符的最长子串是 &quot;abc&quot;，所以其长度为 3。示例 2:输入: &quot;bbbbb&quot;输出: 1解释: 因为无重复字符的最长子串是 &quot;b&quot;，所以其长度为 1。示例 3:输入: &quot;pwwkew&quot;输出: 3解释: 因为无重复字符的最长子串是 &quot;wke&quot;，所以其长度为 3。 请注意，你的答案必须是 子串 的长度，&quot;pwke&quot; 是一个子序列，不是子串。 分析该题目可以借鉴two-sum使用字典存储字符串每个字符和对应的索引下标。字典st记录字符出现的位置，两个相同字符直接的距离就是无重复字符的长度。 Solution1234567891011121314class Solution(object): def lengthOfLongestSubstring(self, s): &quot;&quot;&quot; :type s: str :rtype: int &quot;&quot;&quot; st = &#123;&#125; i, res = -1, 0 for j in range(len(s)): if s[j] in st: i = max(st[s[j]], i) res = max(res, j - i) st[s[j]] = j return res","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://huangzhiyuan.github.io/tags/leetcode/"}]},{"title":"leetcode-20-valid-parentheses","slug":"leetcode-20-valid-parentheses","date":"2020-02-13T13:02:03.000Z","updated":"2020-03-01T12:16:06.000Z","comments":true,"path":"2020/02/13/leetcode-20-valid-parentheses/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/13/leetcode-20-valid-parentheses/","excerpt":"题目描述：给定一个只包括 ‘(‘，’)’，’{‘，’}’，’[‘，’]’ 的字符串，判断字符串是否有效。 有效字符串需满足： 左括号必须用相同类型的右括号闭合。左括号必须以正确的顺序闭合。注意空字符串可被认为是有效字符串。","text":"题目描述：给定一个只包括 ‘(‘，’)’，’{‘，’}’，’[‘，’]’ 的字符串，判断字符串是否有效。 有效字符串需满足： 左括号必须用相同类型的右括号闭合。左括号必须以正确的顺序闭合。注意空字符串可被认为是有效字符串。 1234567891011121314151617181920示例 1:输入: &quot;()&quot;输出: true示例 2:输入: &quot;()[]&#123;&#125;&quot;输出: true示例 3:输入: &quot;(]&quot;输出: false示例 4:输入: &quot;([)]&quot;输出: false示例 5:输入: &quot;&#123;[]&#125;&quot;输出: true Solution by me123456789101112131415161718192021222324class Solution(object): def isValid(self, s): &quot;&quot;&quot; :type s: str :rtype: bool &quot;&quot;&quot; res = [] for i in range(len(s)): if s[i] in &quot;([&#123;&quot;: res.append(s[i]) else: if len(res) == 0: return False k = res.pop() if (s[i] == &quot;)&quot; and k == &#x27;(&#x27;) or \\ (s[i] == &#x27;]&#x27; and k == &#x27;[&#x27;) or \\ (s[i] == &#x27;&#125;&#x27; and k == &#x27;&#123;&#x27;): pass else: return False if len(res) == 0: return True else: return False Solution 21234567891011class Solution(object): def isValid(self, s): &quot;&quot;&quot; :type s: str :rtype: bool &quot;&quot;&quot; while &#x27;&#123;&#125;&#x27; in s or &#x27;()&#x27; in s or &#x27;[]&#x27; in s: s = s.replace(&#x27;&#123;&#125;&#x27;, &#x27;&#x27;) s = s.replace(&#x27;[]&#x27;, &#x27;&#x27;) s = s.replace(&#x27;()&#x27;, &#x27;&#x27;) return s == &#x27;&#x27;","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://huangzhiyuan.github.io/tags/leetcode/"}]},{"title":"leetcode-50-powx-n","slug":"leetcode-50-powx-n","date":"2020-02-13T12:37:40.000Z","updated":"2020-03-01T12:17:14.000Z","comments":true,"path":"2020/02/13/leetcode-50-powx-n/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/13/leetcode-50-powx-n/","excerpt":"题目描述：实现 pow(x, n) ，即计算 x 的 n 次幂函数。","text":"题目描述：实现 pow(x, n) ，即计算 x 的 n 次幂函数。 123456789101112131415161718示例 1:输入: 2.00000, 10输出: 1024.00000示例 2:输入: 2.10000, 3输出: 9.26100示例 3:输入: 2.00000, -2输出: 0.25000解释: 2-2 = 1/22 = 1/4 = 0.25说明:-100.0 &lt; x &lt; 100.0n 是 32 位有符号整数，其数值范围是 [−231, 231 − 1] 。 Solution123456789101112131415161718192021class Solution(object): def myPow(self, x, n): &quot;&quot;&quot; :type x: float :type n: int :rtype: float &quot;&quot;&quot; if n == -1: return 1.0/x elif n == 0: return 1 elif n == 1: return x else: res = x tmp = Solution() if n % 2 == 1: res = tmp.myPow(x, n//2) ** 2 * x else: res = tmp.myPow(x, n//2) ** 2 return res","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://huangzhiyuan.github.io/tags/leetcode/"}]},{"title":"leetcode-60-permutation-sequence","slug":"leetcode-60-permutation-sequence","date":"2020-02-12T15:06:28.000Z","updated":"2020-03-01T12:17:20.000Z","comments":true,"path":"2020/02/12/leetcode-60-permutation-sequence/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/12/leetcode-60-permutation-sequence/","excerpt":"题目描述：给出集合 [1,2,3,…,n]，其所有元素共有 n! 种排列。 按大小顺序列出所有排列情况，并一一标记，当 n = 3 时, 所有排列如下： “123”“132”“213”“231”“312”“321”给定 n 和 k，返回第 k 个排列。","text":"题目描述：给出集合 [1,2,3,…,n]，其所有元素共有 n! 种排列。 按大小顺序列出所有排列情况，并一一标记，当 n = 3 时, 所有排列如下： “123”“132”“213”“231”“312”“321”给定 n 和 k，返回第 k 个排列。 说明： 给定 n 的范围是 [1, 9]。给定 k 的范围是[1, n!]。示例 1: 123456输入: n = 3, k = 3输出: &quot;213&quot;示例 2:输入: n = 4, k = 9输出: &quot;2314&quot; 分析直接用回溯法做的话需要在回溯到第k个排列时终止就不会超时了, 但是效率依旧感人可以用数学的方法来解, 因为数字都是从1开始的连续自然数, 排列出现的次序可以推算出来, 对于n=4, k=15 找到k=15排列的过程: 1 + 对2,3,4的全排列 (3!个)2 + 对1,3,4的全排列 (3!个) 3, 1 + 对2,4的全排列(2!个)3 + 对1,2,4的全排列 (3!个)——-&gt; 3, 2 + 对1,4的全排列(2!个)——-&gt; 3, 2, 1 + 对4的全排列(1!个)——-&gt; 32144 + 对1,2,3的全排列 (3!个) 3, 4 + 对1,2的全排列(2!个) 3, 2, 4 + 对1的全排列(1!个) 确定第一位: k = 14(从0开始计数) index = k / (n-1)! = 2, 说明第15个数的第一位是3 更新k k = k - index*(n-1)! = 2确定第二位: k = 2 index = k / (n-2)! = 1, 说明第15个数的第二位是2 更新k k = k - index*(n-2)! = 0确定第三位: k = 0 index = k / (n-3)! = 0, 说明第15个数的第三位是1 更新k k = k - index*(n-3)! = 0确定第四位: k = 0 index = k / (n-4)! = 0, 说明第15个数的第四位是4最终确定n=4时第15个数为3214 Solution1234567891011121314151617181920212223242526class Solution(object): def getPermutation(self, n, k): &quot;&quot;&quot; :type n: int :type k: int :rtype: str &quot;&quot;&quot; def calculate(i): sum = 1 for j in range(1, i + 1): sum *= j return sum k -= 1 avail = [i for i in range(1, n + 1)] ans = [] for i in range(n - 1, -1, -1): m = calculate(i) d = k // m k = k % m x = avail[d] avail.remove(x) ans.append(str(x)) return &quot;&quot;.join(ans)","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://huangzhiyuan.github.io/tags/leetcode/"}]},{"title":"leetcode-43-multiply-strings","slug":"leetcode-43-multiply-strings","date":"2020-02-12T13:38:03.000Z","updated":"2020-03-01T12:17:02.000Z","comments":true,"path":"2020/02/12/leetcode-43-multiply-strings/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/12/leetcode-43-multiply-strings/","excerpt":"题目描述：给定两个以字符串形式表示的非负整数 num1 和 num2，返回 num1 和 num2 的乘积，它们的乘积也表示为字符串形式。","text":"题目描述：给定两个以字符串形式表示的非负整数 num1 和 num2，返回 num1 和 num2 的乘积，它们的乘积也表示为字符串形式。 示例： 12345678示例 1:输入: num1 = &quot;2&quot;, num2 = &quot;3&quot;输出: &quot;6&quot;示例 2:输入: num1 = &quot;123&quot;, num2 = &quot;456&quot;输出: &quot;56088&quot; 说明： num1 和 num2 的长度小于110。 num1 和 num2 只包含数字 0-9。 num1 和 num2 均不以零开头，除非是数字 0 本身。 不能使用任何标准库的大数类型（比如 BigInteger）或直接将输入转换为整数来处理。 Solution 1123456789101112131415161718192021222324class Solution(object): def multiply(self, num1, num2): &quot;&quot;&quot; :type num1: str :type num2: str :rtype: str &quot;&quot;&quot; num1 = num1[::-1] num2 = num2[::-1] l1, l2 = len(num1), len(num2) res = [] for i in range(l1+l2): res.append(&#x27;0&#x27;) for i in range(l1): for j in range(l2): tmp = int(num1[i]) * int(num2[j]) carry = int(res[i+j]) + tmp % 10 res[i+j] = str(carry % 10) res[i+j+1] = str(int(res[i+j+1]) + tmp // 10 + carry // 10) res = res[::-1] for i in range(l1+l2): if res[i] != &#x27;0&#x27;: break return &#x27;&#x27;.join(res[i:]) Solution 2 (小改动较Solution 1)12345678910111213141516171819202122class Solution(object): def multiply(self, num1, num2): &quot;&quot;&quot; :type num1: str :type num2: str :rtype: str &quot;&quot;&quot; num1 = num1[::-1] num2 = num2[::-1] l1, l2 = len(num1), len(num2) res = [0] * (l1 + l2) for i in range(l1): for j in range(l2): tmp = int(num1[i]) * int(num2[j]) carry = int(res[i+j]) + tmp % 10 res[i+j] = carry % 10 res[i+j+1] = int(res[i+j+1]) + tmp // 10 + carry // 10 res = res[::-1] for i in range(l1+l2): if res[i] != 0: break return &#x27;&#x27;.join(map(str, res)[i:])","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://huangzhiyuan.github.io/tags/leetcode/"}]},{"title":"leetcode-33-search-in-rotated-sorted-array","slug":"leetcode-33-search-in-rotated-sorted-array","date":"2020-02-11T15:15:11.000Z","updated":"2020-03-01T12:16:54.000Z","comments":true,"path":"2020/02/11/leetcode-33-search-in-rotated-sorted-array/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/11/leetcode-33-search-in-rotated-sorted-array/","excerpt":"假设按照升序排序的数组在预先未知的某个点上进行了旋转。( 例如，数组 [0,1,2,4,5,6,7] 可能变为 [4,5,6,7,0,1,2] )。搜索一个给定的目标值，如果数组中存在这个目标值，则返回它的索引，否则返回 -1 。你可以假设数组中不存在重复的元素。你的算法时间复杂度必须是 O(log n) 级别。","text":"假设按照升序排序的数组在预先未知的某个点上进行了旋转。( 例如，数组 [0,1,2,4,5,6,7] 可能变为 [4,5,6,7,0,1,2] )。搜索一个给定的目标值，如果数组中存在这个目标值，则返回它的索引，否则返回 -1 。你可以假设数组中不存在重复的元素。你的算法时间复杂度必须是 O(log n) 级别。 示例： 12345678示例 1:输入: nums = [4,5,6,7,0,1,2], target = 0输出: 4示例 2:输入: nums = [4,5,6,7,0,1,2], target = 3输出: -1 Solution1234567891011121314151617181920212223class Solution(object): def search(self, nums, target): &quot;&quot;&quot; :type nums: List[int] :type target: int :rtype: int &quot;&quot;&quot; left, right = 0, len(nums) - 1 while left&lt;=right: mid = (left + right) // 2 if nums[mid] == target: return mid if nums[mid] &gt;= nums[left]: if nums[left] &lt;= target &lt; nums[mid]: right = mid - 1 else: left = mid + 1 else: if nums[mid] &lt; target &lt;= nums[right]: left = mid + 1 else: right = mid - 1 return -1","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://huangzhiyuan.github.io/tags/leetcode/"}]},{"title":"leetcode-21-merge-two-lists","slug":"leetcode-21-merge-two-lists","date":"2020-02-11T14:08:14.000Z","updated":"2020-03-01T12:16:14.000Z","comments":true,"path":"2020/02/11/leetcode-21-merge-two-lists/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/11/leetcode-21-merge-two-lists/","excerpt":"将两个有序链表合并为一个新的有序链表并返回。新链表是通过拼接给定的两个链表的所有节点组成的。","text":"将两个有序链表合并为一个新的有序链表并返回。新链表是通过拼接给定的两个链表的所有节点组成的。 示例： 12输入：1-&gt;2-&gt;4, 1-&gt;3-&gt;4输出：1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4 Solution1234567891011121314151617181920212223242526# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def mergeTwoLists(self, l1, l2): &quot;&quot;&quot; :type l1: ListNode :type l2: ListNode :rtype: ListNode &quot;&quot;&quot; res = ListNode(0) tmp = res while l1 and l2: if l1.val &lt; l2.val: tmp.next, l1 = l1, l1.next else: tmp.next, l2 = l2, l2.next tmp = tmp.next if l1: tmp.next = l1 else: tmp.next = l2 return res.next","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://huangzhiyuan.github.io/tags/leetcode/"}]},{"title":"leetcode-2-add-two-numbers","slug":"leetcode-2-add-two-numbers","date":"2020-02-11T14:03:48.000Z","updated":"2020-03-01T12:14:50.000Z","comments":true,"path":"2020/02/11/leetcode-2-add-two-numbers/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/11/leetcode-2-add-two-numbers/","excerpt":"给出两个 非空 的链表用来表示两个非负的整数。其中，它们各自的位数是按照 逆序 的方式存储的，并且它们的每个节点只能存储 一位 数字。如果，我们将这两个数相加起来，则会返回一个新的链表来表示它们的和。您可以假设除了数字 0 之外，这两个数都不会以 0 开头。","text":"给出两个 非空 的链表用来表示两个非负的整数。其中，它们各自的位数是按照 逆序 的方式存储的，并且它们的每个节点只能存储 一位 数字。如果，我们将这两个数相加起来，则会返回一个新的链表来表示它们的和。您可以假设除了数字 0 之外，这两个数都不会以 0 开头。 示例： 123输入：(2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)输出：7 -&gt; 0 -&gt; 8原因：342 + 465 = 807 Solution1234567891011121314151617181920212223242526# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def addTwoNumbers(self, l1, l2): &quot;&quot;&quot; :type l1: ListNode :type l2: ListNode :rtype: ListNode &quot;&quot;&quot; tmp = ListNode(0) re = tmp carry = 0 while l1 or l2 or carry: y1 = l1.val if l1 else 0 y2 = l2.val if l2 else 0 k = y1 + y2 + carry carry = k // 10 re.next = ListNode(k%10) re = re.next if l1: l1 = l1.next if l2: l2 = l2.next return tmp.next;","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://huangzhiyuan.github.io/tags/leetcode/"}]},{"title":"leetcode-27-remove-element","slug":"leetcode-27-remove-element","date":"2020-02-10T13:20:03.000Z","updated":"2020-03-01T12:16:46.000Z","comments":true,"path":"2020/02/10/leetcode-27-remove-element/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/10/leetcode-27-remove-element/","excerpt":"给定一个数组 nums 和一个值 val，你需要原地移除所有数值等于 val 的元素，返回移除后数组的新长度。 不要使用额外的数组空间，你必须在原地修改输入数组并在使用 O(1) 额外空间的条件下完成。 元素的顺序可以改变。你不需要考虑数组中超出新长度后面的元素。","text":"给定一个数组 nums 和一个值 val，你需要原地移除所有数值等于 val 的元素，返回移除后数组的新长度。 不要使用额外的数组空间，你必须在原地修改输入数组并在使用 O(1) 额外空间的条件下完成。 元素的顺序可以改变。你不需要考虑数组中超出新长度后面的元素。 示例1： 123给定 nums = [3,2,2,3], val = 3,函数应该返回新的长度 2, 并且 nums 中的前两个元素均为 2。你不需要考虑数组中超出新长度后面的元素。 示例2： 1234给定 nums = [0,1,2,2,3,0,4,2], val = 2,函数应该返回新的长度 5, 并且 nums 中的前五个元素为 0, 1, 3, 0, 4。注意这五个元素可为任意顺序。你不需要考虑数组中超出新长度后面的元素。 说明：为什么返回数值是整数，但输出的答案是数组呢?请注意，输入数组是以“引用”方式传递的，这意味着在函数里修改输入数组对于调用者是可见的。你可以想象内部操作如下: 12345678// nums 是以“引用”方式传递的。也就是说，不对实参作任何拷贝int len = removeElement(nums, val);// 在函数里修改输入数组对于调用者是可见的。// 根据你的函数返回的长度, 它会打印出数组中该长度范围内的所有元素。for (int i = 0; i &lt; len; i++) &#123; print(nums[i]);&#125; Solution112345678910111213141516class Solution(object): def removeElement(self, nums, val): &quot;&quot;&quot; :type nums: List[int] :type val: int :rtype: int &quot;&quot;&quot; k = len(nums) if k == 0: return 0 j = 0 for i in range(k): if nums[i] != val: nums[j] = nums[i] j += 1 return j Solution2123456789101112class Solution(object): def removeElement(self, nums, val): &quot;&quot;&quot; :type nums: List[int] :type val: int :rtype: int &quot;&quot;&quot; k = len(nums) for i in range(k-1,-1,-1): if nums[i] == val: nums.pop(i) return len(nums)","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://huangzhiyuan.github.io/tags/leetcode/"}]},{"title":"leetcode-26-remove-duplicates-from-sorted-array","slug":"leetcode-26-remove-duplicates-from-sorted-array","date":"2020-02-10T12:39:44.000Z","updated":"2020-03-01T12:16:36.000Z","comments":true,"path":"2020/02/10/leetcode-26-remove-duplicates-from-sorted-array/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/10/leetcode-26-remove-duplicates-from-sorted-array/","excerpt":"给定一个排序数组，你需要在原地删除重复出现的元素，使得每个元素只出现一次，返回移除后数组的新长度。 不要使用额外的数组空间，你必须在原地修改输入数组并在使用 O(1) 额外空间的条件下完成。","text":"给定一个排序数组，你需要在原地删除重复出现的元素，使得每个元素只出现一次，返回移除后数组的新长度。 不要使用额外的数组空间，你必须在原地修改输入数组并在使用 O(1) 额外空间的条件下完成。 示例1： 123给定数组 nums = [1,1,2],函数应该返回新的长度 2, 并且原数组 nums 的前两个元素被修改为 1, 2。你不需要考虑数组中超出新长度后面的元素。 示例2： 123给定 nums = [0,0,1,1,1,2,2,3,3,4],函数应该返回新的长度 5, 并且原数组 nums 的前五个元素被修改为 0, 1, 2, 3, 4。你不需要考虑数组中超出新长度后面的元素。 说明: 为什么返回数值是整数，但输出的答案是数组呢?请注意，输入数组是以“引用”方式传递的，这意味着在函数里修改输入数组对于调用者是可见的。你可以想象内部操作如下: 12345678// nums 是以“引用”方式传递的。也就是说，不对实参做任何拷贝int len = removeDuplicates(nums);// 在函数里修改输入数组对于调用者是可见的。// 根据你的函数返回的长度, 它会打印出数组中该长度范围内的所有元素。for (int i = 0; i &lt; len; i++) &#123; print(nums[i]);&#125; Solution123456789101112class Solution(object): def removeDuplicates(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; k = 0 for i in range(1,len(nums)): if nums[k] != nums[i]: k += 1 nums[k] = nums[i] return k+1","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://huangzhiyuan.github.io/tags/leetcode/"}]},{"title":"不散的宴席","slug":"不散的宴席","date":"2020-02-09T07:04:23.000Z","updated":"2020-03-01T12:21:44.000Z","comments":true,"path":"2020/02/09/不散的宴席/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/09/%E4%B8%8D%E6%95%A3%E7%9A%84%E5%AE%B4%E5%B8%AD/","excerpt":"2020年的春节是较为难忘的一个春节了。经历过03年的SARS，08年的大地震，现在又加上20年的“NCP”(新型冠状病毒)，也算得上是国运坎坷多灾多难了。在这个宅在家不出门就算为国家战疫情做贡献的时候，一口气在家竟然憋到大年15没去上班，竟然还有半个月的隔离器要熬。也算得上是没有寒假作业的最长的一个假期了。午后的中午，明光明媚洒满窗台，暖暖的阳光，让人感到久违的惬意。花了小半天将家里收拾了个干干净净，心情也愉悦舒畅了好多。在这片大好的春光中捧起一本散发着书香味的书，慵懒的靠在飘窗也是享受。大有躲进小楼成一统，管它春夏与秋冬之势。","text":"2020年的春节是较为难忘的一个春节了。经历过03年的SARS，08年的大地震，现在又加上20年的“NCP”(新型冠状病毒)，也算得上是国运坎坷多灾多难了。在这个宅在家不出门就算为国家战疫情做贡献的时候，一口气在家竟然憋到大年15没去上班，竟然还有半个月的隔离器要熬。也算得上是没有寒假作业的最长的一个假期了。午后的中午，明光明媚洒满窗台，暖暖的阳光，让人感到久违的惬意。花了小半天将家里收拾了个干干净净，心情也愉悦舒畅了好多。在这片大好的春光中捧起一本散发着书香味的书，慵懒的靠在飘窗也是享受。大有躲进小楼成一统，管它春夏与秋冬之势。 后会暂无期，相聚终有时。 摘抄如下： 你多少岁了？你最向往的生活是什么样的？你曾经触碰过吗？ 十六岁，说起向往的生活，大概就是在蓝天白云下放一群听话或不听话的羊吧，没有城市和人类，没有金钱的诱惑。我很向往自由和回归自然的生活，哪怕这些并不稳定或自己随时都会暴露于危险之下。我也心甘情愿。从未触碰。 十八岁。最向往的生活已经逝去。高中三年，和最好的朋友逃课看NBA，顶着烈日在蝉鸣声下打球，在星空下的溪边喝酒吹水谈论自己喜欢的女孩子，在教室里面看着老师授课，同学们为梦想奋斗。可是现在都过去了。现在大一，更多的是对未来的不安和迷茫。 十九岁。我是个很平凡而且追求平凡的人。没有大多数人想要的什么出去这个世界看看，没有他们宏伟的梦想。我最想的生活就是有三五个好友，有心爱的人在身旁，家里人身体健康，每天都很开心和很充实。看似很简单，其实以后很难做到吧。我现在大一，我想我正在享受这样的生活。 二十三岁。想去顺租一个为期二十年的老宅子，院里栽石榴树，屋外种油菜花。如果生女儿，二十年的青山烟雨也够她成长祸水了；如果生儿子，只要品行端良，行无为之道或造福于民都由他去。清溪，垂柳，落日，书卷，古刹，祠堂，还有令人欣喜的梅花，莲花，插画，稻花以及使人惧怕的老牛野鸭。胸中有佛，心头有爱。触碰过。 现在我们的祖国还有哪个地方能在夜晚看到满天繁星？ 记得小时候的某个夜晚，躺在院子里的卧椅上，看到的天空是满天繁星，很明亮也很震撼，让人有种想去探索宇宙奥秘的冲动。但现在的夜晚却总是只有零星的几颗星出现，曾几何时我还以为是自己近视的原因。 仰望星空，相信这是很多当今都市人的梦想。逃离城市，来到荒野，舒展身体，仰卧这凝望头顶那片深邃的苍穹，自己的心境能得以宁静。城市中的灯火霓虹和大气污染将我们头顶上的星空完全掩盖住了，以致我们太久太久都没有抬头仰望星空，甚至将其遗忘了。 如果发现自己的名字被载入史册，你回觉得是因为什么？ 史书记载，中国首位穿越试验员于首次穿越实验旅途中下落不明。 成为新中国成立以来，第一位女性国家领导人。 除了族谱，我相信不会有更多的其它记载！ 为了世界和平勇敢地于外星人和亲，成了男版的王昭君。 三百年女尸容颜不改。 论优质女大学生为何找不到“男票”的案例。 在世界杯决赛中打入绝杀进球，帮助中国历史上第一次举起大力神杯。 爱读什么书？如何看待俗和雅： 郭德纲：平常喜欢看古典文学和历史作品，特别是《二十四史》、《清史稿》等史书。读史是为了丰富自己的头脑，了解人生是怎么回事。不管是帝王将相，还是才子佳人，清官也罢贪官也罢，几千年来这些故事几乎都是原封不动地反复发生，宋朝的故事和明朝的故事是一样的。人是不会变化的，无非是朝代不一样。我不敢说自己把世事看透了，但很多事情的确能看开，也就这点事，用不着自个儿较真。 至于俗和雅，我觉得高雅不是装出来的，孙子才是装出来的。高雅和低俗，只是切入点不一样。有人喜欢交响乐，高雅音乐，但有人不喜欢，整晚没有包袱没有笑点。比如我带一只小狗去听交响乐，我听得如痴如醉，但小狗可能一直在盯着指挥棒，等着丢过来。泰坦尼克号沉了，对人类来说是一场巨大的灾难，但对船上那些餐厅里活着的海鲜来说，那就是生命的奇迹。高尔基先生教导我们说，喝着咖啡就大蒜，秋水长天共一色。现如今的心态，已经到了阅遍天下A片而心中无码的境界。 你一直在练习微笑，你不是说变成了自己讨厌的人，而是在事故中变得沉稳，总不能累了就放弃，痛了就喊疼。相信别人不如依赖自己，他人报以伤害，时光回给你温暖。 这个世界有那么多未知，每一天世界都不够用，只是我们习惯了，把自己活成不了解自己的人，想要什么，想去哪里，就连想爱的人，都不确定。天南地北转啊转，遇见太多人经历太多事，但都毁在一颗不够坚定的心上。人的命过一天少一天，爱的人见一面少一面，根本没时间矫情。 他们缩在史书的一角，用只字片语书写着一生，只是他们的故事注定 要比那些感天动地、曲折离奇的戏剧更加精彩动人，因为这才是不完美但真实的历史。 也许人生中，总会有一些光亮不会熄灭，就像夏夜里那漫天飞舞的萤火虫尾翼的亮光一样，永远不会被夜色席卷。无论多么深沉的夜","categories":[{"name":"读书","slug":"读书","permalink":"http://huangzhiyuan.github.io/categories/%E8%AF%BB%E4%B9%A6/"}],"tags":[{"name":"读书","slug":"读书","permalink":"http://huangzhiyuan.github.io/tags/%E8%AF%BB%E4%B9%A6/"}]},{"title":"leetcode-16-3sum-closest","slug":"leetcode-16-3sum-closest","date":"2020-02-08T08:53:57.000Z","updated":"2020-03-01T12:16:00.000Z","comments":true,"path":"2020/02/08/leetcode-16-3sum-closest/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/08/leetcode-16-3sum-closest/","excerpt":"给定一个包括 n 个整数的数组 nums 和 一个目标值 target。找出 nums 中的三个整数，使得它们的和与 target 最接近。返回这三个数的和。假定每组输入只存在唯一答案。","text":"给定一个包括 n 个整数的数组 nums 和 一个目标值 target。找出 nums 中的三个整数，使得它们的和与 target 最接近。返回这三个数的和。假定每组输入只存在唯一答案。 12例如，给定数组 nums = [-1，2，1，-4], 和 target = 1.与 target 最接近的三个数的和为 2. (-1 + 2 + 1 = 2). Solution1234567891011121314151617181920212223class Solution(object): def threeSumClosest(self, nums, target): &quot;&quot;&quot; :type nums: List[int] :type target: int :rtype: int &quot;&quot;&quot; nums.sort() res = sum(nums[:3]) for i in range(len(nums) - 2): l = i + 1 r = len(nums) - 1 while l &lt; r: three = nums[i] + nums[l] + nums[r] if abs(three - target) &lt; abs(res - target): res = three if three &gt; target: r -= 1 elif three &lt; target: l += 1 else: return res return res","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://huangzhiyuan.github.io/tags/leetcode/"}]},{"title":"leetcode-15-3sum","slug":"leetcode-15-3sum","date":"2020-02-08T07:33:37.000Z","updated":"2020-03-01T12:15:50.000Z","comments":true,"path":"2020/02/08/leetcode-15-3sum/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/08/leetcode-15-3sum/","excerpt":"给定一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？找出所有满足条件且不重复的三元组。 注意：答案中不可以包含重复的三元组。","text":"给定一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？找出所有满足条件且不重复的三元组。 注意：答案中不可以包含重复的三元组。 示例： 1234567给定数组 nums = [-1, 0, 1, 2, -1, -4]，满足要求的三元组集合为：[ [-1, 0, 1], [-1, -1, 2]] 分析该题目与之前两个数之和类似，可以将a+b+c=0转换成a+b=-c。难点在与排序和去重。 Solution123456789101112131415161718192021222324252627class Solution(object): def threeSum(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: List[List[int]] &quot;&quot;&quot; nums.sort() res = [] for i in range(len(nums)): if i == 0 or nums[i] &gt; nums[i-1]: l = i + 1 r = len(nums) - 1 while l &lt; r: s = nums[i] + nums[l] + nums[r] if s == 0: res.append([nums[i], nums[l], nums[r]]) l += 1 r -= 1 while l &lt; r and nums[l] == nums[l-1]: l += 1 while l &lt; r and nums[r] == nums[r+1]: r -=1 elif s &gt; 0: r -= 1 else: l += 1 return res","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://huangzhiyuan.github.io/tags/leetcode/"}]},{"title":"leetcode-44-trapping-rain-water","slug":"leetcode-44-trapping-rain-water","date":"2020-02-07T06:50:25.000Z","updated":"2020-03-01T12:17:08.000Z","comments":true,"path":"2020/02/07/leetcode-44-trapping-rain-water/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/07/leetcode-44-trapping-rain-water/","excerpt":"题目描述：给定 n 个非负整数表示每个宽度为 1 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水","text":"题目描述：给定 n 个非负整数表示每个宽度为 1 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水 上面是由数组 [0,1,0,2,1,0,1,3,2,1,2,1] 表示的高度图，在这种情况下，可以接 6 个单位的雨水（蓝色部分表示雨水）。 示例： 12输入: [0,1,0,2,1,0,1,3,2,1,2,1]输出: 6 分析对每一列的水，只需要关注当前列，以及左边最高的墙和右边最高的墙种较矮的一个。这里有3种情况： 较矮的墙 &gt; 当前列的墙water = min(max_left, max_right) - current_height 较矮的墙 = 当前列的墙water = min(max_left, max_right) - current_height = 0 较矮的墙 &lt; 当前列的墙water = min(max_left, max_right) - current_height = 0 根据以上思路，只需遍历每一列，然后分别求出这一列对应的两边最高的墙，然后找出较矮的一端和当前墙的高度比较。 12345678910111213141516171819class Solution(object): def trap(self, height): &quot;&quot;&quot; :type height: List[int] :rtype: int &quot;&quot;&quot; sum = 0 for i in range(1, len(height) - 1): # search tallest wall in left left_h = max(height[:i]) # search tallest wall in right right_h = max(height[i+1:]) min_tmp = min(left_h, right_h) if height[i] &lt; min_tmp: sum += (min_tmp - height[i]) return sum 可以注意到，上面解法中，对于每一列，我们在求它左边和右边最高的墙时都是重新遍历了一遍所有高度，在这里我们可以进行优化。用两个数组分别存储第i列左边和右边最高的墙的高度： 12max_left[i] = Max(max_left[i-1], height[i-1])max_right[i] = Max(max_right[i+1], height[i+1]) 实现如下： 12345678910111213141516171819202122class Solution(object): def trap(self, height): &quot;&quot;&quot; :type height: List[int] :rtype: int &quot;&quot;&quot; # search tallest wall in left left_max = [0 for i in range(len(height))] for i in range(1, len(height) - 1): left_max[i] = max(left_max[i-1], height[i-1]) # search tallest wall in right right_max = [0 for i in range(len(height))] for i in range(len(height) - 2, 0, -1): right_max[i] = max(right_max[i+1], height[i+1]) sum = 0 for i in range(1, len(height) - 1): min_tmp = min(left_max[i], right_max[i]) if min_tmp &gt; height[i]: sum += (min_tmp - height[i]) return sum","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://huangzhiyuan.github.io/tags/leetcode/"}]},{"title":"leetcode-11-container-with-most-water","slug":"leetcode-11-container-with-most-water","date":"2020-02-07T06:05:36.000Z","updated":"2020-03-01T12:15:40.000Z","comments":true,"path":"2020/02/07/leetcode-11-container-with-most-water/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/07/leetcode-11-container-with-most-water/","excerpt":"题目描述：给定 n 个非负整数 a1，a2，…，an，每个数代表坐标中的一个点 (i, ai) 。在坐标内画 n 条垂直线，垂直线 i 的两个端点分别为 (i, ai) 和 (i, 0)。找出其中的两条线，使得它们与 x 轴共同构成的容器可以容纳最多的水。 说明：你不能倾斜容器，且 n 的值至少为 2。","text":"题目描述：给定 n 个非负整数 a1，a2，…，an，每个数代表坐标中的一个点 (i, ai) 。在坐标内画 n 条垂直线，垂直线 i 的两个端点分别为 (i, ai) 和 (i, 0)。找出其中的两条线，使得它们与 x 轴共同构成的容器可以容纳最多的水。 说明：你不能倾斜容器，且 n 的值至少为 2。 图中垂直线代表输入数组 [1,8,6,2,5,4,8,3,7]。在此情况下，容器能够容纳水（表示为蓝色部分）的最大值为 49。 示例： 12输入: [1,8,6,2,5,4,8,3,7]输出: 49 分析最朴素的方式，时间复杂度O(n^2),提交结果显示超时 :( 1234567891011121314class Solution(object): def maxArea(self, height): &quot;&quot;&quot; :type height: List[int] :rtype: int &quot;&quot;&quot; sum = 0 l = len(height) for i in range(1, l): for j in range(i): tmp = (i-j) * min(height[i],height[j]) if tmp&gt;sum: sum = tmp return sum 减少内部遍历层数，只遍历一遍的算法，时间复杂度只有O(n)。 12345678910111213141516171819202122class Solution(object): def maxArea(self, height): &quot;&quot;&quot; :type height: List[int] :rtype: int &quot;&quot;&quot; sum = 0 i= 0 j = len(height) - 1 while i&lt;j: h = 0 b = j - 1 if height[i] &lt; height[j]: h = height[i] i += 1 else: h = height[j] j -= 1 tmp = b * h if tmp &gt; sum: sum = tmp return sum","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://huangzhiyuan.github.io/tags/leetcode/"}]},{"title":"leetcode-4-median-of-two-sorted-arrays","slug":"leetcode-4-median-of-two-sorted-arrays","date":"2020-02-07T05:52:06.000Z","updated":"2020-03-01T12:15:24.000Z","comments":true,"path":"2020/02/07/leetcode-4-median-of-two-sorted-arrays/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/07/leetcode-4-median-of-two-sorted-arrays/","excerpt":"题目描述：给定两个大小为 m 和 n 的有序数组 nums1 和 nums2。 请你找出这两个有序数组的中位数，并且要求算法的时间复杂度为 O(log(m + n))。你可以假设 nums1 和 nums2 不会同时为空。","text":"题目描述：给定两个大小为 m 和 n 的有序数组 nums1 和 nums2。 请你找出这两个有序数组的中位数，并且要求算法的时间复杂度为 O(log(m + n))。你可以假设 nums1 和 nums2 不会同时为空。 示例： 123456789101112示例 1:nums1 = [1, 3]nums2 = [2]则中位数是 2.0示例 2:nums1 = [1, 2]nums2 = [3, 4]则中位数是 (2 + 3)/2 = 2.5 分析如果没有算法时间复杂度的要求，最简单朴素的想法如下： 1234567891011121314class Solution(object): def findMedianSortedArrays(self, nums1, nums2): &quot;&quot;&quot; :type nums1: List[int] :type nums2: List[int] :rtype: float &quot;&quot;&quot; tmp = nums1 + nums2 tmp.sort() m = len(tmp) if m%2 == 0: return (tmp[m//2-1] + tmp[m//2]) / 2.0 else: return tmp[m//2]","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://huangzhiyuan.github.io/tags/leetcode/"}]},{"title":"leetcode-1-two-sum","slug":"leetcode-1-two-sum","date":"2020-02-07T05:07:11.000Z","updated":"2020-03-01T12:14:38.000Z","comments":true,"path":"2020/02/07/leetcode-1-two-sum/","link":"","permalink":"http://huangzhiyuan.github.io/2020/02/07/leetcode-1-two-sum/","excerpt":"题目描述：给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那 两个 整数，并返回他们的数组下标。","text":"题目描述：给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那 两个 整数，并返回他们的数组下标。 你可以假设每种输入只会对应一个答案。但是，你不能重复利用这个数组中同样的元素。 示例: 1234给定 nums = [2, 7, 11, 15], target = 9因为 nums[0] + nums[1] = 2 + 7 = 9所以返回 [0, 1] 分析最直接的思路是用首尾查找，需要一次排序，时间复杂度是O(nlogn)。另外一种简洁高效的方法是使用字典做，时间复杂度是O(n)。 Solution1234567891011121314class Solution(object): def twoSum(self, nums, target): &quot;&quot;&quot; :type nums: List[int] :type target: int :rtype: List[int] &quot;&quot;&quot; hashmap = &#123;&#125; for index, num in enumerate(nums): tmp = target - num if tmp in hashmap: return [hashmap[tmp], index] hashmap[num] = index return None","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://huangzhiyuan.github.io/tags/leetcode/"}]},{"title":"gdb实用小技巧","slug":"gdb-tips","date":"2019-10-09T13:55:22.000Z","updated":"2020-03-01T12:11:58.000Z","comments":true,"path":"2019/10/09/gdb-tips/","link":"","permalink":"http://huangzhiyuan.github.io/2019/10/09/gdb-tips/","excerpt":"本文旨在对程序debug利器gdb工具常用技巧进行汇总。从介绍、信息显示、函数、断电、观察点、打印、多进程、线程、core dump文件、汇编、图形化界面等方面展开。","text":"本文旨在对程序debug利器gdb工具常用技巧进行汇总。从介绍、信息显示、函数、断电、观察点、打印、多进程、线程、core dump文件、汇编、图形化界面等方面展开。 信息显示查看gdb版本信息1(gdb) show version 查看gdb版权信息1(gdb) show copying 启动时不显示提示信息1$ gdb -q 退出时不显示提示信息12(gdb) set confirm off可以把这个命令加到.gdbinit文件里 函数列出函数名字123(gdb) info functionsor(gdb) info functions regex 是否进入到调试信息的函数12step (s) 进入函数；next (n) 不进入函数 退出正在调试的函数 finish 函数会继续执行完，并且打印返回值，然后等待输入接下来的命令。 return 函数不会继续执行下面的语句，直接返回。也可以用 return expression 命令指定函数的返回值。 打印函数堆栈帧信息使用gdb调试程序时，可以使用“i frame”命令显示函数堆栈帧信息。 123i framei registersdisassemble func 选择函数堆栈帧123frame n (n是层数)orframe addr (addr是堆栈地址) 断点命名空间设置断点1(gdb) b Foo::foo 在程序地址上打断点1b *address 当调试汇编程序时，或者没有调试信息的程序时，可用此法。 在程序入口处打断点12345678910111213141516171819202122(base) huang@mlt:~/gdb$ strip hello(base) huang@mlt:~/gdb$ readelf -h helloELF Header: Magic: 7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00 Class: ELF64 Data: 2&#x27;s complement, little endian Version: 1 (current) OS/ABI: UNIX - System V ABI Version: 0 Type: EXEC (Executable file) Machine: Advanced Micro Devices X86-64 Version: 0x1 Entry point address: 0x400470 Start of program headers: 64 (bytes into file) Start of section headers: 4472 (bytes into file) Flags: 0x0 Size of this header: 64 (bytes) Size of program headers: 56 (bytes) Number of program headers: 9 Size of section headers: 64 (bytes) Number of section headers: 29 Section header string table index: 28 当调试没有调试信息的程序时，直接运行start命令是没有效果的： 12(gdb) startFunction &quot;main&quot; not defined 如果不知道main在何处，那么可以在入口处打断点。先通过readelf或者进入gdb，执行info files获得入口地址，然后： 12(gdb) b *0x400470(gdb) r 在文件行号上打断点123(gdb) b nor(gdb) b file:linenum 设置临时断点1tb a.c:15 断点只生效一次，使用tbreak命令(tb)。 设置条件断点1break ... if cond 观察点设置观察点12345(gdb) watch nor(gdb) p &amp;a$1 = (int *) 0x6009c8 &lt;a&gt;(gdb) watch *(int*)0x6009c8 只有当一个变量值发生变化时，程序会停下来。 1info watchpoints 查看所有观察点。 打印打印ASCII和宽字符字符串123456789#include &lt;stdio.h&gt;#include &lt;wchar.h&gt;int main()&#123; char str1[] = &quot;abcd&quot;; wchar_t str2[] = L&quot;abcd&quot;; return 0;&#125; 使用gdb调试时，使用“x/s”命令打印ASCII字符串。 123456789(gdb) x/s str10x804779f: &quot;abcd&quot;(gdb) n7 wchar_t str2[] = L&quot;abcd&quot;;(gdb) p sizeof(wchar_t)$1 = 4(gdb) x/ws str20x8047788: U&quot;abcd&quot; 4个字节用“x/ws”，两个字节，则用”x/hs”命令。 123456789101112131415161718(gdb) print a$11 = &quot;\\000\\001\\002\\003\\004\\005\\006\\a\\b\\t\\n\\v\\f\\r\\016\\017&quot;(1)16进制格式打印数组a前16个byte的值(gdb) x/16x a0x7fffffffdf30: 0x00 0x01 0x02 0x03 0x04 0x05 0x06 0x070x7fffffffdf38: 0x08 0x09 0x0a 0x0b 0x0c 0x0d 0x0e 0x0f(2)以无符号10进制格式打印数组a前16个byte的值(gdb) x/16u a0x7fffffffdf30: 0 1 2 3 4 5 6 70x7fffffffdf38: 8 9 10 11 12 13 14 15(3)2进制格式打印数组a前16个byte的值(gdb) x/16t a0x7fffffffdf30: 00000000 00000001 00000010 00000011 00000100 00000101 00000110 000001110x7fffffffdf38: 00001000 00001001 00001010 00001011 00001100 00001101 00001110 00001111(4)以16进制格式打印数组第5到8 4个元素的值(gdb) x/4x a[5]@40x7fffffffdf35: 0x05 0x06 0x07 0x08 打印数组中任意连续元素值12(gdb) p array[index]@numindex是数组索引，从0开始计数，num是连续多少个元素 打印数组元素下标12(gdb) set print array-indexes on(gdb) p num 打印函数局部变量的值123(gdb) bt fullor(gdb) info locals 打印进程内存信息123(gdb) i proc mappingsor(gdb) i files 打印内存的值123456789#include &lt;stdio.h&gt;int main() &#123; int i = 0; char a[100]; for (i=0; i&lt;sizeof(a); i++) &#123; a[i] = i; &#125; return 0;&#125; gdb中使用“x”命令来打印内存的值，格式为“x/nfu addr”。含义为以f格式打印从addr开始的n个长度为u的内存值。具体参数含义如下： n: 输出单元的个数。 f： 输出格式。比如f是16进制；o是8进制。 u：标明一个单位的长度。b是一个byte，h是两个byte(half word)，w是四个byte (word)，g是八个byte (giant word)。 汇编设置汇编指令格式1(gdb) disassemble main 将源程序和汇编指令映射起来源代码： 123456789#include &lt;stdio.h&gt;int main() &#123; int i = 0; char a[100]; for (i=0; i&lt;sizeof(a); i++) &#123; a[i] = i; &#125; return 0;&#125; 使用“disas /m fun”命令将函数代码和汇编指令映射起来。 1234567891011121314151617181920212223242526272829303132333435363738394041(gdb) disas /m mainDump of assembler code for function main:3 int main()&#123; 0x0000000000400546 &lt;+0&gt;: push %rbp 0x0000000000400547 &lt;+1&gt;: mov %rsp,%rbp 0x000000000040054a &lt;+4&gt;: sub $0x30,%rsp 0x000000000040054e &lt;+8&gt;: mov %fs:0x28,%rax 0x0000000000400557 &lt;+17&gt;: mov %rax,-0x8(%rbp) 0x000000000040055b &lt;+21&gt;: xor %eax,%eax4 int i = 0; 0x000000000040055d &lt;+23&gt;: movl $0x0,-0x24(%rbp)5 char a[16];6 for(i=0; i&lt;sizeof(a); ++i) 0x0000000000400564 &lt;+30&gt;: movl $0x0,-0x24(%rbp) 0x000000000040056b &lt;+37&gt;: jmp 0x40057f &lt;main+57&gt; 0x000000000040057b &lt;+53&gt;: addl $0x1,-0x24(%rbp) 0x000000000040057f &lt;+57&gt;: mov -0x24(%rbp),%eax 0x0000000000400582 &lt;+60&gt;: cmp $0xf,%eax 0x0000000000400585 &lt;+63&gt;: jbe 0x40056d &lt;main+39&gt;7 a[i] = i; 0x000000000040056d &lt;+39&gt;: mov -0x24(%rbp),%eax 0x0000000000400570 &lt;+42&gt;: mov %eax,%edx 0x0000000000400572 &lt;+44&gt;: mov -0x24(%rbp),%eax 0x0000000000400575 &lt;+47&gt;: cltq 0x0000000000400577 &lt;+49&gt;: mov %dl,-0x20(%rbp,%rax,1)8 return 0;=&gt; 0x0000000000400587 &lt;+65&gt;: mov $0x0,%eax9 &#125; 0x000000000040058c &lt;+70&gt;: mov -0x8(%rbp),%rcx 0x0000000000400590 &lt;+74&gt;: xor %fs:0x28,%rcx 0x0000000000400599 &lt;+83&gt;: je 0x4005a0 &lt;main+90&gt; 0x000000000040059b &lt;+85&gt;: callq 0x400420 &lt;__stack_chk_fail@plt&gt; 0x00000000004005a0 &lt;+90&gt;: leaveq 0x00000000004005a1 &lt;+91&gt;: retqEnd of assembler dump. 查看某行所对应的地址范围，使用“disassemble [start], [end]”命令： 123456789101112131415161718Breakpoint 4, main () at mem.c:44 int i = 0;(gdb) l1 #include &lt;stdio.h&gt;23 int main()&#123;4 int i = 0;5 char a[16];6 for(i=0; i&lt;sizeof(a); ++i)7 a[i] = i;8 return 0;9 &#125;(gdb) i line 4Line 4 of &quot;mem.c&quot; starts at address 0x40055d &lt;main+23&gt; and ends at 0x400564 &lt;main+30&gt;.(gdb) disassemble 0x40055d, 0x400564Dump of assembler code from 0x40055d to 0x400564:=&gt; 0x000000000040055d &lt;main+23&gt;: movl $0x0,-0x24(%rbp)End of assembler dump. 打印寄存器的值123(gdb) i registers(gdb) i all-registers(gdb) i registers eax 改变字符串的值123456789#incldue &lt;stdio.h&gt;int main() &#123; char p1[] = &quot;sam&quot;; char *p2 = &quot;Bob&quot;; printf(&quot;p1 is %s\\n&quot;, p1); printf(&quot;p2 is %s\\n&quot;, p2); return 0;&#125; 可以使用set命令改变字符串的值： 1234567891011121314151617181920212223242526(gdb) rStarting program: /home/huang/gdb/setp1 is samBreakpoint 1, main () at set.c:77 printf(&quot;p2 is %s\\n&quot;, p2);(gdb) l2 int main() &#123;3 char p1[] = &quot;sam&quot;;4 char *p2 = &quot;Bob&quot;;56 printf(&quot;p1 is %s\\n&quot;, p1);7 printf(&quot;p2 is %s\\n&quot;, p2);8 return 0;9 &#125;(gdb) print p1$1 = &quot;sam&quot;(gdb) print p2$2 = 0x400694 &quot;Bob&quot;(gdb) set p1=&quot;abc&quot;(gdb) set p2=&quot;zzz&quot;(gdb) np2 is zzzBreakpoint 2, main () at set.c:88 return 0; 设置变量的值1(gdb) set var variable=expr 跳转到指定位置1(gdb) jump line 显示共享链接库信息1(gdb) info sharedlibrary regex 脚本配置gdb init文件配置文件：.gdbinit： 123456789101112131415# 保存历史命令set history filename ~/.gdb_historyset history save on# 退出时不显示提示信息set confirm off# 按照派生类型打印对象set print object on# 打印数组的索引下标set print array-indexes on#每行打印一个结构体成员set print pretty on 图形化界面进入和退出图形化调试界面1234567$ gdb -tui program# 显示汇编窗口(gdb) layout asmor(gdb)layout split# 显示寄存器窗口(gdb) layout regs 缩写技巧12345678910111213141516171819202122232425b -&gt; breakc -&gt; continued -&gt; deletef -&gt; framei -&gt; infoj -&gt; jumpl -&gt; listn -&gt; nxetp -&gt; printr -&gt; runs -&gt; stepu -&gt; until----aw -&gt; awatchbt -&gt; backtracedir -&gt; directorydisas -&gt; disassemblefin -&gt; finishig -&gt; ignoreni -&gt; nextirw -&gt; rwatchsi -&gt; stepitb -&gt; tbreakwa -&gt; watchwin -&gt; winheight 来源：https://github.com/hellogcc/100-gdb-tips","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"gdb","slug":"gdb","permalink":"http://huangzhiyuan.github.io/tags/gdb/"}]},{"title":"第二章、编译和链接","slug":"chapter-two-build-and-link","date":"2019-09-23T14:07:30.000Z","updated":"2020-03-01T12:07:46.000Z","comments":true,"path":"2019/09/23/chapter-two-build-and-link/","link":"","permalink":"http://huangzhiyuan.github.io/2019/09/23/chapter-two-build-and-link/","excerpt":"IDE和编译器提供的默认配置、编译和链接参数对于大部分的应用程序开发而言已经足够使用。但是在这样的开发过程中，我们往往会被这些复杂的集成工具所提供的的强大功能所诱惑，很多系统软件的运行机制与机理被掩盖，程序的很多莫名其妙的错误让我们无所适从，面对程序运行时种种性能瓶颈我们束手无策。","text":"IDE和编译器提供的默认配置、编译和链接参数对于大部分的应用程序开发而言已经足够使用。但是在这样的开发过程中，我们往往会被这些复杂的集成工具所提供的的强大功能所诱惑，很多系统软件的运行机制与机理被掩盖，程序的很多莫名其妙的错误让我们无所适从，面对程序运行时种种性能瓶颈我们束手无策。 被隐藏了的过程简单的的“Hello World”程序，在Linux下，当使用GCC来编译时 123$gcc hello.c$./a.outHello World! 事实上，上述过程可以分解为4个步骤，分别是 预处理（prepressing) 编译 （compilation） 汇编 （assembly） 链接 （link) 预编译预编译讲stdio.h等被预编译成一个.i文件。C++程序的头文件可能是.cpp或.cxx，预编译后的文件扩展名是.ii。第一步预编译相当于如下命令(-E表示只预编译) 1$gcc -E hello.c -o hello.i 预编译过程主要处理那些源代码文件中以“#”开始的预编译指令，比如“#include”、“#define”等，主要规则如下： 将所有“#define”删除，并且展开所有的宏定义。 处理所有条件预编译指令，比如 #if、#indef、#elif、#else、#endif。 处理#indclude预编译指令，将被包含的文件插入到该预编译指令的位置。 删除所有的注释 “//”和“/**/”。 添加行号和文件名标识，以便于编译时编译器产生调试编译器产生调试用的行号信息及用于编译时产生编译错误或警告时能够显示行号。 保留所有的#pragma编译器指令。 当无法判断宏定义是否正确或头文件包含是否正确时，可以查看预编译后的文件来确定问题。 编译编译过程就是把预处理的文件进行一系列词法分析、语法分析、语义分析及优化后生成相应的汇编代码文件，这个过程往往是整个程序的核心。 1$gcc -S hello.i -o hello.s 实际上gcc这个命令只是这些后台程序的包装，它会根据不同的参数要求去调用预编译编译程序cc1、汇编器as和链接器ld。 汇编汇编器试讲汇编代码转变成机器可以执行的指令。 12345$as hello.s -o hello.oor$gcc -c hello.s -o hello.lor$gcc -c hello.c -o hello.o 链接1$ld -static 1.o 2.o -lgcc 编译器做了什么编译器是将高级语言翻译成机器语言的一个工具。高级语言使程序开发者可以更加关注程序逻辑的本身，而尽量少考虑计算机本身的限制，如字长、内存大小、通信方式、存储方式等。高级语言的开发效率大大提高。编译过程一般分为6步：扫描、语法分析、语义分析、源代码优化、代码生成和目标代码优化。整个过程如图所示： 词法分析源代码source code被输入到扫描器（Scanner)，扫描器的任务很简单，就是简单的进行语法分析，将代码的字符序列分割成一系列的记号（Token）。词法分析产生的记号一般可以分为如下几类：关键字、标识符、字符量（包含数字、字符串等）和特殊符号（如加好、等号） 语法分析语法分析器（Grammer Parser)将对扫描器产生的记号进行语法分析，从而产生语法书（Syntax Tree)。整个过程采用了上下文无关语法（Contex-free Grammer)。由语法分析器生成的语法书就是以表达式（Expression）为节点的数。 语义分析语法分析仅仅是完成了对表达式的语法层面的分析，但是它并不了解这个语句是否真正有意义。编译器所能分析的是静态语义（Static Semantic)，所谓静态语义是指在编译期可以确定的语义，与之对应的是动态语义（Dynamic Semantic）就是只要在运行期间才能确定的语义。静态语义通常包括声明和类型的匹配。 中间语言的生成 现代的编译器有着很多层次的优化，往往在源代码级别会有一个优化过程。我们这里所描述的源码优化器（Source Code Optimizer)在不同编译器中可能会有不同的定义或有一些其他的差异。源代码级优化器会在源代码级别进行优化。如上例中的表达式（2+6）这个表达式可能会优化掉，因为它的值在编译器就可以被确定。类似的还要其他的复杂的优化过程。经过优化后的语法书如下图所示： 直接在语法树进行优化比较困难，所以源代码优化器往往将整个语法书转换成中间代码（Intermediate Code), 它是语法树的顺序表示。中间代码有很多种类型，在不同的编译器中有着不同的形式，比较常见的是：三地址码（Three-address Code)和P-代码，最基本的三地址码是这样的： 1x = y op z 比如 1234t1 = 2 + 6t2 = index + 4t3 = t2 * t1array[index] = t3 为了使所有的操作都符合三地址码的形式，这里利用了几个临时变量，t1，t2和t3，经过优化： 123t2 = index + 4t2 = t2 * 8array[index] = t2 中间代码可以使得编译器分为前端和后端。编译器前端负责产生机器无关的中间代码，编译器后端将中间代码转换成目标机器代码。 目标代码生成与优化 编译器后端主要包括代码生成器（Code Generate)和目标代码优化器（Target Code Optimizer）。 链接器链接器年龄比编译器长 模块本质-静态链接链接的主要内容是把各个模块直接相互引用的部分都处理好，使得各个模块直接能够正确的衔接。链接过程主要包括了地址和空间分配、符号决议和重定位。 最基本的链接过程如图所示。","categories":[{"name":"读书","slug":"读书","permalink":"http://huangzhiyuan.github.io/categories/%E8%AF%BB%E4%B9%A6/"}],"tags":[{"name":"程序员的自我修养","slug":"程序员的自我修养","permalink":"http://huangzhiyuan.github.io/tags/%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/"}]},{"title":"深度学习框架的来龙去脉","slug":"DeepLearning-Framework","date":"2019-09-19T14:13:09.000Z","updated":"2020-03-01T12:09:42.000Z","comments":true,"path":"2019/09/19/DeepLearning-Framework/","link":"","permalink":"http://huangzhiyuan.github.io/2019/09/19/DeepLearning-Framework/","excerpt":"哪一个深度学习框架是最好用呢？哪一个深度学习框架更适合自己呢？这真是一个十分让人纠结的问题。不过，在你选择站队之前，不妨和我一起来了解一下各个框架的来龙去脉，先看一看哪一个框架更能激发起你的兴趣，有了兴趣，剩下的问题就变得简单了，我们能够透过浮躁的排名和对比，深入研究这些框架背后的设计思想与技术本质。让每一个框架的优点都能为我所用。","text":"哪一个深度学习框架是最好用呢？哪一个深度学习框架更适合自己呢？这真是一个十分让人纠结的问题。不过，在你选择站队之前，不妨和我一起来了解一下各个框架的来龙去脉，先看一看哪一个框架更能激发起你的兴趣，有了兴趣，剩下的问题就变得简单了，我们能够透过浮躁的排名和对比，深入研究这些框架背后的设计思想与技术本质。让每一个框架的优点都能为我所用。 深度学习框架概述与深度学习技术的四大阵营人工智能从学术理论研究到生产应用的产品化开发过程中通常会涉及到多个不同的步骤和工具，这使得人工智能开发依赖的环境安装、部署、测试以及不断迭代改进准确性和性能调优的工作变得非常繁琐耗时也非常复杂。为了简化、加速和优化这个过程，学界和业界都作了很多的努力，开发并完善了多个基础的平台和通用工具，也被称会机器学习框架或深度学习框架。有了这些基础的平台和工具，我们就可以避免重复发明轮子，而专注于技术研究和产品创新。这些框架有早期从学术界走出的Caffe、 Torch和Theano，到现在产业界由Google领导的TensorFlow，Amazon选择押注的MXNet，Facebook倾力打造的PyTorch，Microsoft内部开源的CNTK等等。 当前主流的深度学习框架列表： TensorFlow Keras PyTorch MXNet Caffe Caffe2 Theano FastAI CNTK Gluon PyTorch PaddlePaddle Chainer 其它小众深度学习框架还有如亚马逊曾经使用的深度学习引擎DSSTNE，卡耐基梅隆大学的小众DyNet，Intel开源的基于Spark且只能运行在Intel CPU芯片之上的深度学习库BigDL，Nervana开发的基于Python的深度学习库Neon，简洁无依赖且符合C++11标准的tiny-dnn，基于Theano的轻量级的库Lasagne等。 这其中有几个框架曾经辉煌过，但是现在已经被更新的框架取代了，或者被新一代的版本收编了，我们称他们为没落的贵族，如Torch，基于Python的版本PyTorch已经取代了Torch。Caffe与Caffe2，Facebook已经把Caffe和Caffe2与PyTorch进行了整合，推出了全新的PyTorch 1.0，资历最老的框架之一Theano的创始人已经转投TensorFlow，已经停止更新维护走向没落了。 基于Java和Scala的开源的分布式深度学习库Deeplearning4J在深度学习领域缺少像Python这样强大的社区支持，日本公司Preferred Networks开源的Chainer曾经是动态计算图的首选框架，特别适用于自然语言处理，但后来MXNet，PyTorch等也支持了这个特性，其优势也不复存在了。由于没有巨头的支持，Deeplearning4J和Chainer都只能默默的发展。 看起来我们好像有很多很多选择，但其实如果我们进一步进行细分，就会发现我们的选择也并不是很多，没有巨头背书的框架就只能面临被淘汰和边缘化的命运了，其实顶级深度学习框架只有四大阵营，或者说是四大门派，Google领导的TensorFlow，Amazon选择的MXnet，Facebook倾力打造的PyTorch，Microsoft把内部核心技术开源的CNTK。虽然Keras等框架在深度学习框架中排名很高，但它却不是一个独立框架，而是作为前端对底层引擎进行上层封装的高级API层，提升易用性，此类深度学习框架的目标是只需几行代码就能让你构建一个神经网络，这类框架还有FastAI和Gluon。好在每一个前端上层轻量级框架又都对应一个最适合的基础底层框架，这样就出现了深度学习框架的四大技术方向，每一个技术方向背后又都有一个巨头在背书和推动。 深度学习框架的四大阵营与其技术方向分别为： TensorFlow，前端框架Keras，背后巨头Google； PyTorch，前端框架FastAI，背后巨头Facebook； MXNet，前端框架Gluon，背后巨头Amazon； Cognitive Toolkit (CNTK)，前端框架Keras或Gluon，背后巨头Microsoft。 主流开源深度学习框架的来龙去脉Google在2015年11月正式开源发布TensorFlow，TensorFlow由Google大脑团队开发，其命名来源于本身的运行原理。由于Google的巨大影响力和巨大支持，很快就成为深度学习领域占据绝对统治地位的框架。很多企业都在基于TensorFlow 开发自己的产品或将 TensorFlow整合到自己的产品中去，如Airbnb、Uber、Twitter、英特尔、高通、小米、京东等。 Google大脑一开始是Google X的一个研究项目，2011年，Google大脑的雏形起源于一项斯坦福大学与谷歌公司的联合研究项目，由Google Fellow杰夫·迪恩（Jeff Dean）、研究员格雷科拉多（Greg Corrado）与斯坦福大学人工智能教授吴恩达（Andrew Ng）共同发起，把深度学习技术带到了人工智能问题的解决中，并建立起了第一代大型深度学习软件系统DistBelief，这是一个运行在谷歌云计算平台上的一个服务。随后，Google在其商业产品广泛应用部署了DistBelief的深度学习神经网络，包括搜索、YouTube、语音搜索、广告、相册、地图、街景和Google翻译等。 2013年3月，Google收购了DNNResearch，DNNResearch创始人Geoffrey Hinton也由此进入Google大脑团队工作。Google指派Geoffrey Hinton和Jeff Dean等开始简化和重构DistBelief的代码库，使其变成一个更快、更健壮的应用级别代码库，形成了TensorFlow。对DistBelief 进行了多方面的改进后，使其可在小到一部手机，大到数千台数据中心服务器的各种设备上运行，TensorFlow也成为了基于 DistBelief 进行研发的第二代人工智能学习系统，可被用于语音识别或图像识别等多想机器学习和深度学习领域。一个突破性的成果是在2012年6月Google大脑用16000台电脑模拟人类电脑的活动，并在学习1000万张图像后，成功在YouTube视频中找出了“猫”，这可能意味着机器第一次有了猫的概念。 Google在2014年1月又收购了英国DeepMind，DeepMind成为谷歌大脑之外另一个研究人工智能方向的团队。DeepMind在4年来的首要的人工智能研究平台是开源软件Torch7机器学习库，Torch7非常灵活和快速，能够快速建模。在Google的大力支持下，AlphaGo横空出世，使人工智能第一次战胜人类职业围棋高手，轰动世界，以一己之力推动人工智能应用到了一个新的高度。2016年4月，DeepMind宣布将开始在将来所有的研究中开始使用TensorFlow。这样Google的两大人工智能团队也统一到了统一的深度学习框架TensorFlow上。 TensorFlow的编程接口支持C++和Python，Java、Go、R和Haskell API也将被支持，是所有深度学习框架中对开发语言支持的最全面的，TensorFlow可以在AWS和Google Cloud中运行，支持Windows 7、Windows 10、Windows Server 2016，TenserFlow使用C++ Eigen库，可以在ARM架构上编译和优化，使其可以在各种服务器和移动设备上部署自己的训练模型，也是在所有深度学习框架中支持运行平台最多的。TensorFlow基于计算图实现自动微分系统，使用数据流图进行数值计算，图中的节点代表数学运算，图中的线条则代表在这些节点之间传递的张量（多维数组）。TensorFlow追求对运行平台和开发语言最广泛的支持，力求统一深度学习领域，但是这也带来了过于复杂的系统设计，TensorFlow在GitHub上的总代码数已经超过100万行了，TensorFlow在接口设计中创造了很多新的抽象概念，如图、会话、命名空间和Place-Holder等，同一个功能又提供了多种实现，使用上可能有细微的区别，频繁的接口变动也导致了向后兼容性上的问题。由于直接使用TensorFlow过于复杂，包括Google官方在内的很多开发者尝试构建一个高级API作为TensorFlow更易用的接口，包括Keras、Sonnet、TFLearn、TensorLayer、Slim、Fold、PrettyLayer等，其中Keras在2017年成为第一个被Google添加到TensorFlow核心中的高级别框架，这让Keras变成TensorFlow的默认API，使Keras + TensorFlow的组合成为Google官方认可并大力支持的平台。TensorFlow仍在快速的发展中，是最具野心和最具统治力的深度学习框架，十分期待2019年即将发布的TensorFlow 2.0带来的新技术。 Keras是第二流行的深度学习框架，但并不是独立框架。Keras由纯Python编写而成，以TensorFlow、Theano或CNTK为底层引擎。Keras是在Tensorflow上层封装的高级API层，提升易用性。Keras的目标是只需几行代码就能让你构建一个神经网络。 Keras的创造者是谷歌AI研究员Francois Chollet，也同时参与TensorFlow的开发，最初创建Keras是为了自己有一个好的工具来使用RNNs。在研究LSTM在自然语言处理中的应用时用Theano做了一个可重用的开源实现，逐渐变成了一个框架，并命名为Keras。Keras在2015年3月开源，最初因为同时支持CNN和RNN，可以通过Python代码而不是通过配置文件来定义模型等特点而逐渐流行起来。2017年，Keras成为第一个被Google添加到TensorFlow核心中的高级别框架，这让Keras变成Tensorflow的默认API，使Keras + TensorFlow的组合成为Google官方认可并大力支持的平台。 学习使用Keras很容易，但是大多数时间都在学习如何调用接口，难以真正学习到深度学习的内容，Keras层层封装让用户在新增操作或获取底层的数据信息时过于困难，存在过度封装导致缺乏灵活性的问题，性能也存在瓶颈。Keras有助于快速入门，但是不应该依赖它，需要进一步学习使用TensorFlow。 PyTorch是Facebook开发的用于训练神经网络的Python包，也是Facebook倾力打造的首选深度学习框架，在2017年1月首次推出，Facebook人工智能研究院（FAIR）在GitHub上开源了PyTorch，迅速占领了GitHub热度榜榜首，Facebook用Python重写了基于Lua语言的深度学习库Torch，PyTorch不是简单的封装Torch提供Python接口，而是对Tensor上的全部模块进行了重构，新增了自动求导系统，使其成为最流行的动态图框架，这使得PyTorch对于开发人员更为原生，与TensorFlow相比也更加年轻更有活力，PyTorch继承了Torch灵活、动态的编程环境和用户友好的界面，支持以快速和灵活的方式构建动态神经网络，还允许在训练过程中快速更改代码而不妨碍其性能，即支持动态图形等尖端AI模型的能力，是快速实验的理想选择。 PyTorch专注于快速原型设计和研究的灵活性，很快就成为AI研究人员的热门选择，流行度的增长十分迅猛，现在已经是第二流行的独立框架。PyTorch的社区迅速发展起来。PyTorch 现在是GitHub 上增长速度第二快的开源项目，在过去的12个月里，贡献者增加了2.8倍。这个增速是十分可怕的，意味着PyTorch成为现在最受关注的深度学习框架，能够挑战TensorFlow的霸主地位。 2018年12月08号，在 NeurIPS 大会上，Facebook 正式发布 PyTorch 1.0稳定版，目前领导PyTorch 1.0核心开发工作的是Facebook的AI基础设施技术负责人Dmytro Dzhulgakov。Caffe的作者贾扬清发文介绍PyTorch 1.0 = Caffe2 + PyTorch。现在，PyTorch 1.0已经为大量Facebook产品和服务提供了支持，包括每天执行60亿次文本翻译。 根据贾扬清发文介绍，PyTorch 1.0拥有能在命令式执行模式和声明式执行模式之间无缝转换的混合前端，这样就不需要开发人员通过重写代码来优化性能或从Python迁移，能够无缝地共享用于原型设计的即时模式和用于生产环境的图执行模式之间的大部分代码。PyTorch 1.0将即时模式和图执行模式融合在一起，既具备研究的灵活性，也具备生产所需的最优性能。 PyTorch 1.0重构和统一了Caffe2和PyTorch 0.4框架的代码库，删除了重复的组件并共享上层抽象，得到了一个统一的框架，支持高效的图模式执行、移动部署和广泛的供应商集成等。这让开发人员可以同时拥有PyTorch和Caffe2的优势，同时做到快速验证和优化性能。PyTorch的命令式前端通过其灵活而高效的编程模型实现了更快速的原型设计和实验，又吸取了Caffe2和ONNX的模块化以及面向生产的特点，使得深度学习项目能从研究原型快速无缝衔接到生产部署，在一个框架中统一实验研究和生产能力。 Theano最早始于2007，以一个希腊数学家的名字命名，早期开发者是蒙特利尔大学的Yoshua Bengio 和 Ian Goodfellow。Theano是最老牌和最稳定的库之一，是第一个有较大影响力的Python深度学习框架，早期的深度学习库的开不是Caffe就是Theano。 Theano是一个比较底层的Python库，这一点上和TensorFlow类似，专门用于定义、优化和求值数学表达式，效率高，非常适用于多维数组，所以特别适合做机器学习。Theano可以被理解为一个数学表达式的编译器，Theano框架会对用符号式语言定义的程序进行编译，来高效运行于 GPU 或 CPU上。但是Theano不支持分布式计算，这使其更适合于在实验室的学习入门，并不适用于大型的工业界的项目，这可能是其技术上落后的一个重要原因。 Theano来自学界，它最初是为学术研究而设计，这使得深度学习领域的许多学者至今仍在使用 Theano，但Theano在工程设计上有较大的缺陷，有难调试，构建图慢的缺点，开发人员在它的基础之上，开发了Lasagne、Blocks、PyLearn2和Keras上层接口封装框架。但随着 Tensorflow 在谷歌的大力支持下强势崛起，使用Theano的人已经越来越少了。标志性的事件就是创始者之一的 Ian Goodfellow 放弃 Theano 转去谷歌开发 Tensorflow了。而另一位创始人Yoshua Bengio 于 2017 年 09 月宣布不再维护 Theano，所以这个项目事实上已经宣告死亡了。基于 Theano 的前端轻量级的神经网络库，如 Lasagne和Blocks也同样没落了。但我们可以说，Theano作为第一个主要的Python深度学习框架，已经完成了自己的使命，为早期的研究人员提供了强大的工具和很大的帮助，为后来的深度学习框架奠定了以计算图为框架核心 ，采用GPU加速计算的基本设计理念。 Caffe的全称是Convolutional Architecture for Fast Feature Embedding，意为“用于特征提取的卷积架构”，它是一个清晰、高效的深度学习框架，核心语言是C++。作者是贾扬清，贾扬清在清华大学获得本科和硕士学位。在UC Berkeley获得计算机科学博士学位。他曾在Google Brain工作，参与过TensorFlow的开发。贾扬清现在是Facebook AI架构总监，但据2019年3月的最新消息贾扬清可能加入阿里巴巴任VP。Caffe最初发起于2013年9月，从贾扬清用NVIDIA的学术捐赠的一块K20 GPU开始攒了一个机器开始，然后用大概两个多月的时间写了整个架构和ImageNet所需要的各个实现。12月份正式在Github上发布开源。 Caffe是一款十分适合深度学习入门的开源框架，它的代码和框架都比较简单，代码易于扩展，运行速度快，也适合深入学习分析。正是由于Caffe有着更小的系统框架，使得一些探索性的实验更加容易一些。即使在Google工作时，贾扬清仍然会经常使用Caffe来做一些快速的prototype和实验，Google的研究人员通常使用各种自己熟悉的开源框架来进行小规模的研究，然后用DistBelief（Google Brain的大规模的机器学习框架）来作为一个通用的框架实现大规模产品级别的部署。 在Caffe之前，深度学习领域缺少一个完全公开所有的代码、算法和各种细节的框架，导致很多的研究人员和博士需要一次又一次重复实现相同的算法，所以说Caffe对于深度学习开源社区的贡献非常大，Caffe是学术界和业界公认的最老牌的框架之一，是很多人入门的基础。 Caffe不支持分布式，与其它更新的深度学习框架相比，Caffe确实不够灵活，文档也不够用，Caffe的安装也比较复杂，安装需要解决大量的依赖包。大家会发现套用原有模型很方便，但个性化就要读源代码，灵活性明显不足，为模型做调整常常需要用 C++ 和 CUDA编程，虽然使用Python 和Matlab 也能做一些小调整。与Keras过度封装导致缺乏灵活性不同，Caffe缺乏灵活性主要是由于其自身的设计，在Caffe中最主要的抽象对象是层，每实现一个新层，必须要利用C++实现其前向传播和反向传播代码，如果需要新层在GPU上运行，还需要同时用CUDA实现这一层的前向传播和反向传播，这让不熟悉C++和CUDA的用户扩展Caffe非常困难。这也是由于Caffe最初定位在科研上面，并假设说大家都会有一定的时间和精力来读代码。 与Theano的没落与终结不同，随着贾扬清在2016年2月加入Facebook，2016年11月，贾扬清在 Facebook 官网发文，介绍了Caffe2go，它使用Unix理念构建的轻量级、模块化框架，核心架构非常轻量化，可以附加多个模块，是一个在手机上也能运行神经网络模型，可以在移动平台上实时获取、分析、处理像素。Caffe2go规模更小、训练速度更快、对计算性能要求较低。Caffe2go是Facebook继Torch后的第二个AI平台，因为其大小、速度和灵活性上的优势，Facebook曾将Caffe2go推上了战略地位，和研究工具链Torch一起组成了Facebook 机器学习产品的核心。 2017年4 月 18 日，Facebook 开源了 Caffe2，Facebook 的AI双平台定位已经清晰了，Caffe2 的开发重点是性能和跨平台部署，PyTorch 则专注于快速原型设计和研究的灵活性。Caffe2一开始的定位就是工业界产品级别的一个轻量化的深度学习算法框架，更注重模块化，支持大规模的分布式计算，支持跨平台，如同 TensorFlow，Caffe2 使用 C++ Eigen 库，支持 ARM 架构。并且为移动端实时计算做了很多优化，支持移动端iOS, Android, 服务器端Linux, Mac, Windows, 甚至一些物联网设备如Raspberry Pi, NVIDIA Jetson TX2等平台部署。Caffe2将AI生产工具标准化，目前全球各地的Facebook服务器和超过10亿部手机通过Caffe2运行神经网络，其中包含了最新的iPhone和Android手机。 虽然Facebook的Caffe2和PyTorch两个团队一直在独立的发展，但是二者的组件已经被大量共享，双方也意识到将各自的优势特性整合到一个包中的重要性，实现从快速原型到快速部署执行的平稳过渡是有重要意义的，这样也可以轻松地使用共享工具提高开发效率。最终可以将 PyTorch 前端的灵活用户体验与 Caffe2 后端的扩展、部署和嵌入式功能相结合。在2018年12月的 NeurIPS 大会上，Facebook 正式发布 PyTorch 1.0稳定版，支持AWS、谷歌云、微软Azure等云平台。贾扬清发文介绍PyTorch 1.0 = Caffe2 + PyTorch，至此，Facebook的AI深度学习框架正式统一。Caffe, Caffe2, Torch, PyTorch的用户们都不用再纠结了，有了统一明确的技术架构与技术路线，就是PyTorch 1.0。 Torch在2002年诞生于纽约大学Torch，后续加入了深度学习的内容，是一个著名开源深度学习框架，是BSD3协议下的开源项目。由Facebook的Ronan Collobert和Soumith Chintala，Twitter的Clement Farabet，DeepMind的Koray Kavukcuoglu共同开发和维护，所以Torch7自然也成为Facebook和DeepMind一开始使用的深度学习工具，Twitter和英伟达也都使用定制版的Torch用于人工智能研究，DeepMind在被Google收购后转向了TensorFlow。 Torch的编程语言为1990 年代诞生于巴西的 Lua，Lua相当于一个小型加强版的C，支持类和面向对象，运行效率极高，所以需要先学习Lua语言然后才能使用Torch，在开发人员没有熟练掌握Lua之前，使用Torch很难提高开发的整体生产力。其实Lua和Python都属于比较容易入门的语言，但Python很明显已经抢先统治了机器学习领域，大多数开发人员都不愿意为了使用一个框架而学习一门新语言，相反，一些开发人员在学习并掌握一门新语言后才会愿意使用基于这门语言的框架，这一点使Torch的进一步发展受到了限制，并导致Torch推广的困难。 Torch是命令式的，因此与TensorFlow和Theano相比，Torch的灵活度更高，而前两者是陈述式的（declarative），必须declare一个计算图。Torch 非常适用于卷积神经网络，Torch的原生交互界面比其他框架用起来更自然、更得心应手。第三方的扩展工具包提供了丰富的递归神经网络RNN模型。 FastAI不是一个独立的深度学习框架，而是一个基于PyTorch的上层封装的高级API层，提升PyTorch的易用性，目标是只需几行代码就能让你构建一个神经网络。FastAI并不是简单意义上的将PyTorch封装了一遍，而是类似于Keras与TensorFlow的关系，充分借鉴了Keras，将PyTorch的强大性易用化。 FastAI最初的版本在2018年9月发布，FastAI 1.0版本在2018年10月Facebook开发者大会上和Facebook的PyTorch 1.0 rc1预览版一起发布，在实验和测试比拼中，用5行代码就可以完成Keras用31行才能解决的事情，因此，在Hacker News上关注度比PyTorch 1.0还高，FastAI基于PyTorch 1.0框架，也被Facebook官方重点宣传。 MXNet是一个轻量级、可移植、灵活的分布式的开源深度学习框架，也是Amazon官方主推的深度学习框架，MXNet 支持卷积神经网络（CNN）、循环神经网络（RNN）和长短时间记忆网络（LTSM），为图像、手写文字和语音的识别和预测以及自然语言处理提供了出色的工具。MXNet项目诞生于2015年9月，作者是当时在卡耐基梅隆大学CMU读博士的李沐，MXNet 在2016年11月被亚马逊选为官方开源平台，2017年1月23日，MXNet项目进入Apache基金会，成为Apache的孵化器项目。Amazon和Apache的双重认可使其生命力更加强大，成为能够与Google的TensorFlow，Facebook的PyTorch和微软的CNTK分庭抗礼的顶级深度学习框架。值得一提的是，其实MXNet的很多作者都是中国人，其最大的贡献组织为百度。 Amazon的AWS虽然支持TensorFlow等主流深度学习框架，但是Amazon不会傻到为Google做嫁衣而依赖Tensorflow，深度学习技术平台太重要了，Amazon不会受制于人。Amazon之所以选择MXNet作为首选开源深度学习框架与平台与李沐在 CMU 的两位博士导师之一的Alex Smola有很大的关系，2015 年Alex从CMU重返工业界，加入亚马逊AWS 担任机器学习总监。 在2014年NIPS上，同为上海交大校友的陈天奇和李沐在讨论到各自在做深度学习Toolkits的项目时，发现他们都在做很多重复性的工作，如文件加载等，于是他们又拉来几个优秀的C++机器学习系统的开发人员共同成立了DMLC（Distributed (Deep) Machine Learning Community），一个小的机器学习爱好者圈子，号召大家发挥各自所长，一起合作开发，发起了通过配置来定义和训练神经网络的CXXNet和提供类似numpy一样的张量计算接口的Minerva两个深度学习项目，本意是更方便共享各自项目的代码，并给用户提供一致的体验。CXXNet擅长使用卷积神经网络进行图片分类，但它的灵活性不足，用户只能通过配置来定义模型，而无法进行交互式的编程。Minerva则更灵活，但不够稳定，李沐想同时给两个项目做分布式的扩展，后来自然想到把两个项目合并起来，于是就有了MXNet，可以读作“mix net”，其名字来自Minerva的M和CXXNet的XNet。其中Symbol的想法来自CXXNet，而NDArray的想法来自Minerva。 目前主流的深度学习系统一般采用命令式编程（imperative programming，比如 Torch）或声明式编程（declarative programming，比如 Caffe，theano 和 TensorFlow）两种编程模式中的一种，而 MXNet 尝试将两种模式结合起来，在命令式编程上 MXNet 提供张量运算，而声明式编程中 MXNet 支持符号表达式。用户可以根据需要自由选择，同时，MXNet 支持多种语言的 API 接口，包括 Python、C++（并支持在 Android 和 iOS 上编译）、R、Scala、Julia、Matlab 和 JavaScript。 MXNet长期处于快速迭代的过程中，文档却长时间没有更新，导致新用户难以上手，老用户也需要查阅源码才能理解MXNet接口的用法，为了完善MXNet的生态圈并推广，MXNet先后推出了MinPy，Keras和Gluon等高级API封装接口，但目前前两个高级接口已经停止了开发，Gluon模仿了PyTorch的接口设计，成为李沐和Amazon主推的配套MXNet使用的上层API。 MXNet的优势是分布式支持和对内存、显存的明显优化，同样的模型，MXNet往往占用更小的内存和显存，在分布式环境下，MXNet的扩展性能也显示优于其他框架。Keras作Francois Chollet认为除了TensorFlow，MXNet和它的高级API接口Gluon也很有前景，与TensorFlow一样，MXNet是为数不多的具有实际生产级和可扩展性的框架。亚马逊有一个庞大的团队在很认真的支持MXNet，成为了MXNet背后强大的工程力量。 2017年10月20日，Amazon和 Microsoft 联合发布了Gluon，Gluon是一种新的动态计算图的开源深度学习框架的高级接口，简而言之，是一个基于MXNet深度学习框架的类似Keras和FastAI的上层API接口，但其最大的特点是Gluon同时支持灵活的动态图和高效的静态图，支持符号式和命令式编程的API，支持高度可扩展的训练，能够高效的评估模型，可帮助开发人员更轻松、更快速地构建机器学习模型，而不牺牲任何性能。Gluon现已在Apache MXNet 中可用，后续将支持Microsoft Cognitive Toolkit及其他架构。微软Azure的所有服务、工具和基础结构也将全面支持Gluon。 微软的人工智能工具包是CNTK，CNTK 全名为Computational Network Toolkit，2016年1月26日宣布在GitHub上开源，10月份又更命名为微软认知工具包Microsoft Cognitive Toolkit。CNTK最初是面向语音识别的框架，早在2014年，黄学东博士和他的团队正在对计算机能够理解语音的能力进行改进，但是手上的工具却延缓了他们的进度，一组自发组成的团队构想设计了一个全新的方案，由此诞生了CNTK，微软语音识别研究团队在语音识别上不断打破世界纪录并逼近人类水准，使得微软的技术受到广泛关注，在处理图像、手写字体和语音识别问题上，它都是很好的选择。Cognitive Toolkit工具包在微软内部被广泛使用，微软的人工智能工具包跟其他工具包最大的不同在于数据，Cognitive Toolkit的数据都来自于微软自己的大规模生产数据。包括Cortana、Bing以及Cognitive Services中的Emotion API，这些都是用Cognitive Toolkit创建出来的。 CNTK基于C++架构，Python或C++编程接口，CNTK 支持 64 位的 Linux 和 Windows 系统，在 MIT 许可证下发布。支持跨平台的CPU/GPU 部署。CNTK 在 Azure GPU Lab 上显示出最高效的分布式计算性能。但CNTK现在还不支持ARM 架构，使其在移动设备上的功能受到了限制。 Chainer是由日本深度学习创业公司Preferred Networks于2015年6月发布的深度学习框架。最大的特点是支持动态图，曾经是动态计算图的首选框架，特别适用于自然语言处理。Chainer是用Python开发的，支持多种前馈神经网络，包括卷积网络、循环网络、递归网络，支持运行中动态定义的网络（Define-by-Run）。前馈计算可以引入Python的各种控制流，同时反向传播时不受干扰，简化了调试错误的难度。 开放神经网络交换（ONNX，全称是“Open Neural Network Exchange”）格式的发布于2017年9月横空出世。ONNX最初由微软和Facebook联合发布，后来亚马逊也加入进来，并在12月发布了V1版本，宣布支持ONNX的公司还有AMD、ARM、华为、 IBM、英特尔、Qualcomm等。ONNX是一个表示深度学习模型的开放格式。它使用户可以更轻松地在不同框架之间转移模型。例如，它允许用户构建一个PyTorch模型，然后使用MXNet运行该模型来进行推理。ONNX从一开始就支持Caffe2，Microsoft Cognitive Toolkit，MXNet和PyTorch，Google虽然目前还不在这个阵营中，但与其他开源项目一样，社区也已经为TensorFlow添加了一个转换器。 诞生于中国本土的深度学习框架下面要重点介绍一下诞生于中国本土深度学习框架，他们正在崛起： 华为MindSpore2018年10月10日，华为在上海全联接大会上首次发布华为AI战略与全栈全场景AI解决方案，包括Ascend(昇腾)系列AI芯片以及CANN算子库、MindSpore深度学习框架、AI开发平台ModelArts。华为MindSpore支持端、边、云独立的和协同的统一训练和推理框架。但是目前仍然在开发中，以华为在中国科技界地位和研发投入，自然是最受大家期待的。华为云虽然可以支持其它所有主流的深度学习框架，但就如同Amazon选择MXNet一样，这不是一个可以讨论的问题，为了不受制于人，是一定要有的。我相信为了与其它主流框架进行竞争，MindSpore将来也一定会开源的。 百度PaddlePaddle2016年8月底百度开源了内部使用多年的深度学习平台PaddlePaddle，PaddlePaddle 100% 都在Github上公开，没有内部版本。PaddlePaddle能够应用于自然语言处理、图像识别、推荐引擎等多个领域，其优势在于开放的多个领先的预训练中文模型。PaddlePaddle的2013年版本是百度杰出科学家徐伟主导设计和开发的，其设计思路是每一个模型的表示方式都是“一串Layers”， Caffe的作者贾扬清称赞了百度的 PaddlePaddle，并说“整体的设计感觉和 Caffe 心有灵犀”。三年后，百度AI团队在徐伟的指导下作了两次升级，2017年4月推出PaddlePaddle v2，v2参考TensorFlow增加了Operators的概念，把Layers打碎成更细粒度的Operators，同时支持更复杂的网络拓扑图而不只是“串”。2017 年底推出PaddlePaddleFluid。Fluid类似PyTorch，提供自己的解释器甚至编译器，所以不受限于 Python 的执行速度问题。 阿里巴巴XDL (X-Deep Learning)2018年11月，阿里巴巴宣布，其大数据营销平台阿里妈妈将把其应用于自身广告业务的算法框架XDL (X-Deep Learning)进行开源，正式加入开源学习框架的激烈竞争。XDL主要是针对特定应用场景如广告的深度学习问题的解决方案，是上层高级API框架而不是底层框架。XDL需要采用桥接的方式配合使用 TensorFlow 和 MXNet 作为单节点的计算后端，XDL依赖于阿里提供特定的部署环境。 小米MACE2018年6月28日，小米首席架构师、人工智能与云平台副总裁崔宝秋宣布正式开源小米自研的移动端深度学习框架(MACE) Mobile AI Compute Engine。它针对移动芯片特性进行了大量优化，目前在小米手机上已广泛应用，如人像模式、场景识别等。该框架采用与 Caffe2 类似的描述文件定义模型，因此它能非常便捷地部署移动端应用。目前该框架为 TensorFlow 和 Caffe 模型提供转换工具，并且其它框架定义的模型很快也能得到支持。 如何做深度学习框架的选型进入深度学习领域，基础是学习Python。可以说现在进入深度学习领域是相对容易的，在5年前，研究深度学习需要用C++或Matlab来编写大量的低级算法，这需要研究生教育甚至是博士的教育。现在不一样了，你只需要学习Python，就很容易上手，虽然深度学习正在支持越来越多的编程语言，但Python最简单而且应用最广泛的一个，Python最厉害的地方在于其生态系统非常好，有社区的强大支持，比如要装Python，有方便的Anaconda；要用Python visualization，有Matplotlib可以用；要Numerical computation有NumPy和SciPy可以选择，要做图像处理，还有Scikit-image。有很多现成的工具可以使用，可以节省自己大量的时间，这正是工程师所需要的。 在对所有主流深度学习框架有一个了解后，我想是时候舍弃开发语言（基本都支持Python和C++，Java和Lua面向特定社区）、接口简易、文档完善、运算速度、性能、安装部署方便等方面的纯技术比较了，可能在这些框架诞生的初期我们更看重这些方面，但是随着各个框架的不断的完善与大企业的支持与不断的投入，各个框架之间也在不断的相互借鉴，最后的结果就是大家都差不多，各有千秋，我们现在要进入深一层维度的比拼，应该至少考虑下面几个维度： 深度学习框架是否支持分布式计算，是不是分布式框架？ 分布式：TensorFlow、MXNet、PyTorch、CNTK、Caffe2、DL4J 不支持分布式：Caffe、Theano、Torch 深度学习框架是否支持移动端部署？ 支持：PyTorch、MXNet、TensorFlow、Caffe2 不支持：CNTK 编程接口的设计是命令式编程(imperative programming)还是声明式语言(declarative programing)？ 命令式：简单易懂的编程接口PyTorch，NumPy和Torch、TheanoMXNet通过NDarray模块和Gluon高级接口提供了非常类似PyTorch的编程接口。 声明式：TensorFlow、Theano、Caffe 深度学习框架是基于动态计算图还是静态计算图？ 目前使用动态计算图的框架有PyTorch、MXNet、Chainer。 目前使用静态计算图框架有TensorFlow、Keras、CNTK、Caffe/Caffe2、Theano等，其中TensorFlow主要使用了静态计算图，TensorFlow在2018年10月宣布了一个动态计算选项Eager Execution，但该特性还比较新颖可能并不是很成熟，并且 TensorFlow 的文档和项目依然以静态计算图为主。MXNet同时具有动态计算图和静态计算图两种机制。 深度学习框架是否有强大的社区和生态支持？重金打造的TensorFlow，多方押注的MXNet，正在崛起的PyTorh，技术稳重的CNTK，这四大开源深度学习框架都满足这一点。 深度学习框架背后是否有巨头支持？ Google领导的TensorFlow，Amazon选择的MXNet，Facebook倾力打造的PyTorch，Microsoft把内部核心技术开源的CNTK，这四大开源深度学习框架都满足这一点。 通过对上面六个维度的思考，我想大家应该知道该如何作选择了：首先，静态计算图很好，但是动态图是未来和趋势，对于大多数开发者来说，Python是基础，Python的成熟可用的库、工具和生态与社区的支持太重要了；对于深度学习的商业应用而非纯粹的实验室研究来说，支持分布式和移动端运行平台是必选的，将来一定会用到的；前端的编程接口越灵活超好，我们需要考虑不同的应用场景，因此前端编程接口的设计需要兼容简单高效的命令式和逻辑清晰的声明式；深度学习框架一定要有背后巨头的大力支持和强大的社区，有专业的团队不断的更新并完善代码库。这样来看，只有下面的四大顶级深度学习框架阵营才能够满足我们的要求。 深度学习框架的四大阵营与其技术方向分别为：（1）TensorFlow，前端框架Keras，背后巨头Google；（2）PyTorch，前端框架FastAI，背后巨头Facebook；（3）MXNet，前端框架Gluon，背后巨头Amazon；（4）Cognitive Toolkit (CNTK)，前端框架Keras或Gluon，背后巨头Microsoft。 那么在这四大阵营中又如何选择呢？这就要看具体项目的需要了，看重Google无与伦比的巨大影响力的开发者并不需要太过纠结，TensorFlow会支持最广泛的开发语言与最多的运行平台，开发者很难逃出Google的覆盖范围，更多的开发者会被收编，AlphaGo已经帮助Google证明了Google在人工智能上技术领先地位，Keras+TensorFlow的方案已经被Google官方认可，Google的TensorFlow2.0将带来的新技术与突破；喜欢学习新事物和追求完美的开发者一定不能错过Facebook的PyTorch，PyTorch正在强势崛起，是动态图技术的最佳代表，是当前最活跃最有生命力的深度学习框架，这一次Google遇到了真正的对手；Amazon在云计算和云服务上的领先地位带给开发者更大的信心，选择Amazon人工智能背后的技术一定没有错；微软的技术正在不断挑战人类语音识别和图像识别的极限，长期受益于微软阵营的开发人员对于微软开源其核心技术是非常兴奋的，Cognitive Toolkit (CNTK)可以被Keras和Gluon同时支持，这太棒了，确实带给开发者更多的选择。 最后，我们会发现深度学习框架其实只是一个工具和平台，虽然分为四大阵营和四大技术路线，但是得益于这些主流框架之间的不停的比拼与互相借鉴，最后会发现其实大家都差不多，最棒的是这些主流的深度学习框架都是基于Python的，只要掌握了Python和深度学习算法的设计思想，每一种框架都是一个可用的库或工具集，我们是工程师，工程师需要善于学习并善于选择使用最优的工具。初学者可以从上层高级API框架开始学习，如Keras、Gluon和FastAI，但是不能依赖这些层层封装高级API，不然是无法真正掌握深度学习的技术本质的。深入学习并熟练掌握一种顶级深度学习框架是非常重要的，比如PyTorch，然后再跑一跑TensorFlow和MXNet，我们可以在对比中学习，在深度学习领域，可以深刻理解什么是“纸上得来终觉浅”，我觉得学习深度学习及人工智能技术，一定要动手实践，只有动手做过了才是自己的，不然，一切都还是书本上的。 我还想再多谈一点的一个深度学习领域要面对的问题是“我们是否需要自己重新发明轮子？”我们需要自己重新设计并实现一个深度学习的框架吗？ 我想这取决于你的兴趣和你的时间，如果你立志成为一个大神级的专家，有充足的时间和强烈的兴趣，那么根据自己对算法和数据结构的独特理解，在设计思想上融入自己的哲学和艺术的思考，自己从头开始设计并实现一个深度学习框架也未尝不可，发起一个开源项目并聚合一群志合道同的小伙伴一起奋斗，是一件十分有挑战并且十分有趣的事情，我认为我们应该大力鼓励有这样志向的年轻人，哪怕只是为了更深入的学习研究也好呀。何况我们中国人已经有了业界最成功的榜样，前有UC伯克利的贾扬清开发了Caffe，后有CMU卡耐基梅隆的李沐创造了MXNet，他们都是在读博士的时候创造了自己的深度学习框架并发起了开源项目，再看看他们今天在行业中的地位，都是Google、Facebook、Amazon、Apple等巨头必抢的顶级人才。对于中小企业来说，快速把商业应用做好并服务于消费者才是最重要的，因此选好一个顶级深度学习阵营和成熟的技术路线才是更重要的，确实没有必要重新发明轮子，但是对于一个严重依赖人工智能基础技术的超大型巨头企业来说，比如华为、阿里巴巴或百度，必须要拥有自己的深度学习框架和平台，这不是一个可以讨论的问题。如同Amazon选择了MXNet，AWS虽然支持TensorFlow，但Amazon不会傻到去依赖于Google。 链接：https://zhuanlan.zhihu.com/p/59086302来源：知乎","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"DL Framwork","slug":"DL-Framwork","permalink":"http://huangzhiyuan.github.io/tags/DL-Framwork/"}]},{"title":"第一章、温故而知新","slug":"chapter-one-review-old-and-learn-new","date":"2019-09-18T13:25:50.000Z","updated":"2020-03-01T12:07:26.000Z","comments":true,"path":"2019/09/18/chapter-one-review-old-and-learn-new/","link":"","permalink":"http://huangzhiyuan.github.io/2019/09/18/chapter-one-review-old-and-learn-new/","excerpt":"简单的事物背后往往蕴含着复杂的机制。","text":"简单的事物背后往往蕴含着复杂的机制。 从Hello world说起12345#include &lt;stdio.h&gt;int main() &#123; printf(&quot;Hello world\\n&quot;); return 0;&#125; 程序为什么先编译才能运行？ 编译器把源代码转换成可以执行的机器码过程中都做了些什么？怎么做的？ 编译出来的可执行文件里面有什么？除了机器码还有什么？它们是怎么存放怎么组织的？ 头文件是什么机制？ 不同的编译器和操作系统最终编译出来的结果是否一致？为什么？ 程序是怎么运行起来的？怎么开始？怎么结束的？ 程序在运行时，它在内存中是什么样的？ 万变不离其宗对于系统程序员来说，计算机多如牛毛的硬件设备中，只有3个最为关键，分别是CPU、内存和IO控制芯片。北桥芯片（PCI Bridge）：为了协调CPU、内存和高速的图形设备而设计，以便它们之间高速的交换数据。南桥芯片：处理低速设备。如磁盘、USB、键盘、鼠标等。 SMP与多核 人们总是希望CPU越来越快，过去的20年里，CPU的频率从及时kHz到4GHz之后，制造CPU的工艺已经达到物理极限，很难突破。另外一个角度来提高那就是增加CPU的数量。然而速度的提高并不与CPU增加的数量成正比。 站得高，望得远系统软件分为两块，一块是平台性的，比如操作系统内核、驱动程序、运行库和数以千计的系统工具。另外一块是用于程序开发的，比如编译器、汇编器等开发工具和开发库。 操作系统功能 提供抽象的API接口 管理硬件资源 不要让CPU打盹 分时系统 多任务系统 设备驱动操作系统为硬件层的上层，它是对硬件的管理和抽象。统一硬件访问模式。 内存不够怎么办如何将计算机上有限的物理内存分配个多个程序使用？ 地址空间不隔离 内存使用效率低 程序运行地址不确定 解决上述问题的思路是增加中间层，即虚拟地址。通过映射的方法，将虚拟地址转换为物理地址，就可以保证任意一个程序访问的物理地址与另外一个程序相互不重叠，以达到地址空间隔离的效果。 关于隔离每个进程都有自己的独立空间，并且只能访问自己的地址空间，这样就达到了有效进程隔离。 分段分段可以解决问题1和问题3，然而内存使用效率依旧不高。根据程序的局部性原理，当一个程序运行时，在某个时间段，它只是频繁的用到了一部分数据，也就是说，程序的很多数据其实在一个时间段内都欧式不用被用到的。可以使用更小粒度的内存分割和映射的方法，就是分页（Paging). 分页分页的基本方法是把地址空间人为的等分成固定大小的页，每一页的大小都有硬件决定。 众人拾柴火焰高线程基础线程， 有时也被成为轻量级进程，是程序执行的最小单位。一个标准的线程由线程ID,当前指令指针（PC）、寄存器集合和堆栈组成。 本章小结温故而知新。包括CPU与外围部件的连接方式、SMP与多核、软硬件层次体系结构、如何充分领设备驱动、操作系统、虚拟空间、物理空间、页映射和线程的基本概念。","categories":[{"name":"读书","slug":"读书","permalink":"http://huangzhiyuan.github.io/categories/%E8%AF%BB%E4%B9%A6/"}],"tags":[{"name":"程序员的自我修养","slug":"程序员的自我修养","permalink":"http://huangzhiyuan.github.io/tags/%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/"}]},{"title":"周杰伦新歌登上热搜","slug":"won-t-cry","date":"2019-09-17T14:42:03.000Z","updated":"2020-04-13T12:40:38.000Z","comments":true,"path":"2019/09/17/won-t-cry/","link":"","permalink":"http://huangzhiyuan.github.io/2019/09/17/won-t-cry/","excerpt":"原标题：周杰伦新歌销量：11小时卖570万张，首日有望破3000万元。9月16日晚23点，周杰伦新歌《说好不哭》正式上线全网发售，截至17日上午10点整，周杰伦全网累计售出573万张数字单曲。","text":"原标题：周杰伦新歌销量：11小时卖570万张，首日有望破3000万元。9月16日晚23点，周杰伦新歌《说好不哭》正式上线全网发售，截至17日上午10点整，周杰伦全网累计售出573万张数字单曲。 &#160;&#160;其中QQ音乐平台11小时累计售出502万张，其他两个平台总计售出70万张，由于周杰伦音乐版权在QQ音乐这里，因此网易云音乐并不参与周杰伦新歌发售，因此没有销售数据。 &#160;&#160;按照每张数字单曲在音乐平台3元的售价计算，仅QQ音乐的销售额就高达1507万元，其他两个音乐平台累计销售额也突破200万元。全网总销售额超过1720万元，而这距新歌发售才过去11个小时，按照每小时154万元销售额的速度，周杰伦新歌《说好不哭》发售首日有望突破3000万大关。 &#160;&#160;据周杰伦官方粉丝团官微透露，周杰伦新歌上线后半小时销量增加百万，新歌发行后目前已经喜提9个热搜，同时有7个热搜在榜，甚至还干崩了若干个音乐平台的服务器，连网友都评论调侃“凭一己之力爆了微博、弄瘫QQ音乐的男人，他可以”，“这是实打实的流量，不靠打榜做数据，全网刷屏说明了一切吧”。","categories":[{"name":"其他","slug":"其他","permalink":"http://huangzhiyuan.github.io/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"新闻","slug":"新闻","permalink":"http://huangzhiyuan.github.io/tags/%E6%96%B0%E9%97%BB/"}]},{"title":"写在17年圣诞节的话……","slug":"wordsInChristmas","date":"2017-12-25T15:56:42.000Z","updated":"2020-03-01T12:21:00.000Z","comments":true,"path":"2017/12/25/wordsInChristmas/","link":"","permalink":"http://huangzhiyuan.github.io/2017/12/25/wordsInChristmas/","excerpt":"最近总感觉有很多话要说，想说却一直不知从何说起。","text":"最近总感觉有很多话要说，想说却一直不知从何说起。 17年将尽，又是一年总结时。来实习已经3周零1天，对此的感觉就是好快啊，没感觉做出什么一周一周就又结束了，虽忙碌但却也充实。既然选择了就要坚持走下去，并且要尽自己最大的努力做到最好。我也承认自己无论是工作还是生活，都有着不少的缺点和不足，做事未必高效且精准完美，待人也没皆大欢喜。好在我虽然缺点很多，自甘堕落畏缩不前却绝不在其中。虽然艰辛，但我不会轻易放弃，虽然进步缓慢，但我不曾放慢前进的步伐。哪怕每天有一点点的进步，说什么真理无穷，进一寸有进一寸的欢喜。自勉之！ 自律、节制、慎独、规律，认清自己，知不足懂进退，也能省却无限闲烦闷。就这样吧，睡觉~","categories":[{"name":"其他","slug":"其他","permalink":"http://huangzhiyuan.github.io/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"生活","slug":"生活","permalink":"http://huangzhiyuan.github.io/tags/%E7%94%9F%E6%B4%BB/"}]},{"title":"机器学习术语汇总","slug":"MachineLeaningTerms","date":"2017-12-25T14:12:14.000Z","updated":"2020-03-01T12:17:50.000Z","comments":true,"path":"2017/12/25/MachineLeaningTerms/","link":"","permalink":"http://huangzhiyuan.github.io/2017/12/25/MachineLeaningTerms/","excerpt":"开发者必看，超全机器学习术语表汇总。","text":"开发者必看，超全机器学习术语表汇总。 A准确率（accuracy）分类模型预测准确的比例。在多类别分类中，准确率定义如下：在二分类中，准确率定义为： 激活函数（Activation function）一种函数（例如 ReLU 或 Sigmoid），将前一层所有神经元激活值的加权和输入到一个非线性函数中，然后向下一层传递该函数的输出值（典型的非线性）。 AdaGrad一种复杂的梯度下降算法，重新调节每个参数的梯度，高效地给每个参数一个单独的学习率。 AUC（曲线下面积）一种考虑到所有可能的分类阈值的评估标准。ROC曲线下面积代表分类器随机预测真正类（Ture Positives）要比假正类（False Positives）概率大的确信度。 B反向传播（Backpropagation）神经网络中完成梯度下降的重要算法。首先，在前向传播的过程中计算每个节点的输出值。然后，在反向传播的过程中计算与每个参数对应的误差的偏导数。 基线（Baseline）被用为对比模型表现参考点的简单模型。基线帮助模型开发者量化模型在特定问题上的预期表现。 批量模型训练中一个迭代（指一次梯度更新）使用的样本集。 批量大小（batch size）一个批量中样本的数量。例如，SGD 的批量大小为 1，而 mini-batch 的批量大小通常在 10-1000之间。批量大小通常在训练与推理的过程中确定，然而 TensorFlow 不允许动态批量大小。 偏置（bias）与原点的截距或偏移量。偏置（也称偏置项）被称为机器学习模型中的 b 或者 w0。例如，偏置项是以下公式中的b：y′=b+w_1x_1+w_2x_2+…w_nx_n。注意不要和预测偏差混淆。 二元分类器（binary classification）一类分类任务，输出两个互斥（不相交）类别中的一个。例如，一个评估邮件信息并输出「垃圾邮件」或「非垃圾邮件」的机器学习模型就是一个二元分类器。 binning/bucketing根据值的范围将一个连续特征转换成多个称为buckets或者bins二元特征，称为 buckets 或者bins。例如，将温度表示为单一的浮点特征，可以将温度范围切割为几个离散的bins。假如给定的温度的敏感度为十分之一度那么分布在 0.0 度和 15.0度之间的温度可以放入一个bin中，15.1度到30.0 度放入第二个 bin，30.1 度到 45.0 度放入第三个 bin。 C标定层（calibration layer）一种调整后期预测的结构，通常用于解释预测偏差。调整后的预期和概率必须匹配一个观察标签集的分布。 候选采样（candidate sampling）一种优化训练时间的，使用Softmax等算法计算所有正标签的概率，同时只计算一些随机取样的负标签的概率。例如，有一个样本标记为「小猎兔狗」和「狗」，候选取样将计算预测概率，和与「小猎兔狗」和「狗」类别输出（以及剩余的类别的随机子集，比如「猫」、「棒棒糖」、「栅栏」）相关的损失项。这个想法的思路是，负类别可以通过频率更低的负强化（negative reinforcement）进行学习，而正类别经常能得到适当的正强化，实际观察确实如此。候选取样的动力是计算有效性从所有负类别的非计算预测的得益。 检查点（checkpoint）在特定的时刻标记模型的变量的状态的数据。检查点允许输出模型的权重，也允许通过多个阶段训练模型。检查点还允许跳过错误继续进行（例如，抢占作业）。注意其自身的图式并不包含于检查点内。 类别（class）所有同类属性的目标值作为一个标签。例如，在一个检测垃圾邮件的二元分类模型中，这两个类别分别是垃圾邮件和非垃圾邮件。而一个多类别分类模型将区分狗的种类，其中的类别可以是贵宾狗、小猎兔狗、哈巴狗等等。 类别不平衡数据集（class-imbalanced data set）这是一个二元分类问题，其中两个类别的标签的分布频率有很大的差异。比如，一个疾病数据集中若0.01%的样本有正标签，而99.99%的样本有负标签，那么这就是一个类别不平衡数据集。但对于一个足球比赛预测器数据集，若其中 51% 的样本标记一队胜利，而49%的样本标记其它队伍胜利，那么这就不是一个类别不平衡数据集。 分类模型（classification）机器学习模型的一种，将数据分离为两个或多个离散类别。例如，一个自然语言处理分类模型可以将一句话归类为法语、西班牙语或意大利语。分类模型与回归模型（regression model）成对比。 分类阈值（classification threshold）应用于模型的预测分数以分离正类别和负类别的一种标量值标准。当需要将 logistic 回归的结果映射到二元分类模型中时就需要使用分类阈值。例如，考虑一个确定给定邮件为垃圾邮件的概率的logistic回归模型，如果分类阈值是 0.9，那么logistic回归值在0.9以上的被归为垃圾邮件，而在 0.9 以下的被归为非垃圾邮件。 混淆矩阵（confusion matrix）总结分类模型的预测结果的表现水平（即，标签和模型分类的匹配程度）的 NxN 表格。混淆矩阵的一个轴列出模型预测的标签，另一个轴列出实际的标签。N 表示类别的数量。在一个二元分类模型中，N=2。例如，以下为一个二元分类问题的简单的混淆矩阵：| None | Tumor(predicted) | Non-Predicted || ——– | —–: | :—-: || Tumor(actual) | 18 | 1 || Non-Tumor | 6 | 452 |上述混淆矩阵展示了在19个确实为肿瘤的样本中，有18个被模型正确的归类（18 个真正），有1个被错误的归类为非肿瘤（1个假负类）。类似的，在 458 个确实为非肿瘤的样本中，有452个被模型正确的归类（452个真负类），有 6 个被错误的归类（6 个假正类）。多类别分类的混淆矩阵可以帮助发现错误出现的模式。例如，一个混淆矩阵揭示了一个识别手写数字体的模型倾向于将 4 识别为9，或者将7识别为 1。混淆矩阵包含了足够多的信息可以计算很多的模型表现度量，比如精度（precision）和召回（recall）率。 连续特征（continuous feature）拥有无限个取值点的浮点特征。和离散特征（discrete feature）相反。 收敛（convergence）训练过程达到的某种状态，其中训练损失和验证损失在经过了确定的迭代次数后，在每一次迭代中，改变很小或完全不变。换句话说就是，当对当前数据继续训练而无法再提升模型的表现水平的时候，就称模型已经收敛。在深度学习中，损失值下降之前，有时候经过多次迭代仍保持常量或者接近常量，会造成模型已经收敛的错觉。 凸函数（concex function）一种形状大致呈字母U形或碗形的函数。然而，在退化情形中，凸函数的形状就像一条线。例如，以下几个函数都是凸函数： L2 损失函数 Log 损失函数 L1 正则化函数 L2 正则化函数 凸函数是很常用的损失函数。因为当一个函数有最小值的时候（通常就是这样），梯度下降的各种变化都能保证找到接近函数最小值的点。类似的，随机梯度下降的各种变化有很大的概率（虽然无法保证）找到接近函数最小值的点。两个凸函数相加（比如，L2 损失函数+L1 正则化函数）后仍然是凸函数。深度模型通常是非凸的。出乎意料的是，以凸优化的形式设计的算法通常都能在深度网络上工作的很好，虽然很少能找到最小值。 成本（cost）loss 的同义词。 交叉熵（cross-entropy）多类别分类问题中对Log损失函数的推广。交叉熵量化两个概率分布之间的区别。参见困惑度（perplexity）。 D数据集（data set）样本的集合。 决策边界（decision boundary）在一个二元分类或多类别分类问题中模型学习的类别之间的分离器。例如，下图就展示了一个二元分类问题，决策边界即橙点类和蓝点类的边界。 深度模型（deep model）一种包含多个隐藏层的神经网络。深度模型依赖于其可训练的非线性性质。和宽度模型对照（wide model）。 密集特征（dense feature）大多数取值为非零的一种特征，通常用取浮点值的张量（tensor）表示。和稀疏特征（sparse feature）相反。 派生特征（derived feature）合成特征（synthetic feature）的同义词。 离散特征（discrete feature）只有有限个可能取值的一种特征。例如，一个取值只包括动物、蔬菜或矿物的特征就是离散（或类别）特征。和连续特征（continuous feature）对照。 dropout 正则化（dropout regularization）训练神经网络时一种有用的正则化方法。dropout正则化的过程是在单次梯度计算中删去一层网络中随机选取的固定数量的单元。删去的单元越多，正则化越强。 动态模型（dynamic model）以连续更新的方式在线训练的模型。即数据连续不断的输入模型。 E早期停止法（early stopping）一种正则化方法，在训练损失完成下降之前停止模型训练过程。当验证数据集（validation data set）的损失开始上升的时候，即泛化表现变差的时候，就该使用早期停止法了。 嵌入（embeddings）一类表示为连续值特征的明确的特征。嵌入通常指将高维向量转换到低维空间中。例如，将一个英语句子中的单词以以下任何一种方式表示： 拥有百万数量级（高维）的元素的稀疏向量，其中所有的元素都是整数。向量的每一个单元表示一个单独的英语单词，单元中的数字表示该单词在一个句子中出现的次数。由于一个句子中的单词通常不会超过50个，向量中几乎所有的单元都是0。少量的非零的单元将取一个小的整数值（通常为 1）表示句子中一个单词的出现次数。 拥有数百个（低维）元素的密集向量，其中每一个元素取 0 到 1 之间的浮点数。 在 TensorFlow 中，嵌入是通过反向传播损失训练的，正如神经网络的其它参量一样。 经验风险最小化（empirical risk minimization，ERM）选择能最小化训练数据的损失的模型函数的过程。和结构风险最小化（structual risk minimization）对照。 集成（ensemble）多个模型预测的综合考虑。可以通过以下一种或几种方法创建一个集成方法： 设置不同的初始化； 设置不同的超参量； 设置不同的总体结构。 深度和广度模型是一种集成。 评估器（Estimator） tf.Estimator 类的一个例子，封装 logic 以建立一个 TensorFlow 图并运行一个 TensorFlow session。你可以通过以下方式创建自己的评估器： https://www.tensorflow.org/extend/estimators 样本（example）一个数据集的一行内容。一个样本包含了一个或多个特征，也可能是一个标签。参见标注样本（labeled example）和无标注样本（unlabeled example）。 F假负类（false negative，FN）被模型错误的预测为负类的样本。例如，模型推断一封邮件为非垃圾邮件（负类），但实际上这封邮件是垃圾邮件。 假正类（false positive，FP）被模型错误的预测为正类的样本。例如，模型推断一封邮件为垃圾邮件（正类），但实际上这封邮件是非垃圾邮件。 假正类率（false positive rate，FP rate）ROC 曲线（ROC curve）中的 x 轴。FP 率的定义是：假正率=假正类数/(假正类数+真负类数) 特征（feature）输入变量，用于做出预测。 特征列（feature columns/FeatureColumn）具有相关性的特征的集合，比如用户可能居住的所有可能的国家的集合。一个样本的一个特征列中可能会有一个或者多个特征。TensorFlow 中的特征列还可以压缩元数据比如下列情况： 特征的数据类型； 一个特征是固定长度的或应该转换为嵌入。 一个特征列可以仅包含一个特征。「特征列」是谷歌专用的术语。在 VW 系统（Yahoo/Microsoft）中特征列的意义是「命名空间」（namespace），或者场（field）。 特征交叉（feature cross）将特征进行交叉（乘积或者笛卡尔乘积）运算后得到的合成特征。特征交叉有助于表示非线性关系。 特征工程（feature engineering）在训练模型的时候，决定哪些特征是有用的，然后将记录文件和其它来源的原始数据转换成上述特征的过程。在TensorFlow中特征工程通常意味着将原始记录文件输入tf.Example协议缓存中。参见tf.Transform。特征工程有时候也称为特征提取。 特征集（feature set）机器学习模型训练的时候使用的特征群。比如，邮政编码，面积要求和物业状况可以组成一个简单的特征集，使模型能预测房价。 特征定义（feature spec）描述所需的信息从 tf.Example 协议缓存中提取特征数据。因为 tf.Example 协议缓存只是数据的容器，必须明确以下信息： 需要提取的数据（即特征的关键信息） 数据类型（比如，浮点数还是整数） 数据长度（固定的或者变化的） Estimator API 提供了从一群特征列中生成一个特征定义的工具。 完全 softmax（full softmax）参见 softmax。和候选采样对照。 G泛化（generalization）指模型利用新的没见过的数据而不是用于训练的数据作出正确的预测的能力。 广义线性模型（generalized linear model）最小二乘回归模型的推广/泛化，基于高斯噪声，相对于其它类型的模型（基于其它类型的噪声，比如泊松噪声，或类别噪声）。广义线性模型的例子包括： logistic 回归 多分类回归 最小二乘回归 广义线性模型的参数可以通过凸优化得到，它具有以下性质： 最理想的最小二乘回归模型的平均预测结果等于训练数据的平均标签。 最理想的 logistic回归模型的平均概率的预测结果等于训练数据的平均标签。 广义线性模型的能力局限于其特征的性质。和深度模型不同，一个广义线性模型无法「学习新的特征」。 梯度（gradient）所有变量的偏导数的向量。在机器学习中，梯度是模型函数的偏导数向量。梯度指向最陡峭的上升路线。 梯度截断（gradient clipping）在应用梯度之前先修饰数值，梯度截断有助于确保数值稳定性，防止梯度爆炸出现。 梯度下降（gradient descent）通过计算模型的相关参量和损失函数的梯度最小化损失函数，值取决于训练数据。梯度下降迭代地调整参量，逐渐靠近权重和偏置的最佳组合，从而最小化损失函数。 图（graph）在 TensorFlow 中的一种计算过程展示。图中的节点表示操作。节点的连线是有指向性的，表示传递一个操作（一个张量）的结果（作为一个操作数）给另一个操作。使用 TensorBoard 能可视化计算图。 H启发式（heuristic）一个问题的实际的和非最优的解，但能从学习经验中获得足够多的进步。 隐藏层（hidden layer）神经网络中位于输入层（即特征）和输出层（即预测）之间的合成层。一个神经网络包含一个或多个隐藏层。 折页损失函数（Hinge loss）损失函数的一个类型，用于分类模型以寻找距离每个样本的距离最大的决策边界，即最大化样本和边界之间的边缘。KSVMs 使用 hinge 损失函数（或相关的函数，比如平方 hinge 函数）。在二元分类中，hinge 损失函数按以下方式定义： loss=max(0,1−(y′∗y)) 其中 y’是分类器模型的列输出： y′=b+w_1x_1+w_2x_2+…w_nx_n y 是真实的标签，-1 或+1。因此，hinge 损失将是下图所示的样子： 测试数据（holdout data）有意不用于训练的样本。验证数据集（validation data set）和测试数据集（test data set）是测试数据（holdout data）的两个例子。测试数据帮助评估模型泛化到除了训练数据之外的数据的能力。测试集的损失比训练集的损失提供了对未知数据集的损失更好的估计。 超参数（hyperparameter）连续训练模型的过程中可以拧动的「旋钮」。例如，相对于模型自动更新的参数，学习率（learning rate）是一个超参数。和参量对照。 I独立同分布（independently and identically distributed，i.i.d）从不会改变的分布中获取的数据，且获取的每个值不依赖于之前获取的值。i.i.d. 是机器学习的理想情况——一种有用但在现实世界中几乎找不到的数学构建。例如，网页访客的分布可能是短暂时间窗口上的 i.i.d；即分布不会在该时间窗口发生改变，每个人的访问都与其他人的访问独立。但是，如果你扩展了时间窗口，则会出现网页访客的季节性差异。 推断（inference）在机器学习中，通常指将训练模型应用到无标注样本来进行预测的过程。在统计学中，推断指在观察到的数据的基础上拟合分布参数的过程。 输入层（input layer）神经网络的第一层（接收输入数据）。 评分者间一致性（inter-rater agreement）用来衡量一项任务中人类评分者意见一致的指标。如果意见不一致，则任务说明可能需要改进。有时也叫标注者间信度（inter-annotator agreement）或评分者间信度（inter-rater reliability）。 KKernel 支持向量机（Kernel Support Vector Machines/KSVM）一种分类算法，旨在通过将输入数据向量映射到更高维度的空间使正类和负类之间的边际最大化。例如，考虑一个输入数据集包含一百个特征的分类问题。为了使正类和负类之间的间隔最大化，KSVM从内部将特征映射到百万维度的空间。KSVM 使用的损失函数叫作 hinge 损失。 LL1 损失函数（L1 loss）损失函数基于模型对标签的预测值和真实值的差的绝对值而定义。L1损失函数比起 L2 损失函数对异常值的敏感度更小。 L1 正则化（L1 regularization）一种正则化，按照权重绝对值总和的比例进行惩罚。在依赖稀疏特征的模型中，L1 正则化帮助促使（几乎）不相关的特征的权重趋近于0，从而从模型中移除这些特征。###L2 损失（L2 loss）参见平方损失。 L2 正则化（L2 regularization）一种正则化，按照权重平方的总和的比例进行惩罚。L2正则化帮助促使异常值权重更接近 0 而不趋近于0。（可与L1正则化对照阅读。）L2正则化通常改善线性模型的泛化效果。 标签（label）在监督式学习中，样本的「答案」或「结果」。标注数据集中的每个样本包含一或多个特征和一个标签。比如，在房屋数据集中，特征可能包括卧室数量、卫生间数量、房龄，而标签可能就是房子的价格。在垃圾邮件检测数据集中，特征可能包括主题、发出者何邮件本身，而标签可能是「垃圾邮件」或「非垃圾邮件」。 标注样本（labeled example）包含特征和标签的样本。在监督式训练中，模型从标注样本中进行学习。 lambda正则化率的同义词。（该术语有多种含义。这里，我们主要关注正则化中的定义。） 层（layer）神经网络中的神经元序列，可以处理输入特征序列或神经元的输出。它也是 TensorFlow的一种抽象化概念。层是将张量和配置选项作为输入、输出其他张量的 Python函数。一旦必要的张量出现，用户就可以通过模型函数将结果转换成估计器。 学习率（learning rate）通过梯度下降训练模型时使用的一个标量。每次迭代中，梯度下降算法使学习率乘以梯度，乘积叫作 gradient step。学习率是一个重要的超参数。 最小二乘回归（least squares regression）通过 L2 损失最小化进行训练的线性回归模型。 线性回归（linear regression）对输入特征的线性连接输出连续值的一种回归模型。 logistic 回归（logistic regression）将 sigmoid 函数应用于线性预测，在分类问题中为每个可能的离散标签值生成概率的模型。尽管logistic回归常用于二元分类问题，但它也用于多类别分类问题（这种情况下，logistic 回归叫作「多类别 logistic 回归」或「多项式 回归」。 对数损失函数（Log Loss）二元 logistic 回归模型中使用的损失函数。 损失度量模型预测与标签距离的指标，它是度量一个模型有多糟糕的指标。为了确定损失值，模型必须定义损失函数。例如，线性回归模型通常使用均方差作为损失函数，而 logistic 回归模型使用对数损失函数。 M机器学习（machine learning）利用输入数据构建（训练）预测模型的项目或系统。该系统使用学习的模型对与训练数据相同分布的新数据进行有用的预测。机器学习还指与这些项目或系统相关的研究领域。 均方误差（Mean Squared Error/MSE）每个样本的平均平方损失。MSE可以通过平方损失除以样本数量来计算。TensorFlow Playground 展示「训练损失」和「测试损失」的值是 MSE。 小批量（mini-batch）在训练或推断的一个迭代中运行的整批样本的一个小的随机选择的子集。小批量的大小通常在10到1000之间。在小批量数据上计算损失比在全部训练数据上计算损失要高效的多。 小批量随机梯度下降（mini-batch stochastic gradient descent）使用小批量的梯度下降算法。也就是，小批量随机梯度下降基于训练数据的子集对 梯度进行评估。Vanilla SGD 使用 size 为 1 的小批量。 模型（model）机器学习系统从训练数据中所学内容的表示。该术语有多个含义，包括以下两个相关含义： TensorFlow 图，显示如何计算预测的结构。 TensorFlow 图的特定权重和偏差，由训练决定。 模型训练（model training）确定最佳模型的过程。 动量（Momentum）一种复杂的梯度下降算法，其中的学习步不只依赖于当前步的导数，还依赖于先于它的步。动量包括随着时间计算梯度的指数加权移动平均数，类似于物理学中的动量。动量有时可以阻止学习陷于局部最小值。 多类别（multi-class）在多于两类的类别中进行分类的分类问题。例如，有约 128 种枫树，那么分类枫树品种的模型就是多类别的。反之，把电子邮件分成两个类别（垃圾邮件和非垃圾邮件）的模型是二元分类器模型。 NNaN trap训练过程中，如果模型中的一个数字变成了NaN，则模型中的很多或所有其他数字最终都变成 NaN。NaN 是「Not a Number」的缩写。 负类（negative class）在二元分类中，一个类别是正类，另外一个是负类。正类就是我们要找的目标，负类是另外一种可能性。例如，医疗测试中的负类可能是「非肿瘤」，电子邮件分类器中的负类可能是「非垃圾邮件」。 神经网络（neural network）该模型从大脑中获取灵感，由多个层组成（其中至少有一个是隐藏层），每个层包含简单的连接单元或神经元，其后是非线性。 神经元（neuron）神经网络中的节点，通常输入多个值，生成一个输出值。神经元通过将激活函数（非线性转换）应用到输入值的加权和来计算输出值。 归一化（normalization）将值的实际区间转化为标准区间的过程，标准区间通常是-1 到+1 或 0 到 1。例如，假设某个特征的自然区间是800到6000。通过减法和分割，你可以把那些值标准化到区间-1 到+1。参见缩放。 numpyPython 中提供高效数组运算的开源数学库。pandas 基于 numpy 构建。 O目标（objective）算法尝试优化的目标函数。 离线推断（offline inference）生成一组预测并存储，然后按需检索那些预测。可与在线推断对照阅读。 one-hot 编码（one-hot encoding）一个稀疏向量，其中： 一个元素设置为 1。 所有其他的元素设置为 0。 独热编码常用于表示有有限可能值集合的字符串或标识符。例如，假设一个记录了 15000 个不同品种的植物数据集，每一个用独特的字符串标识符来表示。作为特征工程的一部分，你可能将那些字符串标识符进行独热编码，每个向量的大小为 15000。 一对多（one-vs.-all）给出一个有 N 个可能解决方案的分类问题，一对多解决方案包括N个独立的二元分类器——每个可能的结果都有一个二元分类器。例如，一个模型将样本分为动物、蔬菜或矿物，则一对多的解决方案将提供以下三种独立的二元分类器： 动物和非动物 蔬菜和非蔬菜 矿物和非矿物 在线推断（online inference）按需生成预测。可与离线推断对照阅读。运算（Operation/op） TensorFlow 图中的一个节点。在 TensorFlow 中，任何创建、控制或损坏张量的步骤都是运算。例如，矩阵乘法是一个把两个张量作为输入、生成一个张量作为输出的运算。 优化器（optimizer）梯度下降算法的特定实现。TensorFlow的基类优化器是tf.train.Optimizer。不同的优化器（tf.train.Optimizer 的子类）对应不同的概念，如： 动量（Momentum） 更新频率（AdaGrad = ADAptive GRADient descent；Adam = ADAptive with Momentum；RMSProp） 稀疏性／正则化（Ftrl） 更复杂的数学（Proximal 及其他） 你甚至可以想象 NN-driven optimizer。 异常值（outlier）与大多数值差别很大的值。在机器学习中，下列都是异常值： 高绝对值的权重。 与实际值差距过大的预测值。 比平均值多大约 3 个标准差的输入数据的值。 异常值往往使模型训练中出现问题。 输出层（output layer）神经网络的「最后」一层。这一层包含整个·模型所寻求的答案。 过拟合（overfitting）创建的模型与训练数据非常匹配，以至于模型无法对新数据进行正确的预测。 Ppandas一种基于列的数据分析 API。很多机器学习框架，包括 TensorFlow，支持 pandas 数据结构作为输入。参见 pandas 文档。###参数（parameter）机器学习系统自行训练的模型的变量。例如，权重是参数，它的值是机器学习系统通过连续的训练迭代逐渐学习到的。可与超参数对照阅读。 参数服务器（Parameter Server/PS）用于在分布式设置中跟踪模型参数。 参数更新（parameter update）在训练过程中调整模型参数的操作，通常在梯度下降的单个迭代中进行。 偏导数（partial derivative）一个多变量函数的偏导数是它关于其中一个变量的导数，而保持其他变量恒定。例如，f(x, y) 对于 x 的偏导数就是 f(x) 的导数，y保持恒定。x 的偏导数中只有 x 是变化的，公式中其他的变量都不用变化。 分区策略（partitioning strategy）在多个参数服务器中分割变量的算法。 性能（performance）具有多种含义： 在软件工程中的传统含义：软件运行速度有多快／高效？ 在机器学习中的含义：模型的准确率如何？即，模型的预测结果有多好？ 困惑度（perplexity）对模型完成任务的程度的一种度量指标。例如，假设你的任务是阅读用户在智能手机上输入的单词的头几个字母，并提供可能的完整单词列表。该任务的困惑度（perplexity，P）是为了列出包含用户实际想输入单词的列表你需要进行的猜测数量。困惑度和交叉熵的关系如下： 流程（pipeline）机器学习算法的基础架构。管道包括收集数据、将数据放入训练数据文件中、训练一或多个模型，以及最终输出模型。 正类（positive class）在二元分类中，有两种类别：正类和负类。正类是我们测试的目标。（不过必须承认，我们同时测试两种结果，但其中一种不是重点。）例如，医疗测试中正类可能是「肿瘤」，电子邮件分类器中的正类可能是「垃圾邮件」。可与负类对照阅读。 精度（precision）分类模型的一种指标。准确率指模型预测正类时预测正确的频率。即： 预测（prediction）模型在输入样本后的输出结果。 预测偏差（prediction bias）揭示预测的平均值与数据集中标签的平均值的差距。 预制评估器（pre-made Estimator）已经构建好的评估器。TensorFlow 提供多个预制评估器，包括 DNNClassifier、DNNRegressor和LinearClassifier。你可以根据指导（https://www.tensorflow.org/extend/estimators）构建自己的预制评估器。 预训练模型（pre-trained model）已经训练好的模型或模型组件（如嵌入）。有时，你将预训练嵌入馈送至神经网络。其他时候，你的模型自行训练嵌入，而不是依赖于预训练嵌入。 先验信念（prior belief）训练开始之前你对数据的信念。例如，L2正则化依赖于权重值很小且正常分布在 0 周围的信念。 Q队列（queue）实现队列数据结构的TensorFlow操作。通常在输入／输出（I/O）中使用。 R秩（rank）机器学习领域中包含多种含义的术语： 张量中的维度数量。比如，标量有 1 个秩，向量有 1 个秩，矩阵有 2 个秩。（注：在这个词汇表中，「秩」的概念和线性代数中「秩」的概念不一样，例如三阶可逆矩阵的秩为 3。） 机器学习问题中类别的序数位置，按从高到低的顺序给类别分类。比如，行为排序系统可以把狗的奖励按从高（牛排）到低（甘蓝）排序。 评分者（rater）为样本提供标签的人，有时也叫「标注者」。 召回率（recall）分类模型的一个指标，可以回答这个问题：模型能够准确识别多少正标签？即： 修正线性单元（Rectified Linear Unit/ReLU）一种具备以下规则的激活函数： 如果输入为负或零，则输出为 0。 如果输入为正，则输出与输入相同。 回归模型（regression model）一种输出持续值（通常是浮点数）的模型。而分类模型输出的是离散值，如「day lily」或「tiger lily」。 正则化（regularization）对模型复杂度的惩罚。正则化帮助防止过拟合。正则化包括不同种类： L1 正则化 L2 正则化 dropout 正则化 early stopping（这不是正式的正则化方法，但可以高效限制过拟合） 正则化率（regularization rate）一种标量级，用 lambda来表示，指正则函数的相对重要性。从下面这个简化的损失公式可以看出正则化率的作用： minimize(loss function + λ(regularization function)) 提高正则化率能够降低过拟合，但可能会使模型准确率降低。 表征将数据映射到有用特征的过程。 受试者工作特征曲线（receiver operating characteristic/ROC Curve）反映在不同的分类阈值上，真正类率和假正类率的比值的曲线。参见 AUC。 根目录（root directory）指定放置 TensorFlow 检查点文件子目录和多个模型的事件文件的目录。 均方根误差（Root Mean Squared Error/RMSE）均方误差的平方根。 SSaver负责存储模型检查点文件的 TensorFlow 对象。 缩放（scaling）特征工程中常用的操作，用于控制特征值区间，使之与数据集中其他特征的区间匹配。例如，假设你想使数据集中所有的浮点特征的区间为 0 到 1。给定一个特征区间是 0 到 500，那么你可以通过将每个值除以 500，缩放特征值区间。还可参见正则化。 scikit-learn一种流行的开源机器学习平台。网址：www.scikit-learn.org 序列模型（sequence model）输入具有序列依赖性的模型。例如，根据之前观看过的视频序列对下一个视频进行预测。 会话（session）保持 TensorFlow 程序的状态（如变量）。 Sigmoid 函数（sigmoid function）把 logistic 或多项式回归输出（对数几率）映射到概率的函数，返回的值在 0 到 1 之间。sigmoid 函数的公式如下：其中σ在 logistic 回归问题中只是简单的：在有些神经网络中，sigmoid 函数和激活函数一样。 softmax为多类别分类模型中每个可能的类提供概率的函数。概率加起来的总和是 1.0。例如，softmax可能检测到某个图像是一只狗的概率为0.9，是一只猫的概率为 0.08，是一匹马的概率为 0.02。（也叫作 full softmax）。 稀疏特征（sparse feature）值主要为 0 或空的特征向量。比如，一个向量的值有 1 个 1,、一百万个 0，则该向量为稀疏向量。再比如，搜索查询中的单词也是稀疏向量：在一种语言中有很多可以用的单词，但给定的查询中只用了其中的一些。可与稠密特征对照阅读。 平方损失（squared loss）线性回归中使用的损失函数（也叫作L2 Loss）。该函数计算模型对标注样本的预测值和标签真正值之间差的平方。在平方之后，该损失函数扩大了不良预测的影响。即，平方损失比 L1 Loss 对异常值（outlier）的反应更加强烈。 静态模型（static model）离线训练的模型。 稳态（stationarity）数据集中的一种数据属性，数据分布在一或多个维度中保持不变。通常情况下，维度是时间，意味着具备平稳性的数据不会随着时间发生变化。比如，具备平稳性的数据从 9 月到 12 月不会改变。 步（step）一个批量中的前向和后向评估。 步长（step size）学习速率（learning rate）乘以偏导数的值，即梯度下降中的步长。 随机梯度下降（stochastic gradient descent/SGD）批量大小为 1 的梯度下降算法。也就是说，SGD 依赖于从数据集中随机均匀选择出的一个样本，以评估每一步的梯度。 结构风险最小化（structural risk minimization/SRM）这种算法平衡两个目标： 构建预测性最强的模型（如最低损失）。 使模型尽量保持简单（如强正则化）。 比如，在训练集上的损失最小化 + 正则化的模型函数就是结构风险最小化算法。更多信息，参见 http://www.svms.org/srm/。可与经验风险最小化对照阅读。 摘要（summary）在 TensorFlow 中，特定步计算的值或值的集合，通常用于跟踪训练过程中的模型指标。 监督式机器学习（supervised machine learning）利用输入数据及其对应标签来训练模型。监督式机器学习类似学生通过研究问题和对应答案进行学习。在掌握问题和答案之间的映射之后，学生就可以提供同样主题的新问题的答案了。可与非监督机器学习对照阅读。合成特征（synthetic feature）不在输入特征中，而是从一个或多个输入特征中派生出的特征。合成特征的类型包括： 特征与自己或其他特征相乘（叫作特征交叉）。 两个特征相除。 将连续的特征放进 range bin 中。 由归一化或缩放单独创建的特征不是合成特征。 T张量（tensor）TensorFlow 项目的主要数据结构。张量是 N 维数据结构（N 的值很大），经常是标量、向量或矩阵。张量可以包括整数、浮点或字符串值。 张量处理单元（Tensor Processing Unit，TPU）优化 TensorFlow 性能的 ASIC（application-specific integrated circuit，专用集成电路）。 张量形状（Tensor shape）张量的元素数量包含在不同维度中。比如，[5, 10] 张量在一个维度中形状为 5，在另一个维度中形状为 10。 张量大小（Tensor size）张量包含的标量总数。比如，[5, 10] 张量的大小就是 50。 TensorBoard展示一个或多个 TensorFlow 项目运行过程中保存的摘要数据的控制面板。 TensorFlow大型分布式机器学习平台。该术语还指 TensorFlow 堆栈中的基础 API 层，支持数据流图上的通用计算。尽管TensorFlow主要用于机器学习，但是它也适用于要求使用数据流图进行数值运算的非机器学习任务。 TensorFlow Playground一个可以看到不同超参数对模型（主要是神经网络）训练的影响的平台。前往 http://playground.tensorflow.org，使用 TensorFlow Playground。 TensorFlow Serving帮助训练模型使之可部署到产品中的平台。 测试集（test set）数据集的子集。模型经过验证集初步测试之后，使用测试集对模型进行测试。可与训练集和验证集对照阅读。 tf.Example一种标准 protocol buffer，用于描述机器学习模型训练或推断的输入数据。 训练（training）确定组成模型的完美参数的流程。 训练集（training set）数据集子集，用于训练模型。可与验证集和测试集对照阅读。 真负类（true negative，TN）被模型正确地预测为负类的样本。例如，模型推断某封电子邮件不是垃圾邮件，然后该电邮真的不是垃圾邮件。 真正类（true positive，TP）被模型正确地预测为正类的样本。例如，模型推断某封电子邮件是垃圾邮件，结果该电邮真的是垃圾邮件。 真正类率（true positive rate，TP rate）召回率（recall）的同义词。即： TruePositiveRate=TruePositives/(TruePositives+FalseNegatives) 真正类率是 ROC 曲线的 y 轴。 U无标签样本（unlabeled example）包含特征但没有标签的样本。无标签样本是推断的输入。在半监督学习和无监督学习的训练过程中，通常使用无标签样本。 无监督机器学习（unsupervised machine learning）训练一个模型寻找数据集（通常是无标签数据集）中的模式。无监督机器学习最常用于将数据分成几组类似的样本。例如，无监督机器学习算法可以根据音乐的各种属性聚类数据。用这种方式收集的数据可以作为其他机器学习算法（如音乐推荐服务）的输入。聚类在难以获取真正标签的情景中非常有用。例如，在反欺诈和反滥用的情景中，聚类可以帮助人类更好地理解数据。无监督机器学习的另一个例子是主成分分析（principal component analysis，PCA）。如，将 PCA 应用于包含数百万购物车内容的数据集中时，就有可能发现有柠檬的购物车往往也有解酸剂。可与监督式机器学习对照阅读。 V验证集（validation set）数据集的一个子集（与训练集不同），可用于调整超参数。可与训练集和测试集对照阅读。 W权重（weight）线性模型中的特征系数，或者深度网络中的边缘。线性模型的训练目标是为每个特征确定一个完美的权重。如果权重为0，则对应的特征对模型而言是无用的。 宽模型（wide model）线性模型通常具备很多稀疏输入特征。我们称之为「宽」模型，因其具有大量与输出节点直接连接的输入，是一种特殊类型的神经网络。宽模型通常比深度模型更容易调试（debug）和检查。尽管宽模型无法通过隐藏层表达非线性，但它们可以使用特征交叉和bucketization等转换用不同方式对非线性建模。可与深度模型对照阅读。 原文链接","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://huangzhiyuan.github.io/tags/AI/"}]},{"title":"读懂diff","slug":"learn-diff","date":"2017-12-16T07:01:40.000Z","updated":"2020-03-01T12:14:24.000Z","comments":true,"path":"2017/12/16/learn-diff/","link":"","permalink":"http://huangzhiyuan.github.io/2017/12/16/learn-diff/","excerpt":"diff是Unix系统的一个很重要的工具程序。diff就会告诉你，这两个文件有何差异。它的显示结果不太好懂，下面我就来说明，如何读懂diff。","text":"diff是Unix系统的一个很重要的工具程序。diff就会告诉你，这两个文件有何差异。它的显示结果不太好懂，下面我就来说明，如何读懂diff。 diff的三种格式由于历史原因，diff有三种格式： 正常格式（normal diff） 上下文格式（context diff） 合并格式（unified diff） 示例文件为了便于讲解，先新建两个示例文件。第一个文件叫做f1，内容是每行一个a，一共7行。 1234567 a a a a a a a 第二个文件叫做f2，修改f1而成，第4行变成b，其他不变。 1234567 a a a b a a a 正常格式的diff现在对f1和f2进行比较： $ diff f1 f2 这时，diff就会显示正常格式的结果： 12344c4&lt; a---&gt; b 第一行是一个提示，用来说明变动位置。 4c4 它分成三个部分：前面的”4”，表示f1的第4行有变化；中间的”c”表示变动的模式是内容改变（change），其他模式还有”增加”（a，代表addition）和”删除”（d，代表deletion）；后面的”4”，表示变动后变成f2的第4行。 第二行分成两个部分。 . &lt; a 前面的小于号，表示要从f1当中去除该行（也就是第4行），后面的”a”表示该行的内容。第三行用来分割f1和f2。 . — 第四行，类似于第二行。 . &gt; b 前面的大于号表示f2增加了该行，后面的”b”表示该行的内容。最早的Unix（即AT&amp;T版本的Unix），使用的就是这种格式的diff。 上下文格式的diff上个世纪80年代初，加州大学伯克利分校推出BSD版本的Unix时，觉得diff的显示结果太简单，最好加入上下文，便于了解发生的变动。因此，推出了上下文格式的diff。它的使用方法是加入c参数（代表context）。 $ diff -c f1 f2 显示结果如下： 12345678910111213141516171819 *** f1 2012-08-29 16:45:41.000000000 +0800 --- f2 2012-08-29 16:45:51.000000000 +0800 *************** *** 1,7 **** a a a !a a a a --- 1,7 ---- a a a !b a a a 这个结果分成四个部分。第一部分的两行，显示两个文件的基本情况：文件名和时间信息。 12*** f1 2012-08-29 16:45:41.000000000 +0800--- f2 2012-08-29 16:45:51.000000000 +0800 “***”表示变动前的文件，”—“表示变动后的文件。第二部分是15个星号，将文件的基本情况与变动内容分割开。 1*************** 第三部分显示变动前的文件，即f1。 12345678*** 1,7 ****aaa!aaaa 这时不仅显示发生变化的第4行，还显示第4行的前面三行和后面三行，因此一共显示7行。所以，前面的”*** 1,7 ****”就表示，从第1行开始连续7行。另外，文件内容的每一行最前面，还有一个标记位。如果为空，表示该行无变化；如果是感叹号（!），表示该行有改动；如果是减号（-），表示该行被删除；如果是加号（+），表示该行为新增。第四部分显示变动后的文件，即f2。 12345678--- 1,7 ----aaa!baaa 除了变动行（第4行）以外，也是上下文各显示三行，总共显示7行。 合并格式的diff如果两个文件相似度很高，那么上下文格式的diff，将显示大量重复的内容，很浪费空间。1990年，GNU diff率先推出了”合并格式”的diff，将f1和f2的上下文合并在一起显示。它的使用方法是加入u参数（代表unified）。 $ diff -u f1 f2 显示结果如下： 1234567891011--- f1 2012-08-29 16:45:41.000000000 +0800+++ f2 2012-08-29 16:45:51.000000000 +0800@@ -1,7 +1,7 @@aaa-a+baaa 它的第一部分，也是文件的基本信息。 — f1 2012-08-29 16:45:41.000000000 +0800+++ f2 2012-08-29 16:45:51.000000000 +0800 “—“表示变动前的文件，”+++”表示变动后的文件。第二部分，变动的位置用两个@作为起首和结束。 @@ -1,7 +1,7 @@ 前面的”-1,7”分成三个部分：减号表示第一个文件（即f1），”1”表示第1行，”7”表示连续7行。合在一起，就表示下面是第一个文件从第1行开始的连续7行。同样的，”+1,7”表示变动后，成为第二个文件从第1行开始的连续7行。第三部分是变动的具体内容。 12345678aaa-a+baaa 除了有变动的那些行以外，也是上下文各显示3行。它将两个文件的上下文，合并显示在一起，所以叫做”合并格式”。每一行最前面的标志位，空表示无变动，减号表示第一个文件删除的行，加号表示第二个文件新增的行。 git格式的diff版本管理系统git，使用的是合并格式diff的变体。 $ git diff 显示结果如下： 12345678910111213diff --git a/f1 b/f1index 6f8a38c..449b072 100644--- a/f1+++ b/f1@@ -1,7 +1,7 @@aaa-a+baaa 第一行表示结果为git格式的diff。 diff –git a/f1 b/f1 进行比较的是，a版本的f1（即变动前）和b版本的f1（即变动后）。第二行表示两个版本的git哈希值（index区域的6f8a38c对象，与工作目录区域的449b072对象进行比较），最后的六位数字是对象的模式（普通文件，644权限）。 index 6f8a38c..449b072 100644 第三行表示进行比较的两个文件。 — a/f1+++ b/f1 “—“表示变动前的版本，”+++”表示变动后的版本。后面的行都与官方的合并格式diff相同。 123456789@@ -1,7 +1,7 @@aaa-a+baaa","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"diff","slug":"diff","permalink":"http://huangzhiyuan.github.io/tags/diff/"}]},{"title":"理解 LSTM 网络","slug":"about-LSTM-GRU","date":"2017-12-13T11:39:59.000Z","updated":"2020-03-01T12:05:50.000Z","comments":true,"path":"2017/12/13/about-LSTM-GRU/","link":"","permalink":"http://huangzhiyuan.github.io/2017/12/13/about-LSTM-GRU/","excerpt":"人类并不是每时每刻都从一片空白的大脑开始他们的思考。在你阅读这篇文章时候，你都是基于自己已经拥有的对先前所见词的理解来推断当前词的真实含义。我们不会将所有的东西都全部丢弃，然后用空白的大脑进行思考。我们的思想拥有持久性。 传统的神经网络并不能做到这点，看起来也像是一种巨大的弊端。例如，假设你希望对电影中的每个时间点的时间类型进行分类。传统的神经网络应该很难来处理这个问题——使用电影中先前的事件推断后续的事件。 RNN 解决了这个问题。RNN 是包含循环的网络，允许信息的持久化。","text":"人类并不是每时每刻都从一片空白的大脑开始他们的思考。在你阅读这篇文章时候，你都是基于自己已经拥有的对先前所见词的理解来推断当前词的真实含义。我们不会将所有的东西都全部丢弃，然后用空白的大脑进行思考。我们的思想拥有持久性。 传统的神经网络并不能做到这点，看起来也像是一种巨大的弊端。例如，假设你希望对电影中的每个时间点的时间类型进行分类。传统的神经网络应该很难来处理这个问题——使用电影中先前的事件推断后续的事件。 RNN 解决了这个问题。RNN 是包含循环的网络，允许信息的持久化。 在上面的示例图中，神经网络的模块A，正在读取某个输入x_i，并输出一个值 h_i。循环可以使得信息可以从当前步传递到下一步。这些循环使得 RNN 看起来非常神秘。然而，如果你仔细想想，这样也不比一个正常的神经网络难于理解。RNN可以被看做是同一神经网络的多次复制，每个神经网络模块会把消息传递给下一个。所以，如果我们将这个循环展开： 链式的特征揭示了RNN本质上是与序列和列表相关的。他们是对于这类数据的最自然的神经网络架构。并且RNN也已经被人们应用了！在过去几年中，应用 RNN 在语音识别，语言建模，翻译，图片描述等问题上已经取得一定成功，并且这个列表还在增长。我建议大家参考 Andrej Karpathy 的博客文章——The Unreasonable Effectiveness of Recurrent Neural Networks来看看更丰富有趣的 RNN 的成功应用。 而这些成功应用的关键之处就是LSTM的使用，这是一种特别的RNN，比标准的RNN在很多的任务上都表现得更好。几乎所有的令人振奋的关于RNN的结果都是通过 LSTM 达到的。这篇博文也会就 LSTM 进行展开。 长期依赖（Long-Term Dependencies）问题 RNN 的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。如果RNN可以做到这个，他们就变得非常有用。但是真的可以么？答案是，还有很多依赖因素。 有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词。如果我们试着预测 “the clouds are in the sky”最后的词，我们并不需要任何其他的上下文 —— 因此下一个词很显然就应该是sky。在这样的场景中，相关的信息和预测的词位置之间的间隔是非常小的，RNN 可以学会使用先前的信息。 但是同样会有一些更加复杂的场景。假设我们试着去预测“I grew up in France… I speak fluent French”最后的词。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前位置很远的 France 的上下文的。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。 不幸的是，在这个间隔不断增大时，RNN会丧失学习到连接如此远的信息的能力。 在理论上，RNN绝对可以处理这样的长期依赖问题。人们可以仔细挑选参数来解决这类问题中的最初级形式，但在实践中，RNN肯定不能够成功学习到这些知识。Bengio,etal.(1994)等人对该问题进行了深入的研究，他们发现一些使训练 RNN 变得非常困难的相当根本的原因。 然而，幸运的是，LSTM 并没有这个问题！ LSTM 网络 Long Short Term 网络——一般就叫做LSTM——是一种RNN特殊的类型，可以学习长期依赖信息。LSTM由Hochreiter &amp;Schmidhuber(1997)提出，并在近期被AlexGraves进行了改良和推广。在很多问题，LSTM都取得相当巨大的成功，并得到了广泛的使用。LSTM 通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是 LSTM 的默认行为，而非需要付出很大代价才能获得的能力！ 所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层。 LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于 单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互。 不必担心这里的细节。我们会一步一步地剖析LSTM解析图。现在，我们先来熟悉一下图中使用的各种元素的图标。 在上面的图例中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表pointwise的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。 LSTM 的核心思想 LSTM 的关键就是细胞状态，水平线在图上方贯穿运行。细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。 LSTM 有通过精心设计的称作为“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。他们包含一个sigmoid神经网络层和一个pointwise 乘法操作。 Sigmoid 层输出0到1之间的数值，描述每个部分有多少量可以通过。0 代表“不许任何量通过”，1就指“允许任意量通过”！LSTM拥有三个门，来保护和控制细胞状态。 逐步理解 LSTM 在我们 LSTM 中的第一步是决定我们会从细胞状态中丢弃什么信息。这个决定通过一个称为忘记门层完成。该门会读取h_{t-1}和x_t，输出一个在 0 到 1 之间的数值给每个在细胞状态C_{t-1}中的数字。1表示“完全保留”，0 表示“完全舍弃”。 让我们回到语言模型的例子中来基于已经看到的预测下一个词。在这个问题中，细胞状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语。 下一步是确定什么样的新信息被存放在细胞状态中。这里包含两个部分。第一，sigmoid层称“输入门层”决定什么值我们将要更新。然后，一个 tanh 层创建一个新的候选值向量C{t}，会被加入到状态中。下一步，我们会讲这两个信息来产生对状态的更新。 在我们语言模型的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语。 现在是更新旧细胞状态的时间了，C_{t-1}更新为C_t。前面的步骤已经决定了将会做什么，我们现在就是实际去完成。 我们把旧状态与f_t相乘，丢弃掉我们确定需要丢弃的信息。接着加上 i_t * C{t}。这就是新的候选值，根据我们决定更新每个状态的程度进行变化。 在语言模型的例子中，这就是我们实际根据前面确定的目标，丢弃旧代词的性别信息并添加新的信息的地方。 最终，我们需要确定输出什么值。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先，我们运行一个sigmoid层来确定细胞状态的哪个部分将输出出去。接着，我们把细胞状态通过tanh进行处理（得到一个在-1到1之间的值）并将它和sigmoid门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。 在语言模型的例子中，因为他就看到了一个代词，可能需要输出与一个 动词 相关的信息。例如，可能输出是否代词是单数还是负数，这样如果是动词的话，我们也知道动词需要进行的词形变化。 LSTM 的变体 我们到目前为止都还在介绍正常的 LSTM。但是不是所有的 LSTM 都长成一个样子的。实际上，几乎所有包含LSTM的论文都采用了微小的变体。差异非常小，但是也值得拿出来讲一下。 其中一个流形的 LSTM 变体，就是由 Gers &amp; Schmidhuber (2000) 提出的，增加了 “peephole connection”。是说，我们让门层也会接受细胞状态的输入。 上面的图例中，我们增加了peephole到每个门上，但是许多论文会加入部分的 peephole 而非所有都加。 另一个变体是通过使用coupled忘记和输入门。不同于之前是分开确定什么忘记和需要添加什么新的信息，这里是一同做出决定。我们仅仅会当我们将要输入在当前位置时忘记。我们仅仅输入新的值到那些我们已经忘记旧的信息的那些状态 。 另一个改动较大的变体是 *Gated Recurrent Unit (GRU)*，这是由 Cho, et al. (2014)提出。它将忘记门和输入门合成了一个单一的更新门。同样还混合了细胞状态和隐藏状态，和其他一些改动。最终的模型比标准的 LSTM 模型要简单，也是非常流行的变体。 这里只是部分流行的 LSTM 变体。当然还有很多其他的，如Yao, et al.(2015)提出的Depth Gated RNN。还有用一些完全不同的观点来解决长期依赖的问题，如Koutnik, et al. (2014) 提出的 Clockwork RNN。 要问哪个变体是最好的？其中的差异性真的重要吗？Greff, et al. (2015) 给出了流行变体的比较，结论是他们基本上是一样的。Jozefowicz, et al. (2015) 则在超过1万种RNN架构上进行了测试，发现一些架构在某些任务上也取得了比 LSTM 更好的结果。 结论 刚开始，我提到通过RNN得到重要的结果。本质上所有这些都可以使用 LSTM 完成。对于大多数任务确实展示了更好的性能！ 由于 LSTM 一般是通过一系列的方程表示的，使得LSTM有一点令人费解。然而本文中一步一步地解释让这种困惑消除了不少。 LSTM 是我们在RNN中获得的重要成功。很自然地，我们也会考虑：哪里会有更加重大的突破呢？在研究人员间普遍的观点是：“Yes!下一步已经有了——那就是注意力！”这个想法是让RNN的每一步都从更加大的信息集中挑选信息。例如，如果你使用RNN来产生一个图片的描述，可能会选择图片的一个部分，根据这部分信息来产生输出的词。实际上，Xu,etal.(2015)已经这么做了——如果你希望深入探索注意力可能这就是一个有趣的起点！还有一些使用注意力的相当振奋人心的研究成果，看起来有更多的东西亟待探索…… 注意力也不是 RNN研究领域中唯一的发展方向。例如，Kalchbrenner, et al. (2015) 提出的 Grid LSTM 看起来也是很有前途。使用生成模型的 RNN，诸如Gregor, et al. (2015) Chung, et al. (2015) 和 Bayer &amp; Osendorfer (2015) 提出的模型同样很有趣。在过去几年中，RNN 的研究已经相当的燃，而研究成果当然也会更加丰富！ 原文链接","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"LSTM","slug":"LSTM","permalink":"http://huangzhiyuan.github.io/tags/LSTM/"}]},{"title":"一文读懂卷积神经网络CNN","slug":"CNN-introduction","date":"2017-12-06T13:49:06.000Z","updated":"2020-03-01T12:08:02.000Z","comments":true,"path":"2017/12/06/CNN-introduction/","link":"","permalink":"http://huangzhiyuan.github.io/2017/12/06/CNN-introduction/","excerpt":"卷积神经网络（Convolutional Neural Network，CNN）。Deep Learning是全部深度学习算法的总称，CNN是深度学习算法在图像处理领域的一个应用。","text":"卷积神经网络（Convolutional Neural Network，CNN）。Deep Learning是全部深度学习算法的总称，CNN是深度学习算法在图像处理领域的一个应用。 卷积神经网络的基本概念 卷积神经网络是近年发展起来，并引起广泛重视的一种高效识别方法。20世纪60年代，Hubel和Wiesel在研究猫脑皮层中用于局部敏感和方向选择的神经元时发现其独特的网络结构可以有效地降低反馈神经网络的复杂性，继而提出了卷积神经网络（Convolutional Neural Networks-简称CNN）。现在，CNN已经成为众多科学领域的研究热点之一，特别是在模式分类领域，由于该网络避免了对图像的复杂前期预处理，可以直接输入原始图像，因而得到了更为广泛的应用。 K.Fukushima在1980年提出的新识别机是卷积神经网络的第一个实现网络。随后，更多的科研工作者对该网络进行了改进。其中，具有代表性的研究成果是Alexander和Taylor提出的“改进认知机”，该方法综合了各种改进方法的优点并避免了耗时的误差反向传播。 一般地，CNN的基本结构包括两层，其一为特征提取层，每个神经元的输入与前一层的局部接受域相连，并提取该局部的特征。一旦该局部特征被提取后，它与其它特征间的位置关系也随之确定下来；其二是特征映射层，网络的每个计算层由多个特征映射组成，每个特征映射是一个平面，平面上所有神经元的权值相等。特征映射结构采用影响函数核小的sigmoid函数作为卷积网络的激活函数，使得特征映射具有位移不变性。此外，由于一个映射面上的神经元共享权值，因而减少了网络自由参数的个数。卷积神经网络中的每一个卷积层都紧跟着一个用来求局部平均与二次提取的计算层，这种特有的两次特征提取结构减小了特征分辨率。 卷积神经网络的应用 CNN主要用来识别位移、缩放及其他形式扭曲不变性的二维图形。由于CNN的特征检测层通过训练数据进行学习，所以在使用CNN时，避免了显示的特征抽取，而隐式地从训练数据中进行学习；再者由于同一特征映射面上的神经元权值相同，所以网络可以并行学习，这也是卷积网络相对于神经元彼此相连网络的一大优势。卷积神经网络以其局部权值共享的特殊结构在语音识别和图像处理方面有着独特的优越性，其布局更接近于实际的生物神经网络，权值共享降低了网络的复杂性，特别是多维输入向量的图像可以直接输入网络这一特点避免了特征提取和分类过程中数据重建的复杂度。 卷积神经网络的原理神经网络 首先介绍神经网络，这一步的详细可以参考资源1。简要介绍下。神经网络的每个单元如下：其中，该单元也可以被称作是Logistic回归模型。当将多个单元组合起来并具有分层结构时，就形成了神经网络模型。下图展示了一个具有一个隐含层的神经网络。其对应的公式如下： 比较类似的，可以拓展到有2,3,4,5，…个隐含层。神经网络的训练方法也同Logistic类似，不过由于其多层性，还需要利用链式求导法则对隐含层的节点进行求导，即梯度下降+链式求导法则，专业名称为反向传播。关于训练算法，本文暂不涉及。 卷积神经网络 受Hubel和Wiesel对猫视觉皮层电生理研究启发，有人提出卷积神经网络（CNN），Yann Lecun最早将CNN用于手写数字识别并一直保持了其在该问题的霸主地位。近年来卷积神经网络在多个方向持续发力，在语音识别、人脸识别、通用物体识别、运动分析、自然语言处理甚至脑电波分析方面均有突破。 卷积神经网络与普通神经网络的区别在于，卷积神经网络包含了一个由卷积层和子采样层构成的特征抽取器。在卷积神经网络的卷积层中，一个神经元只与部分邻层神经元连接。在CNN的一个卷积层中，通常包含若干个特征平面(featureMap)，每个特征平面由一些矩形排列的的神经元组成，同一特征平面的神经元共享权值，这里共享的权值就是卷积核。卷积核一般以随机小数矩阵的形式初始化，在网络的训练过程中卷积核将学习得到合理的权值。共享权值（卷积核）带来的直接好处是减少网络各层之间的连接，同时又降低了过拟合的风险。子采样也叫做池化（pooling），通常有均值子采样（mean pooling）和最大值子采样（max pooling）两种形式。子采样可以看作一种特殊的卷积过程。卷积和子采样大大简化了模型复杂度，减少了模型的参数。卷积神经网络的基本结构如图所示： 卷积神经网络由三部分构成。第一部分是输入层。第二部分由n个卷积层和池化层的组合组成。第三部分由一个全连结的多层感知机分类器构成。 局部感受野 卷积神经网络有两种神器可以降低参数数目，第一种神器叫做局部感知野。一般认为人对外界的认知是从局部到全局的，而图像的空间联系也是局部的像素联系较为紧密，而距离较远的像素相关性则较弱。因而，每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息。网络部分连通的思想，也是受启发于生物学里面的视觉系统结构。视觉皮层的神经元就是局部接受信息的（即这些神经元只响应某些特定区域的刺激）。如下图所示：左图为全连接，右图为局部连接。 在上右图中，假如每个神经元只和10×10个像素值相连，那么权值数据为1000000×100个参数，减少为原来的万分之一。而那10×10个像素值对应的10×10个参数，其实就相当于卷积操作。 权值共享 但其实这样的话参数仍然过多，那么就启动第二级神器，即权值共享。在上面的局部连接中，每个神经元都对应100个参数，一共1000000个神经元，如果这1000000个神经元的100个参数都是相等的，那么参数数目就变为100了。 怎么理解权值共享呢？我们可以这100个参数（也就是卷积操作）看成是提取特征的方式，该方式与位置无关。这其中隐含的原理则是：图像的一部分的统计特性与其他部分是一样的。这也意味着我们在这一部分学习的特征也能用在另一部分上，所以对于这个图像上的所有位置，我们都能使用同样的学习特征。 更直观一些，当从一个大尺寸图像中随机选取一小块，比如说 8x8 作为样本，并且从这个小块样本中学习到了一些特征，这时我们可以把从这个8x8样本中学习到的特征作为探测器，应用到这个图像的任意地方中去。特别是，我们可以用从 8x8 样本中所学习到的特征跟原本的大尺寸图像作卷积，从而对这个大尺寸图像上的任一位置获得一个不同特征的激活值。 如下图所示，展示了一个3×3的卷积核在5×5的图像上做卷积的过程。每个卷积都是一种特征提取方式，就像一个筛子，将图像中符合条件（激活值越大越符合条件）的部分筛选出来 多卷积核 上面所述只有100个参数时，表明只有1个10*10的卷积核，显然，特征提取是不充分的，我们可以添加多个卷积核，比如32个卷积核，可以学习32种特征。在有多个卷积核时，如下图所示： 上图右，不同颜色表明不同的卷积核。每个卷积核都会将图像生成为另一幅图像。比如两个卷积核就可以将生成两幅图像，这两幅图像可以看做是一张图像的不同的通道。如下图所示，下图有个小错误，即将w1改为w0，w2改为w1即可。下文中仍以w1和w2称呼它们。 下图展示了在四个通道上的卷积操作，有两个卷积核，生成两个通道。其中需要注意的是，四个通道上每个通道对应一个卷积核，先将w2忽略，只看w1，那么在w1的某位置（i,j）处的值，是由四个通道上（i,j）处的卷积结果相加然后再取激活函数值得到的。 所以，在上图由4个通道卷积得到2个通道的过程中，参数的数目为4×2×2×2个，其中4表示4个通道，第一个2表示生成2个通道，最后的2×2表示卷积核大小。 Down-pooling 在通过卷积获得了特征(features)之后，下一步我们希望利用这些特征去做分类。理论上讲，人们可以用所有提取得到的特征去训练分类器，例如softmax分类器，但这样做面临计算量的挑战。例如：对于一个96X96像素的图像，假设我们已经学习得到了400个定义在8X8输入上的特征，每一个特征和图像卷积都会得到一个 (96 − 8 + 1) × (96 − 8 + 1) = 7921 维的卷积特征，由于有 400 个特征，所以每个样例 (example) 都会得到一个 7921 × 400 = 3,168,400 维的卷积特征向量。学习一个拥有超过3百万特征输入的分类器十分不便，并且容易出现过拟合 (over-fitting)。 为了解决这个问题，首先回忆一下，我们之所以决定使用卷积后的特征是因为图像具有一种“静态性”的属性，这也就意味着在一个图像区域有用的特征极有可能在另一个区域同样适用。因此，为了描述大的图像，一个很自然的想法就是对不同位置的特征进行聚合统计，例如，人们可以计算图像一个区域上的某个特定特征的平均值(或最大值)。这些概要统计特征不仅具有低得多的维度 (相比使用所有提取得到的特征)，同时还会改善结果(不容易过拟合)。这种聚合的操作就叫做池化 (pooling)，有时也称为平均池化或者最大池化(取决于计算池化的方法)。 子采样有两种形式，一种是均值子采样（mean-pooling），一种是最大值子采样（max-pooling）。两种子采样看成特殊的卷积过程，如图下图所示： (1)均值子采样的卷积核中每个权重都是0.25，卷积核在原图inputX上的滑动的步长为2。均值子采样的效果相当于把原图模糊缩减至原来的1/4。 (2)最大值子采样的卷积核中各权重值中只有一个为1，其余均为0，卷积核中为1的位置对应inputX被卷积核覆盖部分值最大的位置。卷积核在原图inputX上的滑动步长为2。最大值子采样的效果是把原图缩减至原来的1/4，并保留每个2*2区域的最强输入。 至此，卷积神经网络的基本结构和原理已经阐述完毕。 多卷积层 在实际应用中，往往使用多层卷积，然后再使用全连接层进行训练，多层卷积的目的是一层卷积学到的特征往往是局部的，层数越高，学到的特征就越全局化。 卷积神经网络的训练 本文的主要目的是介绍CNN参数在使用bp算法时该怎么训练，毕竟CNN中有卷积层和下采样层，虽然和MLP的bp算法本质上相同，但形式上还是有些区别的，很显然在完成CNN反向传播前了解bp算法是必须的。 Forward前向传播 前向过程的卷积为典型valid的卷积过程，即卷积核kernalW覆盖在输入图inputX上，对应位置求积再求和得到一个值并赋给输出图OutputY对应的位置。每次卷积核在inputX上移动一个位置，从上到下从左到右交叠覆盖一遍之后得到输出矩阵outputY(如图4.1与图4.3所示)。如果卷积核的输入图inputX为MxNx大小，卷积核为MwNw大小，那么输出图Y为（Mx-Mw+1）*（Nx-Nw+1）大小。 BackForward反向传播 在错误信号反向传播过程中，先按照神经网络的错误反传方式得到尾部分类器中各神经元的错误信号，然后错误信号由分类器向前面的特征抽取器传播。错误信号从子采样层的特征图（subFeatureMap）往前面卷积层的特征图（featureMap）传播要通过一次full卷积过程来完成。这里的卷积和上一节卷积的略有区别。如果卷积核kernalW的长度为MwMw的方阵，那么subFeatureMap的错误信号矩阵Q_err需要上下左右各拓展Mw-1行或列，与此同时卷积核自身旋转180度。subFeatureMap的错误信号矩阵P_err等于featureMap的误差矩阵Q_err卷积旋转180度的卷积核W_rot180。 下图错误信号矩阵Q_err中的A，它的产生是P中左上22小方块导致的，该22的小方块的对A的责任正好可以用卷积核W表示，错误信号A通过卷积核将错误信号加权传递到与错误信号量为A的神经元所相连的神经元a、b、d、e中，所以在下图中的P_err左上角的22位置错误值包含A、2A、3A、4A。同理，我们可以论证错误信号B、C、D的反向传播过程。综上所述，错误信号反向传播过程可以用下图中的卷积过程表示。 权值更新过程中的卷积 卷积神经网络中卷积层的权重更新过程本质是卷积核的更新过程。由神经网络的权重修改策略我们知道一条连接权重的更新量为该条连接的前层神经元的兴奋输出乘以后层神经元的输入错误信号，卷积核的更新也是按照这个规律来进行。 在前向卷积过程中，卷积核的每个元素（链接权重）被使用过四次，所以卷积核每个元素的产生四个更新量。把前向卷积过程当做切割小图进行多个神经网络训练过程，我们得到四个4*1的神经网络的前层兴奋输入和后层输入错误信号，如图所示。 根据神经网络的权重修改策略，我们可以算出如图所示卷积核的更新量W_delta。权重更新量W_delta可由P_out和Q_err卷积得到，如图下图所示。 常见网络结构ImageNet-2010网络结构 ImageNet LSVRC是一个图片分类的比赛，其训练集包括127W+张图片，验证集有5W张图片，测试集有15W张图片。本文截取2010年Alex Krizhevsky的CNN结构进行说明，该结构在2010年取得冠军，top-5错误率为15.3%。值得一提的是，在今年的ImageNet LSVRC比赛中，取得冠军的GoogNet已经达到了top-5错误率6.67%。可见，深度学习的提升空间还很巨大。 下图即为Alex的CNN结构图。需要注意的是，该模型采用了2-GPU并行结构，即第1、2、4、5卷积层都是将模型参数分为2部分进行训练的。在这里，更进一步，并行结构分为数据并行与模型并行。数据并行是指在不同的GPU上，模型结构相同，但将训练数据进行切分，分别训练得到不同的模型，然后再将模型进行融合。而模型并行则是，将若干层的模型参数进行切分，不同的GPU上使用相同的数据进行训练，得到的结果直接连接作为下一层的输入。 上图模型的基本参数为： 输入：224×224大小的图片，3通道 第一层卷积：11×11大小的卷积核96个，每个GPU上48个。 第一层max-pooling：2×2的核。 第二层卷积：5×5卷积核256个，每个GPU上128个。 第二层max-pooling：2×2的核。 第三层卷积：与上一层是全连接，3*3的卷积核384个。分到两个GPU上个192个。 第四层卷积：3×3的卷积核384个，两个GPU各192个。该层与上一层连接没有经过pooling层。 第五层卷积：3×3的卷积核256个，两个GPU上个128个。 第五层max-pooling：2×2的核。 第一层全连接：4096维，将第五层max-pooling的输出连接成为一个一维向量，作为该层的输入。 第二层全连接：4096维 Softmax层：输出为1000，输出的每一维都是图片属于该类别的概率。 DeepID网络结构 DeepID网络结构是香港中文大学的SunYi开发出来用来学习人脸特征的卷积神经网络。每张输入的人脸被表示为160维的向量，学习到的向量经过其他模型进行分类，在人脸验证试验上得到了97.45%的正确率，更进一步的，原作者改进了CNN，又得到了99.15%的正确率。如下图所示，该结构与ImageNet的具体参数类似，所以只解释一下不同的部分吧。 上图中的结构，在最后只有一层全连接层，然后就是softmax层了。论文中就是以该全连接层作为图像的表示。在全连接层，以第四层卷积和第三层max-pooling的输出作为全连接层的输入，这样可以学习到局部的和全局的特征。 参考资源[1] http://deeplearning.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B 栀子花对Stanford深度学习研究团队的深度学习教程的翻译[2] http://blog.csdn.net/zouxy09/article/details/14222605 csdn博主zouxy09深度学习教程系列[3] http://deeplearning.net/tutorial/ theano实现deep learning[4] Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105.[5] Sun Y, Wang X, Tang X. Deep learning face representation from predicting 10,000 classes[C]//Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. IEEE, 2014: 1891-1898.[6] http://blog.csdn.net/stdcoutzyx/article/details/41596663[7] http://blog.csdn.net/yunpiao123456/article/details/52437794","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"CNN","slug":"CNN","permalink":"http://huangzhiyuan.github.io/tags/CNN/"}]},{"title":"神经网络——BP算法","slug":"BP-Algorithm","date":"2017-12-06T13:40:02.000Z","updated":"2020-03-01T12:07:12.000Z","comments":true,"path":"2017/12/06/BP-Algorithm/","link":"","permalink":"http://huangzhiyuan.github.io/2017/12/06/BP-Algorithm/","excerpt":"对于初学者来说，了解了一个算法的重要意义，往往会引起他对算法本身的重视。BP(Back Propagation，后向传播)算法，具有非凡的历史意义和重大的现实意义。","text":"对于初学者来说，了解了一个算法的重要意义，往往会引起他对算法本身的重视。BP(Back Propagation，后向传播)算法，具有非凡的历史意义和重大的现实意义。 BP算法的意义历史意义 1969年,作为人工神经网络创始人的明斯基(Marrin M insky)和佩珀特(Seymour Papert)合作出版了《感知器》一书,论证了简单的线性感知器功能有限,不能解决如“异或”(XOR )这样的基本问题,而且对多层网络也持悲观态度。这些论点给神经网络研究以沉重的打击,很多科学家纷纷离开这一领域,神经网络的研究走向长达10年的低潮时期。 1974年哈佛大学的Paul Werbos发明BP算法时，正值神经外网络低潮期，并未受到应有的重视。1983年，加州理工学院的物理学家John Hopfield利用神经网络，在旅行商这个NP完全问题的求解上获得当时最好成绩，引起了轰动2。然而,Hopfield的研究成果仍未能指出明斯基等人论点的错误所在,要推动神经网络研究的全面开展必须直接解除对感知器——多层网络算法的疑虑。真正打破明斯基冰封魔咒的是，David Rumelhart等学者出版的《平行分布处理:认知的微观结构探索》一书。书中完整地提出了BP算法,系统地解决了多层网络中隐单元连接权的学习问题,并在数学上给出了完整的推导。这是神经网络发展史上的里程碑，BP算法迅速走红，掀起了神经网络的第二次高潮。 因此，BP算法的历史意义：明确地否定了明斯基等人的错误观点，对神经网络第二次高潮具有决定性意义。 现实意义 这一点是说BP算法在神经网络领域中的地位和意义。 BP算法是迄今最成功的神经网络学习算法，现实任务中使用神经网络时，大多是在使用BP算法进行训练2,包括最近炙手可热的深度学习概念下的卷积神经网络(CNNs)。 什么是BP算法BP神经网络 BP神经网络是这样一种神经网络模型，它是由一个输入层、一个输出层和一个或多个隐层构成，它的激活函数采用sigmoid函数，采用BP算法训练的多层前馈神经网络。 BP算法基本思想 BP算法全称叫作误差反向传播(error Back Propagation，或者也叫作误差逆传播)算法。其算法基本思想为：在2.1所述的前馈网络中，输入信号经输入层输入，通过隐层计算由输出层输出，输出值与标记值比较，若有误差，将误差反向由输出层向输入层传播，在这个过程中，利用梯度下降算法对神经元权值进行调整。 BP算法介绍 要想搞懂BP算法，需要先直观理解多层神经网络的训练。 机器学习可以看做是数理统计的一个应用，在数理统计中一个常见的任务就是拟合，也就是给定一些样本点，用合适的曲线揭示这些样本点随着自变量的变化关系。深度学习同样也是为了这个目的，只不过此时，样本点不再限定为(x, y)点对，而可以是由向量、矩阵等等组成的广义点对(X,Y)。而此时，(X,Y)之间的关系也变得十分复杂，不太可能用一个简单函数表示。然而，人们发现可以用多层神经网络来表示这样的关系，而多层神经网络的本质就是一个多层复合的函数。借用网上找到的一幅图1，来直观描绘一下这种复合关系。 其对应的表达式如下： 上面式中的Wij就是相邻两层神经元之间的权值，它们就是深度学习需要学习的参数，也就相当于直线拟合y=k*x+b中的待求参数k和b。 和直线拟合一样，深度学习的训练也有一个目标函数，这个目标函数定义了什么样的参数才算一组“好参数”，不过在机器学习中，一般是采用成本函数（cost function），然后，训练目标就是通过调整每一个权值Wij来使得cost达到最小。cost函数也可以看成是由所有待求权值Wij为自变量的复合函数，而且基本上是非凸的，即含有许多局部最小值。但实际中发现，采用我们常用的梯度下降法就可以有效的求解最小化cost函数的问题。 梯度下降法需要给定一个初始点，并求出该点的梯度向量，然后以负梯度方向为搜索方向，以一定的步长进行搜索，从而确定下一个迭代点，再计算该新的梯度方向，如此重复直到cost收敛。那么如何计算梯度呢？ 在图中，引入了中间变量c,d。 为了求出a=2, b=1时，e的梯度，我们可以先利用偏导数的定义求出不同层之间相邻节点的偏导关系，如下图所示。 大家也许已经注意到，这样做是十分冗余的，因为很多路径被重复访问了。比如上图中，a-c-e和b-c-e就都走了路径c-e。对于权值动则数万的深度模型中的神经网络，这样的冗余所导致的计算量是相当大的。同样是利用链式法则，BP算法则机智地避开了这种冗余，它对于每 一个路径只访问一次就能求顶点对所有下层节点的偏导值。 正如反向传播(BP)算法的名字说的那样，BP算法是反向(自上往下)来寻找路径的。从最上层的节点e开始，初始值为1，以层为单位进行处理。对于e的下一层的所有子节点，将1乘以e到某个节点路径上的偏导值，并将结果“堆放”在该子节点中。等e所在的层按照这样传播完毕后，第二层的每一个节点都“堆放”些值，然后我们针对每个节点，把它里面所有“堆放”的值求和，就得到了顶点e对该节点的偏导。然后将这些第二层的节点各自作为起始顶点，初始值设为顶点e对它们的偏导值，以”层”为单位重复上述传播过程，即可求出顶点e对每一层节点的偏导数。 以上图为例，节点c接受e发送的12并堆放起来，节点d接受e发送的13并堆放起来，至此第二层完毕，求出各节点总堆放量并继续向下一层发送。节点c向a发送21并对堆放起来，节点c向b发送21并堆放起来，节点d向b发送31并堆放起来，至此第三层完毕，节点a堆放起来的量为2，节点b堆放起来的量为21+3*1=5, 即顶点e对b的偏导数为5. 举个不太恰当的例子，如果把上图中的箭头表示欠钱的关系，即c→e表示e欠c的钱。以a, b为例，直接计算e对它们俩的偏导相当于a, b各自去讨薪。a向c讨薪，c说e欠我钱，你向他要。于是a又跨过c去找e。b先向c讨薪，同样又转向e，b又向d讨薪，再次转向e。可以看到，追款之路，充满艰辛，而且还有重复，即a, b 都从c转向e。 而BP算法就是主动还款。e把所欠之钱还给c，d。c，d收到钱，乐呵地把钱转发给了a，b，皆大欢喜。 BP算法数学工具BP算法中核心的数学工具就是微积分的链式求导法则。 BP算法的推导 BP算法的缺点局部极小值问题BP算法的缺点，首当其冲就是局部极小值问题。 算法训练非常慢BP算法本质上是梯度下降，而它所要优化的目标函数又非常复杂，这使得BP算法效率低下。 参考1、《BP算法的哲学思考》，成素梅、郝中华著 2、《机器学习》，周志华著 3、Deep Learning论文笔记之（四）CNN卷积神经网络推导和实现 4、知乎：如何直观地解释 back propagation 算法？ 另一种较为直观的介绍，参见知乎：Deep learning 原文链接","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"BP算法","slug":"BP算法","permalink":"http://huangzhiyuan.github.io/tags/BP%E7%AE%97%E6%B3%95/"}]},{"title":"CNN/RNN/DNN","slug":"CNN-RNN-DNN","date":"2017-12-05T11:55:05.000Z","updated":"2020-03-01T12:28:16.000Z","comments":true,"path":"2017/12/05/CNN-RNN-DNN/","link":"","permalink":"http://huangzhiyuan.github.io/2017/12/05/CNN-RNN-DNN/","excerpt":"从广义上来说，NN(或是更美的DNN)确实可以认为包含了CNN、RNN这些具体的变种形式。在实际应用中，所谓的深度神经网络DNN，往往融合了多种已知的结构，包括卷积层或是LSTM单元。但是就题主的意思来看，这里的DNN应该特指全连接的神经元结构，并不包含卷积单元或是时间上的关联。","text":"从广义上来说，NN(或是更美的DNN)确实可以认为包含了CNN、RNN这些具体的变种形式。在实际应用中，所谓的深度神经网络DNN，往往融合了多种已知的结构，包括卷积层或是LSTM单元。但是就题主的意思来看，这里的DNN应该特指全连接的神经元结构，并不包含卷积单元或是时间上的关联。 因此，题主一定要将DNN、CNN、RNN等进行对比，也未尝不可。其实，如果我们顺着神经网络技术发展的脉络，就很容易弄清这几种网络结构发明的初衷，和他们之间本质的区别。 神经网络技术起源于上世纪五、六十年代，当时叫感知机(perceptron)，拥有输入层、输出层和一个隐含层。输入的特征向量通过隐含层变换达到输出层，在输出层得到分类结果。早期感知机的推动者是Rosenblatt。(扯一个不相关的：由于计算技术的落后，当时感知器传输函数是用线拉动变阻器改变电阻的方法机械实现的，脑补一下科学家们扯着密密麻麻的导线的样子…)但是，Rosenblatt的单层感知机有一个严重得不能再严重的问题，即它对稍复杂一些的函数都无能为力(比如最为典型的“异或”操作)。连异或都不能拟合，你还能指望这货有什么实际用途么，随着数学的发展，这个缺点直到上世纪八十年代才被Rumelhart、Williams、Hinton、LeCun等人(反正就是一票大牛)发明的多层感知机(multilayer perceptron)克服。多层感知机，顾名思义，就是有多个隐含层的感知机。先看一下多层感知机的结构： 多层感知机可以摆脱早期离散传输函数的束缚，使用sigmoid或tanh等连续函数模拟神经元对激励的响应，在训练算法上则使用Werbos发明的反向传播BP算法。对，这货就是我们现在所说的神经网络NN——神经网络听起来不知道比感知机高端到哪里去了!多层感知机解决了之前无法模拟异或逻辑的缺陷，同时更多的层数也让网络更能够刻画现实世界中的复杂情形。 多层感知机给我们带来的启示是，神经网络的层数直接决定了它对现实的刻画能力——利用每层更少的神经元拟合更加复杂的函数。即便大牛们早就预料到神经网络需要变得更深，但是有一个梦魇总是萦绕左右。随着神经网络层数的加深，优化函数越来越容易陷入局部最优解，并且这个“陷阱”越来越偏离真正的全局最优。利用有限数据训练的深层网络，性能还不如较浅层网络。 同时，另一个不可忽略的问题是随着网络层数增加，“梯度消失”现象更加严重。具体来说，我们常常使用sigmoid作为神经元的输入输出函数。对于幅度为1的信号，在BP反向传播梯度时，每传递一层，梯度衰减为原来的0.25。层数一多，梯度指数衰减后低层基本上接受不到有效的训练信号。 2006年，Hinton利用预训练方法缓解了局部最优解问题，将隐含层推动到了7层，神经网络真正意义上有了“深度”，由此揭开了深度学习的热潮。这里的“深度”并没有固定的定义——在语音识别中4层网络就能够被认为是“较深的”，而在图像识别中20层以上的网络屡见不鲜。为了克服梯度消失，ReLU、maxout等传输函数代替了sigmoid，形成了如今DNN的基本形式。单从结构上来说，全连接的DNN和图1的多层感知机是没有任何区别的。 值得一提的是，今年出现的高速公路网络(highway network)和深度残差学习(deep residual learning)进一步避免了梯度消失，网络层数达到了前所未有的一百多层(深度残差学习：152层)! 如图1所示，我们看到全连接DNN的结构里下层神经元和所有上层神经元都能够形成连接，带来的潜在问题是参数数量的膨胀。假设输入的是一幅像素为1K*1K的图像，隐含层有1M个节点，光这一层就有10^12个权重需要训练，这不仅容易过拟合，而且极容易陷入局部最优。另外，图像中有固有的局部模式(比如轮廓、边界，人的眼睛、鼻子、嘴等)可以利用，显然应该将图像处理中的概念和神经网络技术相结合。此时我们可以祭出题主所说的卷积神经网络CNN。 对于CNN来说，并不是所有上下层神经元都能直接相连，而是通过“卷积核”作为中介。同一个卷积核在所有图像内是共享的，图像通过卷积操作后仍然保留原先的位置关系。两层之间的卷积传输的示意图如下： 通过一个例子简单说明卷积神经网络的结构。假设图3中m-1=1是输入层，我们需要识别一幅彩色图像，这幅图像具有四个通道ARGB(透明度和红绿蓝，对应了四幅相同大小的图像)，假设卷积核大小为100100，共使用100个卷积核w1到w100(从直觉来看，每个卷积核应该学习到不同的结构特征)。 用w1在ARGB图像上进行卷积操作，可以得到隐含层的第一幅图像;这幅隐含层图像左上角第一个像素是四幅输入图像左上角100100区域内像素的加权求和，以此类推。同理，算上其他卷积核，隐含层对应100幅“图像”。每幅图像对是对原始图像中不同特征的响应。按照这样的结构继续传递下去。CNN中还有max-pooling等操作进一步提高鲁棒性。 注意到最后一层实际上是一个全连接层，在这个例子里，我们注意到输入层到隐含层的参数瞬间降低到了100100100=10^6个!这使得我们能够用已有的训练数据得到良好的模型。题主所说的适用于图像识别，正是由于CNN模型限制参数了个数并挖掘了局部结构的这个特点。顺着同样的思路，利用语音语谱结构中的局部信息，CNN照样能应用在语音识别中。全连接的DNN还存在着另一个问题——无法对时间序列上的变化进行建模。然而，样本出现的时间顺序对于自然语言处理、语音识别、手写体识别等应用非常重要。对了适应这种需求，就出现了题主所说的另一种神经网络结构——循环神经网络RNN。在普通的全连接网络或CNN中，每层神经元的信号只能向上一层传播，样本的处理在各个时刻独立，因此又被成为前向神经网络(Feed-forward Neural Networks)。而在RNN中，神经元的输出可以在下一个时间戳直接作用到自身，即第i层神经元在m时刻的输入，除了(i-1)层神经元在该时刻的输出外，还包括其自身在(m-1)时刻的输出!表示成图就是这样的： 我们可以看到在隐含层节点之间增加了互连。为了分析方便，我们常将RNN在时间上进行展开，得到如图6所示的结构： (t+1)时刻网络的最终结果O(t+1)是该时刻输入和所有历史共同作用的结果!这就达到了对时间序列建模的目的。 不知题主是否发现，RNN可以看成一个在时间上传递的神经网络，它的深度是时间的长度!正如我们上面所说，“梯度消失”现象又要出现了，只不过这次发生在时间轴上。 对于t时刻来说，它产生的梯度在时间轴上向历史传播几层之后就消失了，根本就无法影响太遥远的过去。因此，之前说“所有历史”共同作用只是理想的情况，在实际中，这种影响也就只能维持若干个时间戳。 为了解决时间上的梯度消失，机器学习领域发展出了长短时记忆单元LSTM，通过门的开关实现时间上记忆功能，并防止梯度消失，一个LSTM单元长这个样子： 除了上面提到的深度残差学习、LSTM外，深度学习还有许多其他的结构。举个例子，RNN既然能继承历史信息，是不是也能吸收点未来的信息呢?因为在序列信号分析中，如果我能预知未来，对识别一定也是有所帮助的。因此就有了双向RNN、双向LSTM，同时利用历史和未来的信息。 事实上，不论是那种网络，他们在实际应用中常常都混合着使用，比如CNN和RNN在上层输出之前往往会接上全连接层，很难说某个网络到底属于哪个类别。不难想象随着深度学习热度的延续，更灵活的组合方式、更多的网络结构将被发展出来。尽管看起来千变万化，但研究者们的出发点肯定都是为了解决特定的问题。题主如果想进行这方面的研究，不妨仔细分析一下这些结构各自的特点以及它们达成目标的手段。 原文链接","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"CNN","slug":"CNN","permalink":"http://huangzhiyuan.github.io/tags/CNN/"},{"name":"RNN","slug":"RNN","permalink":"http://huangzhiyuan.github.io/tags/RNN/"}]},{"title":"CPU、GPU","slug":"CPU-GPU","date":"2017-12-05T11:49:48.000Z","updated":"2020-03-01T12:09:20.000Z","comments":true,"path":"2017/12/05/CPU-GPU/","link":"","permalink":"http://huangzhiyuan.github.io/2017/12/05/CPU-GPU/","excerpt":"首先需要解释CPU和GPU这两个缩写分别代表什么。CPU即中央处理器，GPU即图形处理器。其次，要解释两者的区别，要先明白两者的相同之处：两者都有总线和外界联系，有自己的缓存体系，以及数字和逻辑运算单元。一句话，两者都为了完成计算任务而设计。","text":"首先需要解释CPU和GPU这两个缩写分别代表什么。CPU即中央处理器，GPU即图形处理器。其次，要解释两者的区别，要先明白两者的相同之处：两者都有总线和外界联系，有自己的缓存体系，以及数字和逻辑运算单元。一句话，两者都为了完成计算任务而设计。 两者的区别在于存在于片内的缓存体系和数字逻辑运算单元的结构差异：CPU虽然有多核，但总数没有超过两位数，每个核都有足够大的缓存和足够多的数字和逻辑运算单元，并辅助有很多加速分支判断甚至更复杂的逻辑判断的硬件；GPU的核数远超CPU，被称为众核（NVIDIA Fermi有512个核）。每个核拥有的缓存大小相对小，数字逻辑运算单元也少而简单（GPU初始时在浮点计算上一直弱于CPU）。从结果上导致CPU擅长处理具有复杂计算步骤和复杂数据依赖的计算任务，如分布式计算，数据压缩，人工智能，物理模拟，以及其他很多很多计算任务等。GPU由于历史原因，是为了视频游戏而产生的（至今其主要驱动力还是不断增长的视频游戏市场），在三维游戏中常常出现的一类操作是对海量数据进行相同的操作，如：对每一个顶点进行同样的坐标变换，对每一个顶点按照同样的光照模型计算颜色值。GPU的众核架构非常适合把同样的指令流并行发送到众核上，采用不同的输入数据执行。在2003-2004年左右，图形学之外的领域专家开始注意到GPU与众不同的计算能力，开始尝试把GPU用于通用计算（即GPGPU）。之后NVIDIA发布了CUDA，AMD和Apple等公司也发布了OpenCL，GPU开始在通用计算领域得到广泛应用，包括：数值分析，海量数据处理（排序，Map-Reduce等），金融分析等等。 简而言之，当程序员为CPU编写程序时，他们倾向于利用复杂的逻辑结构优化算法从而减少计算任务的运行时间，即Latency。当程序员为GPU编写程序时，则利用其处理海量数据的优势，通过提高总的数据吞吐量（Throughput）来掩盖Lantency。目前，CPU和GPU的区别正在逐渐缩小，因为GPU也在处理不规则任务和线程间通信方面有了长足的进步。另外，功耗问题对于GPU比CPU更严重。 CPU和GPU之所以大不相同，是由于其设计目标的不同，它们分别针对了两种不同的应用场景。CPU需要很强的通用性来处理各种不同的数据类型，同时又要逻辑判断又会引入大量的分支跳转和中断的处理。这些都使得CPU的内部结构异常复杂。而GPU面对的则是类型高度统一的、相互无依赖的大规模数据和不需要被打断的纯净的计算环境。 于是CPU和GPU就呈现出非常不同的架构（示意图）： 图片来自nVidia CUDA文档。其中绿色的是计算单元，橙红色的是存储单元，橙黄色的是控制单元。 GPU采用了数量众多的计算单元和超长的流水线，但只有非常简单的控制逻辑并省去了Cache。而CPU不仅被Cache占据了大量空间，而且还有有复杂的控制逻辑和诸多优化电路，相比之下计算能力只是CPU很小的一部分。 从上图可以看出： Cache, local memory： CPU &gt; GPU Threads(线程数): GPU &gt; CPU Registers: GPU &gt; CPU &gt;* 多寄存器可以支持非常多的Thread,thread需要用到register,thread数目大，register也必须得跟着很大才行。 SIMD Unit(单指令多数据流,以同步方式，在同一时间内执行同一条指令): GPU &gt; CPU。 CPU 基于低延时的设计： CPU有强大的ALU（算术运算单元）,它可以在很少的时钟周期内完成算术计算。当今的CPU可以达到64bit双精度。执行双精度浮点源算的加法和乘法只需要1～3个时钟周期。CPU的时钟周期的频率是非常高的，达到1.532～3GHz。 大的缓存也可以降低延时。保存很多的数据放在缓存里面，当需要访问的这些数据，只要在之前访问过的，如今直接在缓存里面取即可。 复杂的逻辑控制单元。当程序含有多个分支的时候，它通过提供分支预测的能力来降低延时。 数据转发。 当一些指令依赖前面的指令结果时，数据转发的逻辑控制单元决定这些指令在pipeline中的位置并且尽可能快的转发一个指令的结果给后续的指令。这些动作需要很多的对比电路单元和转发电路单元。 GPU是基于大的吞吐量设计。 GPU的特点是有很多的ALU和很少的cache. 缓存的目的不是保存后面需要访问的数据的，这点和CPU不同，而是为thread提高服务的。如果有很多线程需要访问同一个相同的数据，缓存会合并这些访问，然后再去访问dram（因为需要访问的数据保存在dram中而不是cache里面），获取数据后cache会转发这个数据给对应的线程，这个时候是数据转发的角色。但是由于需要访问dram，自然会带来延时的问题。GPU的控制单元（左边黄色区域块）可以把多个的访问合并成少的访问。 GPU的虽然有dram延时，却有非常多的ALU和非常多的thread. 为啦平衡内存延时的问题，我们可以中充分利用多的ALU的特性达到一个非常大的吞吐量的效果。尽可能多的分配多的Threads.通常来看GPU ALU会有非常重的pipeline就是因为这样。 所以与CPU擅长逻辑控制，串行的运算。和通用类型数据运算不同，GPU擅长的是大规模并发计算，这也正是密码破解等所需要的。所以GPU除了图像处理，也越来越多的参与到计算当中来。 GPU的工作大部分就是这样，计算量大，但没什么技术含量，而且要重复很多很多次。就像你有个工作需要算几亿次一百以内加减乘除一样，最好的办法就是雇上几十个小学生一起算，一人算一部分，反正这些计算也没什么技术含量，纯粹体力活而已。而CPU就像老教授，积分微分都会算，就是工资高，一个老教授资顶二十个小学生，你要是富士康你雇哪个？GPU就是这样，用很多简单的计算单元去完成大量的计算任务，纯粹的人海战术。这种策略基于一个前提，就是小学生A和小学生B的工作没有什么依赖性，是互相独立的。很多涉及到大量计算的问题基本都有这种特性，比如你说的破解密码，挖矿和很多图形学的计算。这些计算可以分解为多个相同的简单小任务，每个任务就可以分给一个小学生去做。但还有一些任务涉及到“流”的问题。比如你去相亲，双方看着顺眼才能继续发展。总不能你这边还没见面呢，那边找人把证都给领了。这种比较复杂的问题都是CPU来做的。 总而言之，CPU和GPU因为最初用来处理的任务就不同，所以设计上有不小的区别。而某些任务和GPU最初用来解决的问题比较相似，所以用GPU来算了。GPU的运算速度取决于雇了多少小学生，CPU的运算速度取决于请了多么厉害的教授。教授处理复杂任务的能力是碾压小学生的，但是对于没那么复杂的任务，还是顶不住人多。当然现在的GPU也能做一些稍微复杂的工作了，相当于升级成初中生高中生的水平。但还需要CPU来把数据喂到嘴边才能开始干活，究竟还是靠CPU来管的。 什么类型的程序适合在GPU上运行？ 计算密集型的程序。所谓计算密集型(Compute-intensive)的程序，就是其大部分运行时间花在了寄存器运算上，寄存器的速度和处理器的速度相当，从寄存器读写数据几乎没有延时。可以做一下对比，读内存的延迟大概是几百个时钟周期；读硬盘的速度就不说了，即便是SSD, 也实在是太慢了。 易于并行的程序。GPU其实是一种SIMD(Single Instruction Multiple Data)架构， 他有成百上千个核，每一个核在同一时间最好能做同样的事情。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"GPU","slug":"GPU","permalink":"http://huangzhiyuan.github.io/tags/GPU/"}]},{"title":"重返实习","slug":"return-to-internship","date":"2017-12-03T08:21:38.000Z","updated":"2020-03-01T12:19:04.000Z","comments":true,"path":"2017/12/03/return-to-internship/","link":"","permalink":"http://huangzhiyuan.github.io/2017/12/03/return-to-internship/","excerpt":"在经历了17年秋招和毕业答辩之后，几经周折，还是返回了上海继续实习。新的开始从租房开始……","text":"在经历了17年秋招和毕业答辩之后，几经周折，还是返回了上海继续实习。新的开始从租房开始…… 11月中旬到12月初从合肥回家到来这之前的半个月，在家大都过着吃吃睡睡玩玩的类似养猪日子。日子虽清闲，时间一久也呆不住了，增长的除了体重秤上的数字，其他貌似也是没啥了。用我妈的话说，你的书呢？上了这么多年书，马上就要解放作为上班族的我，鬼知道经历了这么多次数的搬来搬去书都去哪了:(，以后再买呗。 我是12月2号早上到上海的。这天是周六，1号线的地铁人还是这么多。房子是之前同学LL的，还是之前的小区。在把帮我搬运东西的好心同学送走后，就开始了改天换地大扫除…… 房子是租来的，生活却是自己的。这场清洗运动在废了两条毛巾、烧了N壶开水持续到晚上7点半左右才告一段落。好在锅碗瓢勺都在，明天开火！ 火有点大，豆腐有点软，卖相相当一般，味道还是不错滴。 一切都才刚刚开始，努力，奋斗！","categories":[{"name":"其他","slug":"其他","permalink":"http://huangzhiyuan.github.io/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"生活","slug":"生活","permalink":"http://huangzhiyuan.github.io/tags/%E7%94%9F%E6%B4%BB/"}]},{"title":"那些看到就想摘抄下来的句子","slug":"sentence-excerpt","date":"2017-11-27T02:35:05.000Z","updated":"2020-03-01T12:19:22.000Z","comments":true,"path":"2017/11/27/sentence-excerpt/","link":"","permalink":"http://huangzhiyuan.github.io/2017/11/27/sentence-excerpt/","excerpt":"愿你出走半生，归来仍是少年。","text":"愿你出走半生，归来仍是少年。 关于想你这件事，躲得过对酒当歌的夜，躲不过四下无人的街。 是谁来自山川湖海，却囿于昼夜、厨房与爱。 那时我们有梦，关于文学，关于爱情，关于穿越世界的旅行。如今我们深夜饮酒，杯子碰到一起，都是梦破碎的声音。 螃蟹在剥我的壳，笔记本在写我，漫天的我落在枫叶上，雪花上，而你在想我 人这一辈子都在躲雨，雨会不会很难过？爱之于我，不是肌肤之亲，不是一蔬一饭，他是一种不死的欲望，是疲惫生活的英雄梦想。 你记得也好，最好你忘掉，在这交会时互放的光亮。 相见亦无事，别后常忆君。 若我遇见你，事隔经年，我将如何致你，以眼泪？以沉默？ 纵豆蔻词工，青楼梦好，难赋深情。 从前的日色变得慢车，马，邮件都慢一生只够爱一个人 与君初相识，犹如故人归。 你如今的气质里，藏着你读过的书，走过的路，和爱过的人。 人生若只如初见，何事秋风悲画扇。等闲变却故人心，却道故人心易变。 山有木兮木有枝，心悦君兮君不知。 “你见过凌晨两点半的太阳吗？”“见过，噩梦醒来身旁你熟睡的脸” 无人与我立黄昏，无人问我粥可温。 我走过山时，山不说话我路过海时，海不说话小毛驴滴滴答答，倚天剑伴我走天涯大家都说我因为爱着杨过大侠才在峨眉山上出了家其实我只是爱上了峨眉山上的云和霞像极了十六岁那年的烟花…… 所爱隔山海，山海不可平。 海底月是天上月，眼前人是心上人。向来心是看客心，奈何人是剧中人。 你那么会安慰人，一定度过了许多自己安慰自己的时光吧。 我爱你，不光是因为你的样子，还因为和你在一起时，我的样子。 如果你没有如期归来，这正是离别的意义。 草在结它的种子，风在摇它的叶子。我们站着，不说话，就十分美好。 情不知所起一往情深，再而衰三而竭 夜阑卧听风吹雨，铁马是你，冰河也是你。 你是年少的欢喜倒过来念你会发现 你是我温暖的手套冰冷的啤酒带着阳光味道的衬衫日复一日的梦想 世间安得双全法，不负如来不负卿。 别人稍一注意你，你就敞开心扉，你觉得这是坦率，其实这是孤独。 你要做一个不动声色的大人了。不准情绪化，不准偷偷想念，不准回头看。去过自己另外的生活。 我所有的自负都来自我的自卑，所有的英雄气概都来自于我内心的软弱，所有的振振有词都因为心中满是怀疑。我假装无情，其实是痛恨自己的深情。我以为人生的意义在于四处游荡流亡，其实只是掩饰至今没有找到愿意驻足的地方。 你站在桥上看风景，看风景的人在楼上看你。明月装饰了你的窗子，你装饰了别人的梦。 很抱歉啊，我白天都没什么机会和你说话，只能憋到晚上给你发句晚安。但你可别小看了这两个字，它可包含着我今天清晨见到的阳光，中午看到的白云，傍晚遇见的微风，包含着我这一天每句想对你说的话，晚安。 瘦不了的永远在骚动，吃不胖的都有恃无恐。 某天，你会无端想起一个人她曾经让你对明天有所期许但是却完全没有出现在你的明天里 我希望有个如你一般的人如山间清爽的风，如古城温暖的光从清晨到夜晚，由山野到书房只要最后是你，就好 我这一辈子走过许多地方的路行过许多地方的桥看过许多次数的云喝过许多种类的酒却只爱过一个正当最好年龄的人","categories":[{"name":"读书","slug":"读书","permalink":"http://huangzhiyuan.github.io/categories/%E8%AF%BB%E4%B9%A6/"}],"tags":[{"name":"摘抄","slug":"摘抄","permalink":"http://huangzhiyuan.github.io/tags/%E6%91%98%E6%8A%84/"}]},{"title":"起跑线","slug":"start-line","date":"2017-11-26T06:59:06.000Z","updated":"2020-03-01T12:19:42.000Z","comments":true,"path":"2017/11/26/start-line/","link":"","permalink":"http://huangzhiyuan.github.io/2017/11/26/start-line/","excerpt":"“绝对不能让我的孩子输在起跑线上！”这部印度印度神片拍出了中国教育之痛。","text":"“绝对不能让我的孩子输在起跑线上！”这部印度印度神片拍出了中国教育之痛。 先说剧情，《起跑线》讲述的就是一对中产阶级的印度夫妇拉吉和米图，为了让女儿皮雅接受好的教育，费尽心思让她进入名校的故事。题材较以往票房大卖的影片类似，都是在轻松娱乐中表现社会热门问题，此次中枪的是教育和阶级固化。 看到剧中男主为了给你女儿争取名校录取名额凌晨排队的一幕，心中不禁觉得好笑，这和国人提前排队为了抢公立小学或幼儿园报名资格或者抢房团架势一模一样好吧。看完这部电影，心里很不是一番滋味。回想起自己从幼儿园到现在研究生毕业这一路走来，所经历所见闻所思所感，太感同身受了。 中国有一句很热门的口号：再穷不能穷教育，不能让孩子输在起跑线上！而这条“起跑线”指的就是孩子接受的教育，中国人认为孩子接受了好的教育，才能具备强大的竞争力，才能在激烈的社会竞争机制中获得先机与优势。换言之，中国人认为，教育可以决定人的命运。印度虽然与中国文化迥异的国家，但也认同这个观点。 2015年《人民日报》就曾发表长篇通讯，提出一个疑问：穷会成为穷的原因，富会成为富的原因吗？文章感叹，贫富差距正在快速加大，阶层固化的趋势越发严重。文章指出：干部子女成为干部的机会，是非干部子女的2.8倍多，并且这个数字还在增长。与此同时，诸如“贫二代”、“富二代”、“官二代”等等名词的涌现，昭示着我们的国家已经进入“拼爹时代”，教育被商品化后待价而沽的趋势，则无疑更是雪上加霜，让人绝望，因为它曾被认为是对抗阶级固化最原始、最基本的渠道。 阶级固化，这四个字则又一次让国人扎心。 联想到近年来一直被炒的火热的寒门再难出贵子言论。不可否认的是随着这些年中国的发展，一些地区一些人或凭借自己努力奋斗（改革开放下海热），或及时把握市场趋势（90年代互联网，10年代房地产），亦或城中村拆迁补偿等等等等各种方法先富裕了起来，并逐渐开始掌握了分量不小的各种社会资源。随之产生的社会问题就是教育资源分配不均，贫富差距越来越大，不仅仅是输在起跑线，在娘胎里恐怕都被拉了好几条街。如今的社会现实是，培养出贵子，想要受到良好的教育，似乎一定要建立在一定的经济基础上。寒门逆袭仅仅凭借天天读死书，无德智体美劳，最多也只能沦落为考试的高分机器而已。何况由此引发的一系列社会问题如留守儿童、豫章书院、红黄蓝虐童等等等等，一切的一切都不是毫无原由的发生。及时是孩子从小上学一路读出来考上不错的大学，在大学全面发展综合发展的挑战下，从小积攒的社交、兴趣爱好、职业规划等问题也都会成为成长的困惑。今年北京高考文科状元在考试结束后分享过这样的一段关于对教育资源的理解，引起了大范围的议论，但你也不得不承认有一定的道理： 住房，教育，医疗，作为最关乎国计民生的问题，也就是我等凡夫俗子穷其一生不可避免的问题。往往一个相关社会热点便能引爆社会舆论成为头条，因为大家都在这个体制下，都在这个圈内，有人得益就意味着某些人群相关权益被剥夺，而这些恰恰是每日兢兢业业辛苦上班的国人能容忍的。在当下的体制中，不是光靠努力就能改变一切的，你任劳任怨殚精竭虑的的付出也许还没别人轻松“作弊”得到的回报大。这种体制之殇，也更是国内影视讳莫如深的。在惊叹泰国、印度、韩国能拍出这种针砭时弊、反映社会痛点的作品时，我们是不是也该好好反思？ 孩子的健康成长（身体健康、心理健康）离不开家庭和社会的保障。家庭的保障主要是父母。父母是世界上最容易成为的职业，也是不需要任何相关育儿培训就能立马上岗的职业。本应言传身教、循循善诱，然而父母本身认知或职业等因素限制，也使得家庭教育质量参差不齐。电影中有句这样的台词：“你们使尽办法让他们入学，不是在帮他们，事实上，是他们在帮你们。”是的，孩子们帮助你们得到了亲戚的羡慕、邻居的称赞、老一辈的赏识，让你们觉得做人真成功啊。孩子，是你以爱的名义，赚取面子的道具。如果家长们不以身作则，言传身教，那么孩子生而为人，从一开始就已经输在了起跑线上。至于社会的保障更是需要每个社会人的共同维护，每个父母都是孩子的父母，每个孩子都是父母的孩子。今天的路人，就有可能是明天的当事人。身在社会，永远不可能置身事外。 最后是电影插曲MV。","categories":[{"name":"电影","slug":"电影","permalink":"http://huangzhiyuan.github.io/categories/%E7%94%B5%E5%BD%B1/"}],"tags":[{"name":"影评","slug":"影评","permalink":"http://huangzhiyuan.github.io/tags/%E5%BD%B1%E8%AF%84/"}]},{"title":"博客自传","slug":"about-this-blog","date":"2017-11-22T11:26:39.000Z","updated":"2020-03-01T12:06:26.000Z","comments":true,"path":"2017/11/22/about-this-blog/","link":"","permalink":"http://huangzhiyuan.github.io/2017/11/22/about-this-blog/","excerpt":"一直以来都想搭建一个自己的博客，哪怕很小，哪怕很low，刚开始以为难度应该很大，要求应该很多，我又不会前端，会不会超麻烦，超费事？现在想来就是自己太懒，人都是有惰性的啊！","text":"一直以来都想搭建一个自己的博客，哪怕很小，哪怕很low，刚开始以为难度应该很大，要求应该很多，我又不会前端，会不会超麻烦，超费事？现在想来就是自己太懒，人都是有惰性的啊！ 刚开始是听说可以用Django结合Python搭建个人博客，年前也的确热血沸腾的按照教程折腾了一个周末（[Django:快速搭建自己打第一个的Blog][1]），只做成了一个大概框架，可惜感觉功能太过于简单，一点也不高大上，我自己都懒得用：( 直到这次看到被人用hexo搭建的博客，感觉这个不错啊，这就是我想要的样子啊。于是折腾了一个下午，就有现在这个了。开心！ 世上无难事，只要肯攀登。以后要好好维护这个工具，每天坚持写点东西，点点滴滴记录自己的成长经历，加油！","categories":[{"name":"其他","slug":"其他","permalink":"http://huangzhiyuan.github.io/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"http://huangzhiyuan.github.io/tags/%E9%9A%8F%E7%AC%94/"}]},{"title":"TCP三次握手","slug":"tcp-three-hands","date":"2017-07-25T15:26:13.000Z","updated":"2020-03-01T12:20:20.000Z","comments":true,"path":"2017/07/25/tcp-three-hands/","link":"","permalink":"http://huangzhiyuan.github.io/2017/07/25/tcp-three-hands/","excerpt":"Http协议三次握手过程TCP是主机对主机层的传输控制协议，提供可靠的连接服务，采用三次握手确认建立一个连接:位码即tcp标志位,有6种标示:SYN(synchronous建立联机)ACK(acknowledgement 确认)PSH(push传送)FIN(finish结束)RST(reset重置)URG(urgent紧急)Sequence number(顺序号码)Acknowledge number(确认号码)","text":"Http协议三次握手过程TCP是主机对主机层的传输控制协议，提供可靠的连接服务，采用三次握手确认建立一个连接:位码即tcp标志位,有6种标示:SYN(synchronous建立联机)ACK(acknowledgement 确认)PSH(push传送)FIN(finish结束)RST(reset重置)URG(urgent紧急)Sequence number(顺序号码)Acknowledge number(确认号码) 第一次握手：主机A发送位码为syn＝1,随机产生seq number=1234567的数据包到服务器，主机B由SYN=1知道，A要求建立联机； 第二次握手：主机B收到请求后要确认联机信息，向A发送ack number=(主机A的seq+1),syn=1,ack=1,随机产生seq=7654321的包 第三次握手：主机A收到后检查ack number是否正确，即第一次发送的seq number+1,以及位码ack是否为1，若正确，主机A会再发送ack number=(主机B的seq+1),ack=1，主机B收到后确认seq值与ack=1则连接建立成功。 完成三次握手，主机A与主机B开始传送数据。 实例: IP 192.168.1.116.3337 &gt; 192.168.1.123.7788: S 3626544836:3626544836IP 192.168.1.123.7788 &gt; 192.168.1.116.3337: S 1739326486:1739326486 ack 3626544837IP 192.168.1.116.3337 &gt; 192.168.1.123.7788: ack 1739326487,ack 1 第一次握手：192.168.1.116发送位码syn＝1,随机产生seq number=3626544836的数据包到192.168.1.123,192.168.1.123由SYN=1知道192.168.1.116要求建立联机; 第二次握手：192.168.1.123收到请求后要确认联机信息，向192.168.1.116发送ack number=3626544837,syn=1,ack=1,随机产生seq=1739326486的包; 第三次握手：192.168.1.116收到后检查ack number是否正确，即第一次发送的seq number+1,以及位码ack是否为1，若正确，192.168.1.116会再发送ack number=1739326487,ack=1，192.168.1.123收到后确认seq=seq+1,ack=1则连接建立成功。 关闭连接： 由于TCP连接是全双工的，因此每个方向都必须单独进行关闭。这个原则是当一方完成它的数据发送任务后就能发送一个FIN来终止这个方向的连接。收到一个 FIN只意味着这一方向上没有数据流动，一个TCP连接在收到一个FIN后仍能发送数据。首先进行关闭的一方将执行主动关闭，而另一方执行被动关闭。 CP的连接的拆除需要发送四个包，因此称为四次挥手(four-way handshake)。客户端或服务器均可主动发起挥手动作，在socket编程中，任何一方执行close()操作即可产生挥手操作。（1）客户端A发送一个FIN，用来关闭客户A到服务器B的数据传送。（2）服务器B收到这个FIN，它发回一个ACK，确认序号为收到的序号加1。和SYN一样，一个FIN将占用一个序号。（3）服务器B关闭与客户端A的连接，发送一个FIN给客户端A。（4）客户端A发回ACK报文确认，并将确认序号设置为收到序号加1。TCP采用四次挥手关闭连接如图2所示。","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://huangzhiyuan.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://huangzhiyuan.github.io/tags/TCP-IP/"}]},{"title":"Java集合框架","slug":"java-mm-setStructure","date":"2017-07-20T14:55:34.000Z","updated":"2020-03-01T12:13:48.000Z","comments":true,"path":"2017/07/20/java-mm-setStructure/","link":"","permalink":"http://huangzhiyuan.github.io/2017/07/20/java-mm-setStructure/","excerpt":"定义：Java中的集合类：是一种工具类，就像是容器，存储任意数量的具有共同属性的对象。","text":"定义：Java中的集合类：是一种工具类，就像是容器，存储任意数量的具有共同属性的对象。 作用：集合可以：1、在类的内部，对数据进行组织；2、简单而快速的搜索大数量的条目；3、有的集合接口，提供了一系列排列有序的元素，并且可以在序列中间快速的插入或者删除元素；4、有的集合接口，提供了映射关系，可以通过关键字（key）快速的查找到对应的唯一对象，而这个关键字可以是任意类型。 与数组的对比：1、数组的长度固定，集合长度可变；2、数组只能通过下标访问元素，下标类型只能是数字型，而有的集合可以通过任意类型查找所映射的具体对象。 Java集合框架的体系结构： Collection接口1、是List、Set和Queue接口的父接口；2、定义了可用于操作List、Set和Queue的方法——增删改查； List接口及其实现类——ArrayList List（序列），元素有序，并且可重复； List可以精确控制元素的插入位置，或删除指定位置的元素； ArrayList——数组序列，是List的一个重要实现类； ArrayList底层是由数组实现的泛型集合中，不能添加泛型规定的类型及其子类型以外的对象，否则会报错。泛型集合中的限定类型，不能使用基本类型，必须用基本类型的包装类。 Set接口及其实现类 Set（集），元素无序，并且不可以重复； HashSet——哈希集，是Set的一个重要实现类； Map接口 Map提供了一种映射关系，元素是以键值对（key-value）的形式存储的，能根据key快速查找value； Map中的键值对以Entry类型的对象实例形式存在； key值不能重复，value值可以重复； key对value是多（一）对一的关系； Map接口提供了返回key值集合、value值集合、Entry值集合，的方法； Map支持泛型，形式如：Map&lt;K,V&gt;HashMap类 HashMap是Map的一个重要实现类，也是最常用的，基于哈希表实现； HashMap中的Entry对象是无序排列的； Key值和value值都可以为null，但是一个HashMap只能有一个Key值为null的映射（Key值不可重复）；","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"Java 集合框架","slug":"Java-集合框架","permalink":"http://huangzhiyuan.github.io/tags/Java-%E9%9B%86%E5%90%88%E6%A1%86%E6%9E%B6/"}]},{"title":"Java内存管理与多线程","slug":"java-mm-memorymanager","date":"2017-07-20T14:37:58.000Z","updated":"2020-03-01T12:13:30.000Z","comments":true,"path":"2017/07/20/java-mm-memorymanager/","link":"","permalink":"http://huangzhiyuan.github.io/2017/07/20/java-mm-memorymanager/","excerpt":"1. 什么是线程？什么是进程？同一进程下的线程共享线程：程序在执行过程中，能够执行程序代码的一个执行单元，一个线程可以创建和撤销另一个线程;同一个进程中的多个线程之间可以并发执行。在Java语言中有4种状态：运行、就绪、挂起、结束。","text":"1. 什么是线程？什么是进程？同一进程下的线程共享线程：程序在执行过程中，能够执行程序代码的一个执行单元，一个线程可以创建和撤销另一个线程;同一个进程中的多个线程之间可以并发执行。在Java语言中有4种状态：运行、就绪、挂起、结束。 进程：指一段正在执行的程序。线程有时也被称为轻量级进程，它是程序执行的最小单元，一个进程可以拥有多个线程，各个线程之间共享程序的内存空间及一些进程级的资源，但是各个线程拥有自己的栈空间。 进程的作用和定义：进程是为了提高CPU的执行效率，减少因为程序等待带来的CPU空转以及其他计算机软硬件资源的浪费而提出来的。进程是为了完成用户任务所需要的程序的一次执行过程和为其分配资源的一个基本单位，是一个具有独立功能的程序段对某个数据集的一次执行活动。 线程和进程的区别：A. 地址空间和其它资源：进程间相互独立，同一进程的各线程间共享。某进程内的线程在其它进程不可见。B. 通信：进程间通信IPC，线程间可以直接读写进程数据段（如全局变量）来进行通信——需要进程同步和互斥手段的辅助，以保证数据的一致性。C. 调度和切换：线程上下文切换比进程上下文切换要快得多。D. 在多线程OS中，进程不是一个可执行的实体。E. 线程是进程的一部分，所以线程有的时候被称为是轻权进程或者轻量级进程。 线程与进程资源分配：线程共享的内容包括： 进程代码段、进程的公有数据(利用这些共享的数据，线程很容易的实现相互之间的通讯)、进程打开的文件描述符、信号的处理器、进程的当前目录和、进程用户ID、进程组ID。线程独有的内容包括： 线程ID 、寄存器组的值 、线程的堆栈 、错误返回码 、线程的信号屏蔽码 。 2. Java的内存机制Java 把内存划分成两种：一种是栈内存，另一种是堆内存。堆和栈相同点：栈(stack)与堆(heap)都是Java用来在Ram中存放数据的地方。与C++不同，Java自动管理栈和堆，程序员不能直接地设置栈或堆。堆和栈区别：栈的优势是，存取速度比堆要快，仅次于直接位于CPU中的寄存器。但缺点是，存在栈中的数据大小与生存期必须是确定的，缺乏灵活性。另外，栈数据可以共享。堆的优势是可以动态地分配内存大小，生存期也不必事先告诉编译器，Java的垃圾收集器会自动收走这些不再使用的数据。但缺点是，由于要在运行时动态分配内存，存取速度较慢。 在函数中定义的一些基本类型的变量和对象的引用变量都是在函数的栈内存中分配，当在一段代码块定义一个变量时，Java 就在栈中为这个变量分配内存空间，当超过变量的作用域后，Java 会自动释放掉为该变量分配的内存空间，该内存空间可以立即被另作它用。 堆内存用来存放由 new 创建的对象和数组，在堆中分配的内存，由 Java 虚拟机的自动垃圾回收器来管理。在堆中产生了一个数组或者对象之后，还可以在栈中定义一个特殊的变量，让栈中的这个变量的取值等于数组或对象在堆内存中的 首地址，栈中的这个变量就成了数组或对象的引用变量，以后就可以在程序中使用栈中的引用变量来访问堆中的数组或者对象，引用变量就相当于是为数组或者对象 起的一个名称。引用变量是普通的变量，定义时在栈中分配，引用变量在程序运行到其作用域之外后被释放。而数组和对象本身在堆中分配，即使程序运行到使用 new 产生数组或者对象的语句所在的代码块之外，数组和对象本身占据的内存不会被释放，数组和对象在没有引用变量指向它的时候，才变为垃圾，不能在被使用，但仍 然占据内存空间不放，在随后的一个不确定的时间被垃圾回收器收走（释放掉）。 3. java中变量在内存中的分配（1）类变量（static修饰的变量）：在程序加载时系统就为它在堆中开辟了内存，堆中的内存地址存放于栈以便于高速访问。静态变量的生命周期–一直持续到整个”系统”关闭 （2）实例变量：当你使用java关键字new的时候，系统在堆中开辟并不一定是连续的空间分配给变量（比如说类实例），然后根据零散的堆内存地址，通过哈希算法换算为一长串数字以表征这个变量在堆中的”物理位置”。 实例变量的生命周期–当实例变量的引用丢失后，将被GC（垃圾回收器）列入可回收“名单”中，但并不是马上就释放堆中内存 （3）局部变量：局部变量，由声明在某方法，或某代码段里（比如for循环），执行到它的时候在栈中开辟内存，当局部变量一但脱离作用域，内存立即释放 4. JVM内存分配JVM 将内存区域划分为： Method Are（Non-Heap）（方法区）,Heap（堆）,Program Counter Register（程序计数器）,VM Stack（虚拟机栈，也有翻译成JAVA 方法栈的）,Native Method Stack（本地方法栈）。方法区和堆是线程共享的，虚拟机栈，程序计数器和本地方法栈是非线程共享的。一般性的 Java 程序的工作过程：一个 Java 源程序文件，会被编译为字节码文件（以 class 为扩展名），每个java程序都需要运行在自己的JVM上，然后告知 JVM 程序的运行入口，再被 JVM 通过字节码解释器加载运行。那么程序开始运行后，都是如何涉及到各内存区域的呢？概括地说来，JVM初始运行的时候都会分配好方法区和堆，而JVM每遇到一个线程，就为其分配一个程序计数器,虚拟机栈和本地方法栈，当线程终止时，三者（虚拟机栈，本地方法栈和程序计数器）所占用的内存空间也会被释放掉。这也是为什么我把内存区域分为线程共享和非线程共享的原因，非线程共享的那三个区域的生命周期与所属线程相同，而线程共享的区域与JAVA程序运行的生命周期相同，所以这也是系统垃圾回收的场所只发生在线程共享的区域（实际上对大部分虚拟机来说知发生在Heap上）的原因。 5. Java实现多线程，创建并启动线程的过程（1）定义线程：1）扩展java.lang.Thread类。 2）实现java.lang.Runnable接口。 （2）实例化线程：1）如果是扩展java.lang.Thread类的线程，则直接new即可。2）如果是实现了java.lang.Runnable接口的类，则用Thread的构造方法：Thread(Runnable target)Thread(Runnable target, String name)Thread(ThreadGroup group, Runnable target)Thread(ThreadGroup group, Runnable target, String name)Thread(ThreadGroup group, Runnable target, String name, long stackSize) （3）启动线程：在线程的Thread对象上调用start()方法，而不是run()或者别的方法。 通过继承Thread类创建线程：public class ThreadDemo02 extends Thread{public void run(){System.out.println(“线程启动！”);}public static void main(String[] args) {ThreadDemo02 thread = new ThreadDemo02();thread.start();}}通过实现Runnable接口创建线程：public class ThreadDemo03 implements Runnable{public void run() {System.out.println(“线程启动02”);}public static void main(String[] args) {Thread thread01 = new Thread(new ThreadDemo03());thread01.start();}} 6. start()方法和run()方法的区别：（1）启动一个线程是start()方法，启动线程之后start()方法会去调用run方法内容。（2）start是创建并启动一个线程，而run是要运行线程中的代码。（3）run()方法 : 在本线程内调用该Runnable对象的run()方法，可以重复多次调用；start()方法 : 启动一个线程，调用该Runnable对象的run()方法，不能多次启动一个线程； 7. 在JAVA中，有六个不同的地方可以存储数据（1） 寄存器（register）：这是最快的存储区，因为它位于不同于其他存储区的地方——处理器内部。但是寄存器的数量极其有限，所以寄存器由编译器根据需求进行分配。你不能直接控制，也不能在程序中感觉到寄存器存在的任何迹象。（2） 堆栈（stack）：位于通用RAM中，但通过它的“堆栈指针”可以从处理器哪里获得支持。堆栈指针若向下移动，则分配新的内存；若向上移动，则释放那些内存。这是一种快速有效的分配存储方法，仅次于寄存器。创建程序时候，JAVA编译器必须知道存储在堆栈内所有数据的确切大小和生命周期，因为它必须生成相应的代码，以便上下移动堆栈指针。这一约束限制了程序的灵活性，所以虽然某些JAVA数据存储在堆栈中——特别是对象引用，但是JAVA对象不存储其中。（3）堆（heap）：一种通用性的内存池（也存在于RAM中），用于存放所以的JAVA对象。堆不同于堆栈的好处是：编译器不需要知道要从堆里分配多少存储区域，也不必知道存储的数据在堆里存活多长时间。因此，在堆里分配存储有很大的灵活性。当你需要创建一个对象的时候，只需要new写一行简单的代码，当执行这行代码时，会自动在堆里进行存储分配。当然，为这种灵活性必须要付出相应的代码。用堆进行存储分配比用堆栈进行存储存储需要更多的时间。（4）静态存储（static storage）：这里的“静态”是指“在固定的位置”。静态存储里存放程序运行时一直存在的数据。你可用关键字static来标识一个对象的特定元素是静态的，但JAVA对象本身从来不会存放在静态存储空间里。（5）常量存储（constant storage）：常量值通常直接存放在程序代码内部，这样做是安全的，因为它们永远不会被改变。有时，在嵌入式系统中，常量本身会和其他部分分割离开，所以在这种情况下，可以选择将其放在ROM中（6）非RAM存储：如果数据完全存活于程序之外，那么它可以不受程序的任何控制，在程序没有运行时也可以存在。","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"Java memory","slug":"Java-memory","permalink":"http://huangzhiyuan.github.io/tags/Java-memory/"}]},{"title":"从ftp服务器自动下载文件夹到本地","slug":"ftp-download","date":"2017-07-04T13:33:41.000Z","updated":"2020-03-01T12:10:38.000Z","comments":true,"path":"2017/07/04/ftp-download/","link":"","permalink":"http://huangzhiyuan.github.io/2017/07/04/ftp-download/","excerpt":"最近有用到ftp文件传输协议从服务器端下载文件到本地，其中需要设计到bat实现ftp的自动登录，以及登录之后对ftp目录下特定文件夹进行遍历下载传输，并统计下载所需的时间。","text":"最近有用到ftp文件传输协议从服务器端下载文件到本地，其中需要设计到bat实现ftp的自动登录，以及登录之后对ftp目录下特定文件夹进行遍历下载传输，并统计下载所需的时间。网上很多版本都是下载某一目录下面所有文件，并不能实现文件夹的递归遍历。另外比较普遍的处理方案是将所有需要备份的文件夹进行压缩，在特定时间将该压缩包网络传输备份（多用于定时备份服务器端日志文件）。 考虑到特殊的需要，这里实现上述功能。并将使用到的源码解析如下： ftp自动登录脚本auto.bat，运行后会将ftp服务器端的pxe目录下面的所有文件下载到本地C:/temp目录。 1234567891011121314151617181920212223rem filename : auto.batrem function : auto login in and download from ftp://efi_testing:Pass_123@192.168.12.10/pxe to c:/temp@echo offrem 指定FTP用户名set ftpUser=&#x27;efi_testing&#x27;rem 指定FTP密码set ftpPass=&#x27;Pass_123&#x27;rem 指定FTP服务器地址set ftpIP=192.168.12.10rem 指定待下载的文件位于FTP服务器的哪个目录set ftpFolder=/pxerem 指定从FTP下载下来的文件存放到本机哪个目录set LocalFolder=C:/tempecho open %ftpIP% &gt;&gt; ftp.txtecho cd &quot;%ftpFolder%&quot; &gt;&gt; ftp.txtecho lcd &quot;%LocalFolder%&quot; &gt;&gt;ftp.txtecho mget *.* &gt;&gt; ftp.txtecho bye &gt;&gt;ftp.txtftp -i -n -s:ftp.txt //易出错，注意pause 其中ftp语法如下： 123456789101112ftp [-v] [-d] [-i] [-n] [-g] [-s:FileName] [-a] [-w:WindowSize] [-A] [Host]参数：-v：禁止显示远程服务器响应。-d：启用调试、显示在客户端和服务器之间传递的所有ftp命令。-i：多个文件传送时关闭交互提示。-n：禁止自动登录到初始连接。-g：禁用文件名组，它允许在本地文件和路径名中使用通配符字符（*和?）。-s:FileName：指定包含ftp命令的文本文件；当ftp启动后，这些命令将自动运行。该参数中不允许有空格。使用该开关而不是重定向（&gt;）。-a：在捆绑数据连接时使用任何本地接口。-w:WindowSize：替代默认大小为4096的传送缓冲区。-A：匿名登陆。Host：指定要连接到远程计算机的计算机名或IP地址。如果指定，计算机必须是最后一个参数。 ftpget.bat文件，在这个文件中完成ftp自动登录和ftp下载文件到本地功能，并统计出所需时间（时：分：秒）。其中会调用下面maketree.bat脚本，生成遍历后的文件夹递归目录。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879rem filename: ftpget.batrem function: auto login in and download from ftp://efi_testing:Pass_123@192.168.12.10/client to %cd%@echo offset host=192.168.12.10set user=efi_testingset pass=Pass_123set dirname=clientset homedir=%cd%rem 生成FTP目录结构if exist tree.txt del tree.txtset time_begin=%time:~0,-3%start /w cmd /c makeTree %dirname% 0del result*.txtfor /f &quot;delims=/&quot; %%i in (tree.txt) do (rem 获取dirname的所有文件rem =============================cd %homedir%md %%icd /d %%iecho open %host%&gt;ftp.txtecho user %user% %pass%&gt;&gt;ftp.txtecho cd %%i&gt;&gt;ftp.txtrem mget *.* 下载所有文件，但不能下载无扩展名的文件；rem 如果使用mget * 可以下载所有文件，但会把子目录的文件也下载过来echo mget *.*&gt;&gt;ftp.txtecho bye&gt;&gt;ftp.txtftp -i -n -s:ftp.txtdel ftp.txtrem ==============================)cd %homedir%del tree.txtset time_end=%time:~0,-3%call :time_lapseecho All copy cost %hour_% hours %munite_% minutes %second_% secondspause&gt;nulgoto :eof:time_lapse:: 一定要按照 秒=&gt;分钟=&gt;小时 的顺序操作for /f &quot;tokens=1,2,3 delims=:&quot; %%i in (&quot;%time_begin%&quot;) do (set /a hour_b=%%iset /a munite_b=%%jset /a second_b=%%k)for /f &quot;tokens=1,2,3 delims=:&quot; %%i in (&quot;%time_end%&quot;) do (set /a hour_e=%%iset /a munite_e=%%jset /a second_e=%%k)if %second_e% lss %second_b% (set /a munite_e=%munite_e%-1set /a second_e=%second_e%+60)set /a second_=%second_e%-%second_b%if %munite_e% lss %munite_b% (set /a hour_e=%hour_e%-1set /a munite_e=%munite_e%+60)set /a munite_=%munite_e%-%munite_b%if %hour_e% lss %hour_b% (set /a hour_e=%hour_e%+24)set /a hour_=%hour_e%-%hour_b% 123456789101112131415161718192021rem filename: maketree.batrem function: generate ftp directory structurerem 调用方法 maketree dir1\\dir2 n (n&gt;=1)rem %1 == dir1\\dir2rem %2 == n@echo offset /a treetime=%2+1echo %1 &gt;&gt;tree.txtecho open %host% &gt; ftp%treetime%.txtecho user %user% %pass% &gt;&gt;ftp%treetime%.txtecho cd %1 &gt;&gt;ftp%treetime%.txtecho dir &gt;&gt;ftp%treetime%.txtecho bye &gt;&gt;ftp%treetime%.txtftp -i -n -s:ftp%treetime%.txt |find &quot;&lt;DIR&gt;&quot;&gt;result%treetime%.txtfor /f &quot;tokens=2,3*&quot; %%i in (result%treetime%.txt) do (start /w cmd /c Maketree.bat %1\\%%k %treetime%)rem del result%treetime%.txtrem del ftp%treetime%.txt 不积跬步,无以至千里;不积小流,无以成江海。每天进步一点点。","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://huangzhiyuan.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"ftp","slug":"ftp","permalink":"http://huangzhiyuan.github.io/tags/ftp/"}]},{"title":"获取微信好友头像","slug":"get_friend_photo","date":"2017-06-26T13:51:19.000Z","updated":"2020-03-01T12:12:16.000Z","comments":true,"path":"2017/06/26/get_friend_photo/","link":"","permalink":"http://huangzhiyuan.github.io/2017/06/26/get_friend_photo/","excerpt":"今天偶尔在GitHub上面看到分享的使用python模块itchat+pillow获取朋友圈微信头像，感觉蛮有意思。下班折腾了下，技术上不算难实现，idea却很重要。最先想到这个idea的才是真会玩啊。纸上得来终觉浅，好记性不如丑网页，要充分把自己的博客利用起来。","text":"今天偶尔在GitHub上面看到分享的使用python模块itchat+pillow获取朋友圈微信头像，感觉蛮有意思。下班折腾了下，技术上不算难实现，idea却很重要。最先想到这个idea的才是真会玩啊。纸上得来终觉浅，好记性不如丑网页，要充分把自己的博客利用起来。 废话少说，先看效果图： 效果还不错吧:)。好友头像个性一目了然，好有喜感。那么如何实现呢？ 首先是要有python环境，下载源码到本地目录。 安装抓图所需模块文件： 1pip install -r requirements.txt requirements.txt文件中包含了需要的各个模块及其版本号。 安装itchat模块： 1pip install itchat wxImage.py源码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import itchat #爬取头像import osimport PIL.Image as Image #拼接图片from os import listdirimport math#首先登陆python版本微信itchat，生成二维码itchat.auto_login(enableCmdQR=True)#获取好友列表friends = itchat.get_friends(update=True)[0:]user = friends[0][&quot;UserName&quot;]print(user)os.mkdir(user)num = 0#爬取好友头像列表并下载到本地for i in friends: img = itchat.get_head_img(userName=i[&quot;UserName&quot;]) fileImage = open(user + &quot;/&quot; + str(num) + &quot;.jpg&quot;,&#x27;wb&#x27;) fileImage.write(img) fileImage.close() num += 1pics = listdir(user)numPic = len(pics)print(numPic)#计算出每张头像缩小后的尺寸，计算每张头像缩小后的边长(默认为正方形)eachsize = int(math.sqrt(float(640 * 640) / numPic))print(eachsize)#计算合成图片每一边分为多少小边numline = int(640 / eachsize)toImage = Image.new(&#x27;RGBA&#x27;, (640, 640))print(numline)# 缩小并拼接图片x = 0y = 0for i in pics: try: #打开图片 img = Image.open(user + &quot;/&quot; + i) except IOError: print(&quot;Error: not find file or file not exist&quot;) else: #缩小图片 img = img.resize((eachsize, eachsize), Image.ANTIALIAS) #拼接图片 toImage.paste(img, (x * eachsize, y * eachsize)) x += 1 if x == numline: x = 0 y += 1toImage.save(user + &quot;.jpg&quot;)# 在微信的文件传输助手发合成后的图片给使用者：itchat.send_image(user + &quot;.jpg&quot;, &#x27;filehelper&#x27;) 运行脚本文件： 1python wxImage.py 出现如下二维码图案，用手机微信右上角的扫一扫，确认登陆即可。 稍候手机文件传输助手会收到效果图片： 接下来可以继续改进的是头像显示不完全问题，还可以对朋友圈用户信息进行进一步的数据挖掘。 感谢分享。","categories":[{"name":"python","slug":"python","permalink":"http://huangzhiyuan.github.io/categories/python/"}],"tags":[{"name":"webchat","slug":"webchat","permalink":"http://huangzhiyuan.github.io/tags/webchat/"}]},{"title":"少年斯派维的奇异旅行","slug":"少年斯派维的奇异旅行","date":"2017-05-02T14:01:51.000Z","updated":"2020-03-01T12:24:00.000Z","comments":true,"path":"2017/05/02/少年斯派维的奇异旅行/","link":"","permalink":"http://huangzhiyuan.github.io/2017/05/02/%E5%B0%91%E5%B9%B4%E6%96%AF%E6%B4%BE%E7%BB%B4%E7%9A%84%E5%A5%87%E5%BC%82%E6%97%85%E8%A1%8C/","excerpt":"今天下班的挺早，B站发现部电影，评分还不错，就这个《少年斯派维的奇异旅行》，标签治愈系，一口气看完，果然慢节奏暖心。","text":"今天下班的挺早，B站发现部电影，评分还不错，就这个《少年斯派维的奇异旅行》，标签治愈系，一口气看完，果然慢节奏暖心。 《少年斯派维的奇异旅行》是由让·皮埃尔·热内执导，凯尔·凯特莱特、海伦娜·伯翰·卡特等主演的电影。T.S.Spivet是个10岁的奇才，酷爱制图学和科学图解。有一天，史密森尼博物馆打电话给他说：享有盛誉的贝尔德奖授予了他，他也被邀请去做次演讲。不为人知的是，他决定乘运货的火车穿越美国去华盛顿。但那儿没有一个人料想到他只是个小孩儿。带着一个望远镜，4个圆规，和他曾曾祖母留给他的回忆，T.S.开始了他神秘的旅程。 这部影片主要讲了一个10岁小天才的孤独成长史，说是成长是指一家人都彻底从弟弟意外丧生的阴影中走了出来，又重新过上了幸福的家庭田园生活，其乐融融，好不温馨。幸福的家庭总是相似的，不幸的家庭各有各的不幸。全片从小正太的视角展开，一路搭顺风车去华府领奖，好笑有趣的经历。一路上以年幼孩子和超高智商的眼光观察记录世界。荣誉、财富的诱惑并不是能最终让他敞开心扉，当真那么多知名学者的面万人瞩目的颁奖典礼上说出自己心里的死结，坦诚弟弟的死因，并最终获得父母谅解，一家人重新幸福的生活在一起才是真正开心所在。孩子的世界，真没那么复杂，尽管是个少年天才，也是需要有亲情有亲人的理解和支持的…… 看评论，《天使爱美丽》也是部类似的片子，哪天抑郁不开心就翻出来看看好了。 Ｂ站链接","categories":[{"name":"电影","slug":"电影","permalink":"http://huangzhiyuan.github.io/categories/%E7%94%B5%E5%BD%B1/"}],"tags":[{"name":"电影","slug":"电影","permalink":"http://huangzhiyuan.github.io/tags/%E7%94%B5%E5%BD%B1/"}]},{"title":"嫌疑人X的献身","slug":"嫌疑人X的献身","date":"2017-04-09T08:29:18.000Z","updated":"2020-03-01T12:23:48.000Z","comments":true,"path":"2017/04/09/嫌疑人X的献身/","link":"","permalink":"http://huangzhiyuan.github.io/2017/04/09/%E5%AB%8C%E7%96%91%E4%BA%BAX%E7%9A%84%E7%8C%AE%E8%BA%AB/","excerpt":"4月份某个周末的下午，外面下雨没法出去玩耍，就在家看电影，看啥好呢？刚好发现了一部最近上映的日本推理作家东野圭吾的同名小说改编的悬疑电影《嫌疑人X的献身》，好，那就看这个吧。","text":"4月份某个周末的下午，外面下雨没法出去玩耍，就在家看电影，看啥好呢？刚好发现了一部最近上映的日本推理作家东野圭吾的同名小说改编的悬疑电影《嫌疑人X的献身》，好，那就看这个吧。 先说剧情。《嫌疑人X的献身》是根据日本推理作家东野圭吾的同名小说改编的悬疑电影，由苏有朋执导，王凯、张鲁一、叶祖新、邓恩熙等主演，林心如特别主演。该片于2017年3月31日全球同步上映。 该片是苏有朋首度执导悬疑题材的电影，也是东野圭吾推理小说首度在中国进行的影视化改编的作品。讲述的是物理学教授唐川和数学老师石泓多年后因石泓邻居陈婧涉及的一桩杀人案重逢，后来被迫站在对立面，由此展开了一场高智商的对决，一步步走向了既震撼人心又令人扼腕的结局。 再说感悟。记得先前被几个同学极度推荐过东野圭吾的侦探小说，比如其中的《放学后》、《白夜行》，当然了还有这个《嫌疑人X的献身》。当时也是热血沸腾下到手机里，想要一睹为快。可是只记得看了个开头，感觉后来剧情发展太慢，情节铺垫过于啰嗦，现在想想还是自己沉不住气，静不下心来（这个毛病得改啊）。今天看完了电影，当时看的原著中的某些情节反而逐渐清晰并回想了起来。毕竟原著的对人物心理、环境等细节描写是无法通过100分钟的视频画面全部显现出来的，尤其是面对着故事背景从日本迁移到天朝。讲述的故事条理还是很清晰，主要的线有两条，一是两大男主的智力对决，而是数学老师对自己爱的人的付出和牺牲（这个牺牲着实够大，够狠啊，果然外表看起来沉默的人发起飙来连自己都害怕）。从小天赋异禀只对数学感兴趣的男主，倘若不是命案的发生，估计也根本不会与女主的生活发生任何的交集了。要说交集顶多也只能是隔着墙壁听女主女儿晚上演奏乐器和每天早上固定去她开的小吃店要份套餐时的简短交流罢了。果然天才都是孤独的，也都是偏激的，上帝在赋予他们异于常人的智力时，也决定了他们与常人交流的格格不入。倘若没有物理学教授唐川的出现，打破这个平静，想必男主也会感到孤独吧。尽管最终自己为了守护爱人设局的结局是自己锒铛入狱，心里还是相当欣慰的：这局我赢了。面对老实巴交一向只知道研究数学的老同学的结局，唐川的心里想必也是惋惜和困惑的：这还是自己心目中那个只知道求最优解的同学吗？是什么值得他为女主如此付出？故事的最后也给出了结局：女主救过他的命。在他最虚弱，人生低谷的时候，她和女生的出现给他的生活带来了生气和活力，之前无论房间或生活的颜色都是灰暗而毫无生机可言的，直到她们的出现，仿佛茂密不透风阴暗潮湿的原始森林里开始射进了一束阳光，温暖、舒服、希望…… 我一向固执认为世上本没有无缘无故的爱，更不会有无缘无故的恨，更不会有真正意义上的感同身受。生物学的知识告诉我们，生物的表现型受基因的决定，但受外界环境的影响。每个人的独特的人生阅历造就了独特的人物性格、人物思想以及气度的格局。每个让我们爱之欲其生、恶之欲其死的人物形象的产生都是有原因的，屁股决定脑袋，位置决定高度。我们批判官员腐败不干实事，只因我们是平民，倘若某天我们真正拥有权力未必就会廉洁奉公；我们批判社会关系复杂事事找关系，只因我们没特权，倘若某天我们遇见这事，未必不会求爷爷告奶奶托关系走后门；我们批判公务员医生警察态度恶劣，倘若我们也经常通宵加班或者每天机器般重复单调无聊的工作，未必也会视每个群众为亲人客气周到……幸福的人生都是类似的，不幸的人生缺各有各的不幸。我想同理心换位思考宽容而不是上帝旁观者的角度看事，应该能够为日常的生活省却无限嫌烦闷吧。 就这样，下面是还记得的台词： 拟一个别人无法解答的问题和解开那个问题,何者更困难?","categories":[{"name":"电影","slug":"电影","permalink":"http://huangzhiyuan.github.io/categories/%E7%94%B5%E5%BD%B1/"}],"tags":[{"name":"电影","slug":"电影","permalink":"http://huangzhiyuan.github.io/tags/%E7%94%B5%E5%BD%B1/"}]},{"title":"Markdown 编辑器语法指南","slug":"Markdown-编辑器语法指南","date":"2017-04-08T08:13:07.000Z","updated":"2020-03-01T12:18:12.000Z","comments":true,"path":"2017/04/08/Markdown-编辑器语法指南/","link":"","permalink":"http://huangzhiyuan.github.io/2017/04/08/Markdown-%E7%BC%96%E8%BE%91%E5%99%A8%E8%AF%AD%E6%B3%95%E6%8C%87%E5%8D%97/","excerpt":"现在是我在学习Markdown时做的笔记。学完这些Markdown的基本使用已经不成问题。","text":"现在是我在学习Markdown时做的笔记。学完这些Markdown的基本使用已经不成问题。 标题设置（让字体变大，和word的标题意思一样）在Markdown当中设置标题，有两种方式：第一种：通过在文字下方添加“=”和“-”，他们分别表示一级标题和二级标题。第二种：在文字开头加上 “#”，通过“#”数量表示几级标题。（一共只有1~6级标题，1级标题字体最大） 块注释（blockquote）通过在文字开头添加“&gt;”表示块注释。（当&gt;和文字之间添加五个blank时，块注释的文字会有变化。） 斜体将需要设置为斜体的文字两端使用1个“*”或者“_”夹起来 粗体将需要设置为斜体的文字两端使用2个“*”或者“_”夹起来 无序列表在文字开头添加(, +, and -)实现无序列表。但是要注意在(, +, and -)和文字之间需要添加空格。（建议：一个文档中只是用一种无序列表的表示方式） 有序列表使用数字后面跟上句号。（还要有空格） 链接（Links）Markdown中有两种方式，实现链接，分别为内联方式和引用方式。内联方式：This is an [example link](http://example.com/).引用方式：I get 10 times more traffic from [Google]1 than from Yahoo or MSN. 图片（Images）图片的处理方式和链接的处理方式，非常的类似。内联方式：![alt text](/path/to/img.jpg “Title”)引用方式：![alt text]id 代码（HTML中所谓的Code）实现方式有两种：第一种：简单文字出现一个代码框。使用&lt;blockquote&gt;。（不是单引号而是左上角的ESC下面~中的）第二种：大片文字需要实现代码框。使用Tab和四个空格。 脚注（footnote）实现方式如下：hello^hello 下划线在空白行下方添加三条“-”横线。（前面讲过在文字下方添加“-”，实现的2级标题） References： 以上内容根据官方文档基本文档进行整理。http://daringfireball.net/projects/markdown/basicsMarkdown官方网站：http://daringfireball.net/projects/markdown/ 推荐一款在线的Markdown编辑器：https://stackedit.io/","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"markdown","slug":"markdown","permalink":"http://huangzhiyuan.github.io/tags/markdown/"}]},{"title":"技术人员的发展之路","slug":"技术人员的发展之路","date":"2017-04-08T07:18:07.000Z","updated":"2020-03-01T12:24:30.000Z","comments":true,"path":"2017/04/08/技术人员的发展之路/","link":"","permalink":"http://huangzhiyuan.github.io/2017/04/08/%E6%8A%80%E6%9C%AF%E4%BA%BA%E5%91%98%E7%9A%84%E5%8F%91%E5%B1%95%E4%B9%8B%E8%B7%AF/","excerpt":"2012年的时候写过一篇叫《程序算法与人生选择》的文章，我用算法来类比如何做选择，说白了就是怎么去计算，但是并没有讲程序员可以发展的方向有哪些。所以，就算是有这些所谓的方法论，我们可能对自己的发展还是会很纠结和无所事从，尤其是人到了30岁，这种彷徨和迷惑越来越重。","text":"2012年的时候写过一篇叫《程序算法与人生选择》的文章，我用算法来类比如何做选择，说白了就是怎么去计算，但是并没有讲程序员可以发展的方向有哪些。所以，就算是有这些所谓的方法论，我们可能对自己的发展还是会很纠结和无所事从，尤其是人到了30岁，这种彷徨和迷惑越来越重。虽然我之前也写过一篇《编程年龄和编程技能》的文章，但是还是有很多做技术的人对于自己能否在年纪大时还能去做技术感到没有信心。我猜测，这其中，最大的问题的是，目前从事技术工作的种种负面的经历（比如经常性的加班，被当成棋子或劳动力等等），让人完全看不到希望和前途，尤其是随着年纪越来越大，对未来的越来越没有信心。 同时，也是因为在GIAC的大会被问到，程序员老了怎么办？而在年底这段时间，也和几个朋友在交流中不断地重复谈到个人发展的这个话题。我的人生过半，活到“不惑”的年纪，自然经常性的对什么事都会回头看看总结归纳，所以，在交谈过程中和交谈过后，自己也有一些思考想记录下来。因为我本人也是在这条路上的人，所以，谈不上给他人指导，我同样也是在瞎乱折腾同样每天在思考自己要去哪儿的“一尘世间迷途老生”。况且，我的经历和眼界非常有限，因此，下面的这些关于个人发展的文字和思考必然是受我的眼界和经历所局限的。也欢迎大家补充和指正。 这些东西不一定对，也不一定就是全部，期许可以让你在年底的时候有所思考，在明年的时候有所计划。 一个重要阶段和标志 在讲个人发展之前，我需要先说一下人生中的一个非常重要的阶段——20到30岁！ 这个阶段的首要任务，就是提升自己学习能力和解决难题的能力。这是一个非常非常关键的时间段！这个时间段几乎决定着你的未来。 30岁以前，这个时间段，应该是人学习和积累的时间段，这个时间段，就是努力学习的时间段。这个时间段，你一定要把时间花在解决问题的技能上。就是说，你一定要练就成的技能是——你能解决大多数人不能解决的问题。使蛮力埋头加班苦干，当一个搬砖老黄牛的是肯定没有前途的。如果你不幸呆在了一个搬砖的地方，天天被业务压得喘不过气来，我建议你宁可让你的项目延期被老板骂，也要把时间挤出来努力学习基础知识，多掌握一些技术（很多技术在思路上是相通的），然后才能有机会改变自己目前的状况。因为，比起你的个人未来，项目延期被老板骂、绩效不好拿不到奖金，都不是什么事儿。 总结一下，你在30岁前，工作5-7年，你需要拥有： 高效的学习能力。这意味着——基础知识扎实、触类旁通、读英文文档不费劲、有寻找前沿知识的能力、能够看到问题和技术的本质、善于思辩、能独立思考。 解决问题的能力。这意味着——你要高效的学习能力、见过很多的场景、犯过或是处理很多错误、能够防火而不是救火。 如果你拥有这两个能力的现象是——在团队或身边的人群中的显现出Leadership。 Leadership并不是当领导和经理，而是一种特征，这种特征有如下两个简单的表象： 帮人解问题。团队或身边中大多数人都在问：“这问题怎么办？”，而总是你能站出来告诉大家这事该怎么办？ 被人所依赖。团队或身边中大多数人在做比较关键的决定时，都会来找你咨询你的意见和想法。 一但你在在30岁之间出现了Leadership这样的特征，那么，你会进入一个正循环的阶段： 因为你学习能力强，所以，你会有更多的机会解决难题。你有更多的机会解决难题，你就会学更多的东西，于是你就会更强。上面这个循环，只要循环上几年，就会让你人生的各种可能性大大的增加。 【 注意 】 要达到这样的特质，需要找到自己的长处、以及适合自己的环境。就像鱼的特长是呆在水里，让鱼儿去追求陆上动物的刺激生活并不靠谱。 一般说来，有这样的潜质的人，在学校中就应该要出现。如果你在大学中还没有出现这样的潜质，那么，你在工作当中要加倍努力了（注：所谓的加倍努力，不是让你使蛮力加班，而是让你多学习成长，使蛮力拼命是弥补不了能力、思维、眼界上的缺陷的）。 Leadership也有范围的，比如，身边的朋友，工作中的团队/部分，圈内，整个行业。Leadership的范围越大，你的个人发展的选择性就越高。反之则越小。 如果已到了30岁左右，还是没有出现这样的特征。那么，可能未来你也很难有这样的Leadership了。而你的个人发展的可能性可能也就不多了（sigh…） 读到这里，我必需要说一下，如果你已开始显现出你的Leadership，那么你才谈得上个人发展，这篇文章后续的内容也可能才会对你有意义。 个人发展的三个方向 以我个人短浅的经历和视野，目前只看到的人的发展有如下三个大方向（他们之间可能会有重叠）： 1）在职场中打拼 2）去经历有意义有价值的事 3）追求一种自由的生活 这三个方向，我个人或多或少都体验过，我也见过身边的很多人走这三个方向走的比较成功。也许还有别的方向，没办法，现在，我的视野就这么大，所以，我在这里，我主要就是谈谈这三个方向。Again，人有资格去走这三个方向的前提是——已有了上面我说的Leadership那种特质！ 一、在职场中发展 在职场中发展应该是绝大多数人的选择。通过加入公司来达到人生的发展。 我们经常可以看到很多所谓的“职业规划”，但是大多数职业规划只不过人力资源搞出来的东西，和实际其实是有很大出入的。我的人生经历中，有18年左右是在公司中度过的，在过银行，小公司，大公司，民营公司，外国公司，传统IT公司，互联网公司，不同的公司完全有不同的玩法和文化，我的经历还算丰富，但也不算特别成功，这里只分享一些我在职场中的心得（不一定对，仅供参考）。 1、去顶尖公司去顶尖公司的一个目的就是让你的Leadership的范围的可能性扩大。 因为公司和公司的差距也不小，所以，就算你在低端公司里是骨干份子，但在高端公司里可能只是一个普通员工（就像中国足球队的主力到了英超可能都无法入选）。所以，在职场中，如果你要让你的个人价值最大化的话，你一定要去顶尖的公司。因为顶尖公司里有非常不错的工作方法和场景，这并不是能看书或是交流得来的，这是必需要去亲身体验的。所以说，在顶尖公司掌握的技能，开阔的眼界，通常来说都会比低端公司的要多得多。 另外，每个公司的工作级别都是有相互对标的，比如：阿里的P几对应于百度的T几。国内的一线公司职位还相当，但是如果和国外一线公司的比，那就有差距了，而且差距还很大。比如，Google或Facebook的某个高级工程师，可能就对应于阿里的P8/P9甚至更高。 是的，对于职场来说，如果你在顶尖公司是骨干，那么，你去低端公司，则有很大机会会成为他们高管和核心。就好像你在Facebook里干三五年成为他们的技术骨干，那么你到BAT去成成为高管概率是非常大的。反过来，如果你毕业主去了BAT成为了一个螺丝钉，在天天加班中度过你的青春，你干个十年能成为BAT的高管的概率可能会非常的低。 2、去真正的创业公司 去顶尖公司和去创业公司在某些时候并不冲突。不过，这里我想讲的是，一个技术能力强的人在大公司可能会被埋没掉。因为大公司业务成功后， 成功的公司在招聘各种高级技术人才都不会成为问题，于是少你一个不少，多你一个不多。 成功的公司其整个技术体系已经完成，Legacy的问题也比较多，所以，可以供你发挥的余地不大。 成功的公司更多的可能会想要稳定的系统，稳定必然会产生保守，而保守则产生不思进取。所以，对于中高级人才来说，在大公司里的能产生的个人价值，可能远远不如那些求贤若渴、没有包袱、可以尽情施展、相对更为灵活和自由的创业型公司。 不过，去创业公司需要小心仔细的挑选和评估，创业公司的不确定因素很多，也和创始人的因素太大了，所以，你需要小心了解创始人和他们的业务情况，想法和理念差不多才能更好的共事。 好多创业公司其实并不是真正的创业公司，他们创业有很大的侥幸和驱利心理，要小心甄别。因为那不是真正的创业公司。 3、职业生涯的发展阶段 首先，有一个不争事实——整个社会是会把最重要的工作交给30岁左右的这群人的。也就是说，30岁左右这群人是这个社会的做事的中坚力量。 所以，这是一个机遇！如果你有了Leadership，你就一定能在这个时间段内赶得上这个机遇——公司和领导对你寄于信任和厚望，并把重要的团队和工作交给你。 于是，你的30岁到40岁就成了一个职业生涯的发展期，也就是你的事业上升期。如果你到40岁都没有赶上，那么你的职业生涯也就这样了，老有所成的人是少数。 在你事业的上升期，你需要更多的软技能，比如： 带领产品和业务的发展的能力 推行自己喜欢的文化的能力 项目管理的能力——在任务重、时间紧中求全 沟通和说服别人的能力 解决冲突的能力 管理和发展团队的能力 解决突发事件的应急能力 …… …… 另外，你还要明白在职场里的几个冷酷的事实： 你开始要关心并处理复杂的人事。尤其在大公司，大量的人都是屁股决定脑袋，利益关系复杂，目标不一致，每个人心里都有不一样的想法。这个时候再也不是talk is cheap, show me the code！而是，code is cheap，talk is the matter。你需要花大量的时间去思考和观察形形色色的人。需要耗费大量的精力在不同的人之间周旋，而不是花时间去创造些什么有价值的东西。 你要开始学会使用各种政治手段。办公室政治不可避免，越大的公司越重，自从你开始成为一线的leader的那一天起，你就开始成为“里外不是人”的角色，需要在下属和领导，员工和公司之间周旋。随而你的级别越来越高，你需要使用更多的政治手段，你会学会审时度世的站队，学会迎合员工和领导，学会用官员的语言说话，学会此一时彼一时，学会妥协和交换，学会忍气吞声，学会在在适当的时机表现自己，学会波澜不惊，学会把自己隐藏起来，甚至你还会迷失自我，开始学会一些厚黑学，比如不得不在适当的时机在背后捅人刀子……你可能会成为一个你自己都讨厌的人。 听上去真的好无聊，所以，你现在也明白为什么高层们都看上去很忙很累，而且抽不出时间来关心细节问题，因为，他们更多的是要协调整个组织和系统来运转，甚至还要四处周旋，各种博弈，没办法，这是职场的必需的东西！听起来是不是感觉人类很愚蠢？这真是没办法的事。如果你不想或是也没有能力玩这些东西，那么你需要去那些可以让技术人员安安心心做技术的公司。这类的公司，我见过Microsoft、Google、Amazon或是一些创业公司里都有。国内的大公司中也有让技术人员成长的职业成长线，但老实说，表面上看似是一个让人专心做技术的升职成长线，但其实还是管理岗位。 所以，技术人员在职场中的归宿有两条路 —— 到真正的技术公司成为一个专心做技术的人，或是在成为一个职业的经理人。 二、追求人生的经历先说三个故事， 第一个，是在阿里的时候，有一天在内网里看到一个贴子，一个做产品的女孩说自己准备离职要去法国学烘培厨艺，引得大家热评。 第二个，是在亚马逊的美国老板，他每年都要去报个培训班学一个技能，比如：厨艺、开双翼飞机、夜总会里的DJ……、甚至去华盛顿去学当一个政客。 第三个，是在汤森路透工作时，一个英国的同事，有一天他说他离职了，和自己的老婆准备用余生去周游世界，我问他是不是有足够多的钱了？他和我说，钱不够，他俩口子的计划是，边旅游边打工，打工打够到下一站的钱就走。他还说，那种用假期去另一个城市的旅游太没意思了，如果你不在那个地方生活上一段时间，你怎么能算是好的旅游体验呢？好吧，无法反驳。我是觉得他们把自己的人生过得如此有意思，令我很佩服。虽然跨界跨得有点猛，但是 Why Not？ 在这里，我想说，去追求一种和众人不一样的人生经历也是一件挺好的事，我个人感觉，比起在职场里有趣地多多了。如果你厌倦了职场，其实为什么不去追求一下不同的人生经历呢。就算你不想去追求跨度比较大的人生经历，那么，在技术圈里，也有很多有价值有意思的经历也可以去的。追求刺激有意义的与众不同的经历的人，其实也能算是一种人生的成功，不是吗？ 如果只说技术方面，我个人看到的去追求经历的人，有两种追求的人其实也很成功的： 到技术创新的发源地去经历创新。计算机互联网各种技术的创新引擎，基本上来说，就是在美国了。我们赶上了这个时代，也选对了这个时代最火热的行业，那么，有什么理由不去这个时代的技术发动机那里去经历呢？在美国硅谷湾区，无论是大公司，还是创业公司，都在迸发着各式各样的创新，如果有能力有机会，为什么不努力去经历一下呢？不经历一下，老了不会觉得错过了是一种后悔吗？ 去经历下一个热点技术的发展。从IT，到互联网、再到移动互联网、云计算、大数据，再到未来的AI，VR，IoT……，技术创新的浪潮一波接一波的过来，你是想在那继续搬砖搬下去，是想迎浪而上去经历浪潮，还是想成为一个随波逐流的人？ 打工也好，创业也好，在国内也好，在国外也好，这些都是形式，不是内容。内容则是你有没有和有想法的人去经历有意义有价值事？人生苦短，白驹过隙，我们技术人员最大的幸运就是生在这样一个刺激的时代，那么，你还有什么理由不去追逐这些前沿刺激的经历呢？ 三、追求自由的生活 我相信“自由”这个事，是所有人的心中都会想去追求的。“生命诚可贵，爱情价更高，…… ”（哈哈） 但一说起自由，绝大多数人都想到的是“财富自由”或是“财务自由”，其实，并不完全是这样的，在自由的通路上，我个人的经历告诉我，其实，你会有很多的不同类型的自由。下面，是我对几个层次的“自由”的理解。 第一层自由——工作自由。人的第一层自由的境界是——“工作自由”，我到不是说你在工作单位上可以很自由，虽然有特例，但并不普遍。我想说的“工作自由”是——你不会有失业危机感了。也就是说，你成了各个公司的抢手货，你不但不愁找不到工作，而且你是完全不愁找不到好工作。试想一下，如果是工作来找你，一方面，你就有真正意义上的工作选择权了，另一方面，你都不愁工作了，你完全就可以随时离职去干你想干的事了。此时，你就达到了“工作自由”。 第二层自由——技能自由。工作自由已是不错，不过前提是你还是需要依赖于别人提供的工作机会。而技能自由则是你可以用自己的技能养活自己，而不需要去公司里工作。也就是所谓的自由职业者了，社会上，这样的人也不少，比如，一些健身体育教练、设计师、翻译者、作者……这些都可以算是自由职业者，程序员这个职业中只要不是搬砖的，有想法的，就有可以成为自由积业者的潜质，想一想，你拥有的编程能力，其实是一种创造的能力，也就是创造力，只要你Make Something People Want（YC创业公司的slogan），你是完全可以通过自己的技能来养活自己的。如果你通过某些自动化的东西，或是你在App上做了一个软件个体户，让自己的收入不断，甚至你做了一个开源软件，社区每个月都给你捐款捐到比你打工挣的还多，那么你就真正的有了技能自由了。 第三层自由——物质自由。我把财务自由换了一种说法。我个人觉得，除了有个好爸爸之外这种特例的情况，如果你想有物质自由的话，本质上来说，你一定要学会投资，投资不一定是你的钱，时间也是一种财富，年轻更是，你怎么投资你的时间还有你的青春？你要把你的投资投到什么样的事，什么样的人？对于投资这个事，风险也比较大。但是，人生不敢冒险可能才是最大的冒险。这个世界有很多技术不是你能看书学来的，而要只能在实战中学会的，比如：游泳。投资可能也是一种。只有真正懂投资的人，或是运气非常好的人，才可能实现物质自由。 追求自由的生活，其实也是个人发展道路上的一个不错的选择。通常来说，自由的人，能力都不差，钱也不会少。因为，他们懂得投资。 也就是说，拥有追求自由能力的的人， 不但有领导力和创造力（也可指导大多数人并走在大多数人前面） 同时他还懂得怎么投资（知道时间和精力和金钱应该投在什么地方）（注：这里我没有提精神自由，老实说，精神上的自由我也不清楚是什么东西，因为我还没有见过，眼界有限，所以先按不表了，不然真成鸡汤文了） 总结 无论是在职场中打拼，还是追求精彩的经历，还是去实现自由，我觉得都是不错的个人发展的方向。 他们都有重叠，比如： 你可以在职场中去追求那些刺激的经历的公司。 同样也可以通过加入有潜力高速发展的公司来达到自由。 你也可以通过追寻不一样的经历来达到人生的自由。…… 总之，这里的逻辑是—— 能够去规划自己的个人发展的人，通常都是有很多机会和可能性的人。 有很多机会和可能性的人，通常都是有Leadership，喜欢冒险的人。 有Leadership喜欢冒险的人，通常都是学习能力强，思维活跃，喜欢折腾，懂得“投资”的人。 学习能力强思维活跃的人，通常来说，都是喜欢看书，喜欢实践和新鲜事物，不怕艰难和挑战，用智力而不是使蛮力的人。 懂得“投资”的人，通常来说，他们更多的关注的是未来和长远的成长，而不是当下的KPI、奖金和晋升。 (全文完) 文章来源：酷壳网","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"职业","slug":"职业","permalink":"http://huangzhiyuan.github.io/tags/%E8%81%8C%E4%B8%9A/"}]},{"title":"关于励志","slug":"关于励志","date":"2017-04-05T15:32:22.000Z","updated":"2020-03-01T12:23:30.000Z","comments":true,"path":"2017/04/05/关于励志/","link":"","permalink":"http://huangzhiyuan.github.io/2017/04/05/%E5%85%B3%E4%BA%8E%E5%8A%B1%E5%BF%97/","excerpt":"世之奇伟、瑰怪、非常之观，常在于险远，而人之所罕至焉，故非有志者不能至也。","text":"世之奇伟、瑰怪、非常之观，常在于险远，而人之所罕至焉，故非有志者不能至也。 一个相似的问题，是一句摘抄，回答是：新闻有条微博说7成网友赞成数学退出高考，下边一片叫好声。有个朋友淡淡回了句：“数学就是用来把这7成人筛出去的。”这句话我永远都记得，所有被千夫所指的困难，都是为了淘汰掉懦夫，仅此而已。在我看来，这句话影响了我接下来的努力和成果，考公成功，起码目前像是往更好的方向走去。人年轻的时候一定要打拼，将来的你一定会感谢现在拼命的你，我想起了一句话，也就是这个问题的回答：“命是弱者的借口，运是强者的谦辞。” 当韩寒去办公室办理退学手续的时候，老师们问他，你不念书了，将来靠什么生活，年少的韩寒天真的说：靠我的稿费啊。老师们全笑了。马云去肯德基应聘，他落选了。马云跟大老板们讲了什么叫电子商务，大老板们得出个结论， 这是个骗子。谢霆锋15岁的时候，父母离了婚，他独自一人去日本学习音乐，只有一把吉他，有时候上晚课回住的地方晚了，就抱着吉他在街上睡。当时只有1米83的艾弗森第一次走进职业篮球场的时候，那些人告诉他，你最终的目标就是每场 得10分和5次助攻，因为你太矮了，永远不可能主宰 这里。北京，张国立带着邓婕在地下室睡门板的时候，两个人都觉得，北漂时能有个伴，真是莫大的幸福。有一天，朴树的妈妈非常难为的问他，你要不要去饭店端个盘子？朴树这才意识到，自己已经在家里白吃白喝很久了。俞敏洪连续考了3年大学，不干农活不打工不赚钱，村里人谁见了都笑话，并且最关键的，他在第3次努力的时候，仍然不知道自己能否考的上。那一年，郭德纲饿的实在没招了，用BB机换了两个馒头。余华把小说投遍了全国各个大小刊物，紧接着，接到了来自全国各地的退稿信。但他没有放弃，他继续写，继续投，紧接着他又接二连三的遭到退稿。艾米纳姆在学校里常常受人欺负，最艰难的是，他是生活在黑人区的一个白人，一天放学回家，他看到一具无名尸体躺在自家门前。有人对年轻的李宗盛说，你那么丑，也没什么天赋，怎么能唱歌呢？林书豪在NBA四处流浪，连续被几家俱乐部横扫出门。艾薇儿在15岁抱着吉他独自离家出走，随着音乐流浪。崔永元第一次主持节目的时候，才主持了一会，身后传来一个声音，这孙子是谁?自从吴宗宪给了一个工作机会后，周杰伦终于不用去餐厅里刷盘子了，于是他写了一些歌，但是没有一个歌手肯唱他写的那些玩意。Lady Gaga在面试的时候，音乐评委说：你的歌声太戏剧性又有流行的风格，没法分类，你另谋高就吧。柳传志拿着二十万开公司，一上来被人骗了14万，气的整宿睡不着。新加坡人阿杜在建筑工地的日子，也好不到哪去。42岁的宗庆发现，做儿童营养液有巨大市场，但亲戚都不支持他，都说那是痴人说梦。他老泪纵横，你能理解一个40多岁的人最后邂逅机遇的心情么。不久后，他给自己的营养液取名娃哈哈。有一天，洗车行来了一辆劳斯莱斯，有个擦车小弟非常欣喜的摸了下方向盘，客人发现后扇了他一巴掌，告诉他，你这辈子都不可能买得起这种车。后来，这个擦车小弟买了6辆劳斯莱斯。擦车小弟名叫周润发。李安毕业后6年没有活干，靠老婆赚钱养着。他曾经想放弃电影，报了电脑班想学点技术打打工补贴家用，他老婆知道后直接告诉他，世界上懂电脑的人那么多，不差你李安一个，你该去只有你能做的事。后来，他拍出了一些世界上只有他能拍出的电影。如果霍华德·舒尔茨被银行拒绝了242次之后放弃了，现在就不会有星巴克。如果沃尔特迪士尼在他的主题公园设计理念被打回302次了之后就放弃了，现在就不会有迪士尼的品牌。如果JK罗琳在稿子被无数次退回连续N年就放弃了，现在就不会有《哈利·波特》系列小说和电影。如果你有个梦想，你就要去捍卫它。 《疯狂原始人》里面有句台词：“你那能叫活着么？你那只能叫没死。” 身不饥寒，天未曾负我；学无长进，我何以对天 你是活了一万多天？还是活了一天 却重复了一万多次？ 如果不走出去，你就会认为你看到的就是你的全部。 生平第一次被邀，当然认真对待！ 在我写出那句话之前，请允许我叙述这句话一直以来激励我汲取有益知识和不断上进的原因：是因为我一直深深怀疑并且渴求了解——“我所处的世界究竟如何运作？”。并且我这种怀疑和求索是真正发自内心的，是真诚的。（不为名利和欲求）后来我得到的信息多了，就愈发向着我心里某个阴暗角落走去。各种线索也不断佐证着我的直觉和推断。我料想我可能会用尽一生去做这个实验，我希望到我离开这个世界前知晓“我所处的世界究竟如何运作？”当我读到这句话时，我深深地知道了在我前面还有多少路要走。 ”每个人都是在洗脑的信息中长大的，后来有的人明白了，有的人去世了。“每当我想起这句话，我就知道我脑海里还有很多没有去除掉的垃圾，我就必须汲取更多有益的信息来使我摆脱这种蒙昧状态；每当我想起这句话，我就十分渴求自己成为一个把目光从洞穴墙壁移向洞穴外世界的明白人；每当我想起这句话，我就知道以知识求独立的旅程是那么地诱惑我，使我昼夜难寐。 这就是我认为最能激励我的话了。 说出来被人嘲笑的梦想，才有实现的价值。 今我之志何在？如笼鸟，如困兽，如鹰隼之失翼，如潜龙之失鳞。耽于三尺之安乐，而忘万里之晴空，蛰于小泽而意殆，渐失无垠之苍穹。惧俗世而裹足，恋今逸而忘初。意沉沉而愈下，年日日而益增。学不见长，闻不见博，行不见谨，言不见传。所以立身者无，能以丧志者有。无恒者唯我，玩物者在斯！ 所谓的光辉岁月，并不是以后，闪耀的日子，而是无人问津时，你对梦想的偏执。 我要这天，再遮不住我眼，要这地，再埋不了我心，要这众生，都明白我意，要那诸佛，都烟消云散！ 你必须非常努力，才能看起来毫不费力。 小时候我发现自己可能穷尽一生也无法读完自己想看的书，学完自己想学的技能的时候，我会告诉自己：学海无涯苦作舟，天道酬勤。但现在会告诉自己：“怕什么真理无穷 进一寸有一寸的欢喜” 你不必跑在任何人后边。 种一棵树最好的时间是十年前，其次是现在。 不要把这个世界，让给你恶心的人。 If not me, who?If not now, when? 生活不可能像你想象得那么好，但也不会像你想象得那么糟。我觉得人的脆弱和坚强都超乎自己的想象。有时，我可能脆弱得一句话就泪流满面；有时，也发现自己咬着牙走了很长的路。 “Everything negative，pressure，challenges，is all an opportunity for me to rise.” —— Kobe Bryant 在一个你不满意的地方，你都无法做领跑者，那么你就没有资格对它不满。 对不起，这把我要赢! 总有一天，我要擒住生活的喉咙，揪着它的衣领，盯着它的眼睛，告诉它：it’s my turn to fuck you!！我装得下所有的卑微，背叛，辛苦，和努力，我经得起任何的漠视，忽略，肮脏和悲寒。献给所有正在牙关紧咬与生活对峙的人们。永不妥协！ 来源：你觉得最励志的一句话是什么？","categories":[{"name":"其他","slug":"其他","permalink":"http://huangzhiyuan.github.io/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"摘抄","slug":"摘抄","permalink":"http://huangzhiyuan.github.io/tags/%E6%91%98%E6%8A%84/"}]},{"title":"《人民的名义》男人戏中的女人们","slug":"《人民的名义》男人戏中的女人们","date":"2017-04-05T15:09:40.000Z","updated":"2020-03-01T12:28:32.000Z","comments":true,"path":"2017/04/05/《人民的名义》男人戏中的女人们/","link":"","permalink":"http://huangzhiyuan.github.io/2017/04/05/%E3%80%8A%E4%BA%BA%E6%B0%91%E7%9A%84%E5%90%8D%E4%B9%89%E3%80%8B%E7%94%B7%E4%BA%BA%E6%88%8F%E4%B8%AD%E7%9A%84%E5%A5%B3%E4%BA%BA%E4%BB%AC/","excerpt":"在抠图剧、替身戏横行的时代，谁都没想到一部主旋律政治剧能成为“爆款”和“网红”。","text":"在抠图剧、替身戏横行的时代，谁都没想到一部主旋律政治剧能成为“爆款”和“网红”。 自首播至今8个播出日，收视率均为双网第一，《人民的名义》带着“反腐反到副国级”的大胆突破，陆续获得了官方、业内和坊间的高度评价。如果说“高冷”知乎上，一个个找影射、扒线索的深度分析贴还不算什么，“八卦”豆瓣上，刷屏的“成了wuli达康书记的迷妹！”也足以反映这部剧在整合不同口味观众、用老戏骨的演技“自产流量”的成功。 在这部反腐版《纸牌屋》营造的“叔控的天堂”里，同样有形形色色行走在权力尖峰上的女性。不同于“糖衣炮弹蛇蝎女+形同保姆老正妻”的脸谱式人物，《人民的名义》在解剖中国式贪腐的同时，更塑造了一批生动的女性群像：精英剩女、高干夫人、蛇蝎情妇……她们在中国官场上，如何在有限的空间中生存？ 反贪局女处长：女强人的感情路 《人民的名义》中，最先出现的女性角色是由柯蓝饰演的反贪局处长陆亦可。刚一出场，唐菀饰演的下属林华华为了不加班，用“相亲”直戳其痛处，让人立刻感到事业型女强人的无奈。但两分钟后，整装出发、执行追捕任务的陆亦可，职业女性的个人魅力和干练气质油然而出，一身检察官制服英姿飒爽，让众多“制服控”欲罢不能。 和激动人心的“大事业”相比，大龄相亲的“小私情”，又算什么烦恼？ 更何况，精英女强人的感情世界，岂是你们这些围观群众能一眼参透的？在事业上不让须眉、冷眼无畏的女检察官，在感情上也有着柔情百转的内心。观众在得知陆亦可“大龄女青年”的身份属性后，也开始好奇她的芳心所属。在陈海家招待侯亮平一场戏让观众捕捉到爱的小线索：陆亦可在陈海面前温柔贤惠、低眉顺眼，而一旦侯亮平开起陈海玩笑，她就开始极为警觉地维护陈海。 还没等到陈海、亦可CP发糖，陈海便遭人陷害重伤昏迷。陆亦可尚未表达的深沉爱意只能藏在心底，不得不强忍住悲愤去探求真相。公义与私情、刚毅与柔情构建起人物性格的诸多侧面，这些源自本性的气质对于柯蓝来说，既是“难演的”，也是“难掩的”。最终，柯蓝交出了一份令人满意的答卷，使观众既欣赏亦可的坚毅，又心疼她的坚强。 当然，鬼马精灵、刀子嘴豆腐心的下属林华华，则反映了职业女性的另一面，如果说陆亦可是“冷面”担当，林华华就是“热血”担当。小处吐槽调笑，大处严肃认真，和同事组成“欢喜冤家”，谁说女检察官就不能职场、情场都玩得转？ 高干夫人：用婚姻交换权力？ 作为高干夫人，张凯丽出演的明史教授吴惠芬，不是“高干文”里的恶婆婆，反而自带知性优雅，同时还有难以捉摸的神秘…… 作为省委副书记高育良的妻子，两人的谈话每次都暗藏玄机，犹如高手过招。我们不难发现，吴惠芬对高育良、高育良的学生、甚至整个汉东的政商圈子都极为熟悉。在高副书记招待下属时，她周旋其中，用温情敲边鼓，但又不时保持着独立和抽离。 这对“官场模范夫妻”之间到底发生了什么？ 以往的荧幕形象里，张凯丽总是接地气的家常妈妈。首次尝试“高干夫人”+“大学教授”，她的演绎与拿捏还是很到位的。除了书记夫人的优雅、大学教授的涵养，吴教授还带着一丝心事重重的隐忍持重，光鲜身份下似乎隐藏着不为人知的秘密，甚至让人拿不准：她究竟是出淤泥而不染的白莲，还是重重黑幕之后的大BOSS？面对手握重权的高育良，作为一个女人，她对婚姻和人生做出的选择，正等待剧情揭晓。 “高干夫人”的反面，是由赵子琪扮演的钟小艾。她是男主角侯亮平的“官配”，更是比老公级别还高的中纪委某室副主任。和扑朔迷离的高氏夫妇不同，这一对夫妻则成为了贪腐阴云中的一道阳光。 陈海车祸后，侯亮平在咖啡厅跟小艾商量回汉东出任反贪局代理局长一职，小艾的回应中有担心、有不舍，但在一番内心挣扎中同意了组织的派遣侯亮平的决定。与其说是深明大义、尊重丈夫，倒不如说是同样奋战在纪检一线、与丈夫在“追求公平正义”上共同的价值观。 在侯亮平临行前，小艾不忘帮他准备好给陈海父母的慰问金，细心与善良见于细节之处，为检察官家庭的日常生活平添了一份温情与暖意。 蛇蝎情妇：从渔家女到企业家的代价 国家部委项目处处长赵德汉贪腐败露、京州市副市长丁义珍仓皇出逃、大风厂强拆引发火灾及工人群体骚动……这些密切交织的情节线索，或明或隐地牵涉到同一个神秘的女性角色——山水集团女掌门人高小琴。 作为剧中“蛇蝎美人”的代表，高小琴的首次亮相就引人注目。在与吴刚饰演的市委书记一起前往拆迁区视察拆迁情况时，高小琴以吴侬软语，怨而不怒地将工作困难娓娓道来，让对方不忍拒绝，足见其长袖善舞、巧捷圆滑的处事手腕。 在最近一集的剧情中，高小琴与祁同伟的暧昧关系已经坐实，大风厂与山水集团股权纠纷的背后，延展出政商之间错综复杂的利益关系和人性欲望。站在各方矛盾风口浪尖的高小琴是个笑里藏刀的狠角色，却又无时不散发着轻熟女干练魅惑的独特韵味，堪称游走在霸气十足与柔媚娇嗔之间的“多面女王”。 这位艳丽又狠辣的蛇蝎美人是如何炼成的？她从一个渔家女子，转身成为女企业家，真的如同自述般全凭清白的奋斗？而周旋在达官贵人之间的游刃有余，背后又有多少被命运所迫的无奈、痛楚？ 高小琴的形象之所以不同于以往脸谱式的“糖衣炮弹”，正因为《人民的名义》将她作为一个女性的欲望，做了多个层面的刻画：她想自我奋斗，又想取道捷径；她想保护家人，又被权力和金钱诱惑；她深谙商场官场人心险恶，又轻信祁同伟的爱意和许诺……最终，构成了她复杂的形象，让我们重新反思一个女性是如何堕落和毁灭的。 纵览这部“男人戏”中的“四大女将”，主创通过丰富的细节、多维的视角使人物的完整性格得到呈现，并在网状的人物关系中强化人物特质，每个女性都立体鲜活、生动多面，不存在扁平式的脸谱化角色。同样具备美貌、学识和能力的她们却走上了不同的人生道路，剧中女性的命运起伏也带给戏外的我们诸多思考。 首先，女人该如何处理与事业的关系。在现代社会中，逐渐自信、独立的女性在职场中该扮演怎样的角色呢？是铁面无情的工作狂陆亦可？为了利益而不择手段的蛇蝎美人高小琴？生活贤内兼事业拍档的钟小艾？还是看似贤内，目前人物走向成谜的吴惠芬？ 剧中女性给出了自己的答案，我们在现实生活中也需要审慎选择。 其次，女人该如何处理与家庭的关系。剧中无论是男人还是女人的变质和堕落，往往都不是一个人的问题，而是家庭出了问题。如在李达康、高育良的家庭中，夫妻之间疏于沟通和理解，缺乏温情脉脉的亲情慰藉和积极向上的精神力量。因此，当一方在权力、利益和欲望中迷失，另一方或尚未发觉，或同流合污。作为组成社会的最基础单位，家庭的和谐关乎社会的稳定。而维护家庭的幸福与和睦，少不了女人的付出与担当。 再次，女人该如何掌握自己人生道路的主导权。主创将高小琴、欧阳菁等人物人性中最幽暗复杂之处剖析开来，也是意在向女性观众传达这样一个道理：可能影响女性一生道路的，有家世、伴侣等诸多外在因素；但最终决定女性命运的，只有她自身的价值选择。学会在充满诱惑的环境中坚定信念、弘扬正义，既是每个女人的必修课，也是最崇高的人性期待。 来源：百度新聞","categories":[{"name":"其他","slug":"其他","permalink":"http://huangzhiyuan.github.io/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"影评","slug":"影评","permalink":"http://huangzhiyuan.github.io/tags/%E5%BD%B1%E8%AF%84/"}]},{"title":"雄安雄安，谁的雄安？","slug":"雄安雄安，谁的雄安？","date":"2017-04-05T14:27:29.000Z","updated":"2020-03-01T12:25:58.000Z","comments":true,"path":"2017/04/05/雄安雄安，谁的雄安？/","link":"","permalink":"http://huangzhiyuan.github.io/2017/04/05/%E9%9B%84%E5%AE%89%E9%9B%84%E5%AE%89%EF%BC%8C%E8%B0%81%E7%9A%84%E9%9B%84%E5%AE%89%EF%BC%9F/","excerpt":"雄安新区的成立，很多人首先想到的是炒房，这怕是会错了党中央的意思。","text":"雄安新区的成立，很多人首先想到的是炒房，这怕是会错了党中央的意思。 1 面对雄安的大方略，我首先想到的也是房子的问题。严格地说，是房价，尤其是北京的高房价。但我的看法，不是炒房又有了一个广阔天地，而是炒房可能走到了穷途末路，高房价，尤其北京的这种房价涨法，也许走到了尽头。 新华社通稿说，雄安新区的成立是千年大计、国家大事。 如何千年大计？如何国家大事？官方的定调是：对集中疏解北京非首都功能，探索人口经济密集地区优化开发新模式，调整优化京津冀城市布局和空间结构，培育创新驱动发展新引擎，具有重大现实意义和深远历史意义。 我的觉悟和眼光有限，看不到太多的现实和历史意义，我只首先看到一点：雄安新区的成立，或许是党中央找到了既解决高房价，又推动经济持续发展的一个新办法、大办法、好办法，而不只是像过去那样表明控制房价的决心。 虽然，目前还是构想初成，理论上可行，实践起来如何，还需要时间检验，但这一次，走到了更正确的方向上。 在适度扩大总需求的同时，着力加强供给侧结构性改革，着力提高供给体系和质量，是习近平总书记“供给侧结构性改革”的核心思想。 我以为，雄安新区的成立，是供给侧改革的高举高打，也是供给侧改革的一场大实验。雄安新区真正的现实意义和历史意义，是改革实践走新路的标本意义。 [2] 一些特大城市房价高成这样的负能量已是显而易见，决策层也是清楚的。 以北京为例，前一段流传的数据是，房价持续大涨的背后，去年，从工业到文娱消费，北京的很多行业的发展指标都是大幅下行，甚至历史性下行的。 也就是说，房价的上涨与经济的发展是高度悖离的。高房价对应的不但不是经济的高成长，而且是经济的下行，这个高，就是虚高，就是泡沫。 这些年，实体经济的发展遇到很多问题，关于实体经济的发展也有很多讨论，这些讨论中，矛头大多指向了税费、虚拟经济。个人以为，若论对实体经济的伤害，真正猛于虎的，不是税费、虚拟经济，而是不断虚高和泡沫化的房价。 税费、虚拟经济等等，这些对实体经济都是皮外伤，真正把实体经济打成内伤甚至大出血的是高房价，高房价促使了实体经济的诸多恶循环。 高房价让实体经济，甚至虚拟经济都背上了不必要的高成本，在经济全球化的格局下，高成本是没有竞争力的。这个问题，任正非先生曾经谈过，遗憾的是，他的很多讲话都被炒作得铺天盖地，这个谈话却没太引起重视。 任总说：“深圳房地产太多了，没有大块的工业用地了，现在土地越来越少，越来越贵，产业成长的可能空间就会越来越小。既然要发展大工业、引导大工业，就要算一算大工业需要的要素是什么，这个要素在全世界是怎么平均的，算一算每平方公里承载了多少产值，这些产值需要多少人，这些人要有住房，要有生活设施。生活设施太贵了，企业就承载不起；生产成本太高了，工业就发展不起来。” 以北京为例，普通企业给到普通职员一万月薪已是不堪重负了，因为企业不光人力，其他成本尤其房租成本也是大涨。但现在，普通企业不给到1万块已经很快找到稍微像样一点的人了，因为不给到这个人家没法过日子。为什么？光是房租，一个小单间就要好几千，要是买了房的再供房，1万块是连房贷都不够。 但即便企业咬牙拿出这个薪水，拿到薪水的人如果家底不过硬，日子还是过得一把鼻涕一把泪。为什么？也是因为光是房租，一个小单间就要好几千，要是买了房的再供房，1万块是连房贷都不够。 普通企业和普通职员是经济发展的中流砥柱，中流砥柱都被房子困住了，后果就是生产的不能好好地生产，消费的不能好好地消费，生产消费都被掣肘了，还谈什么实体经济振兴，而且不光实体经济，虚拟经济也同样受累。 [3] 既然高房价猛于虎，政府也一直在调控，为什么房价还是涨啊？ 当然有炒房、投机的驱动，但最根本的，还是供需不合理，是供给测改革还有很大的空间，还需从更高层面去搞得更好。 比如北京，很多人对房价高的怨言大，批评政府的责任。政府其实也是一肚子苦水。领导比你我更清楚高房价的问题，但领导好像也是没有更好的办法。 曾经和一个北京的领导吃饭，谈到房价问题。他的说法是，想进来的人和钱太多，限都限不住啊！人、钱太多，你加大土地供应啊！把六环以内的都推出来，还不够，就到七环。领导笑笑，好啊，只要你能妥妥地解决以下问题。 他说了北京在最近10年挤进多少人口，增加多少城市负担。今天的北京，每天要用多少水，多少电，消耗多少粮食，生产多少垃圾，以及投入多少公共交通运力还被骂娘……具体数字记不清了，但有一点印象深刻，也是他的结论： 房价不能大涨，涨了大家受不住；人口不能猛增，增了城市受不了；但我们的人口就是不断带着钱往城市里涌啊。“很多目标跟现实都是背向的。” 总结起来就是，城市负荷有限，人口不能大增，这就意味着不能大举土地供应，但我们的需求很大啊，全国的购买力都跑过来。 结果呢，一路大涨，脱离经济基本面的涨。 这种涨苦了北京，上海，深圳，也苦了全国人民和全国均衡发展。一是这些地方的房价虚高，给其他地方炒房提供了支撑。更主要的还是，人力、资本都往这些地方奔，其他地方还要不要发展？要不要共同富裕，全面小康了？ 共同富裕靠什么？靠共同的经济发展。经济发展靠什么？靠人才，靠资本。一个地方的人才、资本过度聚集，背后是其他地方的人才和资本失血。 北上深人才、资本聚集的背后，就是其他的地方在人才、资本失血。现在，全国有多少年轻人是卖掉父母在老家的房，带着父母在老家一辈子的积蓄，走进了深圳、走进了上海、走进了北京啊？走进来之后呢，过得一把鼻涕一把泪。 一边聚集，一边失血的结果，是区域发展的更不平衡以及贫富的更悬殊。最近这些年，农村的问题，东北的问题备受关注，甚至令人倍感忧虑。 表面看，这是产业结构的问题，更深层次，是人才、资本流向的问题。很多地方的农村，主要劳动力已经空了。东北的沉沦，是投资不过山海关，还有一个更厉害，是精英人才纷纷跨出山海关。 人才、资本流动的更深层次问题是什么呢？是政策、资源供给的问题。 比如北京，为什么大家都非要往这里聚集啊？因为这里是北京啊。这里聚集了发展机会，这里聚集了优势功能和核心资源。这里是，全国有钱人得个稍微大点的病，都希望到北京的301、协和等等挂个号。 这种背景下，Duang：我给你来个雄安新区，给你来个更深层次的解决问题：集中疏解北京非首都功能，探索人口经济密集地区优化开发新模式，调整优化京津冀城市布局和空间结构，培育创新驱动发展新引擎。 怎么从更深层次解决问题？ 以集中疏解北京非首都功能为例，说白一点，就是把“北京”的“供给”引导到“雄安”去。 什么是“北京”的“供给”？ 中共中央、国务院印发通知；继深圳经济特区和上海浦东新区之后又一具有全国意义的新区。这就是“北京”的“供给”。 此类的“供给”转移与延展，此前也有过，比如让通州成为北京的副中心，甚至传言燕郊等等将划到北京的版图。但这都不够格局，也不够开阔，甚至还有点继续摊大饼走老路。而此次，Duang的一下高开远走，其眼光和手笔都已大不同。 [4] 因此，雄安的意义不只在雄安，而在通过雄安宣誓，中国的经济发展进入到全国均衡发展的新高度，并真正进入到城市反哺农村，大城市反哺小城市的新时代。 这么多年来，虽然我们推行了很多工业反哺农业，城市反哺农村的办法，但基本上可以说，现实里，依然是在农村推动城市，小城市拱卫大城市。 以京津冀为例。背靠北京的河北和天津，这些年的发展一直不太如人意，民间的一种说法是，河北和天津的“血”（人力、资本等要素）都被北京吸了。 雄安的成立宣誓，现在起，北京不再只是“吸血”，而要开始真正的“输血”了。 京津冀如此，全国亦可如此。 如此，雄安是全国人民的雄安。 或许，这才是雄安真正的意义，是真正的千年大计、国家大事。 这千年大计、国家大事，于眼前，有助于扭转大城市房价泡沫问题，于长远，则将有助于区域的均衡发展，实现真正的协同发展，共同发展和持续发展。 也因此，想到雄安，首先想到炒房，想到又多了一个炒房的广阔天地，这恐怕会错了党中央的意思。 雄安一定会有光明的前途，但这个光明的前途一定不是炒出来的，更不应该，也不可能是靠炒房炒出来的。 甚至，我预计，雄安不但不会给投机炒作提供广阔空间，相反还会就炒作式经济作出更多的改革，对炒作投机进行更多的限制。 现在，全国炒房，借钱炒房，很大一个原因是，大家都认为炒房不会亏。北京大学国家发展研究院姚洋院长说，老百姓是需要被教育的，（房价）腰斩一次，他才知道痛。这，我举双手赞成。 这几年，北京房价猛涨，也把燕郊、大厂等临近纷纷炒作起来，雄安一出，应该是给这些炒作泼了点冷水，最好是让一批人亏点钱，给个教训。 国家对雄安的一个定位是，要培育创新驱动发展新引擎，这或许也是在预示，发展雄安，将不那么依靠房地产而将真正依靠产业来拉动经济，甚至不排除会探索一下新的住房解决方案，比如以公租房、廉租房等等，因为只有解决这些问题，才能更好的杜绝炒房经济。 因此，也友情提示已经闻风而动的投机者和投资者，尤其那些加班加点研究某个公司在雄安有多少土地的券商分析师，以及要闻鸡起舞的人： 雄安的机遇确实大，但投机需谨慎，盲目炒作，眼前也许会追出多少个涨停，但不排除潮水退去之后，追高退出不及时，最后会落它个偷鸡不成蚀把米。 比如被重点推荐的华夏幸福，据说已在雄安新区签约了约500平方公里的土地。很多分析师因此强烈推荐，认为其是雄安最大的赢家，甚至私下给出吓死人的目标价，个人认为，这样的分析师或许还没真正读懂雄安。 华夏幸福是眼光独到的中国领先的产业新城运营商，也为推动区域经济均衡发展做了很多创新探索和积极贡献。但在雄安面前，它的眼光恐怕要让一让它的胸怀。真要是500平方公里都按之前的协议给了它，雄安岂不是成了华夏幸福的雄安。 很多事，塞翁失马，焉知祸福。 就在雄安新区成立的前夜，河北省纪委监察厅网站3月28日发布消息称，雄县原县委书记吴亚飞涉嫌严重违纪，目前正接受组织审查。而华夏幸福与雄县的181.2平方公里的整体开发协议，就是在吴亚飞任上落定的。 因此，悲观谨慎一点考虑，市场对华夏幸福一片喜不胜人的乐观背后可能也暗藏着风险的发生。比如，雄安的500平方公里需要重新讨论，而其已经在“北京大七环”的重投入可能会因为雄安新区的横空出世，一时进退维谷。 浦东成立之初，大家宁要浦西一张床，不要浦东一套房。雄安一出，全国沸腾，网络上热传的却是，多少人披星戴月地赶过去炒房。 这些年，人们的投机觉悟，确实是提高了。 但既然是听党中央的，应该听完整一点。 新华社说，党中央、国务院通知要求，对雄安的规划建设，要保持历史耐心，尊重城市建设规律，合理把握开发节奏。关于房子，则有句名言： “房子是用来住的，不是用来炒的”。 –end– 原文链接：谁的雄安？","categories":[{"name":"其他","slug":"其他","permalink":"http://huangzhiyuan.github.io/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"新闻","slug":"新闻","permalink":"http://huangzhiyuan.github.io/tags/%E6%96%B0%E9%97%BB/"}]},{"title":"人工智能风口只有2%的人能够成为赢家！","slug":"人工智能风口只有2-的人能够成为赢家！","date":"2017-03-28T15:45:42.000Z","updated":"2020-03-01T12:22:28.000Z","comments":true,"path":"2017/03/28/人工智能风口只有2-的人能够成为赢家！/","link":"","permalink":"http://huangzhiyuan.github.io/2017/03/28/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E9%A3%8E%E5%8F%A3%E5%8F%AA%E6%9C%892-%E7%9A%84%E4%BA%BA%E8%83%BD%E5%A4%9F%E6%88%90%E4%B8%BA%E8%B5%A2%E5%AE%B6%EF%BC%81/","excerpt":"现在去xxx创业大街向天上扔一沓美金，砸到10个路人，有9个都自称投资人，其中8个投人工智能。 满嘴的Artificial Intelligence、摩尔定律、大数据、新算法、认知技术、计算机视觉、机器学习、自然语言处理、机器人技术、语音识别……要是不懂点Deep Learning基本原理，没看过雷·库兹韦尔的《奇点临近》都不好意思和别人打招呼。 事实真的是这样吗？23年后人工智能会统治地球吗？说实话，这些暂时和你没有太大关系，你只需要知道，人工智能的第一波红利已来临！","text":"现在去xxx创业大街向天上扔一沓美金，砸到10个路人，有9个都自称投资人，其中8个投人工智能。 满嘴的Artificial Intelligence、摩尔定律、大数据、新算法、认知技术、计算机视觉、机器学习、自然语言处理、机器人技术、语音识别……要是不懂点Deep Learning基本原理，没看过雷·库兹韦尔的《奇点临近》都不好意思和别人打招呼。 事实真的是这样吗？23年后人工智能会统治地球吗？说实话，这些暂时和你没有太大关系，你只需要知道，人工智能的第一波红利已来临！ 在2016 Techcrunch论坛上，李开复曾经说过：“当人工智能识别人脸，超过人的时候，保安的工作至少一部分就没有了；当人工智能能够听懂语音的时候，客服和打电话卖东西人的工作就没有了；当人工智能能够更聪明的炒股的时候，很多人的工作就没有了。” 今年是人工智能发展的第61个年头，所有的投资机构都在谈论和布局该领域，但人工智能将在哪些产业爆发？传统企业又该如何挖掘第一桶金？要知道，每个风口都只有2%的人能够成为赢家。 黑马学院特邀CSDN&amp;极客帮基金创始人蒋涛（黑马连营第5期连长），今晚9点-10点，将在103个黑马社群同步直播，讲讲人工智能的第一波红利！ WHY 人工智能为什么会火？ 谈到科技革命，时下最火的莫过于人工智能。 我自己做技术社区，做了十几年，看到过一波一波的技术浪潮过来。当年移动互联网大潮过来的时候，我们原来在PC端做的事情，都可以用移动互联网的理念把它重新做一遍。简单来说，现在的人工智能就相当于原来移动互联网的概念，原来移动端做过的事情，现在又可以结合人工智能的方式再做一遍，而且会比之前更具颠覆性的效果。毫不夸张地说，我认为人工智能所带给我们的冲击，将会像工业革命一样。 今年是「人工智能」诞生的61周年（注：1956年夏“人工智能之父”麦卡锡首次提出这个概念），同时也是它的第三次浪潮。这次浪潮和以往的前两次都不一样，这次有了实质性的突破。以前，相当于你想到对面去，但是面前有一堵墙；现在这堵墙被凿开了，之所以这么讲，是因为我有以下三点的观察思考： 第一点：人工智能虽然还处于技术创新期，但到人工智能的普及期，我认为也就需要十到二十年的时间。现在相当于移动时代的2005年，虽然第一部3G手机2007年才出现，但2005年我们已经很清楚2G是要到3G的。虽然人工智能还没有找到突破口，不知道会怎么商业化，但是大方向是有的。所以，现在只要你在这个领域冲到第一名，就会持续得到投资。 第二点：人工智能的基础已经充实，它是一个逻辑上的发展，这个发展可以分为三个阶段。1）云计算，把信息基础云化，云计算基础设施的完善使得人工智能响应速度更快。2）大数据，计算的过程中累积了数据，数据的极大丰富，使得基于大数据做出行为分析及短期预判成为可能，各个行业的信息化也为此奠定了良好的基础。3）判断决策，对大数据的判断从而产生了更好的决策，决策实际上就是人工智能的进展。 现在我们的生活中就有很多计算机技术在做决定，坐车是滴滴在帮你调度；去餐馆是大众点评用算法把离你最近、人气最高的餐馆选出来等等，所以你的生活已经和人工智能相关了。 第三点：人工智能之所以取得重大的突破，除了前两个阶段的铺垫外，深度学习的发展也贡献了非常重要的力量。 未来是“AIR”的世界 未来你看到的是物理世界和虚拟世界的叠加，这就是VR和AR，也就是R时代；I时代是物联网时代；A时代也就是人工智能时代。 其实你看到的世界很可能不是真的。为什么这么说？因为这是从视神经系统处理出来的。现在我们可以用计算机处理掉，叠加到视网膜上。比如：我不用递名片，只要念头一转，你的名片就应该自动出来了。那怎么能做到这样呢？就是把所有的信息都连接到网络上，这样物理世界和虚拟世界才能叠加在一起。人之所以和其他生物不一样是因为人会做思考、决策，比动物要高一个级别，具有抽象的能力，这是未来20年的大趋势。 AI技术体现在图像识别的突破上，更重要的可能是智能语言的突破，他能理解你讲的话，写出来的文字，甚至能理解照片，当做到这些的时候，行业就会产生变革。 各个行业基本可分为4个阶段：数字化、数据化、自动化和智能化。越到后面它的武器越强。原来是长枪、大矛，练的是武功，后面就变成机关枪了，扫射的时候你会发现不一样。 今年我们做了AI100，也叫人工智能100年。今年是人工智能的第61年，可能再过20年到40年，这个世界90%的人就不用工作了。在未来，我们要培养200万名数据分析师，因为决策和运营都是用数据驱动的。同时我们也会与投资相结合，帮助中国30万家企业走向智能化阶段。 Where 第一波红利的三个产业 人工智能究竟会改变哪些领域？如何改变呢？ 1． 自动驾驶。 任何领域有非常大的数据量，人工智能都可以用上。全世界和运输价值相关的公司，都已经相信无人驾驶的发展是必然的。所以在无人驾驶、电动车的框架之下，未来的司机基本上会被无人驾驶取代。 单车智能与智慧交通是无人驾驶技术发展的两个阶段。其中，单车智能是无人驾驶技术的基础，是实现无人驾驶终极形态的根本路径；车联网与智能交通则是推进无人驾驶技术发展的强力催化剂，将助力无人驾驶技术的普及。无人驾驶技术的成熟将最终构建城市智能驾驶生态圈，为未来出行提供新的解决方案。 2. 客服行业。 做金融服务的宜信，有1万多个客服，携程大概有7、8千个客服，每天负责就接各种投诉电话，每次都是被用户狂骂、抱怨，未来将有更多工业机器人替代这些低效率的人力。 人工智能客服系统主要是整合邮件、电话、微博、微信、网页、API接口、移动SDK等渠道在内的服务渠道，并统一自动分配工单，同时留存用户信息便于下次咨询时识别。 基本能做到：1）24小时机器人客服在线，随时响应客户的相关资讯和需求；2）建立客服机器人的内容库，用深度学习的方式自动回复重复问题；3）接入人工时机器人给予部分回复建议，加快反馈速度；4）接入内部办公系统，推动多部门协作反馈以及用户精准营销；5）后台实时数据统计汇总，管理用户评价，进行数据挖掘和数据分析； 3. 医疗领域。 医疗人员医院里有大量的临床病历数据，而且不断的产出数据。医疗方面的人工智能主要分为两部分：一是图像识别，应用于感知环节，其主要目的是将影像这类非结构化数据进行分析，获取一些有意义的信息；二是深度学习，应用于学习和分析环节，是AI应用的最核心环节，通过大量的影像数据和诊断数据，不断对神经元网络进行深度学习训练，促使其掌握“诊断”的能力。不管是什么病，图像只是其中一个参数，而治疗疾病则需要多个参数。此外，80%的数据属于非结构化数据，亦即报告+影像。未来，人工智能的使用将大大提高治疗效率。 How 传统企业如何获得红利？ 简单来说，现在的人工智能就相当于原来移动互联网的概念，原来移动端做过的事情，现在又可以结合人工智能的方式再做一遍，而且会比之前更具颠覆性的效果。毫不夸张地说，我认为人工智能所带给我们的冲击，将会像工业革命一样。 传统制造业智能化设想 传统企业的未来有两个方向：第一，你有没有用户的服务和连接。第二，在这个基础上，有没有做智能化决策和分析。后来就是本身在生产线上的提升，生产线也面临着一个问题，你的控制有没有数据化。 在一些制造业工厂，升级之后有没有更好的成本上的控制。极客帮有个合作的LP，专门给小米做代工，他就给我看传统的生产线，就是一条生产线，都是自动化的设备。所以它能够做到手环40元钱，还有利润挣。就卖40元钱还能挣钱，而且挣得还不少。总的来说，我们看到的一个方向的趋势。 对制造业来说，意义最大的是把它的制造过程，原来的控制系统做成一定的数据化，再看看能不能做规划。那些大规模生产的时代已经结束，一定要往前端走，差异化可能在产品设计上、包装上、服务上，从大生产时代到个性化时代，未来到一个智能化时代。 “创业者埋头苦干的同时也要抬头看天！” 来源：CSDN","categories":[{"name":"其他","slug":"其他","permalink":"http://huangzhiyuan.github.io/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"新闻","slug":"新闻","permalink":"http://huangzhiyuan.github.io/tags/%E6%96%B0%E9%97%BB/"}]},{"title":"年轻人，你缺少的不仅是一套房子","slug":"年轻人，你缺少的不仅是一套房子","date":"2017-03-28T15:37:17.000Z","updated":"2020-03-01T12:26:36.000Z","comments":true,"path":"2017/03/28/年轻人，你缺少的不仅是一套房子/","link":"","permalink":"http://huangzhiyuan.github.io/2017/03/28/%E5%B9%B4%E8%BD%BB%E4%BA%BA%EF%BC%8C%E4%BD%A0%E7%BC%BA%E5%B0%91%E7%9A%84%E4%B8%8D%E4%BB%85%E6%98%AF%E4%B8%80%E5%A5%97%E6%88%BF%E5%AD%90/","excerpt":"这两天北京、广州相继限购升级，朋友圈又被没买房的人刷屏了——无论买得起买不起房，无一例外都在吐槽，政策打击不了炒房客，只能伤害自己这样的“刚需客”。","text":"这两天北京、广州相继限购升级，朋友圈又被没买房的人刷屏了——无论买得起买不起房，无一例外都在吐槽，政策打击不了炒房客，只能伤害自己这样的“刚需客”。 前两天，知乎有一个问题也很热：北京的房价是不是正在透支着北京年轻人的创造力和生活品质？而其中一个答案更是成为了一个经典的段子： 几人打车到清华，聊起某某几年前就买房了，真是人生赢家。出租车司机大爷默默听了很久说：我家拆迁分了几套房，可我就是个开车的，你们才是国家的未来和希望。如果你们北大清华毕业，人生的目标就是在北京买套房，而不是思考国家的未来，那这个国家真的没有希望了。 作为一个在20岁之前就靠自己搞定了一线城市首付的年轻人，倒是颇有同感。于我看来，绝大多数年轻人缺少的，远远不仅仅是一套房子那么简单。 1.优秀的人太多，不仅是你一个 北京房价高企的原因是什么？因为稀缺。 作为首善之城，北京聚集的是全中国最顶尖的政治，教育，科技，医疗资源，提供的是中国最多的机会——这么好的东西，人人都想要，于是乎物以稀为贵。 你和租在隔壁的小李都想买房，但是房子只有一套，你985毕业，他清华毕业；你月薪一万二，他月薪两万；你家境一般，国企职工，父母半辈子积蓄60万，他家境优渥，私企老板，轻轻松松拿出100万；房子只有一套，于是他买到了，你没买到。 985毕业不优秀吗？优秀。月薪一万二很少吗？挺好了。国企职工的父母很丢人吗？很不错了。 但是这世界上，总有很多人，先天条件比你优秀，后天还比你努力——用一句政治不太正确的话来说，他们比你更应该拥有一套房子。 与其纠结自己为什么买不起房，不如想想自己还有哪里做的不够好。 2.投资的机会太多，不仅是买房一条 为什么我们都想着买房呢？原因不仅仅在于拥有一套房子可以为我们节约房租，能够让我们的下一代有更好的教育机会，更是因为：一年又一年过去，房价涨了一轮又一轮。 换句话说，房地产，早已不仅是居住所用的“商品”，更是一种“投资标的”。而人们热衷于投资房地产，则是因为其具备的低风险、高稀缺性、高杠杆与高升值潜力这些特性。 确实，这几年，房价涨的很厉害。曾几何时我还为通州一万二的价格咋舌，如今已经对三万一平的燕郊司空见惯了。 但是，放眼更广阔，投资的机会实在太多，不仅是卖房一条路。 腾讯股票，2007年市值800亿人民币，2017年市值1.8万亿人民币，10年20多倍； 比特币，2009年9美元，2017年1100美元，8年120多倍； 而北京房价，东三环的富力城2007年1.4万，如今12万，10年9倍，显得有些相形见绌了。 不断刷新自己的认知体系，不断发掘市场上出现的新机会，财富跑赢房价，并不是梦。 3.努力的路还很长，不仅在一朝一夕 有一个很流行的段子：“我10年前卖掉了自己的房子去创业，努力奋斗了十年，刚刚交了首付把之前那套房子买了回来。”我们每个人都不想白干十年，希望的是能过坐收房地产升值的红利。 然而没有什么是可以急于求成的，安家立业更是如此。未雨绸缪的确是好事，但是在自己的收入和财富尚不足以支撑置业，就空为房价忧虑，未免会欲速则不达。再说直接点，对于大多数人而言，买房其实是一种“求稳”的表现。 然而我们还年轻，我们还没到求稳的年龄。 远了说，马云31岁才创办中国黄页，王健林35岁开始创业，任正非43岁创办华为。近了说，2011年，一个32岁的中年人放弃了待遇不错的工作，冒着失败后卖掉唯一一套房子的风险，开始创业。三年出头，他的公司宣告上市，他叫唐岩。 谁能想象，这个如今身价40亿的男人，15年前第一次来北京，全副家当不过是一卷铺盖和2000块钱。 事实上，人生的道路还很长，比起纠结房价，我们更应该做的是做好自己的认知升级。事实上，连我自己在刚刚交完首付时也一度有过“求稳”的心态，但是一瞬间就想到，自己还年轻，还有太多的机会和可能。至于一套房，不过是给纵情向前的自己一个坚实的后盾罢了。 对于年轻的我们而言，奋发向上的努力，清晰准确的眼光，天生我材必有用的决心——这些东西的价值，比起一套房子，真的重要太多。 至于评论区的各位，我只想说一句话：如果你觉得这些是鸡汤你做不到的话，那你又凭什么能买得起房？ 所以你看——你缺少的可不仅仅是一套房子啊！ 来源：知乎","categories":[{"name":"其他","slug":"其他","permalink":"http://huangzhiyuan.github.io/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"新闻","slug":"新闻","permalink":"http://huangzhiyuan.github.io/tags/%E6%96%B0%E9%97%BB/"}]},{"title":"活在中国，为什么要努力上学？","slug":"活在中国，为什么要努力上学？","date":"2017-03-28T15:27:18.000Z","updated":"2020-03-01T12:26:12.000Z","comments":true,"path":"2017/03/28/活在中国，为什么要努力上学？/","link":"","permalink":"http://huangzhiyuan.github.io/2017/03/28/%E6%B4%BB%E5%9C%A8%E4%B8%AD%E5%9B%BD%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%8A%AA%E5%8A%9B%E4%B8%8A%E5%AD%A6%EF%BC%9F/","excerpt":"万达王健林(辽宁大学211)工商银行姜建清（上财211本，上交985硕博）农业银行刘士余（清华985本硕博）交通银行牛锡明（央财211）","text":"万达王健林(辽宁大学211)工商银行姜建清（上财211本，上交985硕博）农业银行刘士余（清华985本硕博）交通银行牛锡明（央财211）三一重工梁稳根（中南大学985）小米雷军（武大985）魅族白永祥（电子科大985）网易丁磊（电子科大985）360周鸿祎（西安交大985）华为任正非（重大985）华为孙亚芳（电子科大985）联想柳传志（西安电子科大211）联想杨元庆（上交大985本，中科大985硕）微软、百度张亚勤（中科大985）京东刘强东（中国人民大学985）当当李国庆（北大985）巨人网络史玉柱（浙大985）百度李彦宏（北大985）搜狐张朝阳（清华985）新浪曹国伟（复旦985）搜狗王小川（清华985）优酷土豆古永锵（加州大学伯克利本，斯坦福硕）世纪佳缘龚海燕（北大985）58同城姚劲波（中国海洋大学985）聚美优品陈欧（南洋理工）盛大陈天桥（复旦985）盛大张向东（北大985）一加手机刘作虎（浙大985）OPPO 陈明永（浙大985）苏宁张近东（南师大211）移动董事长奚国华（合工大211本，同济985博）移动总裁李跃（天津大学985）电信王晓初（北邮211）中兴史立荣（清华985）美的方洪波（华东师大985）TCL李东生（华南理工985）海尔张瑞敏（中科大985）新东方俞敏洪（北大985）携程／如家／汉庭创始人季琦（上海交大985）比亚迪王传福（中南大学985）华润傅育宁（大连理工985）万科郁亮（北大985）富力地产李思廉（香港中文大学） 明白你为什么要上学了吧。。。 来源：知乎","categories":[{"name":"其他","slug":"其他","permalink":"http://huangzhiyuan.github.io/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"新闻","slug":"新闻","permalink":"http://huangzhiyuan.github.io/tags/%E6%96%B0%E9%97%BB/"}]},{"title":"关于中国男足","slug":"关于中国男足","date":"2017-03-23T14:44:37.000Z","updated":"2020-03-01T12:22:48.000Z","comments":true,"path":"2017/03/23/关于中国男足/","link":"","permalink":"http://huangzhiyuan.github.io/2017/03/23/%E5%85%B3%E4%BA%8E%E4%B8%AD%E5%9B%BD%E7%94%B7%E8%B6%B3/","excerpt":"男足都赢了，没有理由不努力了！","text":"男足都赢了，没有理由不努力了！ 追梦赤子心作曲 : GALA作词 : GALA充满鲜花的世界到底在哪里如果它真的存在那么我一定会去我想在那里最高的山峰矗立不在乎它是不是悬崖峭壁 用力活着用力爱哪怕肝脑涂地不求任何人满意只要对得起自己关于理想我从来没选择放弃即使在灰头土脸的日子里 也许我没有天分但我有梦的天真我将会去证明用我的一生也许我手比较笨但我愿不停探寻付出所有的青春不留遗憾 向前跑 迎着冷眼和嘲笑生命的广阔不历经磨难怎能感到命运它无法让我们跪地求饶就算鲜血洒满了怀抱 继续跑 带着赤子的骄傲生命的闪耀不坚持到底怎能看到与其苟延残喘不如纵情燃烧吧有一天会再发芽 未来迷人绚烂总在向我召唤哪怕只有痛苦作伴也要勇往直前我想在那里最蓝的大海扬帆绝不管自己能不能回还 失败后郁郁寡欢那是懦夫的表现只要一息尚存请握紧双拳在天色破晓之前我们要更加勇敢等待日出时最耀眼的瞬间 向前跑 迎着冷眼和嘲笑生命的广阔不历经磨难怎能感到命运它无法让我们跪地求饶就算鲜血洒满了怀抱 继续跑 带着赤子的骄傲生命的闪耀不坚持到底怎能看到与其苟延残喘不如纵情燃烧为了心中的美好不妥协直到变老 ![\"加油奋斗\"](/img/2017/0323/1.jpg)","categories":[{"name":"其他","slug":"其他","permalink":"http://huangzhiyuan.github.io/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"体育","slug":"体育","permalink":"http://huangzhiyuan.github.io/tags/%E4%BD%93%E8%82%B2/"}]},{"title":"从0到1笔记","slug":"从0到1笔记","date":"2017-03-23T13:37:55.000Z","updated":"2020-03-01T12:21:40.000Z","comments":true,"path":"2017/03/23/从0到1笔记/","link":"","permalink":"http://huangzhiyuan.github.io/2017/03/23/%E4%BB%8E0%E5%88%B01%E7%AC%94%E8%AE%B0/","excerpt":"照搬他人的成果要比创造新东西容易，做我们已知如何去做的事情，会使世界发生从0到1的改变，增添许多类似的东西，但是每次我们创造新事物的时候，却使世界发生从0到1的改变。创造的行为是独一无二的，创造发生的瞬间也是独一无二的。","text":"照搬他人的成果要比创造新东西容易，做我们已知如何去做的事情，会使世界发生从0到1的改变，增添许多类似的东西，但是每次我们创造新事物的时候，却使世界发生从0到1的改变。创造的行为是独一无二的，创造发生的瞬间也是独一无二的。 任何科技公司，无论现在多么挣钱，如果不在艰难的创新上进行投资，将来都会以失败而告终。（目光要长远，格局要大，也许很难，也许很苦，也许一个人默默艰难在一条谁也不知道尽头是什么的路上走了很远，坚持下去，以后的某一天，蓦然回首你曾经走过的路，原来早已跨过了千山万水而不自觉……） ## 第一章 未来的挑战 - “你有没有在什么重要的问题上与其他人看法不同？” - 未来只是还没有到来的时刻的集合。 - 没有人能精确地预测未来，但我们知道两件事：世界必然会变得不同，但变化必须基于当今的世界。 - 我们期待的未来是进步的。进步可以呈两种形式。第一，水平进步，也称广泛进步，意思是照搬已经取得的经验——直接从1跨越到n。水平进步很容易想象，因为我们已经知道了它是什么样的。第二，垂直进步，也称深入进步，意思是要探索新的道路——从0到1的进步。垂直进步很难想象，人们需要去尝试从未做过的事。 - 创业思维：初创公司往往是新科技的诞生地。一个初创公司就是一群人，人数规模适当，这群人可以采纳同一计划去铸就新未来。一个新公司最重要的力量是新思想，新思想甚至比灵活性更重要，而小规模给思考留下了空间。而每个初创公司都不得不做一场思维运动：质疑现有观念，从零开始重新审视自己所从事的业务。 第二章 像1999年那样狂欢 企业的目的就是盈利，不是赔钱。 20世纪90年代会联网热潮 1993年 马赛克浏览器 1994年 网景导航者 网景通信公司（Netscape） 1996年 雅虎上市（估值8.48亿美元） 1997年 亚马逊上市（估值4.38亿美元） 1999年 Pay Pal创建 泡沫，90年代从“砖块到网络”的转变没有取得预计的成果，投资者又重新把目光放回砖块，产生了另一个泡沫——房地产。遭受硅谷之劫，经验教训： 逐渐超前不要总沉溺在宏大的愿景中，否则会使泡沫膨胀。自称可以做出丰功伟绩的人都不能相信，心存改变世界之雄心的人通常要更加谦逊。小幅循序渐进地成长是唯一安全前进的道路。 保持精简和灵活性所有的公司都必须留出一定空间，不要事事都严格计划。你不该知道你的事业会变成事先规划通常既死板又不现实。相反，你应该多做些尝试，反复实践，把创业当成未知的实验。 改进竞争方式 不要贸然创造一个新市场。以现成的客户作为出发点创业才更有保障，在成功者已经创造出的产品的基础上，将这些已经被认可的产品加以改进，才是可取之道。 专注于产品，而非销量如果你的产品需要广告或营销人员去推销，说明你的产品还不够好：科技应用于商业应该主打产品开发，而不是销售物流。泡沫年代的广告显然都是浪费，唯一持久的增长是病毒式增长。 以上对立面可能也会更正确： 大胆尝试胜过平庸保守 坏计划也好过没有计划 竞争性市场对收益有负面影响 营销和产品同样重要 第三章 所有成功的企业都是不同的 “还有什么有价值的公司没有成立？” 如果你想获得持久的价值，不要只是跟风建立一个没有特色的企业 完全竞争和垄断（多家航空公司进行竞争，但谷歌只有一家） 垄断企业推动了社会进步，因为数年甚至数十年的垄断利润诱使人们去进行创新。之后垄断企业会不断创新，因为利润给了他们规划长远未来的资本，它们有能力投资野心勃勃的研究项目，而这些事困在竞争中的企业想都不敢想的。 托尔斯泰在《安娜-卡列尼娜》中以下面这段文字作为开头“幸福的家庭总是相似的，不幸的家庭各有各的不幸。”在商业中，恰恰相反。成功的企业的原因各有不同：每个垄断企业都有解决一个独一无二的问题。而失败的企业失败的原因却相同：它们都没有成功避免竞争。 第四章 竞争意识 竞争是一种观念——这种观念在整个社会中蔓延，扭曲了我们的思想。我们宣扬竞争，把竞争的必要性内化，颁布竞争的条律；结果就是，尽管竞争越来越激烈，我们实际获得的却越来越少，我们把自己困在了竞争中。 胜利肯定比失败好，但是如果这场战役不值得打，那每个参与者都是输家。 如果你不能打败对手，那就和对手联手。 需要的时候，不仅要战斗，还必须得赢，没有中间选择：不管通过什么方法，和风细雨润物无声，还是暴风骤雨速战速决。 第五章 后发优势 规避竞争会帮助你打造垄断企业，但是只有经受住未来考验的企业才是成功的企业。 一个企业今天的价值是它以后创造利润的综合。 一个公司要想有价值，不但必须成长，还必须能持续发展 垄断企业特征 专利技术专利技术是一家公司最实质性的优势，它使你的产品很难或不能被被别的公司复制。如谷歌的搜索算法。一般而言，专利技术在有些方面必须必它最相近的替代品好上10倍才能拥有真正的垄断技术。事实上，做出10倍改进最明确的方法就是创造全新的事物，或者彻底改进已经存在的事物，就可以避开竞争。 网络效应网络效应使一项产品随着越来越多的人使用变得更加有用。（Facebook、webchat、Alipay）。矛盾的是，享有网络效应的企业必须从非常小的市场做起。 规模经济垄断企业越大越强，开发一项产品的固定成本需要更高的销量来分摊。一个好的初创企业在刚开始设计时就应该考虑到之后的大规模发展潜能。 品牌推广一家公司最显而易见的垄断是最自己品牌的垄断，因此打造一个强势品牌是形成垄断的有力方式。 建立垄断企业方法 占领小市场 扩大规模 不要破坏性创新 后来者居上（后期现金流，及现在的融资很重要） 第六章 成功不是中彩票 “成功是靠运气还是靠技能？” “你能掌握自己的未来吗” 如果你认为自己的未来是明确的，那么提前了解未来，并且努力打造未来就是有意义的。但如果你脑海里的未来只是一团迷雾，无法预测，那你就会萌生放弃掌控它的念头。 直到进入大学，才发现，这10年的努力只不过是为了完全不了解的未来填写了一张令人困惑的多元化的简历罢了。 长期规划仍是最重要的 规划优先于机遇 你不是一张彩票 一个初创公司是你可以明确掌握尽最大努力的机会。你不只是拥有自己生命的代理权，还拥有这世界上某个重要角落的代理权。而这一切都要从抵制不公平的概率主宰开始，因为你并不是一张被概率决定命运的彩票。 第七章 向钱看 “天知道，损有余而补不足” 幂次法则（二八定律） 风投的最大密码就是，成功基金的最佳投资所获的回报要等于或超过其他所有投资对象的总和。 保证未来价值，最普遍的回答是多样化的投资组合——“别把所有鸡蛋都放在一个篮子里” “你做什么并不重要，重要的是你要把它做好”。你做什么并不重要？真是彻头彻尾的错误。你应该将全部注意力放在你擅长的事情上，而且在这之前要仔细想一想未来这件事情是否会变得很有价值。 第八章 秘密 “那些有价值的公司还没有人创建？” “人们为什么不探索秘密？” 渐进主义。从小接受教育如何，为了好成绩，也为了应付考试 风险规避。害怕犯错。如果你的目标是一生不犯错，你就不应该去探索秘密。将一生奉献于别人不相信的事情，在正确的路途上孑孓独行已是艰难，而在错误的路途上独自前行更是不可忍受。 自满。社会精英享有最大的自由也最有能力去探索新想法，但他们似乎最不相信秘密。既然能够舒舒服服地享用已有成果，为什么还要大力气去探索秘密？ “扁平化”。全球化的推进，世界成了一个同质的、激烈竞争的市场。任何一个由雄心壮志的人，在探索秘密之前都会先问自己一个问题：如果有可能发现新事物，难道全球人才库中更加聪明、更加有创造力的人还没有发现吗？这种怀疑的声音阻止了人们去继续探索秘密。 为了幸福，每个人都“需要制定目标，并付诸努力，而且至少要实现几个这样的目标”，目标可分为以下三类： 稍作努力即可达到的目标（简单） 不懈努力才能达到的目标（困难） 如何努力都不可能达到的目标（不可为） 如何发现秘密？ 秘密分为两种：关于自然和和关于人的。 自然没有告诉你的秘密是什么？人类没有告诉你的秘密是什么？ 不断向前延伸的道路，是从家门开始的。《魔戒》 第九章 基础决定命运 基础没有打好的初创企业是无法挽救的 初创时的“联姻” 合伙人的选择：技术能力和才华互补固然重要，创始人之间的了解程度和他们合作的默契程度也同样重要。 所有权、经营权和控制权 所有权：谁在法律上拥有公司的资产 经营权：谁实际上管理着公司的日常事务 控制权：谁在形式上管理公司事务 现金奖励不是王道（现在难说啊） 只要公司还在持续创新，创业就还没结束 第十章 黑手党式的机制 “理想的公司文化应该是什么样的？” 第十一章 顾客不会自动找上门 销售在虚拟世界里不足为道，而在现实世界中却极为重要。 技术宅男VS 销售人员 有时产品本身就是一种销售 第十二章 人类和机器 “未来机器会取代你吗？” 每个人都期待未来的计算机能干更多的事情，躲到有些人怀疑：30年后，人类自己还有什么可做的事情吗？ 计算机是辅助人类的工具，而非替代物。 科技是这这个全球化的世界中逃避竞争的唯一方式。虽然计算机越来越强大，但它们不能取代人类：它们只起补充作用。 先进的软件为此提供了可能性，但更为重要的是人类分析师、检察官、科学家、金融专家，没有他们的积极参与，软件毫无用处。 机器学习、大数据、人工智能 第十三章 绿色能源与特斯拉 “创业七问” 工程问题：你的技术具有突破用，而不仅仅是稍有改进吗？ 时机问题：现在开创事业，时机合适吗？ 垄断问题：开创之初，是在一个小市场抱占大份额吗？ 人员问题：你有合适的团队吗？ 推广问题：除了创造产品，你有么有办法推广产品？ 持久问题：未来10年获20年，你能保住自己的市场地位吗？ 秘密问题：你有没有找到一个其他人没有发现的独特的机会？ 第十四章 创始人的悖论 特立独行的个性是驱动公司进步的引擎 王者归来 公司应该汲取的教训是企业离不开创始人。对于创始人看似极端怪异的行为，要有更大的容忍度，我们需要靠非同寻常的人来领导公司，取得大的飞跃，而非限于小的进步。 创始人应该汲取的教训是不要沉醉于自己的声望和他人对自己的追捧中。否则会使自己臭名远播，或是被人妖魔化。 打造更加美好未来 人类未来的可能出现的四种模式 模式一：历史是繁荣与衰败之前的不断更替。直至最近，人们才敢于期待永远避免灾难，但还是怀疑我们视为理所当然的稳定能否维持下去 模式二：全世界大融合，最终达到一个稳定发展的状态，类似于今天最富有的国家的生活（个人觉得这个不可能，贫富差距这么好消除吗） 模式三：毁灭性的灾难，我们无法幸免于难。考虑到当今世界紧密相连的地理环境，以及现代武器史无前例的破坏力，能否组织大规模的社会灾难发生，是必须要问的问题。 模式四：加速腾飞，奔向更美好的未来 未来从来都不是自行发生的，万物归零还是有所突破，这取决于我们自己 重要的是我妈妈是否能抓住独一无二的机会，在日常工作中创新。对我们——全宇宙、全球、全国、全公司、整个生命乃至此刻最为重要的是——独特！ 从0到1 ，更加美好，从独立思考开始，重新认识这个世界，如同古人第一眼看见这个世界一样的新奇，我们才能重构世界，并为未来守护这个世界。The End...","categories":[{"name":"读书","slug":"读书","permalink":"http://huangzhiyuan.github.io/categories/%E8%AF%BB%E4%B9%A6/"}],"tags":[{"name":"读书","slug":"读书","permalink":"http://huangzhiyuan.github.io/tags/%E8%AF%BB%E4%B9%A6/"}]},{"title":"机器学习实战---KNN近邻算法","slug":"机器学习实战-KNN近邻算法","date":"2017-03-22T14:49:07.000Z","updated":"2020-03-01T12:25:00.000Z","comments":true,"path":"2017/03/22/机器学习实战-KNN近邻算法/","link":"","permalink":"http://huangzhiyuan.github.io/2017/03/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-KNN%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/","excerpt":"1. 近邻算法概述 简单说，k-近邻算法采用测量不同特征值之间的距离进行分类。 优点：精度高、对异常值不敏感、无数据输入假定 缺点：计算复杂度高、空间复杂度高 适用数据范围：数值型和标称型","text":"1. 近邻算法概述 简单说，k-近邻算法采用测量不同特征值之间的距离进行分类。 优点：精度高、对异常值不敏感、无数据输入假定 缺点：计算复杂度高、空间复杂度高 适用数据范围：数值型和标称型 工作原理：存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道每项数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征月样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似（最近邻）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中K的出处。 一般流程： １）收集数据：任何方法 ２）准备数据：距离计算所需要的数值，最好是结构化的数据格式 ３）分析数据：任何方法 4）该步骤不适用于k-近邻算法 ４）测试算法：计算错误率 ５）使用算法：首先需要输入样本数据和结构化的输出结果，然后运行k-近邻算法判定输入数据分别属于哪个分类，最后应用对计算出的分类执行后续的处理。 2. 准备：使用python导入数据 新建knn.py，添加以下代码，用来得到数据集和标签，其中group矩阵每行包含一个不同的数据（可看做某个日志文件中不同的测量点），label包含了每个数据点的标签信息，label包含的元素个数等于group矩阵行数。 1234567from numpy import *import operatordef createDataSet(): group = array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]]) labels = [&#x27;A&#x27;,&#x27;A&#x27;,&#x27;B&#x27;,&#x27;B&#x27;] return group, labels 为了方便使用createDataSet()函数，它创建数据集和标签。在Python环境下执行下列命令查看该模块加载情况： 12&gt;&gt;&gt; import kNN&gt;&gt;&gt; group,labels=kNN.createDataSet() 2.1.2 文本文件解析数据解析数据伪代码：对未知类别属性的数据集中的每个点依次执行以下操作：1）计算已知类别数据集中的点与当前点之间的距离；2）按照距离递增次序排序；3）选取与当前点距离最小的K个点4）确定前K个点所在类别的出现频率；5）返回前K个点出现频率最高的类别作为当前点的预测分类代码如下： 12345678910111213def classify0(inX, dataSet, labels, k): dataSetSize = dataSet.shape[0] diffMat = tile(inX, (dataSetSize,1)) - dataSet sqDiffMat = diffMat**2 sqDistances = sqDiffMat.sum(axis=1) distances = sqDistances**0.5 sortedDistIndicies = distances.argsort() classCount=&#123;&#125; for i in range(k): voteIlabel = labels[sortedDistIndicies[i]] classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1 sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0] classify0函数有4个输入参数：用于分类的输入向量inX，输入的训练样本集为dataSet，标签向量为labels，最后的参数k表示用于选择最近邻的数目，其中标签向量的元素数目和矩阵dataSet的行数相同。程序中使用的是欧式距离公式。计算两个向量点xA和xB之间距离。 为了预测数据所在分类，在Python提示符中输入下列命令： 1kNN.classify0([0,0], group, labels, 3) 输出结果为B，也可以改变输入的[0,0]为其他值，测试程序的运行结果。到现在为止，我们已经构造了第一个分类器，使用这个分类器可以完成很多分类任务，从这个实例出发，构造使用多个分类算法将会更加容易。 2.2 使用k_近邻算法改进约会网站的配对效果2.2.1 准备数据收集到的约会数据存放在文本文件datingTestSet.txt中，每个样本数据占据一行，总共有1000行。样本主要包涵以下3种特征。 每年获得的飞行常客里程数 玩视频游戏所耗费时间百分百 每周消费的冰淇淋公升数 魅力程度形如： 40920 8.326976 0.953952 largeDoses 14488 7.153469 1.673904 smallDoses 26052 1.441871 0.805124 didntLike 75136 13.147394 0.428964 didntLike在将上述特征数据输入到分类器之前，需要格式化处理，下面函数的输入为文件名字符串，输出为训练样本矩阵和类标签向量。 123456789101112131415def file2matrix(filename): fr = open(filename) numberOfLines = len(fr.readlines()) #get the number of lines in the file returnMat = zeros((numberOfLines,3)) #prepare matrix to return classLabelVector = [] #prepare labels return fr = open(filename) index = 0 type=&#123;&#x27;largeDoses&#x27;:3,&#x27;smallDoses&#x27;:2,&#x27;didntLike&#x27;:1&#125; for line in fr.readlines(): line = line.strip() listFromLine = line.split(&#x27;\\t&#x27;) returnMat[index,:] = listFromLine[0:3] classLabelVector.append(type.get(listFromLine[-1],0)) index += 1 return returnMat,classLabelVector 解析文本记录，结果如下： 12reload(kNN)datingDataMat, datingLabels = kNN.file2matrix(&#x27;datingTestSet.txt&#x27;) 2.2.2 分析数据：使用Matplotlib创建散点图首先使用使用Matplotlib制作原始数据的散点图。 123456import matplotlibimport matplotlib.pyplot as pltfig = plt.figure()ax = fig.ad_subplot(111)ax.scatter(datingDataMat[:,1], datingDataMat[:,2])plt.show() 输出效果如下。散点图使用datingDataMat矩阵的第二、第三列数据，分别表示特征值“玩视频游戏所耗时间百分比”和“每周所消费的冰淇淋公升数”。 没有使用样本分类的特征值，很难从上图中看到任何有用的数据模式信息。matplotlib库提供的scatter函数支持个性化标记散点图上的点。重新输入上面的代码，调用scatter函数时使用下列参数： 1ax.scatter(datingDataMat[:,1], datingDataMat[:,2], 15.0*array(datingLabels), 15.0*(datingLabels)) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#!/usr/bin/python2.7# _*_ coding: utf-8 _*_from matplotlib import pyplot as pltfrom matplotlib import font_managerimport file2matrixmatrix, labels = file2matrix.file2matrix(&#x27;datingTestSet.txt&#x27;)print matrixprint labelszhfont = matplotlib.font_manager.FontProperties(fname=&#x27;/usr/share/fonts/truetype/arphic/ukai.ttc&#x27;)&quot;&quot;&quot; 比较好看的绘制方法 &quot;&quot;&quot;plt.figure(figsize=(8, 5), dpi=80)axes = plt.subplot(111)# 将三类数据分别取出来# x轴代表飞行的里程数# y轴代表玩视频游戏的百分比type1_x = []type1_y = []type2_x = []type2_y = []type3_x = []type3_y = []print &#x27;range(len(labels)):&#x27;print range(len(labels))for i in range(len(labels)): if labels[i] == 1: # 不喜欢 type1_x.append(matrix[i][0]) type1_y.append(matrix[i][1]) if labels[i] == 2: # 魅力一般 type2_x.append(matrix[i][0]) type2_y.append(matrix[i][1]) if labels[i] == 3: # 极具魅力 print i, &#x27;：&#x27;, labels[i], &#x27;:&#x27;, type(labels[i]) type3_x.append(matrix[i][0]) type3_y.append(matrix[i][1])type1 = axes.scatter(type1_x, type1_y, s=20, c=&#x27;red&#x27;)type2 = axes.scatter(type2_x, type2_y, s=40, c=&#x27;green&#x27;)type3 = axes.scatter(type3_x, type3_y, s=50, c=&#x27;blue&#x27;)# plt.scatter(matrix[:, 0], matrix[:, 1], s=20 * numpy.array(labels),# c=50 * numpy.array(labels), marker=&#x27;o&#x27;,# label=&#x27;test&#x27;)plt.xlabel(u&#x27;每年获取的飞行里程数&#x27;, fontproperties=zhfont)plt.ylabel(u&#x27;玩视频游戏所消耗的事件百分比&#x27;, fontproperties=zhfont)axes.legend((type1, type2, type3), (u&#x27;不喜欢&#x27;, u&#x27;魅力一般&#x27;, u&#x27;极具魅力&#x27;), loc=2, prop=zhfont)plt.show() 2.2.3 准备数据：归一化数值 处理不同取值范围的特征值时，通常采用的方法是数值归一化，如将取值范围处理为0到1或者-1到1之间。下面的公式可以将任意取值范围的特征值转换为0到1之间内的值。 newValue = (oldValue - min) / (max - min) 归一化特征值代码： 123456789def autoNorm(dataSet): minVals = dataSet.min(0) maxVals = dataSet.max(0) ranges = maxVals - minVals normDataSet = zeros(shape(dataSet)) m = dataSet.shape[0] normDataSet = dataSet - tile(minVals, (m,1)) normDataSet = normDataSet/tile(ranges, (m,1)) #element wise dividereturn normDataSet, ranges, minVals 在Python命令提示符下，重新加载kNN.py模块 2.2.4 算法测试 机器学习算法一个很重要的工作就是评估算法的正确率，通常我们只提供已有数据的90%作为训练样本类训练分类器，而使用其余的10%数据去测试分类器，检测分类器的正确率。 分类器针对约会网站的测试代码： 12345678910111213def datingClassTest(): hoRatio = 0.50 #hold out 10% datingDataMat,datingLabels = file2matrix(&#x27;datingTestSet2.txt&#x27;) #load data setfrom file normMat, ranges, minVals = autoNorm(datingDataMat) m = normMat.shape[0] numTestVecs = int(m*hoRatio) errorCount = 0.0 for i in range(numTestVecs): classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:],datingLabels[numTestVecs:m],3) print &quot;the classifier came back with: %d, the real answer is: %d&quot; % (classifierResult, datingLabels[i]) if (classifierResult != datingLabels[i]): errorCount += 1.0 print &quot;the total error rate is: %f&quot; % (errorCount/float(numTestVecs))print errorCount 2.2.5 使用算法：构建完整可用系统 约会网站预测函数: 12345678910def clssifyPerson(): resultList = [&#x27;not at all&#x27;,&#x27;in small doses&#x27;,&#x27;in large doses&#x27;] percentTats = float(raw_input(&quot;percentage of time spent playing video games?&quot;)) ffmiles = float(raw_input(&quot;frequent fliter miles earned per year?&quot;)) iceCream = float(raw_input(&quot;liters of ice cream consumed per year?&quot;)) datingDataMat,datingLabels = file2matrix(&#x27;datingTestSet2.txt&#x27;) normMat,ranges,minVals = autoNorm(datingDataMat) inArr = array([ffMiles,percentTats,iceCream]) classifierResult = classify0((inArr-minVals)/ranges,normMat,datingLabels,3) print &quot;you will probably like this person: &quot; + resultList[classifierResult - 1] 下一节，将会接着学习使用k-近邻算法实现的手写识别系统。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"KNN","slug":"KNN","permalink":"http://huangzhiyuan.github.io/tags/KNN/"}]},{"title":"欢迎使用 Cmd Markdown 编辑阅读器","slug":"欢迎使用-Cmd-Markdown-编辑阅读器","date":"2017-03-22T13:46:26.000Z","updated":"2020-03-01T12:25:22.000Z","comments":true,"path":"2017/03/22/欢迎使用-Cmd-Markdown-编辑阅读器/","link":"","permalink":"http://huangzhiyuan.github.io/2017/03/22/%E6%AC%A2%E8%BF%8E%E4%BD%BF%E7%94%A8-Cmd-Markdown-%E7%BC%96%E8%BE%91%E9%98%85%E8%AF%BB%E5%99%A8/","excerpt":"我们理解您需要更便捷更高效的工具记录思想，整理笔记、知识，并将其中承载的价值传播给他人，Cmd Markdown 是我们给出的答案 —— 我们为记录思想和分享知识提供更专业的工具。 您可以使用 Cmd Markdown：","text":"我们理解您需要更便捷更高效的工具记录思想，整理笔记、知识，并将其中承载的价值传播给他人，Cmd Markdown 是我们给出的答案 —— 我们为记录思想和分享知识提供更专业的工具。 您可以使用 Cmd Markdown： 整理知识，学习笔记 发布日记，杂文，所见所想 撰写发布技术文稿（代码支持） 撰写发布学术论文（LaTeX 公式支持） 除了您现在看到的这个 Cmd Markdown 在线版本，您还可以前往以下网址下载： Windows/Mac/Linux 全平台客户端 请保留此份 Cmd Markdown 的欢迎稿兼使用说明，如需撰写新稿件，点击顶部工具栏右侧的 新文稿 或者使用快捷键 Ctrl+Alt+N。 什么是 MarkdownMarkdown 是一种方便记忆、书写的纯文本标记语言，用户可以使用这些标记符号以最小的输入代价生成极富表现力的文档：譬如您正在阅读的这份文档。它使用简单的符号标记不同的标题，分割不同的段落，粗体 或者 斜体 某些文字，更棒的是，它还可以 1. 制作一份待办事宜 Todo 列表 支持以 PDF 格式导出文稿 改进 Cmd 渲染算法，使用局部渲染技术提高渲染效率 新增 Todo 列表功能 修复 LaTex 公式渲染问题 新增 LaTex 公式编号功能 2. 书写一个质能守恒公式[^LaTeX]$$E=mc^2$$ 3. 高亮一段代码1234567@requires_authorizationclass SomeClass: passif __name__ == &#x27;__main__&#x27;: # A comment print &#x27;hello world&#x27; 4. 高效绘制 流程图12345678st=&gt;start: Startop=&gt;operation: Your Operationcond=&gt;condition: Yes or No?e=&gt;endst-&gt;op-&gt;condcond(yes)-&gt;econd(no)-&gt;op 5. 高效绘制 序列图123Alice-&gt;Bob: Hello Bob, how are you?Note right of Bob: Bob thinksBob--&gt;Alice: I am good thanks! 6. 高效绘制 甘特图12345678910111213title 项目开发流程section 项目确定 需求分析 :a1, 2016-06-22, 3d 可行性报告 :after a1, 5d 概念验证 : 5dsection 项目实施 概要设计 :2016-07-05 , 5d 详细设计 :2016-07-08, 10d 编码 :2016-07-15, 10d 测试 :2016-07-22, 5dsection 发布验收 发布: 2d 验收: 3d 7. 绘制表格 项目 价格 数量 计算机 $1600 5 手机 $12 12 管线 $1 234 8. 更详细语法说明想要查看更详细的语法说明，可以参考我们准备的 Cmd Markdown 简明语法手册，进阶用户可以参考 Cmd Markdown 高阶语法手册 了解更多高级功能。 总而言之，不同于其它 所见即所得 的编辑器：你只需使用键盘专注于书写文本内容，就可以生成印刷级的排版格式，省却在键盘和工具栏之间来回切换，调整内容和格式的麻烦。Markdown 在流畅的书写和印刷级的阅读体验之间找到了平衡。 目前它已经成为世界上最大的技术分享网站 GitHub 和 技术问答网站 StackOverFlow 的御用书写格式。 什么是 Cmd Markdown您可以使用很多工具书写 Markdown，但是 Cmd Markdown 是这个星球上我们已知的、最好的 Markdown 工具——没有之一 ：）因为深信文字的力量，所以我们和你一样，对流畅书写，分享思想和知识，以及阅读体验有极致的追求，我们把对于这些诉求的回应整合在 Cmd Markdown，并且一次，两次，三次，乃至无数次地提升这个工具的体验，最终将它演化成一个 编辑/发布/阅读 Markdown 的在线平台——您可以在任何地方，任何系统/设备上管理这里的文字。 1. 实时同步预览我们将 Cmd Markdown 的主界面一分为二，左边为编辑区，右边为预览区，在编辑区的操作会实时地渲染到预览区方便查看最终的版面效果，并且如果你在其中一个区拖动滚动条，我们有一个巧妙的算法把另一个区的滚动条同步到等价的位置，超酷！ 2. 编辑工具栏也许您还是一个 Markdown 语法的新手，在您完全熟悉它之前，我们在 编辑区 的顶部放置了一个如下图所示的工具栏，您可以使用鼠标在工具栏上调整格式，不过我们仍旧鼓励你使用键盘标记格式，提高书写的流畅度。 3. 编辑模式完全心无旁骛的方式编辑文字：点击 编辑工具栏 最右侧的拉伸按钮或者按下 Ctrl + M，将 Cmd Markdown 切换到独立的编辑模式，这是一个极度简洁的写作环境，所有可能会引起分心的元素都已经被挪除，超清爽！ 4. 实时的云端文稿为了保障数据安全，Cmd Markdown 会将您每一次击键的内容保存至云端，同时在 编辑工具栏 的最右侧提示 已保存 的字样。无需担心浏览器崩溃，机器掉电或者地震，海啸——在编辑的过程中随时关闭浏览器或者机器，下一次回到 Cmd Markdown 的时候继续写作。 5. 离线模式在网络环境不稳定的情况下记录文字一样很安全！在您写作的时候，如果电脑突然失去网络连接，Cmd Markdown 会智能切换至离线模式，将您后续键入的文字保存在本地，直到网络恢复再将他们传送至云端，即使在网络恢复前关闭浏览器或者电脑，一样没有问题，等到下次开启 Cmd Markdown 的时候，她会提醒您将离线保存的文字传送至云端。简而言之，我们尽最大的努力保障您文字的安全。 6. 管理工具栏为了便于管理您的文稿，在 预览区 的顶部放置了如下所示的 管理工具栏： 通过管理工具栏可以： 发布：将当前的文稿生成固定链接，在网络上发布，分享 新建：开始撰写一篇新的文稿 删除：删除当前的文稿 导出：将当前的文稿转化为 Markdown 文本或者 Html 格式，并导出到本地 列表：所有新增和过往的文稿都可以在这里查看、操作 模式：切换 普通/Vim/Emacs 编辑模式 7. 阅读工具栏 通过 预览区 右上角的 阅读工具栏，可以查看当前文稿的目录并增强阅读体验。 工具栏上的五个图标依次为： 目录：快速导航当前文稿的目录结构以跳转到感兴趣的段落 视图：互换左边编辑区和右边预览区的位置 主题：内置了黑白两种模式的主题，试试 黑色主题，超炫！ 阅读：心无旁骛的阅读模式提供超一流的阅读体验 全屏：简洁，简洁，再简洁，一个完全沉浸式的写作和阅读环境 8. 阅读模式在 阅读工具栏 点击 或者按下 Ctrl+Alt+M 随即进入独立的阅读模式界面，我们在版面渲染上的每一个细节：字体，字号，行间距，前背景色都倾注了大量的时间，努力提升阅读的体验和品质。 9. 标签、分类和搜索在编辑区任意行首位置输入以下格式的文字可以标签当前文档： 标签： 未分类 标签以后的文稿在【文件列表】（Ctrl+Alt+F）里会按照标签分类，用户可以同时使用键盘或者鼠标浏览查看，或者在【文件列表】的搜索文本框内搜索标题关键字过滤文稿，如下图所示： 10. 文稿发布和分享在您使用 Cmd Markdown 记录，创作，整理，阅读文稿的同时，我们不仅希望它是一个有力的工具，更希望您的思想和知识通过这个平台，连同优质的阅读体验，将他们分享给有相同志趣的人，进而鼓励更多的人来到这里记录分享他们的思想和知识，尝试点击 (Ctrl+Alt+P) 发布这份文档给好友吧！ 再一次感谢您花费时间阅读这份欢迎稿，点击 (Ctrl+Alt+N) 开始撰写新的文稿吧！祝您在这里记录、阅读、分享愉快！ [^LaTeX]: 支持 LaTeX 编辑显示支持，例如：$\\sum_{i=1}^n a_i=0$， 访问 MathJax 参考更多使用方法。 [^code]: 代码高亮功能支持包括 Java, Python, JavaScript 在内的，四十一种主流编程语言。","categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"markdown","slug":"markdown","permalink":"http://huangzhiyuan.github.io/tags/markdown/"}]},{"title":"Hexo user guide","slug":"hello-world","date":"2016-02-27T13:06:21.000Z","updated":"2020-03-01T11:45:50.000Z","comments":true,"path":"2016/02/27/hello-world/","link":"","permalink":"http://huangzhiyuan.github.io/2016/02/27/hello-world/","excerpt":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy Delete an article123delete db.json$ hexo clean$ hexo g More info: Deployment","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://huangzhiyuan.github.io/categories/Hexo/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://huangzhiyuan.github.io/tags/hexo/"}]}],"categories":[{"name":"技术总结","slug":"技术总结","permalink":"http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"},{"name":"机器学习","slug":"机器学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"深度学习","slug":"深度学习","permalink":"http://huangzhiyuan.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"GPU","slug":"GPU","permalink":"http://huangzhiyuan.github.io/categories/GPU/"},{"name":"读书","slug":"读书","permalink":"http://huangzhiyuan.github.io/categories/%E8%AF%BB%E4%B9%A6/"},{"name":"其他","slug":"其他","permalink":"http://huangzhiyuan.github.io/categories/%E5%85%B6%E4%BB%96/"},{"name":"CUDA","slug":"CUDA","permalink":"http://huangzhiyuan.github.io/categories/CUDA/"},{"name":"Linux","slug":"Linux","permalink":"http://huangzhiyuan.github.io/categories/Linux/"},{"name":"C++","slug":"C","permalink":"http://huangzhiyuan.github.io/categories/C/"},{"name":"电影","slug":"电影","permalink":"http://huangzhiyuan.github.io/categories/%E7%94%B5%E5%BD%B1/"},{"name":"计算机网络","slug":"计算机网络","permalink":"http://huangzhiyuan.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"python","slug":"python","permalink":"http://huangzhiyuan.github.io/categories/python/"},{"name":"Hexo","slug":"Hexo","permalink":"http://huangzhiyuan.github.io/categories/Hexo/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://huangzhiyuan.github.io/tags/Linux/"},{"name":"pytorch","slug":"pytorch","permalink":"http://huangzhiyuan.github.io/tags/pytorch/"},{"name":"quantization","slug":"quantization","permalink":"http://huangzhiyuan.github.io/tags/quantization/"},{"name":"Memory","slug":"Memory","permalink":"http://huangzhiyuan.github.io/tags/Memory/"},{"name":"Docker","slug":"Docker","permalink":"http://huangzhiyuan.github.io/tags/Docker/"},{"name":"zero-copy","slug":"zero-copy","permalink":"http://huangzhiyuan.github.io/tags/zero-copy/"},{"name":"OpenMP","slug":"OpenMP","permalink":"http://huangzhiyuan.github.io/tags/OpenMP/"},{"name":"加密","slug":"加密","permalink":"http://huangzhiyuan.github.io/tags/%E5%8A%A0%E5%AF%86/"},{"name":"GPU","slug":"GPU","permalink":"http://huangzhiyuan.github.io/tags/GPU/"},{"name":"MLPerf","slug":"MLPerf","permalink":"http://huangzhiyuan.github.io/tags/MLPerf/"},{"name":"mAP","slug":"mAP","permalink":"http://huangzhiyuan.github.io/tags/mAP/"},{"name":"SYCL","slug":"SYCL","permalink":"http://huangzhiyuan.github.io/tags/SYCL/"},{"name":"Vtune","slug":"Vtune","permalink":"http://huangzhiyuan.github.io/tags/Vtune/"},{"name":"atomic","slug":"atomic","permalink":"http://huangzhiyuan.github.io/tags/atomic/"},{"name":"hook","slug":"hook","permalink":"http://huangzhiyuan.github.io/tags/hook/"},{"name":"affinity","slug":"affinity","permalink":"http://huangzhiyuan.github.io/tags/affinity/"},{"name":"hyperthreading","slug":"hyperthreading","permalink":"http://huangzhiyuan.github.io/tags/hyperthreading/"},{"name":"NUMA","slug":"NUMA","permalink":"http://huangzhiyuan.github.io/tags/NUMA/"},{"name":"compiler","slug":"compiler","permalink":"http://huangzhiyuan.github.io/tags/compiler/"},{"name":"license","slug":"license","permalink":"http://huangzhiyuan.github.io/tags/license/"},{"name":"topk","slug":"topk","permalink":"http://huangzhiyuan.github.io/tags/topk/"},{"name":"面试","slug":"面试","permalink":"http://huangzhiyuan.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"读书","slug":"读书","permalink":"http://huangzhiyuan.github.io/tags/%E8%AF%BB%E4%B9%A6/"},{"name":"batch-size","slug":"batch-size","permalink":"http://huangzhiyuan.github.io/tags/batch-size/"},{"name":"新闻","slug":"新闻","permalink":"http://huangzhiyuan.github.io/tags/%E6%96%B0%E9%97%BB/"},{"name":"volatile","slug":"volatile","permalink":"http://huangzhiyuan.github.io/tags/volatile/"},{"name":"tensorboard","slug":"tensorboard","permalink":"http://huangzhiyuan.github.io/tags/tensorboard/"},{"name":"cuda","slug":"cuda","permalink":"http://huangzhiyuan.github.io/tags/cuda/"},{"name":"cs179","slug":"cs179","permalink":"http://huangzhiyuan.github.io/tags/cs179/"},{"name":"DNNL","slug":"DNNL","permalink":"http://huangzhiyuan.github.io/tags/DNNL/"},{"name":"stream/event","slug":"stream-event","permalink":"http://huangzhiyuan.github.io/tags/stream-event/"},{"name":"tensoriterator","slug":"tensoriterator","permalink":"http://huangzhiyuan.github.io/tags/tensoriterator/"},{"name":"深度学习框架","slug":"深度学习框架","permalink":"http://huangzhiyuan.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/"},{"name":"conda/pip","slug":"conda-pip","permalink":"http://huangzhiyuan.github.io/tags/conda-pip/"},{"name":"AI","slug":"AI","permalink":"http://huangzhiyuan.github.io/tags/AI/"},{"name":"大牛","slug":"大牛","permalink":"http://huangzhiyuan.github.io/tags/%E5%A4%A7%E7%89%9B/"},{"name":"深入理解计算机系统","slug":"深入理解计算机系统","permalink":"http://huangzhiyuan.github.io/tags/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"},{"name":"cpu","slug":"cpu","permalink":"http://huangzhiyuan.github.io/tags/cpu/"},{"name":"抢红包","slug":"抢红包","permalink":"http://huangzhiyuan.github.io/tags/%E6%8A%A2%E7%BA%A2%E5%8C%85/"},{"name":"Horovod","slug":"Horovod","permalink":"http://huangzhiyuan.github.io/tags/Horovod/"},{"name":"知识图谱","slug":"知识图谱","permalink":"http://huangzhiyuan.github.io/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"},{"name":"TVM","slug":"TVM","permalink":"http://huangzhiyuan.github.io/tags/TVM/"},{"name":"geohash","slug":"geohash","permalink":"http://huangzhiyuan.github.io/tags/geohash/"},{"name":"git","slug":"git","permalink":"http://huangzhiyuan.github.io/tags/git/"},{"name":"分布式","slug":"分布式","permalink":"http://huangzhiyuan.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"高并发","slug":"高并发","permalink":"http://huangzhiyuan.github.io/tags/%E9%AB%98%E5%B9%B6%E5%8F%91/"},{"name":"hash","slug":"hash","permalink":"http://huangzhiyuan.github.io/tags/hash/"},{"name":"CPU-cache","slug":"CPU-cache","permalink":"http://huangzhiyuan.github.io/tags/CPU-cache/"},{"name":"价值观","slug":"价值观","permalink":"http://huangzhiyuan.github.io/tags/%E4%BB%B7%E5%80%BC%E8%A7%82/"},{"name":"embedding","slug":"embedding","permalink":"http://huangzhiyuan.github.io/tags/embedding/"},{"name":"Code Optimization","slug":"Code-Optimization","permalink":"http://huangzhiyuan.github.io/tags/Code-Optimization/"},{"name":"VIM","slug":"VIM","permalink":"http://huangzhiyuan.github.io/tags/VIM/"},{"name":"AWK","slug":"AWK","permalink":"http://huangzhiyuan.github.io/tags/AWK/"},{"name":"memory","slug":"memory","permalink":"http://huangzhiyuan.github.io/tags/memory/"},{"name":"vscode","slug":"vscode","permalink":"http://huangzhiyuan.github.io/tags/vscode/"},{"name":"剑指offer","slug":"剑指offer","permalink":"http://huangzhiyuan.github.io/tags/%E5%89%91%E6%8C%87offer/"},{"name":"C++","slug":"C","permalink":"http://huangzhiyuan.github.io/tags/C/"},{"name":"健身","slug":"健身","permalink":"http://huangzhiyuan.github.io/tags/%E5%81%A5%E8%BA%AB/"},{"name":"摘抄","slug":"摘抄","permalink":"http://huangzhiyuan.github.io/tags/%E6%91%98%E6%8A%84/"},{"name":"副业","slug":"副业","permalink":"http://huangzhiyuan.github.io/tags/%E5%89%AF%E4%B8%9A/"},{"name":"leetcode","slug":"leetcode","permalink":"http://huangzhiyuan.github.io/tags/leetcode/"},{"name":"ml2014","slug":"ml2014","permalink":"http://huangzhiyuan.github.io/tags/ml2014/"},{"name":"gdb","slug":"gdb","permalink":"http://huangzhiyuan.github.io/tags/gdb/"},{"name":"程序员的自我修养","slug":"程序员的自我修养","permalink":"http://huangzhiyuan.github.io/tags/%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/"},{"name":"DL Framwork","slug":"DL-Framwork","permalink":"http://huangzhiyuan.github.io/tags/DL-Framwork/"},{"name":"生活","slug":"生活","permalink":"http://huangzhiyuan.github.io/tags/%E7%94%9F%E6%B4%BB/"},{"name":"diff","slug":"diff","permalink":"http://huangzhiyuan.github.io/tags/diff/"},{"name":"LSTM","slug":"LSTM","permalink":"http://huangzhiyuan.github.io/tags/LSTM/"},{"name":"CNN","slug":"CNN","permalink":"http://huangzhiyuan.github.io/tags/CNN/"},{"name":"BP算法","slug":"BP算法","permalink":"http://huangzhiyuan.github.io/tags/BP%E7%AE%97%E6%B3%95/"},{"name":"RNN","slug":"RNN","permalink":"http://huangzhiyuan.github.io/tags/RNN/"},{"name":"影评","slug":"影评","permalink":"http://huangzhiyuan.github.io/tags/%E5%BD%B1%E8%AF%84/"},{"name":"随笔","slug":"随笔","permalink":"http://huangzhiyuan.github.io/tags/%E9%9A%8F%E7%AC%94/"},{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://huangzhiyuan.github.io/tags/TCP-IP/"},{"name":"Java 集合框架","slug":"Java-集合框架","permalink":"http://huangzhiyuan.github.io/tags/Java-%E9%9B%86%E5%90%88%E6%A1%86%E6%9E%B6/"},{"name":"Java memory","slug":"Java-memory","permalink":"http://huangzhiyuan.github.io/tags/Java-memory/"},{"name":"ftp","slug":"ftp","permalink":"http://huangzhiyuan.github.io/tags/ftp/"},{"name":"webchat","slug":"webchat","permalink":"http://huangzhiyuan.github.io/tags/webchat/"},{"name":"电影","slug":"电影","permalink":"http://huangzhiyuan.github.io/tags/%E7%94%B5%E5%BD%B1/"},{"name":"markdown","slug":"markdown","permalink":"http://huangzhiyuan.github.io/tags/markdown/"},{"name":"职业","slug":"职业","permalink":"http://huangzhiyuan.github.io/tags/%E8%81%8C%E4%B8%9A/"},{"name":"体育","slug":"体育","permalink":"http://huangzhiyuan.github.io/tags/%E4%BD%93%E8%82%B2/"},{"name":"KNN","slug":"KNN","permalink":"http://huangzhiyuan.github.io/tags/KNN/"},{"name":"hexo","slug":"hexo","permalink":"http://huangzhiyuan.github.io/tags/hexo/"}]}