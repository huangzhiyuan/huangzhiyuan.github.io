神经网络——BP算法

# 一、BP算法的意义

对于初学者来说，了解了一个算法的重要意义，往往会引起他对算法本身的重视。BP(Back Propagation，后向传播)算法，具有非凡的历史意义和重大的现实意义。

<--!more-->

## 1.1、历史意义

　　1969年,作为人工神经网络创始人的明斯基(Marrin M insky)和佩珀特(Seymour Papert)合作出版了《感知器》一书,论证了简单的线性感知器功能有限,不能解决如“异或”(XOR )这样的基本问题,而且对多层网络也持悲观态度。这些论点给神经网络研究以沉重的打击,很多科学家纷纷离开这一领域,神经网络的研究走向长达10年的低潮时期。

　　1974年哈佛大学的Paul　Werbos发明BP算法时，正值神经外网络低潮期，并未受到应有的重视。1983年，加州理工学院的物理学家John Hopfield利用神经网络，在旅行商这个NP完全问题的求解上获得当时最好成绩，引起了轰动[2]。然而,Hopfield的研究成果仍未能指出明斯基等人论点的错误所在,要推动神经网络研究的全面开展必须直接解除对感知器——多层网络算法的疑虑。
真正打破明斯基冰封魔咒的是，David　Rumelhart等学者出版的《平行分布处理:认知的微观结构探索》一书。书中完整地提出了BP算法,系统地解决了多层网络中隐单元连接权的学习问题,并在数学上给出了完整的推导。这是神经网络发展史上的里程碑，BP算法迅速走红，掀起了神经网络的第二次高潮。
　　因此，BP算法的历史意义：明确地否定了明斯基等人的错误观点，对神经网络第二次高潮具有决定性意义。

## 1.2、现实意义
　　这一点是说BP算法在神经网络领域中的地位和意义。

　　***BP算法是迄今最成功的神经网络学习算法，现实任务中使用神经网络时，大多是在使用BP算法进行训练[2],包括最近炙手可热的深度学习概念下的卷积神经网络(CNNs)。***

# 二、什么是BP算法

## 2.1、BP神经网络

　　BP神经网络是这样一种神经网络模型，它是由一个输入层、一个输出层和一个或多个隐层构成，它的激活函数采用sigmoid函数，采用BP算法训练的多层前馈神经网络。

## 2.2、BP算法基本思想

　　BP算法全称叫作误差反向传播(error Back　Propagation，或者也叫作误差逆传播)算法。其算法基本思想为：在2.1所述的前馈网络中，输入信号经输入层输入，通过隐层计算由输出层输出，输出值与标记值比较，若有误差，将误差反向由输出层向输入层传播，在这个过程中，利用梯度下降算法对神经元权值进行调整。

## 2.3 BP算法介绍

　　要想搞懂BP算法，需要先直观理解多层神经网络的训练。
　　机器学习可以看做是数理统计的一个应用，在数理统计中一个常见的任务就是拟合，也就是给定一些样本点，用合适的曲线揭示这些样本点随着自变量的变化关系。深度学习同样也是为了这个目的，只不过此时，样本点不再限定为(x, y)点对，而可以是由向量、矩阵等等组成的广义点对(X,Y)。而此时，(X,Y)之间的关系也变得十分复杂，不太可能用一个简单函数表示。然而，人们发现可以用多层神经网络来表示这样的关系，而多层神经网络的本质就是一个多层复合的函数。借用网上找到的一幅图[1]，来直观描绘一下这种复合关系。


---pic 11
其对应的表达式如下：
---pic 12

　　上面式中的Wij就是相邻两层神经元之间的权值，它们就是深度学习需要学习的参数，也就相当于直线拟合y=k*x+b中的待求参数k和b。
　　和直线拟合一样，深度学习的训练也有一个目标函数，这个目标函数定义了什么样的参数才算一组“好参数”，不过在机器学习中，一般是采用成本函数（cost function），然后，训练目标就是通过调整每一个权值Wij来使得cost达到最小。cost函数也可以看成是由所有待求权值Wij为自变量的复合函数，而且基本上是非凸的，即含有许多局部最小值。但实际中发现，采用我们常用的梯度下降法就可以有效的求解最小化cost函数的问题。
　　梯度下降法需要给定一个初始点，并求出该点的梯度向量，然后以负梯度方向为搜索方向，以一定的步长进行搜索，从而确定下一个迭代点，再计算该新的梯度方向，如此重复直到cost收敛。那么如何计算梯度呢？

---pic 13-14

在图中，引入了中间变量c,d。

　　为了求出a=2, b=1时，e的梯度，我们可以先利用偏导数的定义求出不同层之间相邻节点的偏导关系，如下图所示。

---pic 15-16

　　大家也许已经注意到，这样做是十分冗余的，因为很多路径被重复访问了。比如上图中，a-c-e和b-c-e就都走了路径c-e。对于权值动则数万的深度模型中的神经网络，这样的冗余所导致的计算量是相当大的。同样是利用链式法则，BP算法则机智地避开了这种冗余，它对于每　　一个路径只访问一次就能求顶点对所有下层节点的偏导值。
　　正如反向传播(BP)算法的名字说的那样，BP算法是反向(自上往下)来寻找路径的。
从最上层的节点e开始，初始值为1，以层为单位进行处理。对于e的下一层的所有子节点，将1乘以e到某个节点路径上的偏导值，并将结果“堆放”在该子节点中。等e所在的层按照这样传播完毕后，第二层的每一个节点都“堆放"些值，然后我们针对每个节点，把它里面所有“堆放”的值求和，就得到了顶点e对该节点的偏导。然后将这些第二层的节点各自作为起始顶点，初始值设为顶点e对它们的偏导值，以"层"为单位重复上述传播过程，即可求出顶点e对每一层节点的偏导数。

　　以上图为例，节点c接受e发送的1*2并堆放起来，节点d接受e发送的1*3并堆放起来，至此第二层完毕，求出各节点总堆放量并继续向下一层发送。节点c向a发送2*1并对堆放起来，节点c向b发送2*1并堆放起来，节点d向b发送3*1并堆放起来，至此第三层完毕，节点a堆放起来的量为2，节点b堆放起来的量为2*1+3*1=5, 即顶点e对b的偏导数为5.
　　举个不太恰当的例子，如果把上图中的箭头表示欠钱的关系，即c→e表示e欠c的钱。以a, b为例，直接计算e对它们俩的偏导相当于a, 　　b各自去讨薪。a向c讨薪，c说e欠我钱，你向他要。于是a又跨过c去找e。b先向c讨薪，同样又转向e，b又向d讨薪，再次转向e。可以看到，追款之路，充满艰辛，而且还有重复，即a, b 都从c转向e。
　　而BP算法就是主动还款。e把所欠之钱还给c，d。c，d收到钱，乐呵地把钱转发给了a，b，皆大欢喜。


## 2.4、BP算法数学工具

BP算法中核心的数学工具就是微积分的链式求导法则。

----pic 1

## 2.5、BP算法的推导

----pic 2-10

# 三、BP算法的缺点

## 3.1、局部极小值问题

BP算法的缺点，首当其冲就是局部极小值问题。

## 3.2、算法训练非常慢

BP算法本质上是梯度下降，而它所要优化的目标函数又非常复杂，这使得BP算法效率低下。

四、参考

[1]、《BP算法的哲学思考》，成素梅、郝中华著

[2]、《机器学习》，周志华著

[3]、[Deep Learning论文笔记之（四）CNN卷积神经网络推导和实现][1]

[4]、[知乎：如何直观地解释 back propagation 算法？][4]


另一种较为直观的介绍，参见知乎：[Deep learning][2]


[**原文链接**][3]

  
 

  [1]: http://blog.csdn.net/zouxy09/article/details/9993371
  [2]: https://www.zhihu.com/question/27239198
  [3]: http://www.jianshu.com/p/c5cda5a52ee4
  [4]: https://www.zhihu.com/question/27239198