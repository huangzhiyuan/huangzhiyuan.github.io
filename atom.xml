<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>靡不有初 鲜克有终</title>
  
  
  <link href="http://huangzhiyuan.github.io/atom.xml" rel="self"/>
  
  <link href="http://huangzhiyuan.github.io/"/>
  <updated>2021-05-25T14:10:12.451Z</updated>
  <id>http://huangzhiyuan.github.io/</id>
  
  <author>
    <name>黄志远</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Linux性能优化实战之IO性能篇</title>
    <link href="http://huangzhiyuan.github.io/2021/05/25/Linux-perf-optimize-actual-combat-IO/"/>
    <id>http://huangzhiyuan.github.io/2021/05/25/Linux-perf-optimize-actual-combat-IO/</id>
    <published>2021-05-24T16:53:03.000Z</published>
    <updated>2021-05-25T14:10:12.451Z</updated>
    
    <content type="html"><![CDATA[<p>　　实际上，性能优化一直都是大多数软件工程师头上的紧箍咒，甚至许多工作多年的资深工程师，有时也无法准确地分析出线上的很多性能问题。<br>　　性能分析问题为什么这么难呢？性能优化是个系统工具，常常牵一发而动全身。它涉及了从程序设计、算法分析、编程语言，再到系统、存储、网络等各种底层基础设施的方方面面。每一个组件都可能出现问题，甚至多个组件同时出现问题。毫无疑问，性能优化是软件系统中最有挑战的工作指引，也是最考验体现你综合能力的工作之一。该篇blog记录总结了倪朋飞老师在极客时间开设的[《Linux性能优化实战》][1]课程笔记。</p><span id="more"></span>]]></content>
    
    
    <summary type="html">&lt;p&gt;　　实际上，性能优化一直都是大多数软件工程师头上的紧箍咒，甚至许多工作多年的资深工程师，有时也无法准确地分析出线上的很多性能问题。&lt;br&gt;　　性能分析问题为什么这么难呢？性能优化是个系统工具，常常牵一发而动全身。它涉及了从程序设计、算法分析、编程语言，再到系统、存储、网络等各种底层基础设施的方方面面。每一个组件都可能出现问题，甚至多个组件同时出现问题。毫无疑问，性能优化是软件系统中最有挑战的工作指引，也是最考验体现你综合能力的工作之一。该篇blog记录总结了倪朋飞老师在极客时间开设的[《Linux性能优化实战》][1]课程笔记。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Linux" scheme="http://huangzhiyuan.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Linux性能优化实战之CPU性能篇</title>
    <link href="http://huangzhiyuan.github.io/2021/05/25/Linux-perf-optimize-actual-combat-CPU/"/>
    <id>http://huangzhiyuan.github.io/2021/05/25/Linux-perf-optimize-actual-combat-CPU/</id>
    <published>2021-05-24T16:53:03.000Z</published>
    <updated>2021-05-28T05:26:13.649Z</updated>
    
    <content type="html"><![CDATA[<p>　　实际上，性能优化一直都是大多数软件工程师头上的紧箍咒，甚至许多工作多年的资深工程师，有时也无法准确地分析出线上的很多性能问题。<br>　　性能分析问题为什么这么难呢？性能优化是个系统工具，常常牵一发而动全身。它涉及了从程序设计、算法分析、编程语言，再到系统、存储、网络等各种底层基础设施的方方面面。每一个组件都可能出现问题，甚至多个组件同时出现问题。毫无疑问，性能优化是软件系统中最有挑战的工作指引，也是最考验体现你综合能力的工作之一。该篇blog记录总结了倪朋飞老师在极客时间开设的<a href="https://time.geekbang.org/column/article/68728">《Linux性能优化实战》</a>课程笔记。</p><span id="more"></span><h2 id="如何理解“平均负载”"><a href="#如何理解“平均负载”" class="headerlink" title="如何理解“平均负载”"></a>如何理解“平均负载”</h2><p>　　我们常用top或者uptime命令，来了解系统的负载情况。比如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zhiyuanhuang@ZhiyuandeMacBook-Pro blog % uptime</span><br><span class="line">22:06  up  2:40, 2 users, load averages: 2.05 2.39 2.51</span><br></pre></td></tr></table></figure><p>　　分别表示当前时间、系统运行时间、正在登陆用户数。最后三个数字则分别表示过去1分钟、5分钟、15分钟的平均负载。但是平均负载究竟代表了什么？简单来说，平均负载是指单位时间内，系统处于<strong>可运行状态</strong>和<strong>不可中断转态</strong>的平均进程数，也就是<strong>平均活跃进程数</strong>，它和CPU使用率并没有直接关系。所谓可运行转态的进程，是指正在使用CPU或者正在等待CPU的进程，也即是我们用ps命令看到的R状态（running或Runnable）的的进程。不可中断的进程是指正处于内核状态关键流程中的进程，并且这些进程是不可打断的。比如最常见的是等待硬件设备的IO响应，也就是我们在ps命令中看到的D状态（Uninterruptible Sleep，也称为Disk Sleep）的进程。<br>　　比如，当一个进程向磁盘读写数据时，为了保证数据的一致性，在得到磁盘回复前，它是不能被其他进程或者中断打断的，这个时候的进程就处于不可中断的转态。否则会出现磁盘数据与进程数据不一致的情况。所以，不可中断状态实际上是系统对进程和硬件设备的一种保护机制。<br>　　既然平均的是活跃进程数，那么最理想的情况就是每个CPU上面都刚好运行着一个进程，这个利用率最高。比如平均负载为2时，</p><ul><li>在只有2个CPU的系统上，所有的CPU都刚好被完全占用。</li><li>在4个CPU的系统上，意味着有50%的空闲。</li><li>在只有1个CPU的系统中，有一半的进程竞争不到CPU。</li></ul><h3 id="平均负载多少最合适"><a href="#平均负载多少最合适" class="headerlink" title="平均负载多少最合适"></a>平均负载多少最合适</h3><p>　　要评判平均负载，首先你要知道系统有几个CPU，这个可以通过top命令或者从文件/proc/cpuinfo获取，比如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ grep &quot;model name&quot; /proc/cpuinfo | wc -l</span><br></pre></td></tr></table></figure><p>　　从例子中可以看出，平均负载有三个数值，到底该参考哪一个呢？实际上都要看，三个不同时间间隔的平均值，其实给我们提供了，分析系统负载趋势的数据来源，让我们更加全面更立体了解目前的负载状况。一般来说，当平均负载高于CPU数量70%的时候就应该分析排查负载高的问题。一旦负载过高，就可能导致进程响应变慢，进而影响服务的正常功能。</p><h3 id="平均负载与CPU使用率"><a href="#平均负载与CPU使用率" class="headerlink" title="平均负载与CPU使用率"></a>平均负载与CPU使用率</h3><p>　　现实中，经常会把平均负载和CPU使用率混淆。可能会困惑，既然平均负载代表的是活跃进程数，那平均负载高了，不就意味着CPU使用率高吗？回到平均负载的含义上来，平均负载是指在单位时间内，处于可运行状态和不可中断状态的进程数。所以，它不仅包括了正在使用的CPU的进程，还包括等待CPU和等待IO的进程。而CPU使用率，是单位时间内繁忙情况的统计，和平均负载并不一定完全对应。比如</p><ul><li>CPU密集型进程，使用大量CPU会导致平均进程负载升高，此时两者是一致的；</li><li>IO密集型进程，等待IO也会导致平均负载升高，但CPU使用率不一定很高；</li><li>大量等待CPU的进程调度也会导致平均负载升高，此时的CPU使用率也会比较高。</li></ul><h3 id="案例分析"><a href="#案例分析" class="headerlink" title="案例分析"></a>案例分析</h3><p>　　工具iostat、mpstat、pidstat。其中stress是一个Linux系统压力的测试工具，这里用作异常进程模拟平均负载升高的场景。而sysstat包括了常用的Linux性能工具，用来监控和分析系统的性能。mpstat是一个常用的多核CPU性能分析工具，用来实时查看每个CPU的性能指标，以及所有CPU的平均指标。pidstat是一个常用的进程性能分析工具，用来实时查看进程的CPU、内存、IO以及上下文切换等性能指标。每种场景都需要打开3个中端。</p><p><strong>场景一： CPU密集型场景</strong><br>终端1运行stress命令，模拟一个CPU使用率100%的场景：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ stress --cpu 1 --timeout 600</span><br></pre></td></tr></table></figure><p>终端2运行uptime查看平均负载的变化情况：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># -d 参数表示高亮显示变化的区域</span><br><span class="line">$ watch -d uptime</span><br><span class="line">..., load average: 1.00, 0.75, 0.39</span><br></pre></td></tr></table></figure><p>终端3运行mpstat查看CPU使用率变化：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># -P ALL 表示监控所有CPU，后面数字5表示间隔5秒后输出一组数据</span><br><span class="line">$ mpstat -P ALL 5</span><br><span class="line">Linux 4.15.0 (ubuntu) 09/22/18 _x86_64_ (2 CPU)</span><br><span class="line">13:30:06     CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle</span><br><span class="line">13:30:11     all   50.05    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   49.95</span><br><span class="line">13:30:11       0    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00  100.00</span><br><span class="line">13:30:11       1  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00</span><br></pre></td></tr></table></figure><p>从终端二中可以看到，1 分钟的平均负载会慢慢增加到 1.00，而从终端三中还可以看到，正好有一个 CPU 的使用率为 100%，但它的 iowait 只有 0。这说明，平均负载的升高正是由于 CPU 使用率为 100%。到底是哪个进程导致了 CPU 使用率为 100% 呢？你可以使用 pidstat 来查询：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 间隔5秒后输出一组数据</span><br><span class="line">$ pidstat -u 5 1</span><br><span class="line">13:37:07      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command</span><br><span class="line">13:37:12        0      2962  100.00    0.00    0.00    0.00  100.00     1  stress</span><br></pre></td></tr></table></figure><p>从这里可以明显看到，stress 进程的 CPU 使用率为 100%。</p><p><strong>场景二： IO密集型场景</strong><br>首先还是运行 stress 命令，但这次模拟 I/O 压力，即不停地执行 sync：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ stress -i 1 --timeout 600</span><br></pre></td></tr></table></figure><p>在第二个终端运行 uptime 查看平均负载的变化情况：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ watch -d uptime</span><br><span class="line">...,  load average: 1.06, 0.58, 0.37</span><br></pre></td></tr></table></figure><p>第三个终端运行 mpstat 查看 CPU 使用率的变化情况：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 显示所有CPU的指标，并在间隔5秒输出一组数据</span><br><span class="line">$ mpstat -P ALL 5 1</span><br><span class="line">Linux 4.15.0 (ubuntu)     09/22/18     _x86_64_    (2 CPU)</span><br><span class="line">13:41:28     CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle</span><br><span class="line">13:41:33     all    0.21    0.00   12.07   32.67    0.00    0.21    0.00    0.00    0.00   54.84</span><br><span class="line">13:41:33       0    0.43    0.00   23.87   67.53    0.00    0.43    0.00    0.00    0.00    7.74</span><br><span class="line">13:41:33       1    0.00    0.00    0.81    0.20    0.00    0.00    0.00    0.00    0.00   98.99</span><br></pre></td></tr></table></figure><p>从这里可以看到，1 分钟的平均负载会慢慢增加到 1.06，其中一个 CPU 的系统 CPU 使用率升高到了 23.87，而 iowait 高达 67.53%。这说明，平均负载的升高是由于 iowait 的升高。那么到底是哪个进程，导致 iowait 这么高呢？我们还是用 pidstat 来查询：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 间隔5秒后输出一组数据，-u表示CPU指标</span><br><span class="line">$ pidstat -u 5 1</span><br><span class="line">Linux 4.15.0 (ubuntu)     09/22/18     _x86_64_    (2 CPU)</span><br><span class="line">13:42:08      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command</span><br><span class="line">13:42:13        0       104    0.00    3.39    0.00    0.00    3.39     1  kworker/1:1H</span><br><span class="line">13:42:13        0       109    0.00    0.40    0.00    0.00    0.40     0  kworker/0:1H</span><br><span class="line">13:42:13        0      2997    2.00   35.53    0.00    3.99   37.52     1  stress</span><br><span class="line">13:42:13        0      3057    0.00    0.40    0.00    0.00    0.40     0  pidstat</span><br></pre></td></tr></table></figure><p>可以发现，还是 stress 进程导致的。</p><p><strong>场景三： 大量进程的场景</strong><br>当系统中运行进程超出 CPU 运行能力时，就会出现等待 CPU 的进程。我们还是使用 stress，但这次模拟的是 8 个进程：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ stress -c 8 --timeout 600</span><br></pre></td></tr></table></figure><p>由于系统只有 2 个 CPU，明显比 8 个进程要少得多，因而，系统的 CPU 处于严重过载状态，平均负载高达 7.97：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ uptime</span><br><span class="line">...,  load average: 7.97, 5.93, 3.02</span><br></pre></td></tr></table></figure><p>接着再运行 pidstat 来看一下进程的情况：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 间隔5秒后输出一组数据</span><br><span class="line">$ pidstat -u 5 1</span><br><span class="line">14:23:25      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command</span><br><span class="line">14:23:30        0      3190   25.00    0.00    0.00   74.80   25.00     0  stress</span><br><span class="line">14:23:30        0      3191   25.00    0.00    0.00   75.20   25.00     0  stress</span><br><span class="line">14:23:30        0      3192   25.00    0.00    0.00   74.80   25.00     1  stress</span><br><span class="line">14:23:30        0      3193   25.00    0.00    0.00   75.00   25.00     1  stress</span><br><span class="line">14:23:30        0      3194   24.80    0.00    0.00   74.60   24.80     0  stress</span><br><span class="line">14:23:30        0      3195   24.80    0.00    0.00   75.00   24.80     0  stress</span><br><span class="line">14:23:30        0      3196   24.80    0.00    0.00   74.60   24.80     1  stress</span><br><span class="line">14:23:30        0      3197   24.80    0.00    0.00   74.80   24.80     1  stress</span><br><span class="line">14:23:30        0      3200    0.00    0.20    0.00    0.20    0.20     0  pidstat</span><br></pre></td></tr></table></figure><p>可以看出，8 个进程在争抢 2 个 CPU，每个进程等待 CPU 的时间（也就是代码块中的 %wait 列）高达 75%。这些超出 CPU 计算能力的进程，最终导致 CPU 过载。</p><h2 id="经常说的CPU上下文切换是什么意思"><a href="#经常说的CPU上下文切换是什么意思" class="headerlink" title="经常说的CPU上下文切换是什么意思"></a>经常说的CPU上下文切换是什么意思</h2><p>　　进程在竞争CPU的时候并没有真正运行，为什么还会导致系统的负载升高？原因就是CPU上下文切换。<br>　　Linux是一个多任务操作系统，它支持远大于CPU数量的任务同时运行。这些任务实际上并不是真的在同时运行，而是因为在很短的时间内，将CPU轮流分配给它们，造成多任务同时运行的错觉。在每个任务开启钱，CPU都需要知道任务从哪里加载，从哪里开始运行。即需要系统事先帮助设置好CPU寄存器和程序计数器。<br>　　CPU寄存器，是CPU内资的容量小、但速度极快的内存。而程序计数器是用来存储CPU正在执行的指令位置、或者即将执行的下一条指令位置。它们都是CPU在运行任何任务钱，必须来来的环境，因此被叫做<strong>CPU上下文</strong>。</p><p><img src="/img/2021/0526/cpu_context.png" alt="&quot;cpu context&quot;"></p><p>　　上下文切换就是把前一个任务的CPU上下文（CPU寄存器和程序计数器）保存起来，然后然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务。而这些保存下来的上下文，会存储在系统内核中，并在任务重新调度执行时再次加载进来。这样就能保证任务原来的状态不受影响，让任务看起来还是连续运行。</p><p>　　有人说，CPU 上下文切换无非就是更新了 CPU 寄存器的值嘛，但这些寄存器，本身就是为了快速运行任务而设计的，为什么会影响系统的 CPU 性能呢？<br>根据任务的不同，CPU 的上下文切换就可以分为几个不同的场景，也就是<strong>进程上下文切换</strong>、<strong>线程上下文切换</strong>以及<strong>中断上下文切换</strong>。</p><h3 id="进程上下文切换"><a href="#进程上下文切换" class="headerlink" title="进程上下文切换"></a>进程上下文切换</h3><p>　　Linux 按照特权等级，把进程的运行空间分为内核空间和用户空间，分别对应着下图中， CPU 特权等级的 Ring 0 和 Ring 3。</p><blockquote><p>内核空间（Ring 0）具有最高权限，可以直接访问所有资源；<br>用户空间（Ring 3）只能访问受限资源，不能直接访问内存等硬件设备，必须通过系统调用陷入到内核中，才能访问这些特权资源。</p></blockquote><p><img src="/img/2021/0526/ring.png" alt="&quot;ring kernel&quot;"></p><p>　　换个角度看，也就是说，进程既可以在用户空间运行，又可以在内核空间中运行。进程在用户空间运行时，被称为进程的用户态，而陷入内核空间的时候，被称为进程的内核态。从用户态到内核态的转变，需要通过系统调用来完成。比如，当我们查看文件内容时，就需要多次系统调用来完成：首先调用 open() 打开文件，然后调用 read() 读取文件内容，并调用 write() 将内容写到标准输出，最后再调用 close() 关闭文件。<br>　　系统调用的过程有没有发生 CPU 上下文的切换呢？答案自然是肯定的。CPU寄存器里原来用户态的指令位置，需要先保存起来。接着，为了执行内核态代码，CPU寄存器需要更新为内核态指令的新位置。最后才是跳转到内核态运行内核任务。而系统调用结束后，CPU 寄存器需要恢复原来保存的用户态，然后再切换到用户空间，继续运行进程。所以，一次系统调用的过程，其实是发生了两次 CPU 上下文切换。<br>　　需要注意的是，系统调动过程中，并不会设计到虚拟内存等进程用户态的资源，也不会切换进程。</p><blockquote><p>进程上下文切换，是指从一个进程切换到另一个进程。<br>而系统调用过程中一直是同一个进程在运行。</p></blockquote><p>　　所以，<strong>系统调用过程通常称为特权模式切换，而不是上下文切换</strong>。实际上，系统调用过程中，CPU的上下文切换还是无法避免的。那么，进程上下文切换和系统调用有什么区别呢？<br>　　首先，进程是由内核来管理和调度，进程的切换只能发生在内核态。所以进程的上下文切换不仅包括了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的状态。因此，进程的上下文切换就比系统调用时多了一步：在保存当前进程的内核状态和 CPU 寄存器之前，需要先把该进程的虚拟内存、栈等保存下来；而加载了下一进程的内核态后，还需要刷新进程的虚拟内存和用户栈。<br>如下图所示，保存上下文和恢复上下文的过程并不是“免费”的，需要内核在 CPU 上运行才能完成。</p><p><img src="/img/2021/0526/context_change.png" alt="&quot;context change&quot;"></p><p>　　根据 Tsuna 的测试报告，每次上下文切换都需要几十纳秒到数微秒的 CPU 时间。这个时间还是相当可观的，特别是在进程上下文切换次数较多的情况下，很容易导致 CPU 将大量时间耗费在寄存器、内核栈以及虚拟内存等资源的保存和恢复上，进而大大缩短了真正运行进程的时间。这也正是上一节中我们所讲的，导致平均负载升高的一个重要因素。</p><p>另外，我们知道， Linux 通过 TLB（Translation Lookaside Buffer）来管理虚拟内存到物理内存的映射关系。当虚拟内存更新后，TLB 也需要刷新，内存的访问也会随之变慢。特别是在多处理器系统上，缓存是被多个处理器共享的，刷新缓存不仅会影响当前处理器的进程，还会影响共享缓存的其他处理器的进程。</p><p>显然，进程切换时才需要切换上下文，换句话说，只有在进程调度的时候，才需要切换上下文。Linux 为每个 CPU 都维护了一个就绪队列，将活跃进程（即正在运行和正在等待 CPU 的进程）按照优先级和等待 CPU 的时间排序，然后选择最需要 CPU 的进程，也就是优先级最高和等待 CPU 时间最长的进程来运行。那么，进程在什么时候才会被调度到 CPU 上运行呢？<br>最容易想到的一个时机，就是进程执行完终止了，它之前使用的 CPU 会释放出来，这个时候再从就绪队列里，拿一个新的进程过来运行。其实还有很多其他场景，也会触发进程调度，在这里我给你逐个梳理下。</p><p>其一，为了保证所有进程可以得到公平调度，CPU 时间被划分为一段段的时间片，这些时间片再被轮流分配给各个进程。这样，当某个进程的时间片耗尽了，就会被系统挂起，切换到其它正在等待 CPU 的进程运行。<br>其二，进程在系统资源不足（比如内存不足）时，要等到资源满足后才可以运行，这个时候进程也会被挂起，并由系统调度其他进程运行。<br>其三，当进程通过睡眠函数 sleep 这样的方法将自己主动挂起时，自然也会重新调度。<br>其四，当有优先级更高的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行。<br>最后一个，发生硬件中断时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序。</p><p>上下文切换引起的性能问题的幕后元凶往往就是以上几个场景。</p><h3 id="线程上下文切换"><a href="#线程上下文切换" class="headerlink" title="线程上下文切换"></a>线程上下文切换</h3><p>线程与进程最大的区别在于，线程是调度的基本单位，而进程则是资源拥有的基本单位。说白了，所谓内核中的任务调度，实际上的调度对象是线程；而进程只是给线程提供了虚拟内存、全局变量等资源。所以，对于线程和进程，我们可以这么理解：</p><ul><li>当进程只有一个线程时，可以认为进程就等于线程。</li><li>当进程拥有多个线程时，这些线程会共享相同的虚拟内存和全局变量等资源。这些资源在上下文切换时是不需要修改的。</li><li>另外，线程也有自己的私有数据，比如栈和寄存器等，这些在上下文切换时也是需要保存的。</li></ul><p>这么一来，线程的上下文切换其实就可以分为两种情况：<br>第一种， 前后两个线程属于不同进程。此时，因为资源不共享，所以切换过程就跟进程上下文切换是一样。<br>第二种，前后两个线程属于同一个进程。此时，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据。<br>到这里你应该也发现了，虽然同为上下文切换，但同进程内的线程切换，要比多进程间的切换消耗更少的资源，而这，也正是多线程代替多进程的一个优势。</p><h3 id="中断上下文切换"><a href="#中断上下文切换" class="headerlink" title="中断上下文切换"></a>中断上下文切换</h3><p>为了快速响应硬件的事件，中断处理会打断进程的正常调度和执行，转而调用中断处理程序，响应设备事件。而在打断其他进程时，就需要将进程当前的状态保存下来，这样在中断结束后，进程仍然可以从原来的状态恢复运行。</p><p>跟进程上下文不同，中断上下文切换并不涉及到进程的用户态。所以，即便中断过程打断了一个正处在用户态的进程，也不需要保存和恢复这个进程的虚拟内存、全局变量等用户态资源。中断上下文，其实只包括内核态中断服务程序执行所必需的状态，包括 CPU 寄存器、内核堆栈、硬件中断参数等。<br>对同一个 CPU 来说，中断处理比进程拥有更高的优先级，所以中断上下文切换并不会与进程上下文切换同时发生。同样道理，由于中断会打断正常进程的调度和执行，所以大部分中断处理程序都短小精悍，以便尽可能快的执行结束。另外，跟进程上下文切换一样，中断上下文切换也需要消耗 CPU，切换次数过多也会耗费大量的 CPU，甚至严重降低系统的整体性能。</p><h3 id="vmstat"><a href="#vmstat" class="headerlink" title="vmstat"></a>vmstat</h3><p>vmstat 是一个常用的系统性能分析工具，主要用来分析系统的内存使用情况，也常用来分析 CPU 上下文切换和中断的次数。用法如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 每隔5秒输出1组数据</span><br><span class="line">$ vmstat 5</span><br><span class="line">procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----</span><br><span class="line"> r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st</span><br><span class="line"> 0  0      0 7005360  91564 818900    0    0     0     0   25   33  0  0 100  0  0</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>需要重点关注下面四列内容：</p><blockquote><p>**cs (context switch)**是每秒上下文切换的次数<br><strong>in (interrupt)</strong> 则是每秒中断的次数<br>**r (Running or Runnable)**是就绪队列的长度，也就是正在运行和等待CPU的进程数。<br>**b (Blocked)**则是处于不可中断睡眠状态的进程数。</p></blockquote><p>vmstat 只给出了系统总体的上下文切换情况，要想查看每个进程的详细情况，就需要使用我们前面提到过的 pidstat 了。给它加上 -w 选项，你就可以查看每个进程上下文切换的情况了。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 每隔5秒输出1组数据</span><br><span class="line">$ pidstat -w 5</span><br><span class="line">Linux 4.15.0 (ubuntu)  09/23/18  _x86_64_  (2 CPU)</span><br><span class="line"></span><br><span class="line">08:18:26      UID       PID   cswch/s nvcswch/s  Command</span><br><span class="line">08:18:31        0         1      0.20      0.00  systemd</span><br><span class="line">08:18:31        0         8      5.40      0.00  rcu_sched</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>这个结果中有两列内容是我们的重点关注对象。一个是 cswch ，表示每秒自愿上下文切换（voluntary context switches）的次数，另一个则是 nvcswch ，表示每秒非自愿上下文切换（non voluntary context switches）的次数。</p><p>这两个概念你一定要牢牢记住，因为它们意味着不同的性能问题：<br>所谓自愿上下文切换，是指进程无法获取所需资源，导致的上下文切换。比如说， I/O、内存等系统资源不足时，就会发生自愿上下文切换。<br>而非自愿上下文切换，则是指进程由于时间片已到等原因，被系统强制调度，进而发生的上下文切换。比如说，大量进程都在争抢 CPU 时，就容易发生非自愿上下文切换。</p><h2 id="CPU哪性能优化的几个思路"><a href="#CPU哪性能优化的几个思路" class="headerlink" title="CPU哪性能优化的几个思路"></a>CPU哪性能优化的几个思路</h2><p>　　CPU 的性能指标那么多，CPU性能分析工具也是一抓一大把，如果离开专栏，换成实际的工作场景，我又该观察什么指标、选择哪个性能工具呢？我们先来回顾下，描述 CPU 的性能指标都有哪些。<br>　　首先，最容易想到的应该是 <strong>CPU 使用率</strong>，这也是实际环境中最常见的一个性能指标。CPU 使用率描述了非空闲时间占总 CPU 时间的百分比，根据 CPU 上运行任务的不同，又被分为用户 CPU、系统 CPU、等待 I/O CPU、软中断和硬中断等。</p><blockquote><p>用户 CPU 使用率，包括用户态 CPU 使用率（user）和低优先级用户态 CPU 使用率（nice），表示 CPU 在用户态运行的时间百分比。用户 CPU 使用率高，通常说明有应用程序比较繁忙。<br>系统 CPU 使用率，表示 CPU 在内核态运行的时间百分比（不包括中断）。系统 CPU 使用率高，说明内核比较繁忙。<br>等待 I/O 的 CPU 使用率，通常也称为 iowait，表示等待 I/O 的时间百分比。iowait 高，通常说明系统与硬件设备的 I/O 交互时间比较长。<br>软中断和硬中断的CPU使用率，分别表示内核调用软中断处理程序、硬中断处理程序的时间百分比。它们的使用率高，通常说明系统发生了大量的中断。<br>除了上面这些，还有在虚拟化环境中会用到的窃取CPU使用率（steal）和客户CPU使用率（guest），分别表示被其他虚拟机占用的 CPU 时间百分比，和运行客户虚拟机的 CPU 时间百分比。</p></blockquote><p>　　第二个比较容易想到的，应该是平均负载（Load Average），也就是系统的平均活跃进程数。它反应了系统的整体负载情况，主要包括三个数值，分别指过去 1 分钟、过去 5 分钟和过去15分钟的平均负载。理想情况下，平均负载等于逻辑CPU个数，这表示每个CPU都恰好被充分利用。如果平均负载大于逻辑 CPU 个数，就表示负载比较重了。</p><p>　　第三个，也是在专栏学习前你估计不太会注意到的，进程上下文切换，包括：</p><blockquote><p>无法获取资源而导致的自愿上下文切换；<br>被系统强制调度导致的非自愿上下文切换。</p></blockquote><p>　　上下文切换，本身是保证 Linux 正常运行的一项核心功能。但过多的上下文切换，会将原本运行进程的CPU时间，消耗在寄存器、内核栈以及虚拟内存等数据的保存和恢复上，缩短进程真正运行的时间，成为性能瓶颈。</p><p>　　除了上面几种，还有一个指标，CPU 缓存的命中率。由于 CPU 发展的速度远快于内存的发展CPU的处理速度就比内存的访问速度快得多。这样，CPU在访问内存的时候，免不了要等待内存的响应。为了协调这两者巨大的性能差距，CPU 缓存（通常是多级缓存）就出现了。</p><p><img src="/img/2021/0526/cache.png" alt="&quot;cpu cache&quot;"></p><p>就像上面这张图显示的，CPU缓存的速度介于CPU和内存之间，缓存的是热点的内存数据。根据不断增长的热点数据，这些缓存按照大小不同分为 L1、L2、L3 等三级缓存，其中 L1 和 L2 常用在单核中，L3则用在多核中。从L1到L3，三级缓存的大小依次增大，相应的，性能依次降低（当然比内存还是好得多）。而它们的命中率，衡量的是 CPU 缓存的复用情况，命中率越高，则表示性能越好。</p><p>　　现将CPU常用性能指标总结如下：</p><p><img src="/img/2021/0526/cpu_perf_base.png" alt="&quot;cpu perf&quot;"></p><p>活学活用，把性能指标和性能工具联系起来第一个维度，从CPU的性能指标出发。也就是说，当你要查看某个性能指标时，要清楚知道哪些工具可以做到。</p><p><img src="/img/2021/0526/perf_tool.png" alt="&quot;perf tool&quot;"></p><p><img src="/img/2021/0526/tool_perf.png" alt="&quot;tool perf&quot;"></p><p>在实际生产环境中，我们通常都希望尽可能快地定位系统的瓶颈，然后尽可能快地优化性能，也就是要又快又准地解决性能问题。所以，为了缩小排查范围，我通常会先运行几个支持指标较多的工具，如 top、vmstat 和 pidstat 。为什么是这三个工具呢？仔细看看下面这张图，你就清楚了。</p><p><img src="/img/2021/0526/tool.png" alt="&quot;tool&quot;"></p><p>这张图里，列出了<strong>top、vmstat和pidstat</strong>分别提供的重要的CPU指标，并用虚线表示关联关系，对应出了性能分析下一步的方向。<br>通过这张图你可以发现，这三个命令，几乎包含了所有重要的 CPU 性能指标，比如：</p><ul><li>从 top 的输出可以得到各种 CPU 使用率以及僵尸进程和平均负载等信息。</li><li>从 vmstat 的输出可以得到上下文切换次数、中断次数、运行状态和不可中断状态的进程数。</li><li>从 pidstat 的输出可以得到进程的用户 CPU 使用率、系统 CPU 使用率、以及自愿上下文切换和非自愿上下文切换情况。</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;　　实际上，性能优化一直都是大多数软件工程师头上的紧箍咒，甚至许多工作多年的资深工程师，有时也无法准确地分析出线上的很多性能问题。&lt;br&gt;　　性能分析问题为什么这么难呢？性能优化是个系统工具，常常牵一发而动全身。它涉及了从程序设计、算法分析、编程语言，再到系统、存储、网络等各种底层基础设施的方方面面。每一个组件都可能出现问题，甚至多个组件同时出现问题。毫无疑问，性能优化是软件系统中最有挑战的工作指引，也是最考验体现你综合能力的工作之一。该篇blog记录总结了倪朋飞老师在极客时间开设的&lt;a href=&quot;https://time.geekbang.org/column/article/68728&quot;&gt;《Linux性能优化实战》&lt;/a&gt;课程笔记。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Linux" scheme="http://huangzhiyuan.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Linux性能优化实战之网络性能篇</title>
    <link href="http://huangzhiyuan.github.io/2021/05/25/Linux-perf-optimize-actual-combat-Network/"/>
    <id>http://huangzhiyuan.github.io/2021/05/25/Linux-perf-optimize-actual-combat-Network/</id>
    <published>2021-05-24T16:53:03.000Z</published>
    <updated>2021-05-25T12:24:25.411Z</updated>
    
    <content type="html"><![CDATA[<p>　　实际上，性能优化一直都是大多数软件工程师头上的紧箍咒，甚至许多工作多年的资深工程师，有时也无法准确地分析出线上的很多性能问题。<br>　　性能分析问题为什么这么难呢？性能优化是个系统工具，常常牵一发而动全身。它涉及了从程序设计、算法分析、编程语言，再到系统、存储、网络等各种底层基础设施的方方面面。每一个组件都可能出现问题，甚至多个组件同时出现问题。毫无疑问，性能优化是软件系统中最有挑战的工作指引，也是最考验体现你综合能力的工作之一。该篇blog记录总结了倪朋飞老师在极客时间开设的[《Linux性能优化实战》][1]课程笔记。</p><span id="more"></span>]]></content>
    
    
    <summary type="html">&lt;p&gt;　　实际上，性能优化一直都是大多数软件工程师头上的紧箍咒，甚至许多工作多年的资深工程师，有时也无法准确地分析出线上的很多性能问题。&lt;br&gt;　　性能分析问题为什么这么难呢？性能优化是个系统工具，常常牵一发而动全身。它涉及了从程序设计、算法分析、编程语言，再到系统、存储、网络等各种底层基础设施的方方面面。每一个组件都可能出现问题，甚至多个组件同时出现问题。毫无疑问，性能优化是软件系统中最有挑战的工作指引，也是最考验体现你综合能力的工作之一。该篇blog记录总结了倪朋飞老师在极客时间开设的[《Linux性能优化实战》][1]课程笔记。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Linux" scheme="http://huangzhiyuan.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Linux性能优化实战之内存性能篇</title>
    <link href="http://huangzhiyuan.github.io/2021/05/25/Linux-perf-optimize-actual-combat-Memory/"/>
    <id>http://huangzhiyuan.github.io/2021/05/25/Linux-perf-optimize-actual-combat-Memory/</id>
    <published>2021-05-24T16:53:03.000Z</published>
    <updated>2021-06-01T13:14:14.328Z</updated>
    
    <content type="html"><![CDATA[<p>　　实际上，性能优化一直都是大多数软件工程师头上的紧箍咒，甚至许多工作多年的资深工程师，有时也无法准确地分析出线上的很多性能问题。<br>　　性能分析问题为什么这么难呢？性能优化是个系统工具，常常牵一发而动全身。它涉及了从程序设计、算法分析、编程语言，再到系统、存储、网络等各种底层基础设施的方方面面。每一个组件都可能出现问题，甚至多个组件同时出现问题。毫无疑问，性能优化是软件系统中最有挑战的工作指引，也是最考验体现你综合能力的工作之一。该篇blog记录总结了倪朋飞老师在极客时间开设的<a href="https://time.geekbang.org/column/article/68728">《Linux性能优化实战》</a>课程笔记。</p><span id="more"></span><h2 id="Linux内存是怎么工作的"><a href="#Linux内存是怎么工作的" class="headerlink" title="Linux内存是怎么工作的"></a>Linux内存是怎么工作的</h2><p>　　同 CPU 管理一样，内存管理也是操作系统最核心的功能之一。内存主要用来存储系统和应用程序的指令、数据、缓存等。　　</p><h3 id="内存映射"><a href="#内存映射" class="headerlink" title="内存映射"></a>内存映射</h3><p>　　我们通常所说的内存容量，提到的笔记本电脑内存8GB，其实指的是物理内存。物理内存也称为主存，大多数计算机用的主存都是动态随机访问内存（DRAM）。只有内核才可以直接访问物理内存。那么，进程要访问内存时，该怎么办呢？<br>　　Linux内核给每个进程都提供了一个独立的虚拟地址空间，并且这个地址空间是连续的。这样，进程就可以很方便地访问内存，更确切地说是访问虚拟内存。<br>　　虚拟地址空间的内部又被分为内核空间和用户空间两部分，不同字长（也就是单个CPU指令可以处理数据的最大长度）的处理器，地址空间的范围也不同。比如最常见的 32 位和 64 位系统，我画了两张图来分别表示它们的虚拟地址空间，如下所示：</p><p><img src="/img/2021/0528/virtual_addr.png" alt="&quot;virtual addr&quot;"></p><p>　　这里可以看出，32 位系统的内核空间占用 1G，位于最高处，剩下的 3G 是用户空间。而 64 位系统的内核空间和用户空间都是 128T，分别占据整个内存空间的最高和最低处，剩下的中间部分是未定义的。<br>　　还记得进程的用户态和内核态吗？进程在用户态时，只能访问用户空间内存；只有进入内核态后，才可以访问内核空间内存。虽然每个进程的地址空间都包含了内核空间，但这些内核空间，其实关联的都是相同的物理内存。这样，进程切换到内核态后，就可以很方便地访问内核空间内存。<br>　　既然每个进程都有一个这么大的地址空间，那么所有进程的虚拟内存加起来，自然要比实际的物理内存大得多。所以，并不是所有的虚拟内存都会分配物理内存，只有那些实际使用的虚拟内存才分配物理内存，并且分配后的物理内存，是通过内存映射来管理的。<br>　　内存映射，其实就是将虚拟内存地址映射到物理内存地址。为了完成内存映射，内核为每个进程都维护了一张页表，记录虚拟地址与物理地址的映射关系，如下图所示：</p><p><img src="/img/2021/0528/virtual_physical.png" alt="&quot;virtual_physical&quot;"></p><p>　　页表实际上存储在 CPU 的内存管理单元 MMU 中，这样，正常情况下，处理器就可以直接通过硬件，找出要访问的内存。而当进程访问的虚拟地址在页表中查不到时，系统会产生一个<strong>缺页异常</strong>，进入内核空间分配物理内存、更新进程页表，最后再返回用户空间，恢复进程的运行。</p><p>　　在 CPU 上下文切换的文章中曾经提到， TLB（Translation Lookaside Buffer，转译后备缓冲器）会影响 CPU 的内存访问性能，在这里其实就可以得到解释。</p><p>　　TLB其实就是MMU中页表的高速缓存。由于进程的虚拟地址空间是独立的，而TLB的访问速度又比MMU快得多，所以，通过减少进程的上下文切换，减少 TLB 的刷新次数，就可以提高 TLB 缓存的使用率，进而提高CPU的内存访问性能。不过要注意，MMU并不以字节为单位来管理内存，而是规定了一个内存映射的最小单位，也就是页，通常是 4 KB 大小。这样，每一次内存映射，都需要关联 4 KB 或者 4KB 整数倍的内存空间。</p><p>　　页的大小只有 4 KB ，导致的另一个问题就是，整个页表会变得非常大。比方说，仅 32 位系统就需要 100 多万个页表项（4GB/4KB），才可以实现整个地址空间的映射。为了解决页表项过多的问题，Linux 提供了两种机制，也就是多级页表和大页（HugePage）。</p><p>　　多级页表就是把内存分成区块来管理，将原来的映射关系改成区块索引和区块内的偏移。由于虚拟内存空间通常只用了很少一部分，那么，多级页表就只保存这些使用中的区块，这样就可以大大地减少页表的项数。</p><p>　　Linux 用的正是四级页表来管理内存页，如下图所示，虚拟地址被分为 5 个部分，前 4 个表项用于选择页，而最后一个索引表示页内偏移。</p><p><img src="/img/2021/0528/multi_page.png" alt="&quot;multi_page&quot;"></p><p>　　再看大页，顾名思义，就是比普通页更大的内存块，常见的大小有 2MB 和 1GB。大页通常用在使用大量内存的进程上，比如 Oracle、DPDK 等。<br>通过这些机制，在页表的映射下，进程就可以通过虚拟地址来访问物理内存了。那么具体到一个 Linux 进程中，这些内存又是怎么使用的呢？</p><h3 id="虚拟内存空间分布"><a href="#虚拟内存空间分布" class="headerlink" title="虚拟内存空间分布"></a>虚拟内存空间分布</h3><p>　　首先，我们需要进一步了解虚拟内存空间的分布情况。最上方的内核空间不用多讲，下方的用户空间内存，其实又被分成了多个不同的段。以 32 位系统为例，我画了一张图来表示它们的关系。</p><p><img src="/img/2021/0528/user_space_mem.png" alt="&quot;user_space_mem&quot;"></p><p>通过这张图你可以看到，用户空间内存，从低到高分别是五种不同的内存段。</p><ol><li>只读段，包括代码和常量等。</li><li>数据段，包括全局变量等。</li><li>堆，包括动态分配的内存，从低地址开始向上增长。</li><li>文件映射段，包括动态库、共享内存等，从高地址开始向下增长。</li><li>栈，包括局部变量和函数调用的上下文等。栈的大小是固定的，一般是 8 MB。</li></ol><p>在这五个内存段中，堆和文件映射段的内存是动态分配的。比如说，使用 C 标准库的 malloc() 或者 mmap() ，就可以分别在堆和文件映射段动态分配内存。其实64位系统的内存分布也类似，只不过内存空间要大得多。那么，更重要的问题来了，内存究竟是怎么分配的呢？</p><h3 id="内存分配和回收"><a href="#内存分配和回收" class="headerlink" title="内存分配和回收"></a>内存分配和回收</h3><p>malloc() 是 C 标准库提供的内存分配函数，对应到系统调用上，有两种实现方式，即**brk()<strong>和</strong>mmap()**。</p><p>对小块内存（小于 128K），C标准库使用brk()来分配，也就是通过移动堆顶的位置来分配内存。这些内存释放后并不会立刻归还系统，而是被缓存起来，这样就可以重复使用。而大块内存（大于 128K），则直接使用内存映射 mmap() 来分配，也就是在文件映射段找一块空闲内存分配出去。</p><p>这两种方式，自然各有优缺点。<br>brk() 方式的缓存，可以减少缺页异常的发生，提高内存访问效率。不过，由于这些内存没有归还系统，在内存工作繁忙时，频繁的内存分配和释放会造成内存碎片。<br>brk() 方式的缓存，可以减少缺页异常的发生，提高内存访问效率。不过，由于这些内存没有归还系统，在内存工作繁忙时，频繁的内存分配和释放会造成内存碎片。<br>而 mmap() 方式分配的内存，会在释放时直接归还系统，所以每次mmap都会发生缺页异常。在内存工作繁忙时，频繁的内存分配会导致大量的缺页异常，使内核的管理负担增大。这也是 malloc 只对大块内存使用 mmap 的原因。</p><p>了解这两种调用方式后，我们还需要清楚一点，那就是，当这两种调用发生后，其实并没有真正分配内存。这些内存，都只在首次访问时才分配，也就是通过缺页异常进入内核中，再由内核来分配内存。</p><p>整体来说，Linux 使用伙伴系统来管理内存分配。前面我们提到过，这些内存在 MMU 中以页为单位进行管理，伙伴系统也一样，以页为单位来管理内存，并且会通过相邻页的合并，减少内存碎片化（比如 brk 方式造成的内存碎片）。</p><p>如果遇到比页更小的对象，比如不到1K的时候，该怎么分配内存呢？实际系统运行中，确实有大量比页还小的对象，如果为它们也分配单独的页，那就太浪费内存了。</p><p>所以，在用户空间，malloc 通过 brk() 分配的内存，在释放时并不立即归还系统，而是缓存起来重复利用。在内核空间，Linux 则通过 slab 分配器来管理小内存。你可以把 slab 看成构建在伙伴系统上的一个缓存，主要作用就是分配并释放内核中的小对象。对内存来说，如果只分配而不释放，就会造成内存泄漏，甚至会耗尽系统内存。所以，在应用程序用完内存后，还需要调用 free() 或 unmap() ，来释放这些不用的内存。</p><p>当然，系统也不会任由某个进程用完所有内存。在发现内存紧张时，系统就会通过一系列机制来回收内存，比如下面这三种方式：</p><ul><li>回收缓存，比如使用 LRU（Least Recently Used）算法，回收最近使用最少的内存页面；</li><li>回收不常访问的内存，把不常用的内存通过交换分区直接写到磁盘中；</li><li>杀死进程，内存紧张时系统还会通过 OOM（Out of Memory），直接杀掉占用大量内存的进程。</li></ul><p>其中，第二种方式回收不常访问的内存时，会用到交换分区（以下简称 Swap）。Swap 其实就是把一块磁盘空间当成内存来用。它可以把进程暂时不用的数据存储到磁盘中（这个过程称为换出），当进程访问这些内存时，再从磁盘读取这些数据到内存中（这个过程称为换入）。</p><p>所以，你可以发现，Swap把系统的可用内存变大了。不过要注意，通常只在内存不足时，才会发生Swap交换。并且由于磁盘读写的速度远比内存慢，Swap 会导致严重的内存性能问题。</p><p>第三种方式提到的 OOM（Out of Memory），其实是内核的一种保护机制。它监控进程的内存使用情况，并且使用 oom_score 为每个进程的内存使用情况进行评分：</p><ul><li>一个进程消耗的内存越大，oom_score 就越大；</li><li>一个进程运行占用的 CPU 越多，oom_score 就越小。</li></ul><p>这样，进程的oom_score越大，代表消耗的内存越多，也就越容易被OOM杀死，从而可以更好保护系统。当然，为了实际工作的需要，管理员可以通过 /proc 文件系统，手动设置进程的 oom_adj ，从而调整进程的 oom_score。oom_adj 的范围是 [-17, 15]，数值越大，表示进程越容易被 OOM 杀死；数值越小，表示进程越不容易被 OOM 杀死，其中 -17 表示禁止 OOM。比如用下面的命令，你就可以把 sshd 进程的 oom_adj 调小为 -16，这样， sshd 进程就不容易被 OOM 杀死。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo -16 &gt; /proc/$(pidof sshd)/oom_adj</span><br></pre></td></tr></table></figure><h3 id="如何查看内存使用情况"><a href="#如何查看内存使用情况" class="headerlink" title="如何查看内存使用情况"></a>如何查看内存使用情况</h3><p>通过了解内存空间的分布，以及内存的分配和回收，我想你对内存的工作原理应该有了大概的认识。当然，系统的实际工作原理更加复杂，也会涉及其他一些机制，这里我只讲了最主要的原理。掌握了这些，你可以对内存的运作有一条主线认识，不至于脑海里只有术语名词的堆砌。</p><p>那么在了解内存的工作原理之后，我们又该怎么查看系统内存使用情况呢？其实前面CPU内容的学习中，我们也提到过一些相关工具。在这里，你第一个想到的应该是 free 工具吧。下面是一个 free 的输出示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 注意不同版本的free输出可能会有所不同</span><br><span class="line">$ free</span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:        8169348      263524     6875352         668     1030472     7611064</span><br><span class="line">Swap:             0           0           0</span><br></pre></td></tr></table></figure><p>free 输出的是一个表格，其中的数值都默认以字节为单位。表格总共有两行六列，这两行分别是物理内存 Mem 和交换分区 Swap 的使用情况，而六列中，每列数据的含义分别为：</p><p>第一列，total 是总内存大小；<br>第二列，used 是已使用内存的大小，包含了共享内存；<br>第三列，free 是未使用内存的大小；<br>第四列，shared 是共享内存的大小；<br>第五列，buff/cache 是缓存和缓冲区的大小；<br>最后一列，available 是新进程可用内存的大小。</p><p>这里尤其注意一下，最后一列的可用内存 available 。available不仅包含未使用内存，还包括了可回收的缓存，所以一般会比未使用内存更大。不过，并不是所有缓存都可以回收，因为有些缓存可能正在使用中。<br>free 显示的是整个系统的内存使用情况。如果你想查看进程的内存使用情况，可以用 top 或者 ps 等工具。比如，下面是 top 的输出示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 按下M切换到内存排序</span><br><span class="line">$ top</span><br><span class="line">...</span><br><span class="line">KiB Mem :  8169348 total,  6871440 free,   267096 used,  1030812 buff/cache</span><br><span class="line">KiB Swap:        0 total,        0 free,        0 used.  7607492 avail Mem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND</span><br><span class="line">  430 root      19  -1  122360  35588  23748 S   0.0  0.4   0:32.17 systemd-journal</span><br><span class="line"> 1075 root      20   0  771860  22744  11368 S   0.0  0.3   0:38.89 snapd</span><br><span class="line"> 1048 root      20   0  170904  17292   9488 S   0.0  0.2   0:00.24 networkd-dispat</span><br><span class="line">    1 root      20   0   78020   9156   6644 S   0.0  0.1   0:22.92 systemd</span><br><span class="line">12376 azure     20   0   76632   7456   6420 S   0.0  0.1   0:00.01 systemd</span><br><span class="line">12374 root      20   0  107984   7312   6304 S   0.0  0.1   0:00.00 sshd</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>top 输出界面的顶端，也显示了系统整体的内存使用情况，这些数据跟free类似，我就不再重复解释。我们接着看下面的内容，跟内存相关的几列数据，比如 VIRT、RES、SHR 以及 %MEM 等。</p><p>这些数据，包含了进程最重要的几个内存使用情况，我们挨个来看。</p><ul><li>VIRT 是进程虚拟内存的大小，只要是进程申请过的内存，即便还没有真正分配物理内存，也会计算在内。</li><li>RES 是常驻内存的大小，也就是进程实际使用的物理内存大小，但不包括 Swap 和共享内存。</li><li>SHR 是共享内存的大小，比如与其他进程共同使用的共享内存、加载的动态链接库以及程序的代码段等。</li><li>%MEM 是进程使用物理内存占系统总内存的百分比。</li></ul><p>除了要认识这些基本信息，在查看 top 输出时，你还要注意两点。<br>第一，虚拟内存通常并不会全部分配物理内存。从上面的输出，你可以发现每个进程的虚拟内存都比常驻内存大得多。<br>第二，共享内存 SHR 并不一定是共享的，比方说，程序的代码段、非共享的动态链接库，也都算在 SHR 里。当然，SHR 也包括了进程间真正共享的内存。所以在计算多个进程的内存使用时，不要把所有进程的 SHR 直接相加得出结果。</p><h2 id="怎么理解内存中的Buffer和Cache"><a href="#怎么理解内存中的Buffer和Cache" class="headerlink" title="怎么理解内存中的Buffer和Cache"></a>怎么理解内存中的Buffer和Cache</h2><p>在上面free命令里面 Buffer 和 Cache 可能不太好区分。从字面上来说，Buffer是缓冲区，而Cache是缓存，两者都是数据在内存中的临时存储。那么，你知道这两种“临时存储”有什么区别吗？</p><h3 id="free数据的来源"><a href="#free数据的来源" class="headerlink" title="free数据的来源"></a>free数据的来源</h3><p>用 man 命令查询 free 的文档，就可以找到对应指标的详细说明。比如，我们执行 man free ，就可以看到下面这个界面。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">buffers  Memory used by kernel buffers (Buffers in /proc/meminfo)</span><br><span class="line"></span><br><span class="line">cache  Memory used by the page cache and slabs (Cached and SReclaimable in /proc/meminfo)</span><br><span class="line"></span><br><span class="line">buff/cache Sum of buffers and cache</span><br></pre></td></tr></table></figure><p>从 free 的手册中，你可以看到 buffer 和 cache 的说明。</p><ul><li>Buffers 是内核缓冲区用到的内存，对应的是 /proc/meminfo 中的 Buffers 值。</li><li>Cache 是内核页缓存和 Slab 用到的内存，对应的是 /proc/meminfo 中的 Cached 与 SReclaimable 之和。</li></ul><p>有没有更简单、更准确的方法，来查询它们的含义呢？</p><h3 id="proc-文件系统"><a href="#proc-文件系统" class="headerlink" title="proc 文件系统"></a>proc 文件系统</h3><p>在前面 CPU 性能模块就曾经提到过，/proc 是 Linux 内核提供的一种特殊文件系统，是用户跟内核交互的接口。比方说，用户可以从 /proc 中查询内核的运行状态和配置选项，查询进程的运行状态、统计数据等，当然，你也可以通过 /proc 来修改内核的配置。</p><p>proc 文件系统同时也是很多性能工具的最终数据来源。比如我们刚才看到的 free ，就是通过读取/proc/meminfo，得到内存的使用情况。</p><p>继续说回/proc/meminfo，既然 Buffers、Cached、SReclaimable 这几个指标不容易理解，那我们还得继续查 proc 文件系统，获取它们的详细定义。</p><p>执行man proc，你就可以得到proc文件系统的详细文档。注意这个文档比较长，你最好搜索一下（比如搜索meminfo），以便更快定位到内存部分。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Buffers %lu</span><br><span class="line">    Relatively temporary storage for raw disk blocks that shouldn&#x27;t get tremendously large (20MB or so).</span><br><span class="line"></span><br><span class="line">Cached %lu</span><br><span class="line">   In-memory cache for files read from the disk (the page cache).  Doesn&#x27;t include SwapCached.</span><br><span class="line">...</span><br><span class="line">SReclaimable %lu (since Linux 2.6.19)</span><br><span class="line">    Part of Slab, that might be reclaimed, such as caches.</span><br><span class="line">    </span><br><span class="line">SUnreclaim %lu (since Linux 2.6.19)</span><br><span class="line">    Part of Slab, that cannot be reclaimed on memory pressure.</span><br></pre></td></tr></table></figure><p>通过这个文档，我们可以看到：</p><ul><li>Buffers 是对原始磁盘块的临时存储，也就是用来缓存磁盘的数据，通常不会特别大（20MB 左右）。这样，内核就可以把分散的写集中起来，统一优化磁盘的写入，比如可以把多次小的写合并成单次大的写等等。</li><li>Cached 是从磁盘读取文件的页缓存，也就是用来缓存从文件读取的数据。这样，下次访问这些文件数据时，就可以直接从内存中快速获取，而不需要再次访问缓慢的磁盘。</li><li>SReclaimable 是 Slab 的一部分。Slab 包括两部分，其中的可回收部分，用 SReclaimable 记录；而不可回收部分，用 SUnreclaim 记录。</li></ul><p>好了，我们终于找到了这三个指标的详细定义。到这里，你是不是长舒一口气，满意地想着，总算弄明白 Buffer 和 Cache 了。不过，知道这个定义就真的理解了吗？这里我给你提了两个问题，你先想想能不能回答出来。</p><p>第一个问题，Buffer 的文档没有提到这是磁盘读数据还是写数据的缓存，而在很多网络搜索的结果中都会提到 Buffer 只是对将要写入磁盘数据的缓存。那反过来说，它会不会也缓存从磁盘中读取的数据呢？</p><p>第二个问题，文档中提到，Cache 是对从文件读取数据的缓存，那么它是不是也会缓存写文件的数据呢？</p><p>　　接下来以实际案例来说明，首先要安装sysstat，是因为我们要用vmstat来观察buffer和cache的变化情况。虽然从/proc/meminfo里可以读到相同的结果，但是还是vmstat的结果更加直观。最后欧，为了减少缓存的影响，运行如下命令来清理系统缓存：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 清理文件页、目录项、Inodes等各种缓存</span><br><span class="line">$ echo 3 &gt; /proc/sys/vm/drop_caches</span><br></pre></td></tr></table></figure><p>这里的 /proc/sys/vm/drop_caches ，就是通过 proc 文件系统修改内核行为的一个示例，写入 3 表示清理文件页、目录项、Inodes 等各种缓存。</p><p><strong>场景1： 磁盘和文件写案例</strong><br>　　先来模拟第一个场景，打开第一个终端，运行vmstat命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 每隔1秒输出1组数据</span><br><span class="line">$ vmstat 1</span><br><span class="line">procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----</span><br><span class="line">r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st</span><br><span class="line">0  0      0 7743608   1112  92168    0    0     0     0   52  152  0  1 100  0  0</span><br><span class="line"> 0  0      0 7743608   1112  92168    0    0     0     0   36   92  0  0 100  0  0</span><br></pre></td></tr></table></figure><ul><li>buff 和 cache 就是我们前面看到的 Buffers 和 Cache，单位是 KB。</li><li>bi 和 bo 则分别表示块设备读取和写入的大小，单位为块 / 秒。因为 Linux 中块的大小是 1KB，所以这个单位也就等价于 KB/s。</li></ul><p>正常情况下，空闲系统中，你应该看到的是，这几个值在多次结果中一直保持不变。接下来，到第二个终端执行 dd 命令，通过读取随机设备，生成一个 500MB 大小的文件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ dd if=/dev/urandom of=/tmp/file bs=1M count=500</span><br></pre></td></tr></table></figure><p>然后再回到第一个终端，观察 Buffer 和 Cache 的变化情况：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----</span><br><span class="line">r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st</span><br><span class="line">0  0      0 7499460   1344 230484    0    0     0     0   29  145  0  0 100  0  0</span><br><span class="line"> 1  0      0 7338088   1752 390512    0    0   488     0   39  558  0 47 53  0  0</span><br><span class="line"> 1  0      0 7158872   1752 568800    0    0     0     4   30  376  1 50 49  0  0</span><br><span class="line"> 1  0      0 6980308   1752 747860    0    0     0     0   24  360  0 50 50  0  0</span><br><span class="line"> 0  0      0 6977448   1752 752072    0    0     0     0   29  138  0  0 100  0  0</span><br><span class="line"> 0  0      0 6977440   1760 752080    0    0     0   152   42  212  0  1 99  1  0</span><br><span class="line">...</span><br><span class="line"> 0  1      0 6977216   1768 752104    0    0     4 122880   33  234  0  1 51 49  0</span><br><span class="line"> 0  1      0 6977440   1768 752108    0    0     0 10240   38  196  0  0 50 50  0</span><br></pre></td></tr></table></figure><p>通过观察 vmstat 的输出，我们发现，在 dd 命令运行时， Cache 在不停地增长，而 Buffer 基本保持不变。再进一步观察 I/O 的情况，你会看到，</p><ul><li>在 Cache 刚开始增长时，块设备 I/O 很少，bi 只出现了一次488 KB/s，bo则只有一次4KB。而过一段时间后，才会出现大量的块设备写，比如 bo 变成了 122880。</li><li>当 dd 命令结束后，Cache 不再增长，但块设备写还会持续一段时间，并且，多次 I/O 写的结果加起来，才是 dd 要写的 500M 的数据。</li></ul><p>运行下面的命令。清理缓存后，向磁盘分区 /dev/sdb1 写入 2GB 的随机数据：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 首先清理缓存</span><br><span class="line">$ echo 3 &gt; /proc/sys/vm/drop_caches</span><br><span class="line"># 然后运行dd命令向磁盘分区/dev/sdb1写入2G数据</span><br><span class="line">$ dd if=/dev/urandom of=/dev/sdb1 bs=1M count=2048</span><br></pre></td></tr></table></figure><p>再回到终端一，观察内存和 I/O 的变化情况：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----</span><br><span class="line"> r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st</span><br><span class="line">1  0      0 7584780 153592  97436    0    0   684     0   31  423  1 48 50  2  0</span><br><span class="line"> 1  0      0 7418580 315384 101668    0    0     0     0   32  144  0 50 50  0  0</span><br><span class="line"> 1  0      0 7253664 475844 106208    0    0     0     0   20  137  0 50 50  0  0</span><br><span class="line"> 1  0      0 7093352 631800 110520    0    0     0     0   23  223  0 50 50  0  0</span><br><span class="line"> 1  1      0 6930056 790520 114980    0    0     0 12804   23  168  0 50 42  9  0</span><br><span class="line"> 1  0      0 6757204 949240 119396    0    0     0 183804   24  191  0 53 26 21  0</span><br><span class="line"> 1  1      0 6591516 1107960 123840    0    0     0 77316   22  232  0 52 16 33  0</span><br></pre></td></tr></table></figure><p>从这里你会看到，虽然同是写数据，写磁盘跟写文件的现象还是不同的。写磁盘时（也就是 bo 大于 0 时），Buffer 和 Cache 都在增长，但显然 Buffer 的增长快得多。这说明，写磁盘用到了大量的 Buffer，这跟我们在文档中查到的定义是一样的。<br>对比两个案例，我们发现，写文件时会用到 Cache 缓存数据，而写磁盘则会用到 Buffer 来缓存数据。所以，回到刚刚的问题，虽然文档上只提到，Cache 是文件读的缓存，但实际上，Cache 也会缓存写文件时的数据。</p><p><strong>场景2： 磁盘和文件读案例</strong><br>我们再反过来想，磁盘和文件读的时候，又是怎样的呢？我们回到第二个终端，运行下面的命令。清理缓存后，从文件 /tmp/file 中，读取数据写入空设备：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 首先清理缓存</span><br><span class="line">$ echo 3 &gt; /proc/sys/vm/drop_caches</span><br><span class="line"># 运行dd命令读取文件数据</span><br><span class="line">$ dd if=/tmp/file of=/dev/null</span><br></pre></td></tr></table></figure><p>然后，再回到终端一，观察内存和 I/O 的变化情况：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----</span><br><span class="line"> r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st</span><br><span class="line"> 0  1      0 7724164   2380 110844    0    0 16576     0   62  360  2  2 76 21  0</span><br><span class="line"> 0  1      0 7691544   2380 143472    0    0 32640     0   46  439  1  3 50 46  0</span><br><span class="line"> 0  1      0 7658736   2380 176204    0    0 32640     0   54  407  1  4 50 46  0</span><br><span class="line"> 0  1      0 7626052   2380 208908    0    0 32640    40   44  422  2  2 50 46  0</span><br></pre></td></tr></table></figure><p>观察 vmstat 的输出，你会发现读取文件时（也就是 bi 大于 0 时），Buffer 保持不变，而 Cache 则在不停增长。这跟我们查到的定义“Cache 是对文件读的页缓存”是一致的。那么，磁盘读又是什么情况呢？我们再运行第二个案例来看看。<br>回到第二个终端，运行下面的命令。清理缓存后，从磁盘分区 /dev/sda1 中读取数据，写入空设备：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 首先清理缓存</span><br><span class="line">$ echo 3 &gt; /proc/sys/vm/drop_caches</span><br><span class="line"># 运行dd命令读取文件</span><br><span class="line">$ dd if=/dev/sda1 of=/dev/null bs=1M count=1024</span><br></pre></td></tr></table></figure><p>再回到终端一，观察内存和 I/O 的变化情况：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----</span><br><span class="line"> r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st</span><br><span class="line">0  0      0 7225880   2716 608184    0    0     0     0   48  159  0  0 100  0  0</span><br><span class="line"> 0  1      0 7199420  28644 608228    0    0 25928     0   60  252  0  1 65 35  0</span><br><span class="line"> 0  1      0 7167092  60900 608312    0    0 32256     0   54  269  0  1 50 49  0</span><br><span class="line"> 0  1      0 7134416  93572 608376    0    0 32672     0   53  253  0  0 51 49  0</span><br><span class="line"> 0  1      0 7101484 126320 608480    0    0 32748     0   80  414  0  1 50 49  0</span><br></pre></td></tr></table></figure><p>观察 vmstat 的输出，你会发现读磁盘时（也就是 bi 大于 0 时），Buffer 和 Cache 都在增长，但显然 Buffer 的增长快很多。这说明读磁盘时，数据缓存到了 Buffer 中。</p><p>当然，我想，经过上一个场景中两个案例的分析，你自己也可以对比得出这个结论：<strong>读文件时数据会缓存到 Cache 中，而读磁盘时数据会缓存到 Buffer 中</strong>。</p><p>到这里你应该发现了，虽然文档提供了对 Buffer 和 Cache 的说明，但是仍不能覆盖到所有的细节。比如说，今天我们了解到的这两点：</p><ul><li>Buffer 既可以用作“将要写入磁盘数据的缓存”，也可以用作“从磁盘读取数据的缓存”。</li><li>Cache 既可以用作“从文件读取数据的页缓存”，也可以用作“写文件的页缓存”。<br>这样，我们就回答了案例开始前的两个问题。</li></ul><p><strong>Buffer 是对磁盘数据的缓存，而 Cache 是文件数据的缓存，它们既会用在读请求中，也会用在写请求中。</strong></p><h2 id="如何利用系统缓存优化程序的运行效率"><a href="#如何利用系统缓存优化程序的运行效率" class="headerlink" title="如何利用系统缓存优化程序的运行效率"></a>如何利用系统缓存优化程序的运行效率</h2><p>既然 Buffer 和 Cache 对系统性能有很大影响，那我们在软件开发的过程中，能不能利用这一点，来优化 I/O 性能，提升应用程序的运行效率呢？答案是肯定的。</p><h3 id="缓存命中率"><a href="#缓存命中率" class="headerlink" title="缓存命中率"></a>缓存命中率</h3><p>我们想利用缓存来提升程序的运行效率，应该怎么评估这个效果呢？换句话说，有没有哪个指标可以衡量缓存使用的好坏呢？<br>我估计你已经想到了，缓存的命中率。所谓缓存命中率，是指直接通过缓存获取数据的请求次数，占所有数据请求次数的百分比。</p><p><strong>命中率越高，表示使用缓存带来的收益越高，应用程序的性能也就越好。</strong><br>实际上，缓存是现在所有高并发系统必需的核心模块，主要作用就是把经常访问的数据（也就是热点数据），提前读入到内存中。这样，下次访问时就可以直接从内存读取数据，而不需要经过硬盘，从而加快应用程序的响应速度。</p><p>这些独立的缓存模块通常会提供查询接口，方便我们随时查看缓存的命中情况。不过Linux系统中并没有直接提供这些接口，所以这里我要介绍一下，cachestat 和 cachetop ，它们正是查看系统缓存命中情况的工具。</p><ul><li>cachestat 提供了整个操作系统缓存的读写命中情况。</li><li>cachetop 提供了每个进程的缓存命中情况。</li></ul><p>这两个工具都是 bcc 软件包的一部分，它们基于 Linux 内核的 eBPF（extended Berkeley Packet Filters）机制，来跟踪内核中管理的缓存，并输出缓存的使用和命中情况。</p><p>使用 cachestat 和 cachetop 前，我们首先要安装 bcc 软件包。比如，在 Ubuntu 系统中，你可以运行下面的命令来安装：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 4052245BD4284CDD</span><br><span class="line">echo &quot;deb https://repo.iovisor.org/apt/xenial xenial main&quot; | sudo tee /etc/apt/sources.list.d/iovisor.list</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install -y bcc-tools libbcc-examples linux-headers-$(uname -r)</span><br></pre></td></tr></table></figure><p>操作完这些步骤，bcc 提供的所有工具就都安装到 /usr/share/bcc/tools 这个目录中了。不过这里提醒你，bcc 软件包默认不会把这些工具配置到系统的 PATH 路径中，所以你得自己手动配置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ export PATH=$PATH:/usr/share/bcc/tools</span><br></pre></td></tr></table></figure><p>配置完，你就可以运行 cachestat 和 cachetop 命令了。比如，下面就是一个 cachestat 的运行界面，它以 1 秒的时间间隔，输出了 3 组缓存统计数据：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ cachestat 1 3</span><br><span class="line">   TOTAL   MISSES     HITS  DIRTIES   BUFFERS_MB  CACHED_MB</span><br><span class="line">       2        0        2        1           17        279</span><br><span class="line">       2        0        2        1           17        279</span><br><span class="line">       2        0        2        1           17        279 </span><br></pre></td></tr></table></figure><p>你可以看到，cachestat 的输出其实是一个表格。每行代表一组数据，而每一列代表不同的缓存统计指标。这些指标从左到右依次表示：</p><ul><li>TOTAL ，表示总的 I/O 次数；</li><li>MISSES ，表示缓存未命中的次数；</li><li>HITS ，表示缓存命中的次数；</li><li>DIRTIES， 表示新增到缓存中的脏页数；</li><li>BUFFERS_MB 表示 Buffers 的大小，以 MB 为单位；</li><li>CACHED_MB 表示 Cache 的大小，以 MB 为单位。</li></ul><p>接下来我们再来看一个 cachetop 的运行界面：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ cachetop</span><br><span class="line">11:58:50 Buffers MB: 258 / Cached MB: 347 / Sort: HITS / Order: ascending</span><br><span class="line">PID      UID      CMD              HITS     MISSES   DIRTIES  READ_HIT%  WRITE_HIT%</span><br><span class="line">   13029 root     python                  1        0        0     100.0%       0.0%</span><br></pre></td></tr></table></figure><p>它的输出跟 top 类似，默认按照缓存的命中次数（HITS）排序，展示了每个进程的缓存命中情况。具体到每一个指标，这里的 HITS、MISSES 和 DIRTIES ，跟 cachestat 里的含义一样，分别代表间隔时间内的缓存命中次数、未命中次数以及新增到缓存中的脏页数。而 READ_HIT 和 WRITE_HIT ，分别表示读和写的缓存命中率。</p><h3 id="指定文件的缓存大小"><a href="#指定文件的缓存大小" class="headerlink" title="指定文件的缓存大小"></a>指定文件的缓存大小</h3><p>除了缓存的命中率外，还有一个指标你可能也会很感兴趣，那就是指定文件在内存中的缓存大小。你可以使用pcstat这个工具，来查看文件在内存中的缓存大小以及缓存比例。pcstat 是一个基于 Go 语言开发的工具，所以安装它之前，你首先应该安装 Go 语言，你可以点击这里下载安装。<br>安装完 Go 语言，再运行下面的命令安装 pcstat：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ export GOPATH=~/go</span><br><span class="line">$ export PATH=~/go/bin:$PATH</span><br><span class="line">$ go get golang.org/x/sys/unix</span><br><span class="line">$ go get github.com/tobert/pcstat/pcstat</span><br></pre></td></tr></table></figure><p>全部安装完成后，你就可以运行 pcstat 来查看文件的缓存情况了。比如，下面就是一个 pcstat 运行的示例，它展示了 /bin/ls 这个文件的缓存情况：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ pcstat /bin/ls</span><br><span class="line">+---------+----------------+------------+-----------+---------+</span><br><span class="line">| Name    | Size (bytes)   | Pages      | Cached    | Percent |</span><br><span class="line">|---------+----------------+------------+-----------+---------|</span><br><span class="line">| /bin/ls | 133792         | 33         | 0         | 000.000 |</span><br><span class="line">+---------+----------------+------------+-----------+---------+</span><br></pre></td></tr></table></figure><p>这个输出中，Cached 就是 /bin/ls 在缓存中的大小，而 Percent 则是缓存的百分比。你看到它们都是 0，这说明 /bin/ls 并不在缓存中。接着，如果你执行一下 ls 命令，再运行相同的命令来查看的话，就会发现 /bin/ls 都在缓存中了：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ ls</span><br><span class="line">$ pcstat /bin/ls</span><br><span class="line">+---------+----------------+------------+-----------+---------+</span><br><span class="line">| Name    | Size (bytes)   | Pages      | Cached    | Percent |</span><br><span class="line">|---------+----------------+------------+-----------+---------|</span><br><span class="line">| /bin/ls | 133792         | 33         | 33        | 100.000 |</span><br><span class="line">+---------+----------------+------------+-----------+---------+</span><br></pre></td></tr></table></figure><p>知道了缓存相应的指标和查看系统缓存的方法后，接下来，我们就进入今天的正式案例。</p><p><strong>案例</strong><br>第一个案例，我们先来看一下上一节提到的 dd 命令。dd 作为一个磁盘和文件的拷贝工具，经常被拿来测试磁盘或者文件系统的读写性能。不过，既然缓存会影响到性能，如果用 dd 对同一个文件进行多次读取测试，测试的结果会怎么样呢？首先，打开两个终端，连接到 Ubuntu 机器上，确保 bcc 已经安装配置成功。</p><p>使用 dd 命令生成一个临时文件，用于后面的文件读取测试：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 生成一个512MB的临时文件</span><br><span class="line">$ dd if=/dev/sda1 of=file bs=1M count=512</span><br><span class="line"># 清理缓存</span><br><span class="line">$ echo 3 &gt; /proc/sys/vm/drop_caches</span><br></pre></td></tr></table></figure><p>继续在第一个终端，运行 pcstat 命令，确认刚刚生成的文件不在缓存中。如果一切正常，你会看到 Cached 和 Percent 都是 0:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ pcstat file</span><br><span class="line">+-------+----------------+------------+-----------+---------+</span><br><span class="line">| Name  | Size (bytes)   | Pages      | Cached    | Percent |</span><br><span class="line">|-------+----------------+------------+-----------+---------|</span><br><span class="line">| file  | 536870912      | 131072     | 0         | 000.000 |</span><br><span class="line">+-------+----------------+------------+-----------+---------+</span><br></pre></td></tr></table></figure><p>还是在第一个终端中，现在运行 cachetop 命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 每隔5秒刷新一次数据</span><br><span class="line">$ cachetop 5</span><br></pre></td></tr></table></figure><p>这次是第二个终端，运行 dd 命令测试文件的读取速度：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ dd if=file of=/dev/null bs=1M</span><br><span class="line">512+0 records in</span><br><span class="line">512+0 records out</span><br><span class="line">536870912 bytes (537 MB, 512 MiB) copied, 16.0509 s, 33.4 MB/s</span><br></pre></td></tr></table></figure><p>从 dd 的结果可以看出，这个文件的读性能是 33.4 MB/s。由于在 dd 命令运行前我们已经清理了缓存，所以 dd 命令读取数据时，肯定要通过文件系统从磁盘中读取。不过，这是不是意味着， dd 所有的读请求都能直接发送到磁盘呢？我们再回到第一个终端， 查看 cachetop 界面的缓存命中情况</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">PID      UID      CMD              HITS     MISSES   DIRTIES  READ_HIT%  WRITE_HIT%</span><br><span class="line">\.\.\.</span><br><span class="line">    3264 root     dd                  37077    37330        0      49.8%      50.2%</span><br></pre></td></tr></table></figure><p>从 cachetop 的结果可以发现，并不是所有的读都落到了磁盘上，事实上读请求的缓存命中率只有 50% 。接下来，我们继续尝试相同的测试命令。先切换到第二个终端，再次执行刚才的 dd 命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ dd if=file of=/dev/null bs=1M</span><br><span class="line">512+0 records in</span><br><span class="line">512+0 records out</span><br><span class="line">536870912 bytes (537 MB, 512 MiB) copied, 0.118415 s, 4.5 GB/s</span><br></pre></td></tr></table></figure><p>看到这次的结果，有没有点小惊讶？磁盘的读性能居然变成了 4.5 GB/s，比第一次的结果明显高了太多。为什么这次的结果这么好呢？不妨再回到第一个终端，看看 cachetop 的情况：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">10:45:22 Buffers MB: 4 / Cached MB: 719 / Sort: HITS / Order: ascending</span><br><span class="line">PID      UID      CMD              HITS     MISSES   DIRTIES  READ_HIT%  WRITE_HIT%</span><br><span class="line">\.\.\.</span><br><span class="line">   32642 root     dd                 131637        0        0     100.0%       0.0%</span><br></pre></td></tr></table></figure><p>cachetop 也有了不小的变化。你可以发现，这次的读的缓存命中率是 100.0%，也就是说这次的 dd 命令全部命中了缓存，所以才会看到那么高的性能。然后，回到第二个终端，再次执行 pcstat 查看文件 file 的缓存情况：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ pcstat file</span><br><span class="line">+-------+----------------+------------+-----------+---------+</span><br><span class="line">| Name  | Size (bytes)   | Pages      | Cached    | Percent |</span><br><span class="line">|-------+----------------+------------+-----------+---------|</span><br><span class="line">| file  | 536870912      | 131072     | 131072    | 100.000 |</span><br><span class="line">+-------+----------------+------------+-----------+---------+</span><br></pre></td></tr></table></figure><p>从 pcstat 的结果你可以发现，测试文件 file 已经被全部缓存了起来，这跟刚才观察到的缓存命中率 100% 是一致的。这两次结果说明，系统缓存对第二次 dd 操作有明显的加速效果，可以大大提高文件读取的性能。但同时也要注意，如果我们把 dd 当成测试文件系统性能的工具，由于缓存的存在，就会导致测试结果严重失真。</p><p><strong>总结</strong></p><p>Buffers 和 Cache 可以极大提升系统的 I/O 性能。通常，我们用缓存命中率，来衡量缓存的使用效率。命中率越高，表示缓存被利用得越充分，应用程序的性能也就越好。你可以用 cachestat 和 cachetop 这两个工具，观察系统和进程的缓存命中情况。其中，</p><ul><li>cachestat 提供了整个系统缓存的读写命中情况。</li><li>cachetop 提供了每个进程的缓存命中情况。<br>不过要注意，Buffers 和 Cache 都是操作系统来管理的，应用程序并不能直接控制这些缓存的内容和生命周期。所以，在应用程序开发中，一般要用专门的缓存组件，来进一步提升性能。比如，程序内部可以使用堆或者栈明确声明内存空间，来存储需要缓存的数据。再或者，使用 Redis 这类外部缓存服务，优化数据的访问效率。</li></ul><h2 id="如何定位和处理内存泄漏"><a href="#如何定位和处理内存泄漏" class="headerlink" title="如何定位和处理内存泄漏"></a>如何定位和处理内存泄漏</h2><h2 id="为什么系统的Swap变高了"><a href="#为什么系统的Swap变高了" class="headerlink" title="为什么系统的Swap变高了"></a>为什么系统的Swap变高了</h2><h2 id="如何快准狠找到系统内存问题"><a href="#如何快准狠找到系统内存问题" class="headerlink" title="如何快准狠找到系统内存问题"></a>如何快准狠找到系统内存问题</h2><h2 id="文件系统与磁盘的区别"><a href="#文件系统与磁盘的区别" class="headerlink" title="文件系统与磁盘的区别"></a>文件系统与磁盘的区别</h2>]]></content>
    
    
    <summary type="html">&lt;p&gt;　　实际上，性能优化一直都是大多数软件工程师头上的紧箍咒，甚至许多工作多年的资深工程师，有时也无法准确地分析出线上的很多性能问题。&lt;br&gt;　　性能分析问题为什么这么难呢？性能优化是个系统工具，常常牵一发而动全身。它涉及了从程序设计、算法分析、编程语言，再到系统、存储、网络等各种底层基础设施的方方面面。每一个组件都可能出现问题，甚至多个组件同时出现问题。毫无疑问，性能优化是软件系统中最有挑战的工作指引，也是最考验体现你综合能力的工作之一。该篇blog记录总结了倪朋飞老师在极客时间开设的&lt;a href=&quot;https://time.geekbang.org/column/article/68728&quot;&gt;《Linux性能优化实战》&lt;/a&gt;课程笔记。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Linux" scheme="http://huangzhiyuan.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Linux性能优化实战</title>
    <link href="http://huangzhiyuan.github.io/2021/05/25/Linux-perf-optimize-actual-combat/"/>
    <id>http://huangzhiyuan.github.io/2021/05/25/Linux-perf-optimize-actual-combat/</id>
    <published>2021-05-24T16:53:03.000Z</published>
    <updated>2021-05-26T14:44:47.377Z</updated>
    
    <content type="html"><![CDATA[<p>　　实际上，性能优化一直都是大多数软件工程师头上的紧箍咒，甚至许多工作多年的资深工程师，有时也无法准确地分析出线上的很多性能问题。<br>　　性能分析问题为什么这么难呢？性能优化是个系统工具，常常牵一发而动全身。它涉及了从程序设计、算法分析、编程语言，再到系统、存储、网络等各种底层基础设施的方方面面。每一个组件都可能出现问题，甚至多个组件同时出现问题。毫无疑问，性能优化是软件系统中最有挑战的工作指引，也是最考验体现你综合能力的工作之一。该篇blog记录总结了倪朋飞老师在极客时间开设的<a href="https://time.geekbang.org/column/article/68728">《Linux性能优化实战》</a>课程笔记。</p><span id="more"></span><h2 id="开篇词"><a href="#开篇词" class="headerlink" title="开篇词"></a>开篇词</h2><p>　　对于我们大多数人来说，最好的学习方式之一就是带着问题学习。这种方法远比抱着大厚本原理书籍高效，且保证不会轻易把自己积累下来的信心打垮。学习更要学会抓住重点。其实只要了解了少数几个系统组件的基本原理和协作方式，掌握基本的性能指标和工具，学会实际工作中性能优化的常用技巧，就可以准确分析和优化大多数的性能问题了。在此基础之上，再去阅读哪些经典的操作系统或者图书，更能事半功倍。<br>　　在这个专栏里面，作者以案例驱动的思路，分别讲解了Linux性能的基本指标、工具、以及相应的观测、分析和调优方法。具体来看分为5个模块。包括<strong>CPU性能、磁盘IO性能、内存性能以及网络性能</strong>。每个模块还浅入深出分为4个不同的篇章。</p><blockquote><p><strong>基础篇</strong>， 介绍Linux必备的基本原理以及对应的性能指标和性能工具。比如理解平均负载，怎么理解上下文切换，Linux内存的工作原理。</p></blockquote><blockquote><p><strong>案例篇</strong>， 通过模拟案例，帮助分析高手在遇到资源瓶颈时，是如何观测、定位、分析并优化这些性能问题的。</p></blockquote><blockquote><p><strong>套路篇</strong>， 理解了基础，亲身体验了模拟案例，梳理出排查问题的整体思路，也就是检查性能问题的一般步骤。</p></blockquote><blockquote><p><strong>答疑篇</strong>， 系统解答出现频次较多的问题。</p></blockquote><h2 id="如何入门学习Linux性能优化"><a href="#如何入门学习Linux性能优化" class="headerlink" title="如何入门学习Linux性能优化"></a>如何入门学习Linux性能优化</h2><h3 id="性能指标的概念"><a href="#性能指标的概念" class="headerlink" title="性能指标的概念"></a>性能指标的概念</h3><p>　　其实，性能问题并没有我们想象中的那么复杂。也不需要我们了解每个组件的所有实现细节。<br>　　当看到性能指标是，首先会想到的就是“高并发”和“响应快”。它们正对应着性能优化的两个核心目标“吞吐”和“延时”，这两个指标是从应用负载的视角来考察性能，直接影响了产品终端的用户体验。跟它们对应的，是从系统资源的视角出发的指标，比如资源利用率和饱和度等。</p><p><img src="/img/2021/0525/1.png" alt="&quot;perf_indicators&quot;"></p><p>　　随着应用负载的增加，系统资源的使用率也会升高，甚至到达极限。而性能问题的本质就是资源以及达到瓶颈，但请求的处理却还不够快，无法支撑更多的请求。性能分析，其实就是找出应用和系统的瓶颈，并设法去避免或者缓解它们。从而更高效的利用资源处理更多请求。常用步骤如下：</p><blockquote><p>选择指标评估应用程序和系统的性能；<br>为应用程序和系统设置性能目标；<br>进行性能基准测试；<br>性能分析定位瓶颈；<br>优化系统和应用程序；<br>性能监控和告警。</p></blockquote><p><img src="/img/2021/0525/linux_perf_tool.png" alt="&quot;perf_tool&quot;"><br>(图片来自：<a href="http://www.brendangregg.com/Perf/linux_perf_tools_full.png">brendangregg.com</a>)</p><p>这个图是Linux性能分析最重要的参考资料之一。它告诉我们，在不同的子系统中出现性能问题后，应该用什么样的工具来观测和分析。<br>总结的思维导图如下：</p><p><img src="/img/2021/0525/si_wei_picture.png" alt="&quot;思维导图&quot;"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;　　实际上，性能优化一直都是大多数软件工程师头上的紧箍咒，甚至许多工作多年的资深工程师，有时也无法准确地分析出线上的很多性能问题。&lt;br&gt;　　性能分析问题为什么这么难呢？性能优化是个系统工具，常常牵一发而动全身。它涉及了从程序设计、算法分析、编程语言，再到系统、存储、网络等各种底层基础设施的方方面面。每一个组件都可能出现问题，甚至多个组件同时出现问题。毫无疑问，性能优化是软件系统中最有挑战的工作指引，也是最考验体现你综合能力的工作之一。该篇blog记录总结了倪朋飞老师在极客时间开设的&lt;a href=&quot;https://time.geekbang.org/column/article/68728&quot;&gt;《Linux性能优化实战》&lt;/a&gt;课程笔记。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Linux" scheme="http://huangzhiyuan.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch中的量化总结</title>
    <link href="http://huangzhiyuan.github.io/2021/01/10/quantization-in-pytorch/"/>
    <id>http://huangzhiyuan.github.io/2021/01/10/quantization-in-pytorch/</id>
    <published>2021-01-10T12:04:42.000Z</published>
    <updated>2021-01-10T13:07:16.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>背景：</strong><br>在深度学习中，量化指的是使用更少的bit来存储原本以浮点数存储的tensor，以及使用更少的bit来完成原本以浮点数完成的计算。这么做的好处主要有如下几点：</p><ul><li>更少的模型体积，接近4倍的减少；</li><li>可以更快的计算，由于更少的内存访问和更快的int8计算，可以快2~4倍。<br>一个量化后的模型，其部分或者全部的tensor操作会使用int类型来计算，而不是使用量化之前的float类型。当然，量化还需要底层硬件支持，x86 CPU（支持AVX2）、ARM CPU、Google TPU、Nvidia Volta/Turing/Ampere、Qualcomm DSP这些主流硬件都对量化提供了支持。<span id="more"></span></li></ul><p>PyTorch 1.1的时候开始添加torch.qint8 dtype、torch.quantize_linear转换函数来开始对量化提供有限的实验性支持。PyTorch 1.3开始正式支持量化，在可量化的Tensor之外，PyTorch开始支持CNN中最常见的operator的量化操作，包括：</p><ul><li>Tensor上的函数: view, clone, resize, slice, add, multiply, cat, mean, max, sort, topk；</li><li>常见的模块（在torch.nn.quantized中）：Conv2d, Linear, Avgpool2d, AdaptiveAvgpool2d, MaxPool2d, AdaptiveMaxPool2d, Interpolate, Upsample；</li><li>为了量化后还维持更高准确率的合并操作（在torch.nn.intrinsic中）：ConvReLU2d, ConvBnReLU2d, ConvBn2d，LinearReLU，add_relu。</li></ul><p>在PyTorch 1.4的时候，PyTorch添加了nn.quantized.Conv3d，与此同时，torchvision 0.5开始提供量化版本的 ResNet、ResNext、MobileNetV2、GoogleNet、InceptionV3和ShuffleNetV2。到PyTorch 1.5的时候，QNNPACK添加了对dynamic quantization的支持，也就为量化版的LSTM在手机平台上使用提供了支撑——也就是添加了对PyTorch mobile的dynamic quantization的支持；增加了量化版本的sigmoid、leaky relu、batch_norm、BatchNorm2d、 Avgpool3d、quantized_hardtanh、quantized ELU activation、quantized Upsample3d、quantized batch_norm3d、 batch_norm3d + relu operators的fused、quantized hardsigmoid。</p><p>在PyTorch 1.6的时候，添加了quantized Conv1d、quantized hardswish、quantized layernorm、quantized groupnorm、quantized instancenorm、quantized reflection_pad1d、quantized adaptive avgpool、quantized channel shuffle op、Quantized Threshold；添加ConvBn3d, ConvBnReLU3d, BNReLU2d, BNReLU3d；per-channel的量化得到增强；添加对LSTMCell、RNNCell、GRUCell的Dynamic quantization支持； 在nn.DataParallel 和 nn.DistributedDataParallel中可以使用Quantization aware training；支持CUDA上的quantized tensor。</p><p>到目前的最新版本的PyTorch 1.7，又添加了Embedding 和EmbeddingBag quantization、aten::repeat、aten::apend、tensor的stack、tensor的fill_、per channel affine quantized tensor的clone、1D batch normalization、N-Dimensional constant padding、CELU operator、FP16 quantization的支持。</p><p><strong>PyTorch对量化的支持目前有如下三种方式：</strong></p><ul><li><strong>Post Training Dynamic Quantization，模型训练完毕后的动态量化；</strong></li><li><strong>Post Training Static Quantization，模型训练完毕后的静态量化；</strong></li><li><strong>QAT（Quantization Aware Training），模型训练中开启量化。</strong></li></ul><p>在开始这三部分之前，Gemfield先介绍下最基础的Tensor的量化。</p><h2 id="Tensor的量化"><a href="#Tensor的量化" class="headerlink" title="Tensor的量化"></a>Tensor的量化</h2><p>PyTorch为了实现量化，首先就得需要具备能够表示量化数据的Tensor，这就是从PyTorch 1.1之后引入的Quantized Tensor。 Quantized Tensor可以存储 int8/uint8/int32类型的数据，并携带有scale、zero_point这些参数。把一个标准的float Tensor转换为量化Tensor的步骤如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = torch.rand(2,3, dtype=torch.float32)</span><br><span class="line">&gt;&gt;&gt; x</span><br><span class="line">tensor([[0.6839, 0.4741, 0.7451],</span><br><span class="line">        [0.9301, 0.1742, 0.6835]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; xq = torch.quantize_per_tensor(x, scale = 0.5, zero_point = 8, dtype=torch.quint8)</span><br><span class="line">tensor([[0.5000, 0.5000, 0.5000],</span><br><span class="line">        [1.0000, 0.0000, 0.5000]], size=(2, 3), dtype=torch.quint8,</span><br><span class="line">       quantization_scheme=torch.per_tensor_affine, scale=0.5, zero_point=8)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; xq.int_repr()</span><br><span class="line">tensor([[ 9,  9,  9],</span><br><span class="line">        [10,  8,  9]], dtype=torch.uint8)</span><br></pre></td></tr></table></figure><p>quantize_per_tensor函数就是使用给定的scale和zp来把一个float tensor转化为quantized tensor，后文你还会遇到这个函数。通过上面这几个数的变化，你可以感受到，量化tensor，也就是xq，和fp32 tensor的关系大概就是:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xq = round(x / scale + zero_point)</span><br></pre></td></tr></table></figure><p>scale这个缩放因子和zero_point是两个参数，建立起了fp32 tensor到量化tensor的映射关系。scale体现了映射中的比例关系，而zero_point则是零基准，也就是fp32中的零在量化tensor中的值。因为当x为零的时候，上述xq就变成了：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xq = round(zero_point) = zero_point</span><br></pre></td></tr></table></figure><p>现在xq已经是一个量化tensor了，我们可以把xq在反量化回来，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># xq is a quantized tensor with data represented as quint8</span><br><span class="line">&gt;&gt;&gt; xdq = xq.dequantize()</span><br><span class="line">&gt;&gt;&gt; xdq</span><br><span class="line">tensor([[0.5000, 0.5000, 0.5000],</span><br><span class="line">        [1.0000, 0.0000, 0.5000]])</span><br></pre></td></tr></table></figure><p>dequantize函数就是quantize_per_tensor的反义词，把一个量化tensor转换为float tensor。也就是：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xdq = (xq - zero_point) * scale</span><br></pre></td></tr></table></figure><p>xdq和x的值已经出现了偏差的事实告诉了我们两个道理：</p><ul><li>量化会有精度损失；</li><li>我们这里随便选取的scale和zp太烂，选择合适的scale和zp可以有效降低精度损失。不信你把scale和zp分别换成scale = 0.0036, zero_point = 0试试。</li></ul><p>而在PyTorch中，选择合适的scale和zp的工作就由各种observer来完成。</p><p>Tensor的量化支持两种模式：per tensor 和 per channel。Per tensor 是说一个tensor里的所有value按照同一种方式去scale和offset； per channel是对于tensor的某一个维度（通常是channel的维度）上的值按照一种方式去scale和offset，也就是一个tensor里有多种不同的scale和offset的方式（组成一个vector），如此以来，在量化的时候相比per tensor的方式会引入更少的错误。PyTorch目前支持conv2d()、conv3d()、linear()的per channel量化。</p><h3 id="Post-Training-Dynamic-Quantization"><a href="#Post-Training-Dynamic-Quantization" class="headerlink" title="Post Training Dynamic Quantization"></a>Post Training Dynamic Quantization</h3><p>这种量化方式经常缩略前面的两个单词从而称之为Dynamic Quantization，中文为动态量化。这是什么意思呢？你看到全称中的两个关键字了吗：<code>Post</code>、<code>Dynamic</code>：</p><ul><li><p>Post：也就是训练完成后再量化模型的权重参数；</p></li><li><p>Dynamic：也就是网络在前向推理的时候动态的量化float32类型的输入。<br>Dynamic Quantization使用下面的API来完成模型的量化：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.quantization.quantize_dynamic(model, qconfig_spec=None, dtype=torch.qint8, mapping=None, inplace=False)</span><br></pre></td></tr></table></figure><p>quantize_dynamic这个API把一个float model转换为dynamic quantized model，也就是只有权重被量化的model，dtype参数可以取值 float16 或者 qint8。当对整个模型进行转换时，默认只对以下的op进行转换：</p></li><li><p>Linear</p></li><li><p>LSTM</p></li><li><p>LSTMCell</p></li><li><p>RNNCell</p></li><li><p>GRUCell</p></li><li><p>为啥呢？因为dynamic quantization只是把权重参数进行量化，而这些layer一般参数数量很大，在整个模型中参数量占比极高，因此边际效益高。对其它layer进行dynamic quantization几乎没有实际的意义。<br>再来说说这个API的第二个参数：qconfig_spec：</p></li><li><p>qconfig_spec指定了一组qconfig，具体就是哪个op对应哪个qconfig ；</p></li><li><p>每个qconfig是QConfig类的实例，封装了两个observer；</p></li><li><p>这两个observer分别是activation的observer和weight的observer；</p></li><li><p>但是动态量化使用的是QConfig子类QConfigDynamic的实例，该实例实际上只封装了weight的observer；</p></li><li><p>activate就是post process，就是op forward之后的后处理，但在动态量化中不包含；</p></li><li><p>observer用来根据四元组（min_val，max_val，qmin, qmax）来计算2个量化的参数：scale和zero_point；</p></li><li><p>qmin、qmax是算法提前确定好的，min_val和max_val是从输入数据中观察到的，所以起名叫observer。</p></li></ul><p>当qconfig_spec为None的时候就是默认行为，如果想要改变默认行为，则可以：</p><ul><li>qconfig_spec赋值为一个set，比如：{nn.LSTM, nn.Linear}，意思是指定当前模型中的哪些layer要被dynamic quantization；</li><li>qconfig_spec赋值为一个dict，key为submodule的name或type，value为QConfigDynamic实例（其包含了特定的Observer，比如MinMaxObserver、MovingAverageMinMaxObserver、PerChannelMinMaxObserver、MovingAveragePerChannelMinMaxObserver、HistogramObserver）。</li></ul><p>事实上，当qconfig_spec为None的时候，quantize_dynamic API就会使用如下的默认值：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">qconfig_spec = &#123;</span><br><span class="line">               nn.Linear : default_dynamic_qconfig,</span><br><span class="line">               nn.LSTM : default_dynamic_qconfig,</span><br><span class="line">               nn.GRU : default_dynamic_qconfig,</span><br><span class="line">               nn.LSTMCell : default_dynamic_qconfig,</span><br><span class="line">               nn.RNNCell : default_dynamic_qconfig,</span><br><span class="line">               nn.GRUCell : default_dynamic_qconfig,</span><br><span class="line">           &#125;</span><br></pre></td></tr></table></figure><p>这就是Gemfield刚才提到的动态量化只量化Linear和RNN变种的真相。而default_dynamic_qconfig是QConfigDynamic的一个实例，使用如下的参数进行构造：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">default_dynamic_qconfig = QConfigDynamic(activation=default_dynamic_quant_observer, weight=default_weight_observer)</span><br><span class="line">default_dynamic_quant_observer = PlaceholderObserver.with_args(dtype=torch.float, compute_dtype=torch.quint8)</span><br><span class="line">default_weight_observer = MinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_tensor_symmetric)</span><br></pre></td></tr></table></figure><p>其中，用于activation的PlaceholderObserver 就是个占位符，啥也不做；而用于weight的MinMaxObserver就是记录输入tensor中的最大值和最小值，用来计算scale和zp。</p><p>对于一个默认行为下的quantize_dynamic调用，你的模型会经历什么变化呢？Gemfield使用一个小网络来演示下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class CivilNet(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(CivilNet, self).__init__()</span><br><span class="line">        gemfieldin = 1</span><br><span class="line">        gemfieldout = 1</span><br><span class="line">        self.conv = nn.Conv2d(gemfieldin, gemfieldout, kernel_size=1, stride=1, padding=0, groups=1, bias=False)</span><br><span class="line">        self.fc = nn.Linear(3, 2,bias=False)</span><br><span class="line">        self.relu = nn.ReLU(inplace=False)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure><p>原始网络和动态量化后的网络如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#原始网络</span><br><span class="line">CivilNet(</span><br><span class="line">  (conv): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">  (fc): Linear(in_features=3, out_features=2, bias=False)</span><br><span class="line">  (relu): ReLU()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">#quantize_dynamic后</span><br><span class="line">CivilNet(</span><br><span class="line">  (conv): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">  (fc): DynamicQuantizedLinear(in_features=3, out_features=2, dtype=torch.qint8, qscheme=torch.per_tensor_affine)</span><br><span class="line">  (relu): ReLU()</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>可以看到，除了Linear，其它op都没有变动。而Linear被转换成了DynamicQuantizedLinear，DynamicQuantizedLinear就是torch.nn.quantized.dynamic.modules.linear.Linear类。没错，quantize_dynamic API的本质就是检索模型中op的type，如果某个op的type属于字典DEFAULT_DYNAMIC_QUANT_MODULE_MAPPINGS的key，那么，这个op将被替换为key对应的value：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Default map for swapping dynamic modules</span><br><span class="line">DEFAULT_DYNAMIC_QUANT_MODULE_MAPPINGS = &#123;</span><br><span class="line">    nn.GRUCell: nnqd.GRUCell,</span><br><span class="line">    nn.Linear: nnqd.Linear,</span><br><span class="line">    nn.LSTM: nnqd.LSTM,</span><br><span class="line">    nn.LSTMCell: nnqd.LSTMCell,</span><br><span class="line">    nn.RNNCell: nnqd.RNNCell,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里，nnqd.Linear就是DynamicQuantizedLinear就是torch.nn.quantized.dynamic.modules.linear.Linear。 但是，type从key换为value，那这个新的type如何实例化呢？更重要的是，实例化新的type一定是要用之前的权重参数的呀。没错，以Linear为例，该逻辑定义在 nnqd.Linear的from_float()方法中，通过如下方式实例化：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">new_mod = mapping[type(mod)].from_float(mod)</span><br></pre></td></tr></table></figure><p>from_float做的事情主要就是：</p><ul><li>使用MinMaxObserver计算模型中op权重参数中tensor的最大值最小值（这个例子中只有Linear op），缩小量化时原始值的取值范围，提高量化的精度；</li><li>通过上述步骤中得到四元组中的min_val和max_val，再结合算法确定的qmin, qmax计算出scale和zp，参考前文“Tensor的量化”小节，计算得到量化后的weight，这个量化过程有torch.quantize_per_tensor和torch.quantize_per_channel两种，默认是前者（因为qchema默认是torch.per_tensor_affine）；</li><li>实例化nnqd.Linear，然后使用qlinear.set_weight_bias将量化后的weight和原始的bias设置到新的layer上。其中最后一步还涉及到weight和bias的打包，在源代码中是这样的：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#ifdef USE_FBGEMM</span><br><span class="line">    if (ctx.qEngine() == at::QEngine::FBGEMM) &#123;</span><br><span class="line">      return PackedLinearWeight::prepack(std::move(weight), std::move(bias));</span><br><span class="line">    &#125;</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">#ifdef USE_PYTORCH_QNNPACK</span><br><span class="line">    if (ctx.qEngine() == at::QEngine::QNNPACK) &#123;</span><br><span class="line">      return PackedLinearWeightsQnnp::prepack(std::move(weight), std::move(bias));</span><br><span class="line">    &#125;</span><br><span class="line">#endif</span><br><span class="line">    TORCH_CHECK(false,&quot;Didn&#x27;t find engine for operation quantized::linear_prepack &quot;,toString(ctx.qEngine()));</span><br></pre></td></tr></table></figure><p>也就是说依赖FBGEMM、QNNPACK这些backend。量化完后的模型在推理的时候有什么不一样的呢？在原始网络中，从输入到最终输出是这么计算的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#input</span><br><span class="line">torch.Tensor([[[[-1,-2,-3],[1,2,3]]]])</span><br><span class="line"></span><br><span class="line">#经过卷积后（权重为torch.Tensor([[[[-0.7867]]]])）</span><br><span class="line">torch.Tensor([[[[ 0.7867,  1.5734,  2.3601],[-0.7867, -1.5734, -2.3601]]]])</span><br><span class="line"></span><br><span class="line">#经过fc后（权重为torch.Tensor([[ 0.4097, -0.2896, -0.4931], [-0.3738, -0.5541,  0.3243]]) )</span><br><span class="line">torch.Tensor([[[[-1.2972, -0.4004], [1.2972,  0.4004]]]])</span><br><span class="line"></span><br><span class="line">#经过relu后</span><br><span class="line">torch.Tensor([[[[0.0000, 0.0000],[1.2972, 0.4004]]]])</span><br></pre></td></tr></table></figure><p>而在动态量化模型中，上述过程就变成了：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#input</span><br><span class="line">torch.Tensor([[[[-1,-2,-3],[1,2,3]]]])</span><br><span class="line"></span><br><span class="line">#经过卷积后（权重为torch.Tensor([[[[-0.7867]]]])）</span><br><span class="line">torch.Tensor([[[[ 0.7867,  1.5734,  2.3601],[-0.7867, -1.5734, -2.3601]]]])</span><br><span class="line"></span><br><span class="line">#经过fc后（权重为torch.Tensor([[ 0.4085, -0.2912, -0.4911],[-0.3737, -0.5563,  0.3259]], dtype=torch.qint8,scale=0.0043458822183310986,zero_point=0) )</span><br><span class="line">torch.Tensor([[[[-1.3038, -0.3847], [1.2856,  0.3969]]]])</span><br><span class="line"></span><br><span class="line">#经过relu后</span><br><span class="line">torch.Tensor([[[[0.0000, 0.0000], [1.2856, 0.3969]]]])</span><br></pre></td></tr></table></figure><p>所以关键点就是这里的Linear op了，因为其它op和量化之前是一模一样的。你可以看到Linear权重的scale为0.0043458822183310986，zero_point为0。scale和zero_point怎么来的呢？由其使用的observer计算得到的，具体来说就是默认的MinMaxObserver，它是怎么工作的呢？还记得前面说过的observer负责根据四元组来计算scale和zp吧：</p><p><strong>在各种observer中，计算权重的scale和zp离不开这四个变量：min_val，max_val，qmin, qmax，分别代表op权重数据/input tensor数据分布的最小值和最大值，以及量化后的取值范围的最小、最大值。</strong>qmin和qmax的值好确定，基本就是8个bit能表示的范围，这里取的分别是-128和127（更详细的计算方式将会在下文的“静态量化”章节中描述）；Linear op的权重为torch.Tensor([[ 0.4097, -0.2896, -0.4931], [-0.3738, -0.5541, 0.3243]])，因此其min_val和max_val分别为-0.5541 和 0.4097，在这个上下文中，max_val将进一步取这俩绝对值的最大值。由此我们就可以得到：</p><ul><li>scale = max_val / (float(qmax - qmin) / 2) = 0.5541 / ((127 + 128) / 2) = 0.004345882…</li><li>zp = 0</li></ul><p>scale和zp的计算细节还会在下文的“静态量化”章节中更详细的描述。从上面我们可以得知，权重部分的量化是“静态”的，是提前就转换完毕的，而之所以叫做“动态”量化，就在于前向推理的时候动态的把input的float tensor转换为量化tensor。</p><p>在forward的时候，nnqd.Linear会调用torch.ops.quantized.linear_dynamic函数，输入正是上面（pack好后的）量化后的权重和float的bias，而torch.ops.quantized.linear_dynamic函数最终会被PyTorch分发到C++中的apply_dynamic_impl函数，在这里，或者使用FBGEMM的实现（x86-64设备），或者使用QNNPACK的实现（ARM设备上）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#ifdef USE_FBGEMM</span><br><span class="line">at::Tensor PackedLinearWeight::apply_dynamic_impl(at::Tensor input, bool reduce_range) &#123;</span><br><span class="line">  ...</span><br><span class="line">  fbgemm::xxxx</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br><span class="line">#endif // USE_FBGEMM</span><br><span class="line"></span><br><span class="line">#ifdef USE_PYTORCH_QNNPACK</span><br><span class="line">at::Tensor PackedLinearWeightsQnnp::apply_dynamic_impl(at::Tensor input) &#123;</span><br><span class="line">  ...</span><br><span class="line">  qnnpack::qnnpackLinearDynamic(xxxx)</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br><span class="line">#endif // USE_PYTORCH_QNNPACK</span><br></pre></td></tr></table></figure><p>等等，input还是float32的啊，这怎么运算嘛。别急，在上述的apply_dynamic_impl函数中，会使用下面的逻辑对输入进行量化：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor q_input = at::quantize_per_tensor(input_contig, q_params.scale, q_params.zero_point, c10::kQUInt8);</span><br></pre></td></tr></table></figure><p>也就是说，动态量化的本质就藏身于此：基于运行时对数据范围的观察，来动态确定对输入进行量化时的scale值。这就确保 input tensor的scale因子能够基于输入数据进行优化，从而获得颗粒度更细的信息。</p><p>而模型的参数则是提前就转换为了INT8的格式（在使用quantize_dynamic API的时候）。这样，当输入也被量化后，网络中的运算就使用向量化的INT8指令来完成。 而在当前layer输出的时候，我们还需要把结果再重新转换为float32——re-quantization的scale值是依据input、 weight和output scale来确定的，定义如下：<br><strong>requant_scale = input_scale_fp32 * weight_scale_fp32 / output_scale_fp32</strong><br>实际上，在apply_dynamic_impl函数中，requant_scales就是这么实现的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">auto output_scale = 1.f</span><br><span class="line">auto inverse_output_scale = 1.f /output_scale;</span><br><span class="line">requant_scales[i] = (weight_scales_data[i] * input_scale) * inverse_output_scale;</span><br></pre></td></tr></table></figure><p>这就是为什么在前面Gemfield提到过，经过量化版的fc的输出为torch.Tensor([[[[-1.3038, -0.3847], [1.2856, 0.3969]]]])，已经变回正常的float tensor了。所以动态量化模型的前向推理过程可以概括如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#原始的模型，所有的tensor和计算都是浮点型</span><br><span class="line">previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32</span><br><span class="line">                 /</span><br><span class="line">linear_weight_fp32</span><br><span class="line"></span><br><span class="line">#动态量化后的模型，Linear和LSTM的权重是int8</span><br><span class="line">previous_layer_fp32 -- linear_int8_w_fp32_inp -- activation_fp32 -- next_layer_fp32</span><br><span class="line">                     /</span><br><span class="line">   linear_weight_int8</span><br></pre></td></tr></table></figure><p>总结下来，我们可以这么说：Post Training Dynamic Quantization，简称为Dynamic Quantization，也就是动态量化，或者叫作Weight-only的量化，是提前把模型中某些op的参数量化为INT8，然后在运行的时候动态的把输入量化为INT8，然后在当前op输出的时候再把结果requantization回到float32类型。动态量化默认只适用于Linear以及RNN的变种。</p><h3 id="Post-Training-Static-Quantization"><a href="#Post-Training-Static-Quantization" class="headerlink" title="Post Training Static Quantization"></a>Post Training Static Quantization</h3><p>与其介绍post training static quantization是什么，我们不如先来说明下它和dynamic quantization的相同点和区别是什么。相同点就是，都是把网络的权重参数转从float32转换为int8；不同点是，需要把训练集或者和训练集分布类似的数据喂给模型（注意没有反向传播），然后通过每个op输入的分布特点来计算activation的量化参数（scale和zp）——称之为Calibrate（定标）。是的，静态量化包含有activation了，也就是post process，也就是op forward之后的后处理。为什么静态量化需要activation呢？因为静态量化的前向推理过程自(始+1)至(终-1)都是INT计算，activation需要确保一个op的输入符合下一个op的输入。</p><p>PyTorch会使用五部曲来完成模型的静态量化：</p><p><strong>1，fuse_model</strong><br>合并一些可以合并的layer。这一步的目的是为了提高速度和准确度：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fuse_modules(model, modules_to_fuse, inplace=False, fuser_func=fuse_known_modules, fuse_custom_config_dict=None)</span><br></pre></td></tr></table></figure><p>比如给fuse_modules传递下面的参数就会合并网络中的conv1、bn1、relu1：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.quantization.fuse_modules(gemfield_model, [[&#x27;conv1&#x27;, &#x27;bn1&#x27;, &#x27;relu1&#x27;]], inplace=True)</span><br></pre></td></tr></table></figure><p>一旦合并成功，那么原始网络中的conv1就会被替换为新的合并后的module（因为其是list中的第一个元素），而bn1、relu1（list中剩余的元素）会被替换为nn.Identity()，这个模块是个占位符，直接输出输入。举个例子，对于下面的一个小网络：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class CivilNet(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(CivilNet, self).__init__()</span><br><span class="line">        syszuxin = 1</span><br><span class="line">        syszuxout = 1</span><br><span class="line">        self.conv = nn.Conv2d(syszuxin, syszuxout, kernel_size=1, stride=1, padding=0, groups=1, bias=False)</span><br><span class="line">        self.fc = nn.Linear(3, 2,bias=False)</span><br><span class="line">        self.relu = nn.ReLU(inplace=False)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure><p>网络结构如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CivilNet(</span><br><span class="line">  (conv): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">  (fc): Linear(in_features=3, out_features=2, bias=False)</span><br><span class="line">  (relu): ReLU()</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>经过torch.quantization.fuse_modules(c, [[‘fc’, ‘relu’]], inplace=True)后，网络变成了：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CivilNet(</span><br><span class="line">  (conv): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">  (fc): LinearReLU(</span><br><span class="line">    (0): Linear(in_features=3, out_features=2, bias=False)</span><br><span class="line">    (1): ReLU()</span><br><span class="line">  )</span><br><span class="line">  (relu): Identity()</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>modules_to_fuse参数的list可以包含多个item list，或者是submodule的op list也可以，比如：[ [‘conv1’, ‘bn1’, ‘relu1’], [‘submodule.conv’, ‘submodule.relu’]]。有的人会说了，我要fuse的module被Sequential封装起来了，如何传参？参考下面的代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.quantization.fuse_modules(a_sequential_module, [&#x27;0&#x27;, &#x27;1&#x27;, &#x27;2&#x27;], inplace=True)</span><br></pre></td></tr></table></figure><p>不是什么类型的op都可以参与合并，也不是什么样的顺序都可以参与合并。就目前来说，截止到pytorch 1.7.1，只有如下的op和顺序才可以：</p><ul><li>Convolution, Batch normalization</li><li>Convolution, Batch normalization, Relu</li><li>Convolution, Relu</li><li>Linear, Relu</li><li>Batch normalization, Relu</li></ul><p>实际上，这个mapping关系就定义在DEFAULT_OP_LIST_TO_FUSER_METHOD中：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">DEFAULT_OP_LIST_TO_FUSER_METHOD : Dict[Tuple, Union[nn.Sequential, Callable]] = &#123;</span><br><span class="line">    (nn.Conv1d, nn.BatchNorm1d): fuse_conv_bn,</span><br><span class="line">    (nn.Conv1d, nn.BatchNorm1d, nn.ReLU): fuse_conv_bn_relu,</span><br><span class="line">    (nn.Conv2d, nn.BatchNorm2d): fuse_conv_bn,</span><br><span class="line">    (nn.Conv2d, nn.BatchNorm2d, nn.ReLU): fuse_conv_bn_relu,</span><br><span class="line">    (nn.Conv3d, nn.BatchNorm3d): fuse_conv_bn,</span><br><span class="line">    (nn.Conv3d, nn.BatchNorm3d, nn.ReLU): fuse_conv_bn_relu,</span><br><span class="line">    (nn.Conv1d, nn.ReLU): nni.ConvReLU1d,</span><br><span class="line">    (nn.Conv2d, nn.ReLU): nni.ConvReLU2d,</span><br><span class="line">    (nn.Conv3d, nn.ReLU): nni.ConvReLU3d,</span><br><span class="line">    (nn.Linear, nn.ReLU): nni.LinearReLU,</span><br><span class="line">    (nn.BatchNorm2d, nn.ReLU): nni.BNReLU2d,</span><br><span class="line">    (nn.BatchNorm3d, nn.ReLU): nni.BNReLU3d,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>2，设置qconfig</strong><br>qconfig是要设置到模型或者模型的子module上的。前文Gemfield就已经说过，qconfig是QConfig的一个实例，QConfig这个类就是维护了两个observer，一个是activation所使用的observer，一个是op权重所使用的observer。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#如果要部署在x86 server上</span><br><span class="line">gemfield_model.qconfig = torch.quantization.get_default_qconfig(&#x27;fbgemm&#x27;)</span><br><span class="line"></span><br><span class="line">#如果要部署在ARM上</span><br><span class="line">gemfield_model.qconfig = torch.quantization.get_default_qconfig(&#x27;qnnpack&#x27;)</span><br></pre></td></tr></table></figure><p>如果是x86和arm之外呢？抱歉，目前不支持。实际上，这里的get_default_qconfig函数的实现如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def get_default_qconfig(backend=&#x27;fbgemm&#x27;):</span><br><span class="line">    if backend == &#x27;fbgemm&#x27;:</span><br><span class="line">        qconfig = QConfig(activation=HistogramObserver.with_args(reduce_range=True),weight=default_per_channel_weight_observer)</span><br><span class="line">    elif backend == &#x27;qnnpack&#x27;:</span><br><span class="line">        qconfig = QConfig(activation=HistogramObserver.with_args(reduce_range=False),weight=default_weight_observer)</span><br><span class="line">    else:</span><br><span class="line">        qconfig = default_qconfig</span><br><span class="line">    return qconfig</span><br></pre></td></tr></table></figure><p>default_qconfig实际上是QConfig(activation=default_observer, weight=default_weight_observer)，所以gemfield这里总结了一个表格：</p><table><thead><tr><th>量化的backend</th><th>activation</th><th>weight</th></tr></thead><tbody><tr><td>fbgemm</td><td>HistogramObserver (reduce_range=True)</td><td>PerChannelMinMaxObserver (default_per_channel_weight_observer)</td></tr><tr><td>qnnpack</td><td>HistogramObserver (reduce_range=False)</td><td>MinMaxObserver (default_weight_observer)</td></tr><tr><td>默认（非fbgemm和qnnpack）</td><td>MinMaxObserver (default_observer)</td><td>MinMaxObserver (default_weight_observer)</td></tr></tbody></table><p><strong>3，prepare</strong><br>prepare调用是通过如下API完成的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gemfield_model_prepared = torch.quantization.prepare(gemfield_model)</span><br></pre></td></tr></table></figure><p>prepare用来给每个子module插入Observer，用来收集和定标数据。以activation的observer为例，就是期望其观察输入数据得到四元组中的min_val和max_val，至少观察个几百个迭代的数据吧，然后由这四元组得到scale和zp这两个参数的值。</p><p>module上安插activation的observer是怎么实现的呢？还记得<a href="https://zhuanlan.zhihu.com/p/53927068%E4%B8%80%E6%96%87%E4%B8%AD%E8%AF%B4%E8%BF%87%E7%9A%84%E2%80%9C_forward_hooks%E6%98%AF%E9%80%9A%E8%BF%87register_forward_hook%E6%9D%A5%E5%AE%8C%E6%88%90%E6%B3%A8%E5%86%8C%E7%9A%84%E3%80%82%E8%BF%99%E4%BA%9Bhooks%E6%98%AF%E5%9C%A8forward%E5%AE%8C%E4%B9%8B%E5%90%8E%E8%A2%AB%E8%B0%83%E7%94%A8%E7%9A%84......%E2%80%9D%E5%90%97%EF%BC%9F%E6%B2%A1%E9%94%99%EF%BC%8CCivilNet%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84Conv2d%E3%80%81Linear%E3%80%81ReLU%E3%80%81QuantStub%E8%BF%99%E4%BA%9Bmodule%E7%9A%84_forward_hooks%E4%B8%8A%E9%83%BD%E8%A2%AB%E6%8F%92%E5%85%A5%E4%BA%86activation%E7%9A%84HistogramObserver%EF%BC%8C%E5%BD%93%E8%BF%99%E4%BA%9B%E5%AD%90module%E8%AE%A1%E7%AE%97%E5%AE%8C%E6%AF%95%E5%90%8E%EF%BC%8C%E7%BB%93%E6%9E%9C%E4%BC%9A%E8%A2%AB%E7%AB%8B%E5%88%BB%E9%80%81%E5%88%B0%E5%85%B6_forward_hooks%E4%B8%AD%E7%9A%84HistogramObserver%E8%BF%9B%E8%A1%8C%E8%A7%82%E5%AF%9F%E3%80%82">https://zhuanlan.zhihu.com/p/53927068一文中说过的“_forward_hooks是通过register_forward_hook来完成注册的。这些hooks是在forward完之后被调用的......”吗？没错，CivilNet模型中的Conv2d、Linear、ReLU、QuantStub这些module的_forward_hooks上都被插入了activation的HistogramObserver，当这些子module计算完毕后，结果会被立刻送到其_forward_hooks中的HistogramObserver进行观察。</a></p><p>这一步完成后，CivilNet网络就被改造成了：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">CivilNet(</span><br><span class="line">  (conv): Conv2d(</span><br><span class="line">    1, 1, kernel_size=(1, 1), stride=(1, 1), bias=False</span><br><span class="line">    (activation_post_process): HistogramObserver()</span><br><span class="line">  )</span><br><span class="line">  (fc): Linear(</span><br><span class="line">    in_features=3, out_features=2, bias=False</span><br><span class="line">    (activation_post_process): HistogramObserver()</span><br><span class="line">  )</span><br><span class="line">  (relu): ReLU(</span><br><span class="line">    (activation_post_process): HistogramObserver()</span><br><span class="line">  )</span><br><span class="line">  (quant): QuantStub(</span><br><span class="line">    (activation_post_process): HistogramObserver()</span><br><span class="line">  )</span><br><span class="line">  (dequant): DeQuantStub()</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><strong>4，喂数据</strong><br>这一步不是训练。是为了获取数据的分布特点，来更好的计算activation的scale和zp。至少要喂上几百个迭代的数据。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#至少观察个几百迭代</span><br><span class="line">for data in data_loader:</span><br><span class="line">    gemfield_model_prepared(data)</span><br></pre></td></tr></table></figure><p><strong>5，转换模型</strong><br>第四步完成后，各个op权重的四元组（min_val，max_val，qmin, qmax）中的min_val，max_val已经有了，各个op activation的四元组（min_val，max_val，qmin, qmax）中的min_val，max_val也已经观察出来了。那么在这一步我们将调用convert API：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gemfield_model_prepared_int8 = torch.quantization.convert(gemfield_model_prepared)</span><br></pre></td></tr></table></figure><p>这个过程和dynamic量化类似，本质就是检索模型中op的type，如果某个op的type属于字典DEFAULT_STATIC_QUANT_MODULE_MAPPINGS的key（注意字典和动态量化的不一样了），那么，这个op将被替换为key对应的value：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">DEFAULT_STATIC_QUANT_MODULE_MAPPINGS = &#123;</span><br><span class="line">    QuantStub: nnq.Quantize,</span><br><span class="line">    DeQuantStub: nnq.DeQuantize,</span><br><span class="line">    nn.BatchNorm2d: nnq.BatchNorm2d,</span><br><span class="line">    nn.BatchNorm3d: nnq.BatchNorm3d,</span><br><span class="line">    nn.Conv1d: nnq.Conv1d,</span><br><span class="line">    nn.Conv2d: nnq.Conv2d,</span><br><span class="line">    nn.Conv3d: nnq.Conv3d,</span><br><span class="line">    nn.ConvTranspose1d: nnq.ConvTranspose1d,</span><br><span class="line">    nn.ConvTranspose2d: nnq.ConvTranspose2d,</span><br><span class="line">    nn.ELU: nnq.ELU,</span><br><span class="line">    nn.Embedding: nnq.Embedding,</span><br><span class="line">    nn.EmbeddingBag: nnq.EmbeddingBag,</span><br><span class="line">    nn.GroupNorm: nnq.GroupNorm,</span><br><span class="line">    nn.Hardswish: nnq.Hardswish,</span><br><span class="line">    nn.InstanceNorm1d: nnq.InstanceNorm1d,</span><br><span class="line">    nn.InstanceNorm2d: nnq.InstanceNorm2d,</span><br><span class="line">    nn.InstanceNorm3d: nnq.InstanceNorm3d,</span><br><span class="line">    nn.LayerNorm: nnq.LayerNorm,</span><br><span class="line">    nn.LeakyReLU: nnq.LeakyReLU,</span><br><span class="line">    nn.Linear: nnq.Linear,</span><br><span class="line">    nn.ReLU6: nnq.ReLU6,</span><br><span class="line">    # Wrapper Modules:</span><br><span class="line">    nnq.FloatFunctional: nnq.QFunctional,</span><br><span class="line">    # Intrinsic modules:</span><br><span class="line">    nni.BNReLU2d: nniq.BNReLU2d,</span><br><span class="line">    nni.BNReLU3d: nniq.BNReLU3d,</span><br><span class="line">    nni.ConvReLU1d: nniq.ConvReLU1d,</span><br><span class="line">    nni.ConvReLU2d: nniq.ConvReLU2d,</span><br><span class="line">    nni.ConvReLU3d: nniq.ConvReLU3d,</span><br><span class="line">    nni.LinearReLU: nniq.LinearReLU,</span><br><span class="line">    nniqat.ConvBn1d: nnq.Conv1d,</span><br><span class="line">    nniqat.ConvBn2d: nnq.Conv2d,</span><br><span class="line">    nniqat.ConvBnReLU1d: nniq.ConvReLU1d,</span><br><span class="line">    nniqat.ConvBnReLU2d: nniq.ConvReLU2d,</span><br><span class="line">    nniqat.ConvReLU2d: nniq.ConvReLU2d,</span><br><span class="line">    nniqat.LinearReLU: nniq.LinearReLU,</span><br><span class="line">    # QAT modules:</span><br><span class="line">    nnqat.Linear: nnq.Linear,</span><br><span class="line">    nnqat.Conv2d: nnq.Conv2d,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>替换的过程也和dynamic一样，使用from_float() API，这个API会使用前面的四元组信息计算出op权重和op activation的scale和zp，然后用于量化。动态量化”章节时Gemfield说过要再详细介绍下scale和zp的计算过程，好了，就在这里。这个计算过程覆盖了如下的几个问题：</p><ul><li>QuantStub的scale和zp是怎么来的（静态量化需要插入QuantStub，后文有说明）？</li><li>conv activation的scale和zp是怎么来的？</li><li>conv weight的scale和zp是怎么来的？</li><li>fc activation的scale和zp是怎么来的？</li><li>fc weight的scale和zp是怎么来的？</li><li>relu activation的scale和zp是怎么来的？</li><li>relu weight的…等等，relu没有weight。</li></ul><p>我们就从conv来说起吧，还记得前面说过的Observer吗？分为activation和weight两种。以Gemfield这里使用的fbgemm后端为例，activation默认的observer是HistogramObserver、weight默认的observer是PerChannelMinMaxObserver。而计算scale和zp所需的四元组都是这些observer观察出来的呀（好吧，其中两个）。</p><p>在convert API调用中，pytorch会将Conv2d op替换为对应的QuantizedConv2d，在这个替换的过程中会计算QuantizedConv2d activation的scale和zp以及QuantizedConv2d weight的scale和zp。在各种observer中，计算scale和zp离不开这四个变量：min_val，max_val，qmin, qmax，分别代表输入的数据/权重的数据分布的最小值和最大值，以及量化后的取值范围的最小、最大值。qmin和qmax的值好确定，基本就是8个bit能表示的范围，在pytorch中，qmin和qmax是使用如下方式确定的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">if self.dtype == torch.qint8:</span><br><span class="line">    if self.reduce_range:</span><br><span class="line">        qmin, qmax = -64, 63</span><br><span class="line">    else:</span><br><span class="line">        qmin, qmax = -128, 127</span><br><span class="line">else:</span><br><span class="line">    if self.reduce_range:</span><br><span class="line">        qmin, qmax = 0, 127</span><br><span class="line">    else:</span><br><span class="line">        qmin, qmax = 0, 255</span><br></pre></td></tr></table></figure><p>比如conv的activation的observer(quint8)是HistogramObserver，又是reduce_range的，因此其qmin,qmax = 0 ，127，而conv的weight(qint8)是PerChannelMinMaxObserver，不是reduce_range的，因此其qmin, qmax = -128, 127。那么min_val，max_val又是怎么确定的呢？对于HistogramObserver，其由输入数据 + 权重值根据L2Norm(An approximation for L2 error minimization)确定；对于PerChannelMinMaxObserver来说，其由输入数据的最小值和最大值确定，比如在上述的例子中，值就是-0.7898和-0.7898。既然现在conv weight的min_val，max_val，qmin, qmax 分别为 -0.7898、-0.7898、-128、 127，那如何得到scale和zp呢？PyTorch就是用下面的逻辑进行计算的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#qscheme 是 torch.per_tensor_symmetric 或者torch.per_channel_symmetric时</span><br><span class="line">max_val = torch.max(-min_val, max_val)</span><br><span class="line">scale = max_val / (float(qmax - qmin) / 2)</span><br><span class="line">scale = torch.max(scale, torch.tensor(self.eps, device=device, dtype=scale.dtype))</span><br><span class="line">if self.dtype == torch.quint8:</span><br><span class="line">    zero_point = zero_point.new_full(zero_point.size(), 128)</span><br><span class="line"></span><br><span class="line">#qscheme 是 torch.per_tensor_affine时</span><br><span class="line">scale = (max_val - min_val) / float(qmax - qmin)</span><br><span class="line">scale = torch.max(scale, torch.tensor(self.eps, device=device, dtype=scale.dtype))</span><br><span class="line">zero_point = qmin - torch.round(min_val / scale)</span><br><span class="line">zero_point = torch.max(zero_point, torch.tensor(qmin, device=device, dtype=zero_point.dtype))</span><br><span class="line">zero_point = torch.min(zero_point, torch.tensor(qmax, device=device, dtype=zero_point.dtype))</span><br></pre></td></tr></table></figure><p>由此conv2d weight的谜团就被我们解开了：</p><ul><li><p>scale = 0.7898 / ((127 + 128)/2 ) = 0.0062</p></li><li><p>zp = 0<br>再说说QuantStub的scale和zp是如何计算的。QuantStub使用的是HistogramObserver，根据输入从[-3,3]的分布，HistogramObserver计算得到min_val、max_val分别是-3、2.9971，而qmin和qmax又分别是0、127，其schema为per_tensor_affine，因此套用上面的per_tensor_affine逻辑可得：</p></li><li><p>scale = (2.9971 + 3) / (127 - 0) = 0.0472</p></li><li><p>zp = 0 - round(-3 /0.0472) = 64</p></li></ul><p>其它计算同理，不再赘述。有了scale和zp，就有了量化版本的module，上面那个CivilNet网络，经过静态量化后，网络的变化如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#原始的CivilNet网络：</span><br><span class="line">CivilNet(</span><br><span class="line">  (conv): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">  (fc): Linear(in_features=3, out_features=2, bias=False)</span><br><span class="line">  (relu): ReLU()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">#静态量化后的CivilNet网络：</span><br><span class="line">CivilNet(</span><br><span class="line">  (conv): QuantizedConv2d(1, 1, kernel_size=(1, 1), stride=(1, 1), scale=0.0077941399067640305, zero_point=0, bias=False)</span><br><span class="line">  (fc): QuantizedLinear(in_features=3, out_features=2, scale=0.002811126410961151, zero_point=14, qscheme=torch.per_channel_affine)</span><br><span class="line">  (relu): QuantizedReLU()</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><strong>静态量化模型如何推理？</strong><br>我们知道，在PyTorch的网络中，前向推理逻辑都是实现在了每个op的forward函数中（参考：Gemfield：详解Pytorch中的网络构造）。而在convert完成后，所有的op被替换成了量化版本的op，那么量化版本的op的forward会有什么不一样的呢？还记得吗？动态量化中可是只量化了op的权重哦，输入的量化所需的scale的值是在推理过程中动态计算出来的。而静态量化中，统统都是提前就计算好的。我们来看一个典型的静态量化模型的推理过程：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line">class CivilNet(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(CivilNet, self).__init__()</span><br><span class="line">        in_planes = 1</span><br><span class="line">        out_planes = 1</span><br><span class="line">        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, groups=1, bias=False)</span><br><span class="line">        self.fc = nn.Linear(3, 2,bias=False)</span><br><span class="line">        self.relu = nn.ReLU(inplace=False)</span><br><span class="line">        self.quant = QuantStub()</span><br><span class="line">        self.dequant = DeQuantStub()</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.quant(x)</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.dequant(x)</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure><p>网络forward的开始和结束还必须安插QuantStub和DeQuantStub，如上所示。否则运行时会报错：RuntimeError: Could not run ‘quantized::conv2d.new’ with arguments from the ‘CPU’ backend. ‘quantized::conv2d.new’ is only available for these backends: [QuantizedCPU]。</p><p>QuantStub在observer阶段会记录参数值，DeQuantStub在prepare阶段相当于Identity；而在convert API调用过程中，会分别被替换为nnq.Quantize和nnq.DeQuantize。在这个章节要介绍的推理过程中，QuantStub，也就是nnq.Quantize在做什么工作呢？如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def forward(self, X):</span><br><span class="line">    return torch.quantize_per_tensor(X, float(self.scale), int(self.zero_point), self.dtype)</span><br></pre></td></tr></table></figure><p>是不是呼应了前文中的“tensor的量化”章节？这里的scale和zero_point的计算方式前文也刚介绍过。而nnq.DeQuantize做了什么呢？很简单，把量化tensor反量化回来。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def forward(self, Xq):</span><br><span class="line">    return Xq.dequantize()</span><br></pre></td></tr></table></figure><p>是不是又呼应了前文中的“tensor的量化”章节？我们就以上面的CivilNet网络为例，当在静态量化后的模型进行前向推理和原始的模型的区别是什么呢？假设网络的输入为torch.Tensor([[[[-1,-2,-3],[1,2,3]]]])：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">c = CivilNet()</span><br><span class="line">t = torch.Tensor([[[[-1,-2,-3],[1,2,3]]]])</span><br><span class="line">c(t)</span><br></pre></td></tr></table></figure><p>假设conv的权重为torch.Tensor([[[[-0.7867]]]])，假设fc的权重为torch.Tensor([[ 0.4097, -0.2896, -0.4931], [-0.3738, -0.5541, 0.3243]])，那么在原始的CivilNet前向中，从输入到输出的过程依次为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#input</span><br><span class="line">torch.Tensor([[[[-1,-2,-3],[1,2,3]]]])</span><br><span class="line"></span><br><span class="line">#经过卷积后（权重为torch.Tensor([[[[-0.7867]]]])）</span><br><span class="line">torch.Tensor([[[[ 0.7867,  1.5734,  2.3601],[-0.7867, -1.5734, -2.3601]]]])</span><br><span class="line"></span><br><span class="line">#经过fc后（权重为torch.Tensor([[ 0.4097, -0.2896, -0.4931], [-0.3738, -0.5541,  0.3243]]) )</span><br><span class="line">torch.Tensor([[[[-1.2972, -0.4004], [1.2972,  0.4004]]]])</span><br><span class="line"></span><br><span class="line">#经过relu后</span><br><span class="line">torch.Tensor([[[[0.0000, 0.0000],[1.2972, 0.4004]]]])</span><br></pre></td></tr></table></figure><p>而在静态量化的模型前向中，总体情况如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">#input</span><br><span class="line">torch.Tensor([[[[-1,-2,-3],[1,2,3]]]])</span><br><span class="line"></span><br><span class="line">#QuantStub后 (scale=tensor([0.0472]), zero_point=tensor([64]))</span><br><span class="line">tensor([[[[-0.9916, -1.9833, -3.0221],[ 0.9916,  1.9833,  3.0221]]]],</span><br><span class="line">       dtype=torch.quint8, scale=0.04722102731466293, zero_point=64)</span><br><span class="line"></span><br><span class="line">#经过卷积后（权重为torch.Tensor([[[[-0.7898]]]], dtype=torch.qint8, scale=0.0062, zero_point=0))</span><br><span class="line">#conv activation（输入）的scale为0.03714831545948982，zp为64</span><br><span class="line">torch.Tensor([[[[ 0.7801,  1.5602,  2.3775],[-0.7801, -1.5602, -2.3775]]]], scale=0.03714831545948982, zero_point=64)</span><br><span class="line"></span><br><span class="line">#经过fc后（权重为torch.Tensor([[ 0.4100, -0.2901, -0.4951],[-0.3737, -0.5562,  0.3259]], dtype=torch.qint8, scale=tensor([0.0039, 0.0043]),zero_point=tensor([0, 0])) )</span><br><span class="line">#fc activation（输入）的scale为0.020418135449290276, zp为64</span><br><span class="line">torch.Tensor([[[[-1.3068, -0.3879],[ 1.3068,  0.3879]]]], dtype=torch.quint8, scale=0.020418135449290276, zero_point=64)</span><br><span class="line"></span><br><span class="line">#经过relu后</span><br><span class="line">torch.Tensor([[[[0.0000, 0.0000],[1.3068, 0.3879]]]], dtype=torch.quint8, scale=0.020418135449290276, zero_point=64)</span><br><span class="line"></span><br><span class="line">#经过DeQuantStub后</span><br><span class="line">torch.Tensor([[[[0.0000, 0.0000],[1.3068, 0.3879]]]])</span><br></pre></td></tr></table></figure><p>Gemfield这里用原始的python语句来分步骤来介绍下。首先是QuantStub的工作：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn.quantized as nnq</span><br><span class="line">#输入</span><br><span class="line">&gt;&gt;&gt; x = torch.Tensor([[[[-1,-2,-3],[1,2,3]]]])</span><br><span class="line">&gt;&gt;&gt; x</span><br><span class="line">tensor([[[[-1., -2., -3.],</span><br><span class="line">          [ 1.,  2.,  3.]]]])</span><br><span class="line"></span><br><span class="line">#经过QuantStub</span><br><span class="line">&gt;&gt;&gt; xq = torch.quantize_per_tensor(x, scale = 0.0472, zero_point = 64, dtype=torch.quint8)</span><br><span class="line">&gt;&gt;&gt; xq</span><br><span class="line">tensor([[[[-0.9912, -1.9824, -3.0208],</span><br><span class="line">          [ 0.9912,  1.9824,  3.0208]]]], size=(1, 1, 2, 3),</span><br><span class="line">       dtype=torch.quint8, quantization_scheme=torch.per_tensor_affine,</span><br><span class="line">       scale=0.0472, zero_point=64)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; xq.int_repr()</span><br><span class="line">tensor([[[[ 43,  22,   0],</span><br><span class="line">          [ 85, 106, 128]]]], dtype=torch.uint8)</span><br></pre></td></tr></table></figure><p>我们特意在网络前面安插的QuantStub完成了自己的使命，其scale = 0.0472、zero_point = 64是静态量化完毕后就已经知道的，然后通过quantize_per_tensor调用把输入的float tensor转换为了量化tensor，然后送给接下来的<strong>Conv2d——量化版本的Conv2d</strong>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; c = nnq.Conv2d(1,1,1)</span><br><span class="line">&gt;&gt;&gt; weight = torch.Tensor([[[[-0.7898]]]])</span><br><span class="line">&gt;&gt;&gt; qweight = torch.quantize_per_channel(weight, scales=torch.Tensor([0.0062]).to(torch.double), zero_points = torch.Tensor([0]).to(torch.int64), axis=0, dtype=torch.qint8)</span><br><span class="line">&gt;&gt;&gt; c.set_weight_bias(qweight, None)</span><br><span class="line">&gt;&gt;&gt; c.scale = 0.03714831545948982</span><br><span class="line">&gt;&gt;&gt; c.zero_point = 64</span><br><span class="line">&gt;&gt;&gt; x = c(xq)</span><br><span class="line">&gt;&gt;&gt; x</span><br><span class="line">tensor([[[[ 0.7801,  1.5602,  2.3775],</span><br><span class="line">          [-0.7801, -1.5602, -2.3775]]]], size=(1, 1, 2, 3),</span><br><span class="line">       dtype=torch.quint8, quantization_scheme=torch.per_tensor_affine,</span><br><span class="line">       scale=0.03714831545948982, zero_point=64)</span><br></pre></td></tr></table></figure><p>同理，Conv2d的权重的scale=0.0062、zero_points=0是静态量化完毕就已知的，其activation的scale = 0.03714831545948982、zero_point = 64也是量化完毕已知的。然后送给nnq.Conv2d的forward函数（参考：<a href="https://zhuanlan.zhihu.com/p/53927068%EF%BC%89%EF%BC%8C%E5%85%B6forward%E9%80%BB%E8%BE%91%E4%B8%BA%EF%BC%9A">https://zhuanlan.zhihu.com/p/53927068），其forward逻辑为：</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def forward(self, input):</span><br><span class="line">    return ops.quantized.conv2d(input, self._packed_params, self.scale, self.zero_point)</span><br></pre></td></tr></table></figure><p>Conv2d计算完了，我们停下来反省一下。如果是按照浮点数计算，那么-0.7898 * -0.9912 大约是0.7828，但这里使用int8的计算方式得到的值是0.7801，这说明已经在引入误差了（大约为0.34%的误差）。这也是前面gemfield说的使用fuse_modules可以提高精度的原因，因为每一层都会引入类似的误差。</p><p>后面Linear的计算同理，其forward逻辑为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def forward(self, x):</span><br><span class="line">    return torch.ops.quantized.linear(x, self._packed_params._packed_params, self.scale, self.zero_point)</span><br></pre></td></tr></table></figure><p>可以看到，所有以量化方式计算完的值现在需要经过activation的计算。这是静态量化和动态量化的本质区别之一：op和op之间不再需要转换回到float tensor了。通过上面的分析，我们可以把静态量化模型的前向推理过程概括为如下的形式：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#原始的模型，所有的tensor和计算都是浮点型</span><br><span class="line">previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32</span><br><span class="line">                    /</span><br><span class="line">    linear_weight_fp32</span><br><span class="line"></span><br><span class="line">#静态量化的模型，权重和输入都是int8</span><br><span class="line">previous_layer_int8 -- linear_with_activation_int8 -- next_layer_int8</span><br><span class="line">                    /</span><br><span class="line">  linear_weight_int8</span><br></pre></td></tr></table></figure><p>最后再来描述下动态量化和静态量化的最大区别：</p><ul><li>静态量化的float输入必经QuantStub变为int，此后到输出之前都是int；</li><li>动态量化的float输入是经动态计算的scale和zp量化为int，op输出时转换回float。</li></ul><h3 id="QAT（Quantization-Aware-Training）"><a href="#QAT（Quantization-Aware-Training）" class="headerlink" title="QAT（Quantization Aware Training）"></a>QAT（Quantization Aware Training）</h3><p>前面两种量化方法都有一个post关键字，意思是模型训练完毕后所做的量化。而QAT则不一样，是指在训练过程中就开启了量化功能。</p><p>QAT需要五部曲，说到这里，你可能想到了静态量化，那不妨对比着来看。<br><strong>1，设置qconfig</strong><br>在设置qconfig之前，模型首先设置为训练模式，这很容易理解，因为QAT的着力点就是T嘛：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cnet = CivilNet()</span><br><span class="line">cnet.train()</span><br></pre></td></tr></table></figure><p>使用get_default_qat_qconfig API来给要QAT的网络设置qconfig：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cnet.qconfig = torch.quantization.get_default_qat_qconfig(&#x27;fbgemm&#x27;)</span><br></pre></td></tr></table></figure><p>不过，这个qconfig和静态量化中的可不一样啊。前文说过qconfig维护了两个observer，activation的和权重的。QAT的qconfig中，activation和权重的observer都变成了FakeQuantize（和observer是has a的关系，也即包含一个observer），并且参数不一样（qmin、qmax、schema,dtype,qschema,reduce_range这些参数），如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#activation的observer的参数</span><br><span class="line">FakeQuantize.with_args(observer=MovingAverageMinMaxObserver,quant_min=0,quant_max=255,reduce_range=True)</span><br><span class="line"></span><br><span class="line">#权重的observer的参数</span><br><span class="line">FakeQuantize.with_args(observer=MovingAveragePerChannelMinMaxObserver,</span><br><span class="line">                                                               quant_min=-128,</span><br><span class="line">                                                               quant_max=127,</span><br><span class="line">                                                               dtype=torch.qint8,</span><br><span class="line">                                                               qscheme=torch.per_channel_symmetric,</span><br><span class="line">                                                               reduce_range=False,</span><br><span class="line">                                                               ch_axis=0)</span><br></pre></td></tr></table></figure><p>这里FakeQuantize包含的observer是MovingAverageMinMaxObserver，继承自前面提到过的MinMaxObserver，但是求最小值和最大值的方法有点区别，使用的是如下公式：</p><p><img src="/img/2021/0110/6.jpg" alt="&quot;CPU Memory&quot;"></p><ul><li>Xmin、Xmax是当前运行中正在求解和最终求解的最小值、最大值；</li><li>X是当前输入的tensor；</li><li>c是一个常数，PyTorch中默认为0.01，也就是最新一次的极值由上一次贡献99%，当前的tensor贡献1%。</li></ul><p>MovingAverageMinMaxObserver在求min、max的方式和其基类MinMaxObserver有所区别之外，scale和zero_points的计算则是一致的。那么在包含了上述的observer之后，FakeQuantize的作用又是什么呢？看下面的步骤。</p><p><strong>2，fuse_modules</strong><br>和静态量化一样，不再赘述。</p><p><strong>3，prepare_qat</strong><br>在静态量化中，我们这一步使用的是prepare API，而在QAT这里使用的是prepare_qat API。最重要的区别有两点：</p><ul><li>prepare_qat要把qconfig安插到每个op上，qconfig的内容本身就不同，参考五部曲中的第一步；</li><li>prepare_qat 中需要多做一步转换子module的工作，需要inplace的把模型中的一些子module替换了，替换的逻辑就是从DEFAULT_QAT_MODULE_MAPPINGS的key替换为value，这个字典的定义如下：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># Default map for swapping float module to qat modules</span><br><span class="line">DEFAULT_QAT_MODULE_MAPPINGS : Dict[Callable, Any] = &#123;</span><br><span class="line">    nn.Conv2d: nnqat.Conv2d,</span><br><span class="line">    nn.Linear: nnqat.Linear,</span><br><span class="line">    # Intrinsic modules:</span><br><span class="line">    nni.ConvBn1d: nniqat.ConvBn1d,</span><br><span class="line">    nni.ConvBn2d: nniqat.ConvBn2d,</span><br><span class="line">    nni.ConvBnReLU1d: nniqat.ConvBnReLU1d,</span><br><span class="line">    nni.ConvBnReLU2d: nniqat.ConvBnReLU2d,</span><br><span class="line">    nni.ConvReLU2d: nniqat.ConvReLU2d,</span><br><span class="line">    nni.LinearReLU: nniqat.LinearReLU</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>因此，同静态量化的prepare相比，prepare_qat在多插入fake_quants、又替换了nn.Conv2d、nn.Linear之后，CivilNet网络就被改成了如下的样子：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">CivilNet(</span><br><span class="line">  (conv): QATConv2d(</span><br><span class="line">    1, 1, kernel_size=(1, 1), stride=(1, 1), bias=False</span><br><span class="line">    (activation_post_process): FakeQuantize(</span><br><span class="line">      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8),            scale=tensor([1.]), zero_point=tensor([0])</span><br><span class="line">      (activation_post_process): MovingAverageMinMaxObserver(min_val=tensor([]), max_val=tensor([]))</span><br><span class="line">    )</span><br><span class="line">    (weight_fake_quant): FakeQuantize(</span><br><span class="line">      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8),            scale=tensor([1.]), zero_point=tensor([0])</span><br><span class="line">      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (fc): QATLinear(</span><br><span class="line">    in_features=3, out_features=2, bias=False</span><br><span class="line">    (activation_post_process): FakeQuantize(</span><br><span class="line">      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8),            scale=tensor([1.]), zero_point=tensor([0])</span><br><span class="line">      (activation_post_process): MovingAverageMinMaxObserver(min_val=tensor([]), max_val=tensor([]))</span><br><span class="line">    )</span><br><span class="line">    (weight_fake_quant): FakeQuantize(</span><br><span class="line">      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8),            scale=tensor([1.]), zero_point=tensor([0])</span><br><span class="line">      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (relu): ReLU(</span><br><span class="line">    (activation_post_process): FakeQuantize(</span><br><span class="line">      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8),            scale=tensor([1.]), zero_point=tensor([0])</span><br><span class="line">      (activation_post_process): MovingAverageMinMaxObserver(min_val=tensor([]), max_val=tensor([]))</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (quant): QuantStub(</span><br><span class="line">    (activation_post_process): FakeQuantize(</span><br><span class="line">      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8),            scale=tensor([1.]), zero_point=tensor([0])</span><br><span class="line">      (activation_post_process): MovingAverageMinMaxObserver(min_val=tensor([]), max_val=tensor([]))</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (dequant): DeQuantStub()</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li></ul><p><strong>4，喂数据</strong><br>和静态量化完全不同，在QAT中这一步是用来训练的。我们知道，在PyTorch的网络中，前向推理逻辑都是实现在了每个op的forward函数中（参考：Gemfield：详解Pytorch中的网络构造）。而在prepare_qat中，所有的op被替换成了QAT版本的op，那么这些op的forward函数有什么特别的地方呢？</p><p>Conv2d被替换为了QATConv2d：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def forward(self, input):</span><br><span class="line">   return self.activation_post_process(self._conv_forward(input, self.weight_fake_quant(self.weight)))</span><br></pre></td></tr></table></figure><p>Linear被替换为了QATLinear:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def forward(self, input):</span><br><span class="line">    return self.activation_post_process(F.linear(input, self.weight_fake_quant(self.weight), self.bias))</span><br></pre></td></tr></table></figure><p>ReLU还是那个ReLU，不说了。总之，你可以看出来，每个op的输入都需要经过self.weight_fake_quant来处理下，输出又都需要经过self.activation_post_process来处理下，这两个都是FakeQuantize的实例，只是里面包含的observer不一样。以Conv2d为例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#conv2d</span><br><span class="line">weight=functools.partial(&lt;class &#x27;torch.quantization.fake_quantize.FakeQuantize&#x27;&gt;,</span><br><span class="line">           observer=&lt;class &#x27;torch.quantization.observer.MovingAveragePerChannelMinMaxObserver&#x27;&gt;,</span><br><span class="line">           quant_min=-128, quant_max=127, dtype=torch.qint8,</span><br><span class="line">           qscheme=torch.per_channel_symmetric, reduce_range=False, ch_axis=0))</span><br><span class="line"></span><br><span class="line">activation=functools.partial(&lt;class &#x27;torch.quantization.fake_quantize.FakeQuantize&#x27;&gt;,</span><br><span class="line">            observer=&lt;class &#x27;torch.quantization.observer.MovingAverageMinMaxObserver&#x27;&gt;,</span><br><span class="line">            quant_min=0, quant_max=255, reduce_range=True)</span><br></pre></td></tr></table></figure><p>而FakeQuantize的forward函数如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def forward(self, X):</span><br><span class="line">        if self.observer_enabled[0] == 1:</span><br><span class="line">            #使用移动平均算法计算scale和zp</span><br><span class="line"></span><br><span class="line">        if self.fake_quant_enabled[0] == 1:</span><br><span class="line">            X = torch.fake_quantize_per_channel_or_tensor_affine(X...)</span><br><span class="line">        return X</span><br></pre></td></tr></table></figure><p>FakeQuantize中的fake_quantize_per_channel_or_tensor_affine实现了quantize和dequantize，用公式表示的话为：out = (clamp(round(x/scale + zero_point), quant_min, quant_max)-zero_point)*scale。也就是说，这是把量化的误差引入到了训练loss之中呀！</p><p>这样，在QAT中，所有的weights和activations就像上面那样被fake quantized了，且参与模型训练中的前向和反向计算。float值被round成了（用来模拟的）int8值，但是所有的计算仍然是通过float来完成的。 这样以来，所有的权重在优化过程中都能感知到量化带来的影响，称之为量化感知训练（支持cpu和cuda），精度也因此更高。</p><p><strong>5，转换</strong><br>这一步和静态量化一样，不再赘述。需要注意的是，QAT中，有一些module在prepare中已经转换成新的module了，所以静态量化中所使用的字典包含有如下的条目：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DEFAULT_STATIC_QUANT_MODULE_MAPPINGS = &#123;</span><br><span class="line">    ......</span><br><span class="line">    # QAT modules:</span><br><span class="line">    nnqat.Linear: nnq.Linear,</span><br><span class="line">    nnqat.Conv2d: nnq.Conv2d,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>总结下来就是：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 原始的模型，所有的tensor和计算都是浮点</span><br><span class="line">previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32</span><br><span class="line">                      /</span><br><span class="line">    linear_weight_fp32</span><br><span class="line"></span><br><span class="line"># 训练过程中，fake_quants发挥作用</span><br><span class="line">previous_layer_fp32 -- fq -- linear_fp32 -- activation_fp32 -- fq -- next_layer_fp32</span><br><span class="line">                           /</span><br><span class="line">   linear_weight_fp32 -- fq</span><br><span class="line"></span><br><span class="line"># 量化后的模型进行推理，权重和输入都是int8</span><br><span class="line">previous_layer_int8 -- linear_with_activation_int8 -- next_layer_int8</span><br><span class="line">                     /</span><br><span class="line">   linear_weight_int8</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>那么如何更方便的在你的代码中使用PyTorch的量化功能呢？一个比较优雅的方式就是使用deepvac规范——这是一个定义了PyTorch工程标准的项目：<a href="https://github.com/DeepVAC/deepvac">https://github.com/DeepVAC/deepvac</a></p><p>原文链接: <a href="https://zhuanlan.zhihu.com/p/299108528">PyTorch</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;背景：&lt;/strong&gt;&lt;br&gt;在深度学习中，量化指的是使用更少的bit来存储原本以浮点数存储的tensor，以及使用更少的bit来完成原本以浮点数完成的计算。这么做的好处主要有如下几点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;更少的模型体积，接近4倍的减少；&lt;/li&gt;
&lt;li&gt;可以更快的计算，由于更少的内存访问和更快的int8计算，可以快2~4倍。&lt;br&gt;一个量化后的模型，其部分或者全部的tensor操作会使用int类型来计算，而不是使用量化之前的float类型。当然，量化还需要底层硬件支持，x86 CPU（支持AVX2）、ARM CPU、Google TPU、Nvidia Volta/Turing/Ampere、Qualcomm DSP这些主流硬件都对量化提供了支持。</summary>
    
    
    
    <category term="技术总结" scheme="http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="pytorch" scheme="http://huangzhiyuan.github.io/tags/pytorch/"/>
    
    <category term="quantization" scheme="http://huangzhiyuan.github.io/tags/quantization/"/>
    
  </entry>
  
  <entry>
    <title>简要解读内存性能</title>
    <link href="http://huangzhiyuan.github.io/2021/01/10/memory-brief-introduction/"/>
    <id>http://huangzhiyuan.github.io/2021/01/10/memory-brief-introduction/</id>
    <published>2021-01-10T03:11:53.000Z</published>
    <updated>2021-01-10T03:25:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>一台服务器，不管是物理机还是虚拟机，必不可少的就是内存，内存的性能又是如何来衡量呢。</p><p><img src="/img/2021/0110/4.jpg" alt="&quot;CPU Memory&quot;"></p><span id="more"></span><h2 id="内存与缓存"><a href="#内存与缓存" class="headerlink" title="内存与缓存"></a>内存与缓存</h2><p>现在比较新的CPU一般都有三级缓存，<code>L1 Cache</code>（32KB-256KB），<code>L2 Cache</code>（128KB-2MB），<code>L3 Cache</code>（1M-32M）。缓存逐渐变大，CPU在取数据的时候，优先从缓存去取数据，取不到才去内存取数据。</p><h2 id="内存与时延"><a href="#内存与时延" class="headerlink" title="内存与时延"></a>内存与时延</h2><p>显然，越靠近CPU，取数据的速度越块，通过LMBench进行了读数延迟的测试。<br><img src="/img/2021/0110/1.png" alt="&quot;CPU Memory&quot;"></p><p>从上图可以看出：</p><ol><li>Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz 这款CPU的L1D Cache，L1I Cache为32KB，而L2 Cache为1M，L3为32M；</li><li>在对应的Cache中，时延是稳定的；</li><li>不同缓存的时延呈现指数级增长；</li></ol><p>所以我们在写业务代码的时候，如果想要更快地提高效率，那么使得计算更加贴近CPU则可以获取更好的性能。但是从上图也可以看出，内存的时延都是纳秒为单位，而实际业务中都是毫秒为单位，优化的重点应该是那些以毫秒为单位的运算，而内存时延优化这块则是长尾部分。</p><h2 id="内存带宽"><a href="#内存带宽" class="headerlink" title="内存带宽"></a>内存带宽</h2><p>内存时延与缓存其实可谓是紧密相关，不理解透彻了，则可能测的是缓存时延。同样测试内存带宽，如果不是正确的测试，则测的是缓存带宽了。<br>为了了解内存带宽，有必要去了解下内存与CPU的架构，早期的CPU与内存的架构还需要经过北桥总线，现在CPU与内存直接已经不需要北桥，直接通过CPU的内存控制器（IMC）进行内存读取操作：</p><p><img src="/img/2021/0110/5.jpg" alt="&quot;CPU arch&quot;"></p><p>那对应的内存带宽是怎样的呢？测试内存带宽有很多很多工具，linux下一般通过stream进行测试。简单介绍下stream的算法：</p><p><img src="/img/2021/0110/2.png" alt="&quot;stream algorithm&quot;"></p><p>stream算法的原理从上图可以看出非常简单：某个内存块之间的数据读取出来，经过简单的运算放入另一个内存块。那所谓的内存带宽：内存带宽=搬运的内存大小/耗时。通过整机合理的测试，可以测出来内存控制器的带宽。下图是某云产品的内存带宽数据：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">-------------------------------------------------------------</span><br><span class="line">Function    Best Rate MB/s  Avg time     Min time     Max time</span><br><span class="line">Copy:          128728.5     0.134157     0.133458     0.136076</span><br><span class="line">Scale:         128656.4     0.134349     0.133533     0.137638</span><br><span class="line">Add:           144763.0     0.178851     0.178014     0.181158</span><br><span class="line">Triad:         144779.8     0.178717     0.177993     0.180214</span><br><span class="line">-------------------------------------------------------------</span><br></pre></td></tr></table></figure><p>内存带宽的重要性自然不言而喻，这意味着操作内存的最大数据吞吐量。但是正确合理的测试非常重要，有几个注意事项需要关注：</p><ol><li>内存数组大小的设置，必须要远大于L3 Cache的大小，否则就是测试缓存的吞吐性能；</li><li>CPU数目很有关系，一般来说，一两个核的计算能力，是远远到不了内存带宽的，整机的CPU全部运行起来，才可以有效地测试内存带宽。当然跑单核的stream测试也有意义，可以测试内存的延时。</li></ol><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ol><li>内存与NUMA的关系：开启NUMA，可以有效地提供内存的吞吐性能，降低内存时延。</li><li>stream算法的编译方法选择：通过icc编译，可以有效地提供内存带宽性能分。原因是Intel优化了CPU的指令，通过指令向量化和指令Prefetch操作，加速了数据的读写操作以及指令操作。当然其他C代码都可以通过icc编译的方法，提供指令的效率。</li></ol><p>原文链接：<a href="https://developer.aliyun.com/article/675905">阿里云技术</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;一台服务器，不管是物理机还是虚拟机，必不可少的就是内存，内存的性能又是如何来衡量呢。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/2021/0110/4.jpg&quot; alt=&quot;&amp;quot;CPU Memory&amp;quot;&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="技术总结" scheme="http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="Memory" scheme="http://huangzhiyuan.github.io/tags/Memory/"/>
    
  </entry>
  
  <entry>
    <title>容器、docker、虚拟机</title>
    <link href="http://huangzhiyuan.github.io/2021/01/10/docker-info/"/>
    <id>http://huangzhiyuan.github.io/2021/01/10/docker-info/</id>
    <published>2021-01-10T02:36:31.000Z</published>
    <updated>2021-01-10T03:03:54.000Z</updated>
    
    <content type="html"><![CDATA[<p>容器技术起源于Linux，是一种内核虚拟化技术，提供轻量级的虚拟化，以便隔离进程和资源。尽管容器技术已经出现很久，却是随着Docker的出现而变得广为人知。Docker是第一个使容器能在不同机器之间移植的系统。它不仅简化了打包应用的流程，也简化了打包应用的库和依赖，甚至整个操作系统的文件系统能被打包成一个简单的可移植的包，这个包可以被用来在任何其他运行Docker的机器上使用。</p><p>容器和虚拟机具有相似的资源隔离和分配方式，容器虚拟化了操作系统而不是硬件，更加便携和高效。</p><span id="more"></span><p><img src="/img/2021/0110/1.jpg" alt="&quot;docker and virtual machine&quot;"></p><h2 id="docker优势"><a href="#docker优势" class="headerlink" title="docker优势"></a>docker优势</h2><p>相比于使用虚拟机，容器有如下优点：</p><ul><li><strong>更高效的利用系统资源</strong><br>由于容器不需要进行硬件虚拟以及运行完整操作系统等额外开销，容器对系统资源的利用率更高。无论是应用执行速度、内存损耗或者文件存储速度，都要比传统虚拟机技术更高效。因此，相比虚拟机技术，一个相同配置的主机，往往可以运行更多数量的应用。</li><li><strong>更快速的启动时间</strong><br>传统的虚拟机技术启动应用服务往往需要数分钟，而Docker容器应用，由于直接运行于宿主内核，无需启动完整的操作系统，因此可以做到秒级、甚至毫秒级的启动时间，大大节约了开发、测试、部署的时间。</li><li><strong>一致的运行环境</strong><br>开发过程中一个常见的问题是环境一致性问题。由于开发环境、测试环境、生产环境不一致，导致有些问题并未在开发过程中被发现。而Docker的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性。</li><li><strong>更轻松的迁移</strong><br>由于Docker确保了执行环境的一致性，使得应用的迁移更加容易。Docker可以在很多平台上运行，无论是物理机、虚拟机，其运行结果是一致的。因此可以很轻易的将在一个平台上运行的应用，迁移到另一个平台上，而不用担心运行环境的变化导致应用无法正常运行的情况。</li><li><strong>更轻松的维护和扩展</strong><br>Docker使用的分层存储以及镜像的技术，使得应用重复部分的复用更为容易，也使得应用的维护更新更加简单，基于基础镜像进一步扩展镜像也变得非常简单。此外，Docker团队同各个开源项目团队一起维护了大批高质量的官方镜像，既可以直接在生产环境使用，又可以作为基础进一步定制，大大的降低了应用服务的镜像制作成本。</li></ul><h2 id="Docker容器典型使用流程"><a href="#Docker容器典型使用流程" class="headerlink" title="Docker容器典型使用流程"></a>Docker容器典型使用流程</h2><h3 id="docker容器概念"><a href="#docker容器概念" class="headerlink" title="docker容器概念"></a>docker容器概念</h3><p>Docker容器有如下三个主要概念：</p><ul><li><strong>镜像：</strong>Docker镜像里包含了已打包的应用程序及其所依赖的环境。它包含应用程序可用的文件系统和其他元数据，如镜像运行时的可执行文件路径。</li><li><strong>镜像仓库：</strong>Docker镜像仓库用于存放Docker镜像，以及促进不同人和不同电脑之间共享这些镜像。当编译镜像时，要么可以在编译它的电脑上运行，要么可以先上传镜像到一个镜像仓库，然后下载到另外一台电脑上并运行它。某些仓库是公开的，允许所有人从中拉取镜像，同时也有一些是私有的，仅部分人和机器可接入。</li><li><strong>容器：</strong>Docker容器通常是一个Linux容器，它基于Docker镜像被创建。一个运行中的容器是一个运行在Docker主机上的进程，但它和主机，以及所有运行在主机上的其他进程都是隔离的。这个进程也是资源受限的，意味着它只能访问和使用分配给它的资源（CPU、内存等）。</li></ul><p>典型的使用流程如图2所示：<br><img src="/img/2021/0110/2.jpg" alt="&quot;docker use&quot;"></p><p>（1）首先开发者在开发环境机器上开发应用并制作镜像。<br>Docker执行命令，构建镜像并存储在机器上。</p><p>（2）开发者发送上传镜像命令。<br>Docker收到命令后，将本地镜像上传到镜像仓库。</p><p>（3）开发者向生产环境机器发送运行镜像命令。<br>生产环境机器收到命令后，Docker会从镜像仓库拉取镜像到机器上，然后基于镜像运行容器。</p><h3 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h3><p>下面使用Docker将基于Nginx镜像打包一个容器镜像，并基于容器镜像运行应用，然后推送到容器镜像仓库。<br><strong>安装Docker</strong><br>Docker几乎支持在所有操作系统上安装，用户可以根据需要选择要安装的Docker版本。<br>在Linux操作系统下，可以使用如下命令快速安装Docker。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -fsSL get.docker.com -o get-docker.sh</span><br><span class="line">sh get-docker.sh</span><br></pre></td></tr></table></figure><p>说明：CentOS 8.0操作系统使用上述脚本安装Docker会出现问题，建议使用如下命令安装较低版本Docker。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget -O /etc/yum.repos.d/docker-ce.repo https://repo.huaweicloud.com/docker-ce/linux/centos/docker-ce.repo sudo</span><br><span class="line">sed -i &#x27;s+download.docker.com+http://repo.huaweicloud.com/docker-ce+&#x27; /etc/yum.repos.d/docker-ce.repo</span><br><span class="line">yum install docker-ce-18.06.3.ce -y</span><br><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure><h3 id="Docker打包镜像"><a href="#Docker打包镜像" class="headerlink" title="Docker打包镜像"></a>Docker打包镜像</h3><p>Docker提供了一种便捷的描述应用打包的方式，叫做Dockerfile，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 使用官方提供的Nginx镜像作为基础镜像</span><br><span class="line">FROM nginx:alpine</span><br><span class="line"></span><br><span class="line"># 执行一条命令修改Nginx镜像index.html的内容</span><br><span class="line">RUN echo &quot;hello world&quot; &gt; /usr/share/nginx/html/index.html</span><br><span class="line"></span><br><span class="line"># 允许外界访问容器的80端口</span><br><span class="line">EXPOSE 80</span><br></pre></td></tr></table></figure><p>执行docker build命令打包镜像。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -t hello .</span><br></pre></td></tr></table></figure><p>其中-t表示给镜像加一个标签，也就是给镜像取名，这里镜像名为hello。. 表示在当前目录下执行该打包命令。</p><p>执行docker images命令查看镜像，可以看到hello镜像已经创建成功。您还可以看到一个Nginx镜像，这个镜像是从镜像仓库下载下来的，作为hello镜像的基础镜像使用。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">hello               latest              d120ec16dcea        17 minutes ago      158MB</span><br><span class="line">nginx               alpine              eeb27ee6b893        2 months ago        148MB</span><br></pre></td></tr></table></figure><p><strong>本地运行容器镜像</strong><br>有了镜像后，您可以在本地执行docker run命令运行容器镜像。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># docker run -p 8080:80 hello</span><br></pre></td></tr></table></figure><p>docker run命令会启动一个容器，命令中-p是将本地机器的8080端口映射到容器的80端口，即本地机器的8080端口的流量会映射到容器的80端口，当您在本地机器访问 <a href="http://127.0.0.1:8080时，就会访问到容器中，此时浏览器中返回的内容应该就是“hello">http://127.0.0.1:8080时，就会访问到容器中，此时浏览器中返回的内容应该就是“hello</a> world”。</p><p>把镜像推送到镜像仓库</p><p>华为云提供了容器镜像服务SWR，您也可以将镜像上传到SWR，下面演示如何将镜像推送到SWR。详细的方法请参见客户端上传镜像，本文档后续的示例中将主要使用SWR作为示例。</p><p>首先登录SWR控制台，在左侧选择“我的镜像”，然后单击右侧“客户端上传镜像”，在弹出的窗口中单击“生成临时登录指令”，然后复制该指令在本地机器上执行，登录到SWR镜像仓库。</p><p><img src="/img/2021/0110/3.jpg" alt="&quot;docker use&quot;"></p><p>上传镜像前需要给镜像取一个完整的名称，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># docker tag hello swr.cn-east-3.myhuaweicloud.com/container/hello:v1</span><br></pre></td></tr></table></figure><p>这里<a href="http://swr.cn-east-3.myhuaweicloud.com是仓库地址,每个华为云区域的地址不同,v1则是hello镜像分配的版本号./">http://swr.cn-east-3.myhuaweicloud.com是仓库地址，每个华为云区域的地址不同，v1则是hello镜像分配的版本号。</a></p><p><a href="http://swr.cn-east-3.myhuaweicloud.com是仓库地址,每个华为云区域的地址不同./">http://swr.cn-east-3.myhuaweicloud.com是仓库地址，每个华为云区域的地址不同。</a><br>container是组织名，组织一般在SWR中创建，如果没有创建则首次上传的时候会自动创建，组织名在单个区域内全局唯一，需要选择合适的组织名称。<br>v1则是hello镜像分配的版本号。</p><p>然后执行docker push命令就可以将镜像上传到SWR。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># docker push swr.cn-east-3.myhuaweicloud.com/container/hello:v1</span><br></pre></td></tr></table></figure><p>当需要使用该镜像时，使用docker pull命令拉取（下载）该命令即可。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># docker pull swr.cn-east-3.myhuaweicloud.com/container/hello:v1</span><br></pre></td></tr></table></figure><p>原文链接：<a href="https://zhuanlan.zhihu.com/p/271846374">华为云技术</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;容器技术起源于Linux，是一种内核虚拟化技术，提供轻量级的虚拟化，以便隔离进程和资源。尽管容器技术已经出现很久，却是随着Docker的出现而变得广为人知。Docker是第一个使容器能在不同机器之间移植的系统。它不仅简化了打包应用的流程，也简化了打包应用的库和依赖，甚至整个操作系统的文件系统能被打包成一个简单的可移植的包，这个包可以被用来在任何其他运行Docker的机器上使用。&lt;/p&gt;
&lt;p&gt;容器和虚拟机具有相似的资源隔离和分配方式，容器虚拟化了操作系统而不是硬件，更加便携和高效。&lt;/p&gt;</summary>
    
    
    
    <category term="技术总结" scheme="http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="Docker" scheme="http://huangzhiyuan.github.io/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Linux IO原理解析和zero-copy技术揭秘</title>
    <link href="http://huangzhiyuan.github.io/2021/01/07/Linux-IO-and-zero-copy-tech/"/>
    <id>http://huangzhiyuan.github.io/2021/01/07/Linux-IO-and-zero-copy-tech/</id>
    <published>2021-01-07T13:38:01.000Z</published>
    <updated>2021-01-07T14:56:40.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/img/2021/0107/1.jpg" alt="&quot;linux-zero-copy&quot;"></p><p>原文链接：<a href="https://strikefreedom.top/linux-io-and-zero-copy">https://strikefreedom.top/linux-io-and-zero-copy</a></p><p>早就想写篇关于介绍IO操作的技术总结，直到今天看到这篇文章介绍的如此逻辑清晰，也把自己之前掌握的知识串了起来，读完收获颇多。这里就做个搬运工，原文请参见上面链接。<br>如今的网络应用早已从 CPU 密集型转向了 I/O 密集型，网络服务器大多是基于 C-S 模型，也即 客户端 - 服务端 模型，客户端需要和服务端进行大量的网络通信，这也决定了现代网络应用的性能瓶颈：I/O。</p><span id="more"></span><p>传统的 Linux 操作系统的标准 I/O 接口是基于数据拷贝操作的，即 I/O 操作会导致数据在操作系统内核地址空间的缓冲区和用户进程地址空间定义的缓冲区之间进行传输。设置缓冲区最大的好处是可以减少磁盘 I/O 的操作，如果所请求的数据已经存放在操作系统的高速缓冲存储器中，那么就不需要再进行实际的物理磁盘 I/O 操作；然而传统的 Linux I/O 在数据传输过程中的数据拷贝操作深度依赖 CPU，也就是说 I/O 过程需要 CPU 去执行数据拷贝的操作，因此导致了极大的系统开销，限制了操作系统有效进行数据传输操作的能力。<br>I/O 是决定网络服务器性能瓶颈的关键，而传统的 Linux I/O 机制又会导致大量的数据拷贝操作，损耗性能，所以我们亟需一种新的技术来解决数据大量拷贝的问题，这个答案就是零拷贝(Zero-copy)。</p><h2 id="计算机存储器"><a href="#计算机存储器" class="headerlink" title="计算机存储器"></a>计算机存储器</h2><p>既然要分析 Linux I/O，就不能不了解计算机的各类存储器。<br>存储器是计算机的核心部件之一，在完全理想的状态下，存储器应该要同时具备以下三种特性：</p><ul><li>速度足够快：存储器的存取速度应当快于 CPU 执行一条指令，这样 CPU 的效率才不会受限于存储器容量足够大：</li><li>容量能够存储计算机所需的全部数据价格足够便宜：</li><li>价格低廉，所有类型的计算机都能配备<br>但是现实往往是残酷的，我们目前的计算机技术无法同时满足上述的三个条件，于是现代计算机的存储器设计采用了一种分层次的结构：</li></ul><p><img src="/img/2021/0107/2.jpg" alt="&quot;memory-hierachy&quot;"></p><p>从顶至底，现代计算机里的存储器类型分别有：寄存器、高速缓存、主存和磁盘，这些存储器的速度逐级递减而容量逐级递增。存取速度最快的是寄存器，因为寄存器的制作材料和 CPU 是相同的，所以速度和 CPU 一样快，CPU 访问寄存器是没有时延的，然而因为价格昂贵，因此容量也极小，一般 32 位的 CPU 配备的寄存器容量是 32 * 32 Bit，64 位的 CPU 则是 64 * 64 Bit，不管是 32 位还是 64 位，寄存器容量都小于 1 KB，且寄存器也必须通过软件自行管理。</p><p>第二层是高速缓存，也即我们平时了解的 CPU 高速缓存 L1、L2、L3，一般 L1 是每个 CPU 独享，L3 是全部 CPU 共享，而 L2 则根据不同的架构设计会被设计成独享或者共享两种模式之一，比如 Intel 的多核芯片采用的是共享 L2 模式而 AMD 的多核芯片则采用的是独享 L2 模式。</p><p>第三层则是主存，也即主内存，通常称作随机访问存储器（Random Access Memory, RAM）。是与 CPU 直接交换数据的内部存储器。它可以随时读写（刷新时除外），而且速度很快，通常作为操作系统或其他正在运行中的程序的临时资料存储介质。</p><p>第三层则是主存，也即主内存，通常称作随机访问存储器（Random Access Memory, RAM）。是与 CPU 直接交换数据的内部存储器。它可以随时读写（刷新时除外），而且速度很快，通常作为操作系统或其他正在运行中的程序的临时资料存储介质。</p><p><strong>主内存是操作系统进行 I/O 操作的重中之重，绝大部分的工作都是在用户进程和内核的内存缓冲区里完成的，因此我们接下来需要提前学习一些主存的相关原理。</strong></p><h3 id="物理内存"><a href="#物理内存" class="headerlink" title="物理内存"></a>物理内存</h3><p>我们平时一直提及的物理内存就是上文中对应的第三种计算机存储器，RAM 主存，它在计算机中以内存条的形式存在，嵌在主板的内存槽上，用来加载各式各样的程序与数据以供 CPU 直接运行和使用。</p><h3 id="虚拟内存"><a href="#虚拟内存" class="headerlink" title="虚拟内存"></a>虚拟内存</h3><p>在计算机领域有一句如同摩西十诫般神圣的哲言：”<strong>计算机科学领域的任何问题都可以通过增加一个间接的中间层来解决</strong>“，从内存管理、网络模型、并发调度甚至是硬件架构，都能看到这句哲言在闪烁着光芒，而虚拟内存则是这一哲言的完美实践之一。</p><p>虚拟内存是现代计算机中的一个非常重要的存储器抽象，主要是用来解决应用程序日益增长的内存使用需求：现代物理内存的容量增长已经非常快速了，然而还是跟不上应用程序对主存需求的增长速度，对于应用程序来说内存还是不够用，因此便需要一种方法来解决这两者之间的容量差矛盾。</p><p>计算机对多程序内存访问的管理经历了 <code>静态重定位</code> –&gt; <code>动态重定位</code> –&gt; <code>交换(swapping)技术</code> –&gt; <code>虚拟内存</code>，最原始的多程序内存访问是直接访问绝对内存地址，这种方式几乎是完全不可用的方案，因为如果每一个程序都直接访问物理内存地址的话，比如两个程序并发执行以下指令的时候：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mov cx, 2</span><br><span class="line">mov bx, 1000H</span><br><span class="line">mov ds, bx</span><br><span class="line">mov [0], cx</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">mov ax, [0]</span><br><span class="line">add ax, ax</span><br></pre></td></tr></table></figure><p>这一段汇编表示在地址 1000:0 处存入数值 2，然后在后面的逻辑中把该地址的值取出来乘以 2，最终存入 ax 寄存器的值就是 4，如果第二个程序存入 cx 寄存器里的值是 3，那么并发执行的时候，第一个程序最终从 ax 寄存器里得到的值就可能是 6，这就完全错误了，得到脏数据还顶多算程序结果错误，要是其他程序往特定的地址里写入一些危险的指令而被另一个程序取出来执行，还可能会导致整个系统的崩溃。所以，为了确保进程间互不干扰，每一个用户进程都需要实时知晓当前其他进程在使用哪些内存地址，这对于写程序的人来说无疑是一场噩梦。</p><p>因此，操作绝对内存地址是完全不可行的方案，那就只能用操作相对内存地址，我们知道每个进程都会有自己的进程地址，从 0 开始，可以通过相对地址来访问内存，但是这同样有问题，还是前面类似的问题，比如有两个大小为 16KB 的程序 A 和 B，现在它们都被加载进了内存，内存地址段分别是 0 ~ 16384，16384 ~ 32768。A 的第一条指令是 jmp 1024，而在地址 1024 处是一条 mov 指令，下一条指令是 add，基于前面的 mov 指令做加法运算，与此同时，B 的第一条指令是 jmp 1028，本来在 B 的相对地址 1028 处应该也是一条 <code>mov</code> 去操作自己的内存地址上的值，但是由于这两个程序共享了段寄存器，因此虽然他们使用了各自的相对地址，但是依然操作的还是绝对内存地址，于是 B 就会跳去执行 <code>add</code> 指令，这时候就会因为非法的内存操作而 crash。</p><p>有一种<code>静态重定位</code>的技术可以解决这个问题，它的工作原理非常简单粗暴：当 B 程序被加载到地址 16384 处之后，把 B 的所有相对内存地址都加上 16384，这样的话当 B 执行 jmp 1028 之时，其实执行的是 <code>jmp 1028+16384</code>，就可以跳转到正确的内存地址处去执行正确的指令了，但是这种技术并不通用，而且还会对程序装载进内存的性能有影响。</p><p>再往后，就发展出来了存储器抽象：地址空间，就好像进程是 CPU 的抽象，地址空间则是存储器的抽象，每个进程都会分配独享的地址空间，但是独享的地址空间又带来了新的问题：如何实现不同进程的相同相对地址指向不同的物理地址？最开始是使用动态重定位技术来实现，这是用一种相对简单的地址空间到物理内存的映射方法。基本原理就是为每一个 CPU 配备两个特殊的硬件寄存器：基址寄存器和界限寄存器，用来动态保存每一个程序的起始物理内存地址和长度，比如前文中的 A，B 两个程序，当 A 运行时基址寄存器和界限寄存器就会分别存入 0 和 16384，而当 B 运行时则两个寄存器又会分别存入 16384 和 32768。然后每次访问指定的内存地址时，CPU 会在把地址发往内存总线之前自动把基址寄存器里的值加到该内存地址上，得到一个真正的物理内存地址，同时还会根据界限寄存器里的值检查该地址是否溢出，若是，则产生错误中止程序，动态重定位技术解决了静态重定位技术造成的程序装载速度慢的问题，但是也有新问题：每次访问内存都需要进行加法和比较运算，比较运算本身可以很快，但是加法运算由于进位传递时间的问题，除非使用特殊的电路，否则会比较慢。</p><p>然后就是 <code>交换（swapping）</code>技术，这种技术简单来说就是动态地把程序在内存和磁盘之间进行交换保存，要运行一个进程的时候就把程序的代码段和数据段调入内存，然后再把程序封存，存入磁盘，如此反复。为什么要这么麻烦？因为前面那两种重定位技术的前提条件是计算机内存足够大，能够把所有要运行的进程地址空间都加载进主存，才能够并发运行这些进程，但是现实往往不是如此，内存的大小总是有限的，所有就需要另一类方法来处理内存超载的情况，第一种便是简单的交换技术：</p><p><img src="/img/2021/0107/3.jpg" alt="&quot;swapping&quot;"></p><p>先把进程 A 换入内存，然后启动进程 B 和 C，也换入内存，接着 A 被从内存交换到磁盘，然后又有新的进程 D 调入内存，用了 A 退出之后空出来的内存空间，最后 A 又被重新换入内存，由于内存布局已经发生了变化，所以 A 在换入内存之时会通过软件或者在运行期间通过硬件（基址寄存器和界限寄存器）对其内存地址进行重定位，多数情况下都是通过硬件。</p><p>另一种处理内存超载的技术就是虚拟内存技术了，它比交换（swapping）技术更复杂而又更高效，是目前最新应用最广泛的存储器抽象技术：</p><p>虚拟内存的核心原理是：为每个程序设置一段”连续”的虚拟地址空间，把这个地址空间分割成多个具有连续地址范围的页 (page)，并把这些页和物理内存做映射，在程序运行期间动态映射到物理内存。当程序引用到一段在物理内存的地址空间时，由硬件立刻执行必要的映射；而当程序引用到一段不在物理内存中的地址空间时，由操作系统负责将缺失的部分装入物理内存并重新执行失败的指令：</p><p><img src="/img/2021/0107/4.jpg" alt="&quot;virtual-memory&quot;"></p><p>虚拟地址空间按照固定大小划分成被称为页（page）的若干单元，物理内存中对应的则是页框（page frame）。这两者一般来说是一样的大小，如上图中的是 4KB，不过实际上计算机系统中一般是 512 字节到 1 GB，这就是虚拟内存的分页技术。因为是虚拟内存空间，每个进程分配的大小是 4GB (32 位架构)，而实际上当然不可能给所有在运行中的进程都分配 4GB 的物理内存，所以虚拟内存技术还需要利用到前面介绍的交换（swapping）技术，在进程运行期间只分配映射当前使用到的内存，暂时不使用的数据则写回磁盘作为副本保存，需要用的时候再读入内存，动态地在磁盘和内存之间交换数据。</p><p>其实虚拟内存技术从某种角度来看的话，很像是糅合了基址寄存器和界限寄存器之后的新技术。它使得整个进程的地址空间可以通过较小的单元映射到物理内存，而不需要为程序的代码和数据地址进行重定位。</p><p>进程在运行期间产生的内存地址都是虚拟地址，如果计算机没有引入虚拟内存这种存储器抽象技术的话，则 CPU 会把这些地址直接发送到内存地址总线上，直接访问和虚拟地址相同值的物理地址；如果使用虚拟内存技术的话，CPU 则是把这些虚拟地址通过地址总线送到内存管理单元（Memory Management Unit，MMU），MMU 将虚拟地址映射为物理地址之后再通过内存总线去访问物理内存：</p><p><img src="/img/2021/0107/5.jpg" alt="&quot;mmu&quot;"></p><p>虚拟地址（比如 16 位地址 8196=0010 000000000100）分为两部分：虚拟页号（高位部分）和偏移量（低位部分），虚拟地址转换成物理地址是通过页表（page table）来实现的，页表由页表项构成，页表项中保存了页框号、修改位、访问位、保护位和 “在/不在” 位等信息，从数学角度来说页表就是一个函数，入参是虚拟页号，输出是物理页框号，得到物理页框号之后复制到寄存器的高三位中，最后直接把 12 位的偏移量复制到寄存器的末 12 位构成 15 位的物理地址，即可以把该寄存器的存储的物理内存地址发送到内存总线：</p><p><img src="/img/2021/0107/6.jpg" alt="&quot;virtual&quot;"></p><p>在 MMU 进行地址转换时，如果页表项的 “在/不在” 位是 0，则表示该页面并没有映射到真实的物理页框，则会引发一个<code>缺页中断</code>，CPU 陷入操作系统内核，接着操作系统就会通过页面置换算法选择一个页面将其换出 (swap)，以便为即将调入的新页面腾出位置，如果要换出的页面的页表项里的修改位已经被设置过，也就是被更新过，则这是一个脏页 (dirty page)，需要写回磁盘更新改页面在磁盘上的副本，如果该页面是”干净”的，也就是没有被修改过，则直接用调入的新页面覆盖掉被换出的旧页面即可。</p><p>最后，还需要了解的一个概念是转换检测缓冲器（Translation Lookaside Buffer，TLB），也叫快表，是用来加速虚拟地址映射的，因为虚拟内存的分页机制，页表一般是保存内存中的一块固定的存储区，导致进程通过 MMU 访问内存比直接访问内存多了一次内存访问，性能至少下降一半，因此需要引入加速机制，即 TLB 快表，TLB 可以简单地理解成页表的高速缓存，保存了最高频被访问的页表项，由于一般是硬件实现的，因此速度极快，MMU收到虚拟地址时一般会先通过硬件 TLB 查询对应的页表号，若命中且该页表项的访问操作合法，则直接从 TLB 取出对应的物理页框号返回，若不命中则穿透到内存页表里查询，并且会用这个从内存页表里查询到最新页表项替换到现有 TLB 里的其中一个，以备下次缓存命中。</p><p>至此，我们介绍完了包含虚拟内存在内的多项计算机存储器抽象技术，虚拟内存的其他内容比如针对大内存的多级页表、倒排页表，以及处理缺页中断的页面置换算法等等，以后有机会再单独写一篇文章介绍，或者各位读者也可以先行去查阅相关资料了解，这里就不再深入了。</p><h2 id="用户态和内核态"><a href="#用户态和内核态" class="headerlink" title="用户态和内核态"></a>用户态和内核态</h2><p>一般来说，我们在编写程序操作 Linux I/O 之时十有八九是在用户空间和内核空间之间传输数据，因此有必要先了解一下 Linux 的用户态和内核态的概念。</p><p>首先是用户态和内核态：</p><p><img src="/img/2021/0107/7.jpg" alt="&quot;kernel&quot;"></p><p>从宏观上来看，Linux 操作系统的体系架构分为用户态和内核态（或者用户空间和内核）。内核从本质上看是一种软件 —— 控制计算机的硬件资源，并提供上层应用程序 (进程) 运行的环境。用户态即上层应用程序 (进程) 的运行空间，应用程序 (进程) 的执行必须依托于内核提供的资源，这其中包括但不限于 CPU 资源、存储资源、I/O 资源等等。</p><p>现代操作系统都是采用虚拟存储器，那么对 32 位操作系统而言，它的寻址空间（虚拟存储空间）为 2^32 B = 4G。操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核（kernel），保证内核的安全，操心系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。针对 Linux 操作系统而言，将最高的 1G 字节（从虚拟地址 0xC0000000 到 0xFFFFFFFF），供内核使用，称为内核空间，而将较低的 3G 字节（从虚拟地址 0x00000000 到 0xBFFFFFFF），供各个进程使用，称为用户空间。</p><p>因为操作系统的资源是有限的，如果访问资源的操作过多，必然会消耗过多的系统资源，而且如果不对这些操作加以区分，很可能造成资源访问的冲突。所以，为了减少有限资源的访问和使用冲突，Unix/Linux 的设计哲学之一就是：对不同的操作赋予不同的执行等级，就是所谓特权的概念。简单说就是有多大能力做多大的事，与系统相关的一些特别关键的操作必须由最高特权的程序来完成。Intel 的 x86 架构的 CPU 提供了 0 到 3 四个特权级，数字越小，特权越高，Linux 操作系统中主要采用了 0 和 3 两个特权级，分别对应的就是内核态和用户态。运行于用户态的进程可以执行的操作和访问的资源都会受到极大的限制，而运行在内核态的进程则可以执行任何操作并且在资源的使用上没有限制。很多程序开始时运行于用户态，但在执行的过程中，一些操作需要在内核权限下才能执行，这就涉及到一个从用户态切换到内核态的过程。比如 C 函数库中的内存分配函数 malloc()，它具体是使用 sbrk() 系统调用来分配内存，当 malloc 调用 sbrk() 的时候就涉及一次从用户态到内核态的切换，类似的函数还有 printf()，调用的是 wirte() 系统调用来输出字符串，等等。</p><p>用户进程在系统中运行时，大部分时间是处在用户态空间里的，在其需要操作系统帮助完成一些用户态没有特权和能力完成的操作时就需要切换到内核态。那么用户进程如何切换到内核态去使用那些内核资源呢？答案是：1) 系统调用（trap），2) 异常（exception）和 3) 中断（interrupt）。</p><ul><li><strong>系统调用：</strong>用户进程主动发起的操作。用户态进程发起系统调用主动要求切换到内核态，陷入内核之后，由操作系统来操作系统资源，完成之后再返回到进程。</li><li><strong>异常：</strong>被动的操作，且用户进程无法预测其发生的时机。当用户进程在运行期间发生了异常（比如某条指令出了问题），这时会触发由当前运行进程切换到处理此异常的内核相关进程中，也即是切换到了内核态。异常包括程序运算引起的各种错误如除 0、缓冲区溢出、缺页等。</li><li><strong>中断：</strong>当外围设备完成用户请求的操作后，会向 CPU 发出相应的中断信号，这时 CPU 会暂停执行下一条即将要执行的指令而转到与中断信号对应的处理程序去执行，如果前面执行的指令是用户态下的程序，那么转换的过程自然就会是从用户态到内核态的切换。中断包括 I/O 中断、外部信号中断、各种定时器引起的时钟中断等。中断和异常类似，都是通过中断向量表来找到相应的处理程序进行处理。区别在于，中断来自处理器外部，不是由任何一条专门的指令造成，而异常是执行当前指令的结果。</li></ul><p>通过上面的分析，我们可以得出 Linux 的内部层级可分为三大部分：</p><ul><li>用户空间；</li><li>内核空间；</li><li>硬件。</li></ul><p><img src="/img/2021/0107/8.jpg" alt="&quot;kernel-space-user-space&quot;"></p><h2 id="Linux-I-O"><a href="#Linux-I-O" class="headerlink" title="Linux I/O"></a>Linux I/O</h2><h3 id="I-O-缓冲区"><a href="#I-O-缓冲区" class="headerlink" title="I/O 缓冲区"></a>I/O 缓冲区</h3><p><img src="/img/2021/0107/9.jpg" alt="&quot;IO-Buffer&quot;"></p><p>在 Linux 中，当程序调用各类文件操作函数后，用户数据（User Data）到达磁盘（Disk）的流程如上图所示。</p><p>图中描述了 Linux 中文件操作函数的层级关系和内存缓存层的存在位置，中间的黑色实线是用户态和内核态的分界线。</p><p>read(2)/write(2) 是 Linux 系统中最基本的 I/O 读写系统调用，我们开发操作 I/O 的程序时必定会接触到它们，而在这两个系统调用和真实的磁盘读写之间存在一层称为 Kernel buffer cache 的缓冲区缓存。在 Linux 中 I/O 缓存其实可以细分为两个：Page Cache 和 Buffer Cache，这两个其实是一体两面，共同组成了 Linux 的内核缓冲区（Kernel Buffer Cache）：</p><ul><li><strong>读磁盘：</strong>内核会先检查 Page Cache 里是不是已经缓存了这个数据，若是，直接从这个内存缓冲区里读取返回，若否，则穿透到磁盘去读取，然后再缓存在 Page Cache 里，以备下次缓存命中；</li><li><strong>写磁盘：</strong>内核直接把数据写入 Page Cache，并把对应的页标记为 dirty，添加到 dirty list 里，然后就直接返回，内核会定期把 dirty list 的页缓存 flush 到磁盘，保证页缓存和磁盘的最终一致性。</li></ul><p>Page Cache 会通过页面置换算法如 LRU 定期淘汰旧的页面，加载新的页面。可以看出，所谓 I/O 缓冲区缓存就是在内核和磁盘、网卡等外设之间的一层缓冲区，用来提升读写性能的。</p><p>在 Linux 还不支持虚拟内存技术之前，还没有页的概念，因此 Buffer Cache 是基于操作系统读写磁盘的最小单位 – 块（block）来进行的，所有的磁盘块操作都是通过 Buffer Cache 来加速，Linux 引入虚拟内存的机制来管理内存后，页成为虚拟内存管理的最小单位，因此也引入了 Page Cache 来缓存 Linux 文件内容，主要用来作为文件系统上的文件数据的缓存，提升读写性能，常见的是针对文件的 read()/write() 操作，另外也包括了通过 mmap() 映射之后的块设备，也就是说，事实上 Page Cache 负责了大部分的块设备文件的缓存工作。而 Buffer Cache 用来在系统对块设备进行读写的时候，对块进行数据缓存的系统来使用，实际上负责所有对磁盘的 I/O 访问：</p><p><img src="/img/2021/0107/10.jpg" alt="&quot;buffer cache &amp; page cache&quot;"></p><p>因为 Buffer Cache 是对粒度更细的设备块的缓存，而 Page Cache 是基于虚拟内存的页单元缓存，因此还是会基于 Buffer Cache，也就是说如果是缓存文件内容数据就会在内存里缓存两份相同的数据，这就会导致同一份文件保存了两份，冗余且低效。另外一个问题是，调用 write 后，有效数据是在 Buffer Cache 中，而非 Page Cache 中。这就导致 mmap 访问的文件数据可能存在不一致问题。为了规避这个问题，所有基于磁盘文件系统的 write，都需要调用 update_vm_cache() 函数，该操作会把调用 write 之后的 Buffer Cache 更新到 Page Cache 去。由于有这些设计上的弊端，因此在 Linux 2.4 版本之后，kernel 就将两者进行了统一，Buffer Cache 不再以独立的形式存在，而是以融合的方式存在于 Page Cache 中：</p><p><img src="/img/2021/0107/11.jpg" alt="&quot;buffer cache in page cache&quot;"></p><p>融合之后就可以统一操作 Page Cache 和 Buffer Cache：处理文件 I/O 缓存交给 Page Cache，而当底层 RAW device 刷新数据时以 Buffer Cache 的块单位来实际处理。</p><h2 id="I-O-模式"><a href="#I-O-模式" class="headerlink" title="I/O 模式"></a>I/O 模式</h2><p>在 Linux 或者其他 Unix-like 操作系统里，I/O 模式一般有三种：</p><ul><li>程序控制 I/O</li><li>中断驱动 I/O</li><li>DMA I/O</li></ul><h3 id="程序控制-I-O"><a href="#程序控制-I-O" class="headerlink" title="程序控制 I/O"></a>程序控制 I/O</h3><p>这是最简单的一种 I/O 模式，也叫忙等待或者轮询：用户通过发起一个系统调用，陷入内核态，内核将系统调用翻译成一个对应设备驱动程序的过程调用，接着设备驱动程序会启动 I/O 不断循环去检查该设备，看看是否已经就绪，一般通过返回码来表示，I/O 结束之后，设备驱动程序会把数据送到指定的地方并返回，切回用户态。<br>比如发起系统调用 read()：</p><p><img src="/img/2021/0107/12.jpg" alt="&quot;io-model&quot;"></p><h3 id="中断驱动-I-O"><a href="#中断驱动-I-O" class="headerlink" title="中断驱动 I/O"></a>中断驱动 I/O</h3><p>第二种 I/O 模式是利用中断来实现的：<br><img src="/img/2021/0107/13.jpg" alt="&quot;io-interrupt&quot;"></p><p>流程如下：</p><ol><li>用户进程发起一个 read() 系统调用读取磁盘文件，陷入内核态并由其所在的 CPU 通过设备驱动程序向设备寄存器写入一个通知信号，告知设备控制器 (我们这里是磁盘控制器)要读取数据；</li><li>磁盘控制器启动磁盘读取的过程，把数据从磁盘拷贝到磁盘控制器缓冲区里；</li><li>完成拷贝之后磁盘控制器会通过总线发送一个中断信号到中断控制器，如果此时中断控制器手头还有正在处理的中断或者有一个和该中断信号同时到达的更高优先级的中断，则这个中断信号将被忽略，而磁盘控制器会在后面持续发送中断信号直至中断控制器受理；</li><li>中断控制器收到磁盘控制器的中断信号之后会通过地址总线存入一个磁盘设备的编号，表示这次中断需要关注的设备是磁盘；</li><li>中断控制器向 CPU 置起一个磁盘中断信号；</li><li>CPU 收到中断信号之后停止当前的工作，把当前的 PC/PSW 等寄存器压入堆栈保存现场，然后从地址总线取出设备编号，通过编号找到中断向量所包含的中断服务的入口地址，压入 PC 寄存器，开始运行磁盘中断服务，把数据从磁盘控制器的缓冲区拷贝到主存里的内核缓冲区；</li><li>最后 CPU 再把数据从内核缓冲区拷贝到用户缓冲区，完成读取操作，read() 返回，切换回用户态。</li></ol><h3 id="DMA-I-O"><a href="#DMA-I-O" class="headerlink" title="DMA I/O"></a>DMA I/O</h3><p>并发系统的性能高低究其根本，是取决于如何对 CPU 资源的高效调度和使用，而回头看前面的中断驱动 I/O 模式的流程，可以发现第 6、7 步的数据拷贝工作都是由 CPU 亲自完成的，也就是在这两次数据拷贝阶段中 CPU 是完全被占用而不能处理其他工作的，那么这里明显是有优化空间的；第 7 步的数据拷贝是从内核缓冲区到用户缓冲区，都是在主存里，所以这一步只能由 CPU 亲自完成，但是第 6 步的数据拷贝，是从磁盘控制器的缓冲区到主存，是两个设备之间的数据传输，这一步并非一定要 CPU 来完成，可以借助 DMA 来完成，减轻 CPU 的负担。</p><p>DMA 全称是 Direct Memory Access，也即直接存储器存取，是一种用来提供在外设和存储器之间或者存储器和存储器之间的高速数据传输。整个过程无须 CPU 参与，数据直接通过 DMA 控制器进行快速地移动拷贝，节省 CPU 的资源去做其他工作。</p><p>目前，大部分的计算机都配备了 DMA 控制器，而 DMA 技术也支持大部分的外设和存储器。借助于 DMA 机制，计算机的 I/O 过程就能更加高效：<br><img src="/img/2021/0107/14.jpg" alt="&quot;io-DMA&quot;"></p><p>DMA 控制器内部包含若干个可以被 CPU 读写的寄存器：一个主存地址寄存器 MAR（存放要交换数据的主存地址）、一个外设地址寄存器 ADR（存放 I/O 设备的设备码，或者是设备信息存储区的寻址信息）、一个字节数寄存器 WC（对传送数据的总字数进行统计）、和一个或多个控制寄存器。</p><ol><li>用户进程发起一个 read() 系统调用读取磁盘文件，陷入内核态并由其所在的 CPU 通过设置 DMA 控制器的寄存器对它进行编程：把内核缓冲区和磁盘文件的地址分别写入 MAR 和 ADR 寄存器，然后把期望读取的字节数写入 WC 寄存器，启动 DMA 控制器；</li><li>DMA 控制器根据 ADR 寄存器里的信息知道这次 I/O 需要读取的外设是磁盘的某个地址，便向磁盘控制器发出一个命令，通知它从磁盘读取数据到其内部的缓冲区里；</li><li>磁盘控制器启动磁盘读取的过程，把数据从磁盘拷贝到磁盘控制器缓冲区里，并对缓冲区内数据的校验和进行检验，如果数据是有效的，那么 DMA 就可以开始了；</li><li>DMA 控制器通过总线向磁盘控制器发出一个读请求信号从而发起 DMA 传输，这个信号和前面的中断驱动 I/O 小节里 CPU 发给磁盘控制器的读请求是一样的，它并不知道或者并不关心这个读请求是来自 CPU 还是 DMA 控制器；</li><li>紧接着 DMA 控制器将引导磁盘控制器将数据传输到 MAR 寄存器里的地址，也就是内核缓冲区；</li><li>数据传输完成之后，返回一个 ack 给 DMA 控制器，WC 寄存器里的值会减去相应的数据长度，如果 WC 还不为 0，则重复第 4 步到第 6 步，一直到 WC 里的字节数等于 0；</li><li>收到 ack 信号的 DMA 控制器会通过总线发送一个中断信号到中断控制器，如果此时中断控制器手头还有正在处理的中断或者有一个和该中断信号同时到达的更高优先级的中断，则这个中断信号将被忽略，而 DMA 控制器会在后面持续发送中断信号直至中断控制器受理；</li><li>中断控制器收到磁盘控制器的中断信号之后会通过地址总线存入一个主存设备的编号，表示这次中断需要关注的设备是主存；</li><li>中断控制器向 CPU 置起一个 DMA 中断的信号；</li><li>CPU 收到中断信号之后停止当前的工作，把当前的 PC/PSW 等寄存器压入堆栈保存现场，然后从地址总线取出设备编号，通过编号找到中断向量所包含的中断服务的入口地址，压入 PC 寄存器，开始运行 DMA 中断服务，把数据从内核缓冲区拷贝到用户缓冲区，完成读取操作，read() 返回，切换回用户态。</li></ol><h3 id="传统-I-O-读写模式"><a href="#传统-I-O-读写模式" class="headerlink" title="传统 I/O 读写模式"></a>传统 I/O 读写模式</h3><p>Linux 中传统的 I/O 读写是通过 read()/write() 系统调用完成的，read() 把数据从存储器 (磁盘、网卡等) 读取到用户缓冲区，write() 则是把数据从用户缓冲区写出到存储器：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;unistd.h&gt;</span><br><span class="line"></span><br><span class="line">ssize_t read(int fd, void *buf, size_t count);</span><br><span class="line">ssize_t write(int fd, const void *buf, size_t count);</span><br></pre></td></tr></table></figure><p>一次完整的读磁盘文件然后写出到网卡的底层传输过程如下：<br><img src="/img/2021/0107/15.jpg" alt="&quot;IO read wirte&quot;"></p><p>可以清楚看到这里一共触发了 4 次用户态和内核态的上下文切换，分别是 read()/write() 调用和返回时的切换，2 次 DMA 拷贝，2 次 CPU 拷贝，加起来一共 4 次拷贝操作。</p><p>通过引入 DMA，我们已经把 Linux 的 I/O 过程中的 CPU 拷贝次数从 4 次减少到了 2 次，但是 CPU 拷贝依然是代价很大的操作，对系统性能的影响还是很大，特别是那些频繁 I/O 的场景，更是会因为 CPU 拷贝而损失掉很多性能，我们需要进一步优化，降低、甚至是完全避免 CPU 拷贝。</p><h2 id="零拷贝-Zero-copy"><a href="#零拷贝-Zero-copy" class="headerlink" title="零拷贝 (Zero-copy)"></a>零拷贝 (Zero-copy)</h2><p><strong>Zero-copy 是什么？</strong><br>Wikipedia 的解释如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;Zero-copy&quot; describes computer operations in which the CPU does not perform the task of copying data from one memory area to another. This is frequently used to save CPU cycles and memory bandwidth when transmitting a file over a network.</span><br></pre></td></tr></table></figure><p>零拷贝技术是指计算机执行操作时，CPU不需要先将数据从某处内存复制到另一个特定区域。这种技术通常用于通过网络传输文件时节省CPU周期和内存带宽。</p><p><strong>Zero-copy 能做什么？</strong></p><ul><li>减少甚至完全避免操作系统内核和用户应用程序地址空间这两者之间进行数据拷贝操作，从而减少用户态 – 内核态上下文切换带来的系统开销。</li><li>减少甚至完全避免操作系统内核缓冲区之间进行数据拷贝操作。</li><li>帮助用户进程绕开操作系统内核空间直接访问硬件存储接口操作数据。</li><li>利用 DMA 而非 CPU 来完成硬件接口和内核缓冲区之间的数据拷贝，从而解放 CPU，使之能去执行其他的任务，提升系统性能。</li></ul><p><strong>Zero-copy 的实现方式有哪些？</strong><br>从 zero-copy 这个概念被提出以来，相关的实现技术便犹如雨后春笋，层出不穷。但是截至目前为止，并没有任何一种 zero-copy 技术能满足所有的场景需求，还是计算机领域那句无比经典的名言：”There is no silver bullet”!<br>而在 Linux 平台上，同样也有很多的 zero-copy 技术，新旧各不同，可能存在于不同的内核版本里，很多技术可能有了很大的改进或者被更新的实现方式所替代，这些不同的实现技术按照其核心思想可以归纳成大致的以下三类：</p><ul><li><strong>减少甚至避免用户空间和内核空间之间的数据拷贝：</strong>在一些场景下，用户进程在数据传输过程中并不需要对数据进行访问和处理，那么数据在 Linux 的 Page Cache 和用户进程的缓冲区之间的传输就完全可以避免，让数据拷贝完全在内核里进行，甚至可以通过更巧妙的方式避免在内核里的数据拷贝。这一类实现一般是通过增加新的系统调用来完成的，比如 Linux 中的 mmap()，sendfile() 以及 splice() 等。</li><li><strong>绕过内核的直接 I/O：</strong>允许在用户态进程绕过内核直接和硬件进行数据传输，内核在传输过程中只负责一些管理和辅助的工作。这种方式其实和第一种有点类似，也是试图避免用户空间和内核空间之间的数据传输，只是第一种方式是把数据传输过程放在内核态完成，而这种方式则是直接绕过内核和硬件通信，效果类似但原理完全不同。</li><li><strong>内核缓冲区和用户缓冲区之间的传输优化：</strong>这种方式侧重于在用户进程的缓冲区和操作系统的页缓存之间的 CPU 拷贝的优化。这种方法延续了以往那种传统的通信方式，但更灵活。</li></ul><p><strong>减少甚至避免用户空间和内核空间之间的数据拷贝</strong><br><code>mmap()</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;sys/mman.h&gt;</span><br><span class="line"></span><br><span class="line">void *mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset);</span><br><span class="line">int munmap(void *addr, size_t length);</span><br></pre></td></tr></table></figure><p>一种简单的实现方案是在一次读写过程中用 Linux 的另一个系统调用 mmap() 替换原先的 read()，mmap() 也即是内存映射（memory map）：把用户进程空间的一段内存缓冲区（user buffer）映射到文件所在的内核缓冲区（kernel buffer）上。</p><p><img src="/img/2021/0107/16.jpg" alt="&quot;IO mmap &amp; write&quot;"></p><ol><li>利用 mmap() 替换 read()，配合 write() 调用的整个流程如下： 用户进程调用 mmap()，从用户态陷入内核态，将内核缓冲区映射到用户缓存区；</li><li>DMA 控制器将数据从硬盘拷贝到内核缓冲区；</li><li> mmap() 返回，上下文从内核态切换回用户态；</li><li> 用户进程调用 write()，尝试把文件数据写到内核里的套接字缓冲区，再次陷入内核态；</li><li> CPU 将内核缓冲区中的数据拷贝到的套接字缓冲区；</li><li> DMA 控制器将数据从套接字缓冲区拷贝到网卡完成数据传输；</li><li> write() 返回，上下文从内核态切换回用户态。</li></ol><p>通过这种方式，有两个优点：一是节省内存空间，因为用户进程上的这一段内存是虚拟的，并不真正占据物理内存，只是映射到文件所在的内核缓冲区上，因此可以节省一半的内存占用；二是省去了一次 CPU 拷贝，对比传统的 Linux I/O 读写，数据不需要再经过用户进程进行转发了，而是直接在内核里就完成了拷贝。所以使用 mmap() 之后的拷贝次数是 2 次 DMA 拷贝，1 次 CPU 拷贝，加起来一共 3 次拷贝操作，比传统的 I/O 方式节省了一次 CPU 拷贝以及一半的内存，不过因为 mmap() 也是一个系统调用，因此用户态和内核态的切换还是 4 次。</p><p>mmap() 因为既节省 CPU 拷贝次数又节省内存，所以比较适合大文件传输的场景。虽然 mmap() 完全是符合 POSIX 标准的，但是它也不是完美的，因为它并不总是能达到理想的数据传输性能。首先是因为数据数据传输过程中依然需要一次 CPU 拷贝，其次是内存映射技术是一个开销很大的虚拟存储操作：这种操作需要修改页表以及用内核缓冲区里的文件数据汰换掉当前 TLB 里的缓存以维持虚拟内存映射的一致性。但是，因为内存映射通常针对的是相对较大的数据区域，所以对于相同大小的数据来说，内存映射所带来的开销远远低于 CPU 拷贝所带来的开销。此外，使用 mmap() 还可能会遇到一些需要值得关注的特殊情况，例如，在 mmap() –&gt; write() 这两个系统调用的整个传输过程中，如果有其他的进程突然截断了这个文件，那么这时用户进程就会因为访问非法地址而被一个从总线传来的 SIGBUS 中断信号杀死并且产生一个 core dump。有两种解决办法：</p><ol><li>设置一个信号处理器，专门用来处理 SIGBUS 信号，这个处理器直接返回， write() 就可以正常返回已写入的字节数而不会被 SIGBUS 中断，errno 错误码也会被设置成 success。然而这实际上是一个掩耳盗铃的解决方案，因为 BIGBUS 信号的带来的信息是系统发生了一些很严重的错误，而我们却选择忽略掉它，一般不建议采用这种方式。</li><li>通过内核的文件租借锁（这是 Linux 的叫法，Windows 上称之为机会锁）来解决这个问题，这种方法相对来说更好一些。我们可以通过内核对文件描述符上读/写的租借锁，当另外一个进程尝试对当前用户进程正在进行传输的文件进行截断的时候，内核会发送给用户一个实时信号：RT_SIGNAL_LEASE 信号，这个信号会告诉用户内核正在破坏你加在那个文件上的读/写租借锁，这时 write() 系统调用会被中断，并且当前用户进程会被 SIGBUS 信号杀死，返回值则是中断前写的字节数，errno 同样会被设置为 success。文件租借锁需要在对文件进行内存映射之前设置，最后在用户进程结束之前释放掉。</li></ol><p><strong>sendfile()</strong><br>在 Linux 内核 2.1 版本中，引入了一个新的系统调用 sendfile()：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;sys/sendfile.h&gt;</span><br><span class="line"></span><br><span class="line">ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);</span><br></pre></td></tr></table></figure><p>从功能上来看，这个系统调用将 mmap() + write() 这两个系统调用合二为一，实现了一样效果的同时还简化了用户接口，其他的一些 Unix-like 的系统像 BSD、Solaris 和 AIX 等也有类似的实现，甚至 Windows 上也有一个功能类似的 API 函数 TransmitFile。</p><p>out_fd 和 in_fd 分别代表了写入和读出的文件描述符，in_fd 必须是一个指向文件的文件描述符，且要能支持类 mmap() 内存映射，不能是 Socket 类型，而 out_fd 在 Linux 内核 2.6.33 版本之前只能是一个指向 Socket 的文件描述符，从 2.6.33 之后则可以是任意类型的文件描述符。off_t 是一个代表了 in_fd 偏移量的指针，指示 sendfile() 该从 in_fd 的哪个位置开始读取，函数返回后，这个指针会被更新成 sendfile() 最后读取的字节位置处，表明此次调用共读取了多少文件数据，最后的 count 参数则是此次调用需要传输的字节总数。</p><p><img src="/img/2021/0107/17.jpg" alt="&quot;IO sendfile&quot;"></p><p>使用 sendfile() 完成一次数据读写的流程如下：</p><ol><li>用户进程调用 sendfile() 从用户态陷入内核态；</li><li>DMA 控制器将数据从硬盘拷贝到内核缓冲区；</li><li>CPU 将内核缓冲区中的数据拷贝到套接字缓冲区；</li><li>DMA 控制器将数据从套接字缓冲区拷贝到网卡完成数据传输；</li><li>sendfile() 返回，上下文从内核态切换回用户态。</li></ol><p>基于 sendfile()， 整个数据传输过程中共发生 2 次 DMA 拷贝和 1 次 CPU 拷贝，这个和 mmap() + write() 相同，但是因为 sendfile() 只是一次系统调用，因此比前者少了一次用户态和内核态的上下文切换开销。读到这里，聪明的读者应该会开始提问了：”sendfile() 会不会遇到和 mmap() + write() 相似的文件截断问题呢？”，很不幸，答案是肯定的。sendfile() 一样会有文件截断的问题，但欣慰的是，sendfile() 不仅比 mmap() + write() 在接口使用上更加简洁，而且处理文件截断时也更加优雅：如果 sendfile() 过程中遭遇文件截断，则 sendfile() 系统调用会被中断杀死之前返回给用户进程其中断前所传输的字节数，errno 会被设置为 success，无需用户提前设置信号处理器，当然你要设置一个进行个性化处理也可以，也不需要像之前那样提前给文件描述符设置一个租借锁，因为最终结果还是一样的。</p><p>sendfile() 相较于 mmap() 的另一个优势在于数据在传输过程中始终没有越过用户态和内核态的边界，因此极大地减少了存储管理的开销。即便如此，sendfile() 依然是一个适用性很窄的技术，最适合的场景基本也就是一个静态文件服务器了。而且根据 Linus 在 2001 年和其他内核维护者的邮件列表内容，其实当初之所以决定在 Linux 上实现 sendfile() 仅仅是因为在其他操作系统平台上已经率先实现了，而且有大名鼎鼎的 Apache Web 服务器已经在使用了，为了兼容 Apache Web 服务器才决定在 Linux 上也实现这个技术，而且 sendfile() 实现上的简洁性也和 Linux 内核的其他部分集成得很好，所以 Linus 也就同意了这个提案。</p><p>然而 sendfile() 本身是有很大问题的，从不同的角度来看的话主要是：</p><ol><li>首先一个是这个接口并没有进行标准化，导致 sendfile() 在 Linux 上的接口实现和其他类 Unix 系统的实现并不相同；</li><li>其次由于网络传输的异步性，很难在接收端实现和 sendfile() 对接的技术，因此接收端一直没有实现对应的这种技术；</li><li>最后从性能方面考量，因为 sendfile() 在把磁盘文件从内核缓冲区（page cache）传输到到套接字缓冲区的过程中依然需要 CPU 参与，这就很难避免 CPU 的高速缓存被传输的数据所污染。</li></ol><p>此外，需要说明下，sendfile() 的最初设计并不是用来处理大文件的，因此如果需要处理很大的文件的话，可以使用另一个系统调用 sendfile64()，它支持对更大的文件内容进行寻址和偏移。</p><p><strong>sendﬁle() with DMA Scatter/Gather Copy</strong></p><p>上一小节介绍的 sendfile() 技术已经把一次数据读写过程中的 CPU 拷贝的降低至只有 1 次了，但是人永远是贪心和不知足的，现在如果想要把这仅有的一次 CPU 拷贝也去除掉，有没有办法呢？<br>当然有！通过引入一个新硬件上的支持，我们可以把这个仅剩的一次 CPU 拷贝也给抹掉：Linux 在内核 2.4 版本里引入了 DMA 的 scatter/gather – 分散/收集功能，并修改了 sendfile() 的代码使之和 DMA 适配。scatter 使得 DMA 拷贝可以不再需要把数据存储在一片连续的内存空间上，而是允许离散存储，gather 则能够让 DMA 控制器根据少量的元信息：一个包含了内存地址和数据大小的缓冲区描述符，收集存储在各处的数据，最终还原成一个完整的网络包，直接拷贝到网卡而非套接字缓冲区，避免了最后一次的 CPU 拷贝：</p><p><img src="/img/2021/0107/18.jpg" alt="&quot;IO sendfile with DMA gather&quot;"></p><p>sendfile() + DMA gather 的数据传输过程如下：</p><ol><li>用户进程调用 sendfile()，从用户态陷入内核态；</li><li>DMA 控制器使用 scatter 功能把数据从硬盘拷贝到内核缓冲区进行离散存储；</li><li>CPU 把包含内存地址和数据长度的缓冲区描述符拷贝到套接字缓冲区，DMA 控制器能够根据这些信息生成网络包数据分组的报头和报尾</li><li>DMA 控制器根据缓冲区描述符里的内存地址和数据大小，使用 scatter-gather 功能开始从内核缓冲区收集离散的数据并组包，最后直接把网络包数据拷贝到网卡完成数据传输；</li><li>sendfile() 返回，上下文从内核态切换回用户态。</li></ol><p>基于这种方案，我们就可以把这仅剩的唯一一次 CPU 拷贝也给去除了（严格来说还是会有一次，但是因为这次 CPU 拷贝的只是那些微乎其微的元信息，开销几乎可以忽略不计），理论上，数据传输过程就再也没有 CPU 的参与了，也因此 CPU 的高速缓存再不会被污染了，也不再需要 CPU 来计算数据校验和了，CPU 可以去执行其他的业务计算任务，同时和 DMA 的 I/O 任务并行，此举能极大地提升系统性能。</p><p><strong>splice()</strong></p><p>sendfile() + DMA Scatter/Gather 的零拷贝方案虽然高效，但是也有两个缺点：</p><ol><li>这种方案需要引入新的硬件支持；</li><li>虽然 sendfile() 的输出文件描述符在 Linux kernel 2.6.33 版本之后已经可以支持任意类型的文件描述符，但是输入文件描述符依然只能指向文件。</li></ol><p>这两个缺点限制了 sendfile() + DMA Scatter/Gather 方案的适用场景。为此，Linux 在 2.6.17 版本引入了一个新的系统调用 splice()，它在功能上和 sendfile() 非常相似，但是能够实现在任意类型的两个文件描述符时之间传输数据；而在底层实现上，splice()又比 sendfile() 少了一次 CPU 拷贝，也就是等同于 sendfile() + DMA Scatter/Gather，完全去除了数据传输过程中的 CPU 拷贝。</p><p>splice() 系统调用函数定义如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;fcntl.h&gt;</span><br><span class="line">#include &lt;unistd.h&gt;</span><br><span class="line"></span><br><span class="line">int pipe(int pipefd[2]);</span><br><span class="line">int pipe2(int pipefd[2], int flags);</span><br><span class="line"></span><br><span class="line">ssize_t splice(int fd_in, loff_t *off_in, int fd_out, loff_t *off_out, size_t len, unsigned int flags);</span><br></pre></td></tr></table></figure><p>fd_in 和 fd_out 也是分别代表了输入端和输出端的文件描述符，这两个文件描述符必须有一个是指向管道设备的，这也是一个不太友好的限制，虽然 Linux 内核开发的官方从这个系统调用推出之时就承诺未来可能会重构去掉这个限制，然而他们许下这个承诺之后就如同石沉大海，如今 14 年过去了，依旧杳无音讯…<br>off_in 和 off_out 则分别是 fd_in 和 fd_out 的偏移量指针，指示内核从哪里读取和写入数据，len 则指示了此次调用希望传输的字节数，最后的 flags 是系统调用的标记选项位掩码，用来设置系统调用的行为属性的，由以下 0 个或者多个值通过『或』操作组合而成：</p><ul><li>SPLICE_F_MOVE：指示 splice() 尝试仅仅是移动内存页面而不是复制，设置了这个值不代表就一定不会复制内存页面，复制还是移动取决于内核能否从管道中移动内存页面，或者管道中的内存页面是否是完整的；这个标记的初始实现有很多 bug，所以从 Linux 2.6.21 版本开始就已经无效了，但还是保留了下来，因为在未来的版本里可能会重新被实现。</li><li>SPLICE_F_NONBLOCK：指示 splice() 不要阻塞 I/O，也就是使得 splice() 调用成为一个非阻塞调用，可以用来实现异步数据传输，不过需要注意的是，数据传输的两个文件描述符也最好是预先通过 O_NONBLOCK 标记成非阻塞 I/O，不然 splice() 调用还是有可能被阻塞。</li><li>SPLICE_F_MORE：通知内核下一个 splice() 系统调用将会有更多的数据传输过来，这个标记对于输出端是 socket 的场景非常有用。</li></ul><p>splice() 是基于 Linux 的管道缓冲区 (pipe buffer) 机制实现的，所以 splice() 的两个入参文件描述符才要求必须有一个是管道设备，一个典型的 splice() 用法是：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">int pfd[2];</span><br><span class="line"></span><br><span class="line">pipe(pfd);</span><br><span class="line"></span><br><span class="line">ssize_t bytes = splice(file_fd, NULL, pfd[1], NULL, 4096, SPLICE_F_MOVE);</span><br><span class="line">assert(bytes != -1);</span><br><span class="line"></span><br><span class="line">bytes = splice(pfd[0], NULL, socket_fd, NULL, bytes, SPLICE_F_MOVE | SPLICE_F_MORE);</span><br><span class="line">assert(bytes != -1);</span><br></pre></td></tr></table></figure><p>数据传输过程图：</p><p><img src="/img/2021/0107/19.jpg" alt="&quot;IO splice&quot;"></p><p>使用 splice() 完成一次磁盘文件到网卡的读写过程如下：</p><ol><li>用户进程调用 pipe()，从用户态陷入内核态，创建匿名单向管道，pipe() 返回，上下文从内核态切换回用户态；</li><li>用户进程调用 splice()，从用户态陷入内核态；</li><li>DMA 控制器将数据从硬盘拷贝到内核缓冲区，从管道的写入端”拷贝”进管道，splice() 返回，上下文从内核态回到用户态；</li><li>用户进程再次调用 splice()，从用户态陷入内核态；</li><li>内核把数据从管道的读取端”拷贝”到套接字缓冲区，DMA 控制器将数据从套接字缓冲区拷贝到网卡；</li><li>splice() 返回，上下文从内核态切换回用户态。</li></ol><p>相信看完上面的读写流程之后，读者肯定会非常困惑：说好的 splice() 是 sendfile() 的改进版呢？sendfile() 好歹只需要一次系统调用，splice() 居然需要三次，这也就罢了，居然中间还搞出来一个管道，而且还要在内核空间拷贝两次，这算个毛的改进啊？<br>我最开始了解 splice() 的时候，也是这个反应，但是深入学习它之后，才渐渐知晓个中奥妙，且听我细细道来：<br>先来了解一下 pipe buffer 管道，管道是 Linux 上用来供进程之间通信的信道，管道有两个端：写入端和读出端，从进程的视角来看，管道表现为一个 FIFO 字节流环形队列：</p><p><img src="/img/2021/0107/20.jpg" alt="&quot;pipe buffer&quot;"></p><p>管道本质上是一个内存中的文件，也就是本质上还是基于 Linux 的 VFS，用户进程可以通过 pipe() 系统调用创建一个匿名管道，创建完成之后会有两个 VFS 的 file 结构体的 inode 分别指向其写入端和读出端，并返回对应的两个文件描述符，用户进程通过这两个文件描述符读写管道；管道的容量单位是一个虚拟内存的页，也就是 4KB，总大小一般是 16 个页，基于其环形结构，管道的页可以循环使用，提高内存利用率。 Linux 中以 pipe_buffer 结构体封装管道页，file 结构体里的 inode 字段里会保存一个 pipe_inode_info 结构体指代管道，其中会保存很多读写管道时所需的元信息，环形队列的头部指针页，读写时的同步机制如互斥锁、等待队列等：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">struct pipe_buffer &#123;</span><br><span class="line">    struct page *page; // 内存页结构</span><br><span class="line">    unsigned int offset, len; // 偏移量，长度</span><br><span class="line">    const struct pipe_buf_operations *ops;</span><br><span class="line">    unsigned int flags;</span><br><span class="line">    unsigned long private;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">struct pipe_inode_info &#123;</span><br><span class="line">    struct mutex mutex;</span><br><span class="line">    wait_queue_head_t wait;</span><br><span class="line">    unsigned int nrbufs, curbuf, buffers;</span><br><span class="line">    unsigned int readers;</span><br><span class="line">    unsigned int writers;</span><br><span class="line">    unsigned int files;</span><br><span class="line">    unsigned int waiting_writers;</span><br><span class="line">    unsigned int r_counter;</span><br><span class="line">    unsigned int w_counter;</span><br><span class="line">    struct page *tmp_page;</span><br><span class="line">    struct fasync_struct *fasync_readers;</span><br><span class="line">    struct fasync_struct *fasync_writers;</span><br><span class="line">    struct pipe_buffer *bufs;</span><br><span class="line">    struct user_struct *user;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>pipe_buffer 中保存了数据在内存中的页、偏移量和长度，以这三个值来定位数据，注意这里的页不是虚拟内存的页，而用的是物理内存的页框，因为管道时跨进程的信道，因此不能使用虚拟内存来表示，只能使用物理内存的页框定位数据；管道的正常读写操作是通过 pipe_write()/pipe_read() 来完成的，通过把数据读取/写入环形队列的 pipe_buffer 来完成数据传输。</p><p>splice() 是基于 pipe buffer 实现的，但是它在通过管道传输数据的时候却是零拷贝，因为它在写入读出时并没有使用 pipe_write()/pipe_read() 真正地在管道缓冲区写入读出数据，而是通过把数据在内存缓冲区中的物理内存页框指针、偏移量和长度赋值给前文提及的 pipe_buffer 中对应的三个字段来完成数据的”拷贝”，也就是其实只拷贝了数据的内存地址等元信息。<br>splice() 在 Linux 内核源码中的内部实现是 do_splice() 函数，而写入读出管道则分别是通过 do_splice_to() 和 do_splice_from()，这里我们重点来解析下写入管道的源码，也就是 do_splice_to()，我现在手头的 Linux 内核版本是 v4.8.17，我们就基于这个版本来分析，至于读出的源码函数 do_splice_from()，原理是相通的，大家举一反三即可。<br>splice() 写入数据到管道的调用链式：do_splice() –&gt;  do_splice_to() –&gt; splice_read()</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">static long do_splice(struct file *in, loff_t __user *off_in,</span><br><span class="line">              struct file *out, loff_t __user *off_out,</span><br><span class="line">              size_t len, unsigned int flags)</span><br><span class="line">&#123;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">    // 判断是写出 fd 是一个管道设备，则进入数据写入的逻辑</span><br><span class="line">    if (opipe) &#123;</span><br><span class="line">        if (off_out)</span><br><span class="line">            return -ESPIPE;</span><br><span class="line">        if (off_in) &#123;</span><br><span class="line">            if (!(in-&gt;f_mode &amp; FMODE_PREAD))</span><br><span class="line">                return -EINVAL;</span><br><span class="line">            if (copy_from_user(&amp;offset, off_in, sizeof(loff_t)))</span><br><span class="line">                return -EFAULT;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            offset = in-&gt;f_pos;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // 调用 do_splice_to 把文件内容写入管道</span><br><span class="line">        ret = do_splice_to(in, &amp;offset, opipe, len, flags);</span><br><span class="line"></span><br><span class="line">        if (!off_in)</span><br><span class="line">            in-&gt;f_pos = offset;</span><br><span class="line">        else if (copy_to_user(off_in, &amp;offset, sizeof(loff_t)))</span><br><span class="line">            ret = -EFAULT;</span><br><span class="line"></span><br><span class="line">        return ret;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return -EINVAL;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>进入 do_splice_to() 之后，再调用 splice_read()：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">static long do_splice_to(struct file *in, loff_t *ppos,</span><br><span class="line">             struct pipe_inode_info *pipe, size_t len,</span><br><span class="line">             unsigned int flags)</span><br><span class="line">&#123;</span><br><span class="line">    ssize_t (*splice_read)(struct file *, loff_t *,</span><br><span class="line">                   struct pipe_inode_info *, size_t, unsigned int);</span><br><span class="line">    int ret;</span><br><span class="line"></span><br><span class="line">    if (unlikely(!(in-&gt;f_mode &amp; FMODE_READ)))</span><br><span class="line">        return -EBADF;</span><br><span class="line"></span><br><span class="line">    ret = rw_verify_area(READ, in, ppos, len);</span><br><span class="line">    if (unlikely(ret &lt; 0))</span><br><span class="line">        return ret;</span><br><span class="line"></span><br><span class="line">    if (unlikely(len &gt; MAX_RW_COUNT))</span><br><span class="line">        len = MAX_RW_COUNT;</span><br><span class="line"></span><br><span class="line">    // 判断文件的文件的 file 结构体的 f_op 中有没有可供使用的、支持 splice 的 splice_read 函数指针</span><br><span class="line">    // 因为是 splice() 调用，因此内核会提前给这个函数指针指派一个可用的函数</span><br><span class="line">    if (in-&gt;f_op-&gt;splice_read)</span><br><span class="line">        splice_read = in-&gt;f_op-&gt;splice_read;</span><br><span class="line">    else</span><br><span class="line">        splice_read = default_file_splice_read;</span><br><span class="line"></span><br><span class="line">    return splice_read(in, ppos, pipe, len, flags);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>in-&gt;f_op-&gt;splice_read 这个函数指针根据文件描述符的类型不同有不同的实现，比如这里的 in 是一个文件，因此是 generic_file_splice_read()，如果是 socket 的话，则是 sock_splice_read()，其他的类型也会有对应的实现，总之我们这里将使用的是 generic_file_splice_read() 函数，这个函数会继续调用内部函数 __generic_file_splice_read 完成以下工作：</p><ol><li>在 page cache 页缓存里进行搜寻，看看我们要读取这个文件内容是否已经在缓存里了，如果是则直接用，否则如果不存在或者只有部分数据在缓存中，则分配一些新的内存页并进行读入数据操作，同时会增加页框的引用计数；</li><li>基于这些内存页，初始化 splice_pipe_desc 结构，这个结构保存会保存文件数据的地址元信息，包含有物理内存页框地址，偏移、数据长度，也就是 pipe_buffer 所需的三个定位数据的值；</li><li>最后，调用 splice_to_pipe()，splice_pipe_desc 结构体实例是函数入参。</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">ssize_t splice_to_pipe(struct pipe_inode_info *pipe, struct splice_pipe_desc *spd)</span><br><span class="line">&#123;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">    for (;;) &#123;</span><br><span class="line">        if (!pipe-&gt;readers) &#123;</span><br><span class="line">            send_sig(SIGPIPE, current, 0);</span><br><span class="line">            if (!ret)</span><br><span class="line">                ret = -EPIPE;</span><br><span class="line">            break;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        if (pipe-&gt;nrbufs &lt; pipe-&gt;buffers) &#123;</span><br><span class="line">            int newbuf = (pipe-&gt;curbuf + pipe-&gt;nrbufs) &amp; (pipe-&gt;buffers - 1);</span><br><span class="line">            struct pipe_buffer *buf = pipe-&gt;bufs + newbuf;</span><br><span class="line"></span><br><span class="line">            // 写入数据到管道，没有真正拷贝数据，而是内存地址指针的移动，</span><br><span class="line">            // 把物理页框、偏移量和数据长度赋值给 pipe_buffer 完成数据入队操作</span><br><span class="line">            buf-&gt;page = spd-&gt;pages[page_nr];</span><br><span class="line">            buf-&gt;offset = spd-&gt;partial[page_nr].offset;</span><br><span class="line">            buf-&gt;len = spd-&gt;partial[page_nr].len;</span><br><span class="line">            buf-&gt;private = spd-&gt;partial[page_nr].private;</span><br><span class="line">            buf-&gt;ops = spd-&gt;ops;</span><br><span class="line">            if (spd-&gt;flags &amp; SPLICE_F_GIFT)</span><br><span class="line">                buf-&gt;flags |= PIPE_BUF_FLAG_GIFT;</span><br><span class="line"></span><br><span class="line">            pipe-&gt;nrbufs++;</span><br><span class="line">            page_nr++;</span><br><span class="line">            ret += buf-&gt;len;</span><br><span class="line"></span><br><span class="line">            if (pipe-&gt;files)</span><br><span class="line">                do_wakeup = 1;</span><br><span class="line"></span><br><span class="line">            if (!--spd-&gt;nr_pages)</span><br><span class="line">                break;</span><br><span class="line">            if (pipe-&gt;nrbufs &lt; pipe-&gt;buffers)</span><br><span class="line">                continue;</span><br><span class="line"></span><br><span class="line">            break;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里可以清楚地看到 splice() 所谓的写入数据到管道其实并没有真正地拷贝数据，而是玩了个 tricky 的操作：只进行内存地址指针的拷贝而不真正去拷贝数据。所以，数据 splice() 在内核中并没有进行真正的数据拷贝，因此 splice() 系统调用也是零拷贝。还有一点需要注意，前面说过管道的容量是 16 个内存页，也就是 16 * 4KB = 64 KB，也就是说一次往管道里写数据的时候最好不要超过 64 KB，否则的话会 splice() 会阻塞住，除非在创建管道的时候使用的是 pipe2() 并通过传入 O_NONBLOCK 属性将管道设置为非阻塞。</p><p>即使 splice() 通过内存地址指针避免了真正的拷贝开销，但是算起来它还要使用额外的管道来完成数据传输，也就是比 sendfile() 多了两次系统调用，这不是又增加了上下文切换的开销吗？为什么不直接在内核创建管道并调用那两次 splice()，然后只暴露给用户一次系统调用呢？实际上因为 splice() 利用管道而非硬件来完成零拷贝的实现比 sendfile() + DMA Scatter/Gather 的门槛更低，因此后来的 sendfile() 的底层实现就已经替换成 splice() 了。</p><p>至于说 splice() 本身的 API 为什么还是这种使用模式，那是因为 Linux 内核开发团队一直想把基于管道的这个限制去掉，但不知道因为什么一直搁置，所以这个 API 也就一直没变化，只能等内核团队哪天想起来了这一茬，然后重构一下使之不再依赖管道，在那之前，使用 splice() 依然还是需要额外创建管道来作为中间缓冲，如果你的业务场景很适合使用 splice()，但又是性能敏感的，不想频繁地创建销毁 pipe buffer 管道缓冲区，那么可以参考一下 HAProxy 使用 splice() 时采用的优化方案：预先分配一个 pipe buffer pool 缓存管道，每次调用 spclie() 的时候去缓存池里取一个管道，用完就放回去，循环利用，提升性能。</p><p><strong>send() with MSG_ZEROCOPY</strong><br>Linux 内核在 2017 年的 v4.14 版本接受了来自 Google 工程师 Willem de Bruijn 在 TCP 网络报文的通用发送接口 send() 中实现的 zero-copy 功能 (MSG_ZEROCOPY) 的 patch，通过这个新功能，用户进程就能够把用户缓冲区的数据通过零拷贝的方式经过内核空间发送到网络套接字中去，这个新技术和前文介绍的几种零拷贝方式相比更加先进，因为前面几种零拷贝技术都是要求用户进程不能处理加工数据而是直接转发到目标文件描述符中去的。Willem de Bruijn 在他的论文里给出的压测数据是：采用 netperf 大包发送测试，性能提升 39%，而线上环境的数据发送性能则提升了 5%~8%，官方文档陈述说这个特性通常只在发送 10KB 左右大包的场景下才会有显著的性能提升。一开始这个特性只支持 TCP，到内核 v5.0 版本之后才支持 UDP。<br>这个功能的使用模式如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">if (setsockopt(socket_fd, SOL_SOCKET, SO_ZEROCOPY, &amp;one, sizeof(one)))</span><br><span class="line">        error(1, errno, &quot;setsockopt zerocopy&quot;);</span><br><span class="line"></span><br><span class="line">ret = send(socket_fd, buffer, sizeof(buffer), MSG_ZEROCOPY);</span><br></pre></td></tr></table></figure><p>首先第一步，先给要发送数据的 socket 设置一个 SOCK_ZEROCOPY option，然后在调用 send() 发送数据时再设置一个 MSG_ZEROCOPY option，其实理论上来说只需要调用 setsockopt() 或者 send() 时传递这个 zero-copy 的 option 即可，两者选其一，但是这里却要设置同一个 option 两次，官方的说法是为了兼容 send() API 以前的设计上的一个错误：send() 以前的实现会忽略掉未知的 option，为了兼容那些可能已经不小心设置了 MSG_ZEROCOPY option 的程序，故而设计成了两步设置。不过我猜还有一种可能：就是给使用者提供更灵活的使用模式，因为这个新功能只在大包场景下才可能会有显著的性能提升，但是现实场景是很复杂的，不仅仅是全部大包或者全部小包的场景，有可能是大包小包混合的场景，因此使用者可以先调用 setsockopt() 设置 SOCK_ZEROCOPY option，然后再根据实际业务场景中的网络包尺寸选择是否要在调用 send() 时使用 MSG_ZEROCOPY 进行 zero-copy 传输。</p><p>因为 send() 可能是异步发送数据，因此使用 MSG_ZEROCOPY 有一个需要特别注意的点是：调用 send() 之后不能立刻重用或释放 buffer，因为 buffer 中的数据不一定已经被内核读走了，所以还需要从 socket 关联的错误队列里读取一下通知消息，看看 buffer 中的数据是否已经被内核读走了：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">pfd.fd = fd;</span><br><span class="line">pfd.events = 0;</span><br><span class="line">if (poll(&amp;pfd, 1, -1) != 1 || pfd.revents &amp; POLLERR == 0)</span><br><span class="line">        error(1, errno, &quot;poll&quot;);</span><br><span class="line"></span><br><span class="line">ret = recvmsg(fd, &amp;msg, MSG_ERRQUEUE);</span><br><span class="line">if (ret == -1)</span><br><span class="line">        error(1, errno, &quot;recvmsg&quot;);</span><br><span class="line"></span><br><span class="line">read_notification(msg);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">uint32_t read_notification(struct msghdr *msg)</span><br><span class="line">&#123;</span><br><span class="line">    struct sock_extended_err *serr;</span><br><span class="line">    struct cmsghdr *cm;</span><br><span class="line"></span><br><span class="line">    cm = CMSG_FIRSTHDR(msg);</span><br><span class="line">    if (cm-&gt;cmsg_level != SOL_IP &amp;&amp;</span><br><span class="line">        cm-&gt;cmsg_type != IP_RECVERR)</span><br><span class="line">            error(1, 0, &quot;cmsg&quot;);</span><br><span class="line"></span><br><span class="line">    serr = (void *) CMSG_DATA(cm);</span><br><span class="line">    if (serr-&gt;ee_errno != 0 ||</span><br><span class="line">        serr-&gt;ee_origin != SO_EE_ORIGIN_ZEROCOPY)</span><br><span class="line">            error(1, 0, &quot;serr&quot;);</span><br><span class="line"></span><br><span class="line">    return serr-&gt;ee _ data;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个技术是基于 redhat 红帽在 2010 年给 Linux 内核提交的 virtio-net zero-copy 技术之上实现的，至于底层原理，简单来说就是通过 send() 把数据在用户缓冲区中的分段指针发送到 socket 中去，利用 page pinning 页锁定机制锁住用户缓冲区的内存页，然后利用 DMA 直接在用户缓冲区通过内存地址指针进行数据读取，实现零拷贝；具体的细节可以通过阅读 Willem de Bruijn 的论文 (PDF) 深入了解。</p><p>目前来说，这种技术的主要缺陷有：</p><ol><li>只适用于大文件 (10KB 左右) 的场景，小文件场景因为 page pinning 页锁定和等待缓冲区释放的通知消息这些机制，甚至可能比直接 CPU 拷贝更耗时；</li><li>因为可能异步发送数据，需要额外调用 poll() 和 recvmsg() 系统调用等待 buffer 被释放的通知消息，增加代码复杂度，以及会导致多次用户态和内核态的上下文切换；</li><li>MSG_ZEROCOPY 目前只支持发送端，接收端暂不支持。</li></ol><p><strong>绕过内核的直接 I/O</strong><br>可以看出，前面种种的 zero-copy 的方法，都是在想方设法地优化减少或者去掉用户态和内核态之间以及内核态和内核态之间的数据拷贝，为了实现避免这些拷贝可谓是八仙过海，各显神通，采用了各种各样的手段，那么如果我们换个思路：其实这么费劲地去消除这些拷贝不就是因为有内核在掺和吗？如果我们绕过内核直接进行 I/O 不就没有这些烦人的拷贝问题了吗？这就是绕过内核直接 I/O 技术：</p><p><img src="/img/2021/0107/21.jpg" alt="&quot;direct hardware control&quot;"></p><p>这种方案有两种实现方式：</p><p><strong>用户直接访问硬件</strong><br>这种技术赋予用户进程直接访问硬件设备的权限，这让用户进程能有直接读写硬件设备，在数据传输过程中只需要内核做一些虚拟内存配置相关的工作。这种无需数据拷贝和内核干预的直接 I/O，理论上是最高效的数据传输技术，但是正如前面所说的那样，并不存在能解决一切问题的银弹，这种直接 I/O 技术虽然有可能非常高效，但是它的适用性也非常窄，目前只适用于诸如 MPI 高性能通信、丛集计算系统中的远程共享内存等有限的场景。</p><p>这种技术实际上破坏了现代计算机操作系统最重要的概念之一 —— 硬件抽象，我们之前提过，抽象是计算机领域最最核心的设计思路，正式由于有了抽象和分层，各个层级才能不必去关心很多底层细节从而专注于真正的工作，才使得系统的运作更加高效和快速。此外，网卡通常使用功能较弱的 CPU，例如只包含简单指令集的 MIPS 架构处理器（没有不必要的功能，如浮点数计算等），也没有太多的内存来容纳复杂的软件。因此，通常只有那些基于以太网之上的专用协议会使用这种技术，这些专用协议的设计要比远比 TCP/IP 简单得多，而且多用于局域网环境中，在这种环境中，数据包丢失和损坏很少发生，因此没有必要进行复杂的数据包确认和流量控制机制。而且这种技术还需要定制的网卡，所以它是高度依赖硬件的。</p><p>与传统的通信设计相比，直接硬件访问技术给程序设计带来了各种限制：由于设备之间的数据传输是通过 DMA 完成的，因此用户空间的数据缓冲区内存页必须进行 page pinning（页锁定），这是为了防止其物理页框地址被交换到磁盘或者被移动到新的地址而导致 DMA 去拷贝数据的时候在指定的地址找不到内存页从而引发缺页错误，而页锁定的开销并不比 CPU 拷贝小，所以为了避免频繁的页锁定系统调用，应用程序必须分配和注册一个持久的内存池，用于数据缓冲。</p><p>用户直接访问硬件的技术可以得到极高的 I/O 性能，但是其应用领域和适用场景也极其的有限，如集群或网络存储系统中的节点通信。它需要定制的硬件和专门设计的应用程序，但相应地对操作系统内核的改动比较小，可以很容易地以内核模块或设备驱动程序的形式实现出来。直接访问硬件还可能会带来严重的安全问题，因为用户进程拥有直接访问硬件的极高权限，所以如果你的程序设计没有做好的话，可能会消耗本来就有限的硬件资源或者进行非法地址访问，可能也会因此间接地影响其他正在使用同一设备的应用程序，而因为绕开了内核，所以也无法让内核替你去控制和管理。</p><p><strong>内核控制访问硬件</strong></p><p>相较于用户直接访问硬件技术，通过内核控制的直接访问硬件技术更加的安全，它比前者在数据传输过程中会多干预一点，但也仅仅是作为一个代理人这样的角色，不会参与到实际的数据传输过程，内核会控制 DMA 引擎去替用户进程做缓冲区的数据传输工作。同样的，这种方式也是高度依赖硬件的，比如一些集成了专有网络栈协议的网卡。这种技术的一个优势就是用户集成去 I/O 时的接口不会改变，就和普通的 read()/write() 系统调用那样使用即可，所有的脏活累活都在内核里完成，用户接口友好度很高，不过需要注意的是，使用这种技术的过程中如果发生了什么不可预知的意外从而导致无法使用这种技术进行数据传输的话，则内核会自动切换为最传统 I/O 模式，也就是性能最差的那种模式。</p><p>这种技术也有着和用户直接访问硬件技术一样的问题：DMA 传输数据的过程中，用户进程的缓冲区内存页必须进行 page pinning 页锁定，数据传输完成后才能解锁。CPU 高速缓存内保存的多个内存地址也会被冲刷掉以保证 DMA 传输前后的数据一致性。这些机制有可能会导致数据传输的性能变得更差，因为 read()/write() 系统调用的语义并不能提前通知 CPU 用户缓冲区要参与 DMA 数据传输传输，因此也就无法像内核缓冲区那样可依提前加载进高速缓存，提高性能。由于用户缓冲区的内存页可能分布在物理内存中的任意位置，因此一些实现不好的 DMA 控制器引擎可能会有寻址限制从而导致无法访问这些内存区域。一些技术比如 AMD64 架构中的 IOMMU，允许通过将 DMA 地址重新映射到内存中的物理地址来解决这些限制，但反过来又可能会导致可移植性问题，因为其他的处理器架构，甚至是 Intel 64 位 x86 架构的变种 EM64T 都不具备这样的特性单元。此外，还可能存在其他限制，比如 DMA 传输的数据对齐问题，又会导致无法访问用户进程指定的任意缓冲区内存地址。</p><p><strong>内核缓冲区和用户缓冲区之间的传输优化</strong><br>到目前为止，我们讨论的 zero-copy 技术都是基于减少甚至是避免用户空间和内核空间之间的 CPU 数据拷贝的，虽然有一些技术非常高效，但是大多都有适用性很窄的问题，比如 sendfile()、splice() 这些，效率很高，但是都只适用于那些用户进程不需要直接处理数据的场景，比如静态文件服务器或者是直接转发数据的代理服务器。</p><p>现在我们已经知道，硬件设备之间的数据可以通过 DMA 进行传输，然而却并没有这样的传输机制可以应用于用户缓冲区和内核缓冲区之间的数据传输。不过另一方面，广泛应用在现代的 CPU 架构和操作系统上的虚拟内存机制表明，通过在不同的虚拟地址上重新映射页面可以实现在用户进程和内核之间虚拟复制和共享内存，尽管一次传输的内存颗粒度相对较大：4KB 或 8KB。</p><p>因此如果要在实现在用户进程内处理数据（这种场景比直接转发数据更加常见）之后再发送出去的话，用户空间和内核空间的数据传输就是不可避免的，既然避无可避，那就只能选择优化了，因此本章节我们要介绍两种优化用户空间和内核空间数据传输的技术：</p><ol><li>动态重映射与写时拷贝 (Copy-on-Write)</li><li>缓冲区共享 (Buffer Sharing)</li></ol><p><strong>动态重映射与写时拷贝 (Copy-on-Write)</strong></p><p>前面我们介绍过利用内存映射技术来减少数据在用户空间和内核空间之间的复制，通常简单模式下，用户进程是对共享的缓冲区进行同步阻塞读写的，这样不会有 data race 问题，但是这种模式下效率并不高，而提升效率的一种方法就是异步地对共享缓冲区进行读写，而这样的话就必须引入保护机制来避免数据冲突问题，写时复制 (Copy on Write) 就是这样的一种技术。</p><p>写入时复制（Copy-on-write，COW）是一种计算机程序设计领域的优化策略。其核心思想是，如果有多个调用者（callers）同时请求相同资源（如内存或磁盘上的数据存储），他们会共同获取相同的指针指向相同的资源，直到某个调用者试图修改资源的内容时，系统才会真正复制一份专用副本（private copy）给该调用者，而其他调用者所见到的最初的资源仍然保持不变。这过程对其他的调用者都是透明的。此作法主要的优点是如果调用者没有修改该资源，就不会有副本（private copy）被创建，因此多个调用者只是读取操作时可以共享同一份资源。</p><p>举一个例子，引入了 COW 技术之后，用户进程读取磁盘文件进行数据处理最后写到网卡，首先使用内存映射技术让用户缓冲区和内核缓冲区共享了一段内存地址并标记为只读 (read-only)，避免数据拷贝，而当要把数据写到网卡的时候，用户进程选择了异步写的方式，系统调用会直接返回，数据传输就会在内核里异步进行，而用户进程就可以继续其他的工作，并且共享缓冲区的内容可以随时再进行读取，效率很高，但是如果该进程又尝试往共享缓冲区写入数据，则会产生一个 COW 事件，让试图写入数据的进程把数据复制到自己的缓冲区去修改，这里只需要复制要修改的内存页即可，无需所有数据都复制过去，而如果其他访问该共享内存的进程不需要修改数据则可以永远不需要进行数据拷贝。</p><p>COW 是一种建构在虚拟内存冲映射技术之上的技术，因此它需要 MMU 的硬件支持，MMU 会记录当前哪些内存页被标记成只读，当有进程尝试往这些内存页中写数据的时候，MMU 就会抛一个异常给操作系统内核，内核处理该异常时为该进程分配一份物理内存并复制数据到此内存地址，重新向 MMU 发出执行该进程的写操作。</p><p>COW 最大的优势是节省内存和减少数据拷贝，不过却是通过增加操作系统内核 I/O 过程复杂性作为代价的。当确定采用 COW 来复制页面时，重要的是注意空闲页面的分配位置。许多操作系统为这类请求提供了一个空闲的页面池。当进程的堆栈或堆要扩展时或有写时复制页面需要管理时，通常分配这些空闲页面。操作系统分配这些页面通常采用称为按需填零的技术。按需填零页面在需要分配之前先填零，因此会清除里面旧的内容。</p><p>COW 这种零拷贝技术比较适用于那种多读少写从而使得 COW 事件发生较少的场景，因为 COW 事件所带来的系统开销要远远高于一次 CPU 拷贝所产生的。此外，在实际应用的过程中，为了避免频繁的内存映射，可以重复使用同一段内存缓冲区，因此，你不需要在只用过一次共享缓冲区之后就解除掉内存页的映射关系，而是重复循环使用，从而提升性能，不过这种内存页映射的持久化并不会减少由于页表往返移动和 TLB 冲刷所带来的系统开销，因为每次接收到 COW 事件之后对内存页而进行加锁或者解锁的时候，页面的只读标志 (read-ony) 都要被更改为 (write-only)。</p><p><strong>缓冲区共享 (Buffer Sharing)</strong><br>从前面的介绍可以看出，传统的 Linux I/O接口，都是基于复制/拷贝的：数据需要在操作系统内核空间和用户空间的缓冲区之间进行拷贝。在进行 I/O 操作之前，用户进程需要预先分配好一个内存缓冲区，使用 read() 系统调用时，内核会将从存储器或者网卡等设备读入的数据拷贝到这个用户缓冲区里；而使用 write() 系统调用时，则是把用户内存缓冲区的数据拷贝至内核缓冲区。</p><p>为了实现这种传统的 I/O 模式，Linux 必须要在每一个 I/O 操作时都进行内存虚拟映射和解除。这种内存页重映射的机制的效率严重受限于缓存体系结构、MMU 地址转换速度和 TLB 命中率。如果能够避免处理 I/O 请求的虚拟地址转换和 TLB 刷新所带来的开销，则有可能极大地提升 I/O 性能。而缓冲区共享就是用来解决上述问题的一种技术</p><p>最早支持 Buffer Sharing 的操作系统是 Solaris。后来，Linux 也逐步支持了这种 Buffer Sharing 的技术，但时至今日依然不够完整和成熟。</p><p>操作系统内核开发者们实现了一种叫 fbufs 的缓冲区共享的框架，也即快速缓冲区（ Fast Buffers ），使用一个 fbuf 缓冲区作为数据传输的最小单位，使用这种技术需要调用新的操作系统 API，用户区和内核区、内核区之间的数据都必须严格地在 fbufs 这个体系下进行通信。fbufs 为每一个用户进程分配一个 buffer pool，里面会储存预分配 (也可以使用的时候再分配) 好的 buffers，这些 buffers 会被同时映射到用户内存空间和内核内存空间。fbufs 只需通过一次虚拟内存映射操作即可创建缓冲区，有效地消除那些由存储一致性维护所引发的大多数性能损耗。</p><p>传统的 Linux I/O 接口是通过把数据在用户缓冲区和内核缓冲区之间进行拷贝传输来完成的，这种数据传输过程中需要进行大量的数据拷贝，同时由于虚拟内存技术的存在，I/O 过程中还需要频繁地通过 MMU 进行虚拟内存地址到物理内存地址的转换，高速缓存的汰换以及 TLB 的刷新，这些操作均会导致性能的损耗。而如果利用 fbufs 框架来实现数据传输的话，首先可以把 buffers 都缓存到 pool 里循环利用，而不需要每次都去重新分配，而且缓存下来的不止有 buffers 本身，而且还会把虚拟内存地址到物理内存地址的映射关系也缓存下来，也就可以避免每次都进行地址转换，从发送接收数据的层面来说，用户进程和 I/O 子系统比如设备驱动程序、网卡等可以直接传输整个缓冲区本身而不是其中的数据内容，也可以理解成是传输内存地址指针，这样就就避免了大量的数据内容拷贝：用户进程/ IO 子系统通过发送一个个的 fbuf 写出数据到内核而非直接传递数据内容，相对应的，用户进程/ IO 子系统通过接收一个个的 fbuf 而从内核读入数据，这样就能减少传统的 read()/write() 系统调用带来的数据拷贝开销：</p><p><img src="/img/2021/0107/22.jpg" alt="&quot;IO buffer sharing by fbufs&quot;"></p><ol><li>发送方用户进程调用 uf_allocate 从自己的 buffer pool 获取一个 fbuf 缓冲区，往其中填充内容之后调用 uf_write 向内核区发送指向 fbuf 的文件描述符；</li><li>I/O 子系统接收到 fbuf 之后，调用 uf_allocb 从接收方用户进程的 buffer pool 获取一个 fubf 并用接收到的数据进行填充，然后向用户区发送指向 fbuf 的文件描述符；</li><li>接收方用户进程调用 uf_get 接收到 fbuf，读取数据进行处理，完成之后调用 uf_deallocate 把 fbuf 放回自己的 buffer pool。</li></ol><p><strong>fbufs 的缺陷</strong><br>共享缓冲区技术的实现需要依赖于用户进程、操作系统内核、以及 I/O 子系统 (设备驱动程序，文件系统等)之间协同工作。比如，设计得不好的用户进程容易就会修改已经发送出去的 fbuf 从而污染数据，更要命的是这种问题很难 debug。虽然这个技术的设计方案非常精彩，但是它的门槛和限制却不比前面介绍的其他技术少：首先会对操作系统 API 造成变动，需要使用新的一些 API 调用，其次还需要设备驱动程序配合改动，还有由于是内存共享，内核需要很小心谨慎地实现对这部分共享的内存进行数据保护和同步的机制，而这种并发的同步机制是非常容易出 bug 的从而又增加了内核的代码复杂度，等等。因此这一类的技术还远远没有到发展成熟和广泛应用的阶段，目前大多数的实现都还处于实验阶段</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文中我主要讲解了 Linux I/O 底层原理，然后介绍并解析了 Linux 中的 Zero-copy 技术，并给出了 Linux 对 I/O 模块的优化和改进思路。</p><p>Linux 的 Zero-copy 技术可以归纳成以下三大类：</p><ul><li><strong>减少甚至避免用户空间和内核空间之间的数据拷贝：</strong>在一些场景下，用户进程在数据传输过程中并不需要对数据进行访问和处理，那么数据在 Linux 的 Page Cache 和用户进程的缓冲区之间的传输就完全可以避免，让数据拷贝完全在内核里进行，甚至可以通过更巧妙的方式避免在内核里的数据拷贝。这一类实现一般是是通过增加新的系统调用来完成的，比如 Linux 中的 mmap()，sendfile() 以及 splice() 等。</li><li><strong>绕过内核的直接 I/O：</strong>允许在用户态进程绕过内核直接和硬件进行数据传输，内核在传输过程中只负责一些管理和辅助的工作。这种方式其实和第一种有点类似，也是试图避免用户空间和内核空间之间的数据传输，只是第一种方式是把数据传输过程放在内核态完成，而这种方式则是直接绕过内核和硬件通信，效果类似但原理完全不同。</li><li><strong>内核缓冲区和用户缓冲区之间的传输优化：</strong>这种方式侧重于在用户进程的缓冲区和操作系统的页缓存之间的 CPU 拷贝的优化。这种方法延续了以往那种传统的通信方式，但更灵活。</li></ul><p>本文从虚拟内存、I/O 缓冲区，用户态&amp;内核态以及 I/O 模式等等知识点全面而又详尽地剖析了 Linux 系统的 I/O 底层原理，分析了 Linux 传统的 I/O 模式的弊端，进而引入 Linux Zero-copy 零拷贝技术的介绍和原理解析，通过将零拷贝技术和传统的 I/O 模式进行区分和对比，带领读者经历了 Linux I/O 的演化历史，通过帮助读者理解 Linux 内核对 I/O 模块的优化改进思路，相信不仅仅是让读者了解 Linux 底层系统的设计原理，更能对读者们在以后优化改进自己的程序设计过程中能够有所启发。</p><h2 id="参考-amp-延伸阅读"><a href="#参考-amp-延伸阅读" class="headerlink" title="参考&amp;延伸阅读"></a>参考&amp;延伸阅读</h2><ul><li><a href="http://index-of.es/Varios-2/Modern%20Operating%20Systems%204th%20Edition.pdf">MODERN OPERATING SYSTEMS</a></li><li><a href="https://www.linuxjournal.com/article/6345">Zero Copy I: User-Mode Perspective</a></li><li><a href="http://www.cs.inf.ethz.ch/group/stricker/sada/archive/chihaia.pdf">Message Passing for Gigabit/s Networks with “Zero-Copy” under Linux</a></li><li><a href="https://static.aminer.org/pdf/PDF/000/253/158/design_and_implementation_of_zero_copy_data_path_for_efficient.pdf">ZeroCopy: Techniques, Benefits and Pitfalls</a></li><li><a href="https://lwn.net/Articles/726917/">Zero-copy networking</a></li><li><a href="https://lwn.net/Articles/28548/">Driver porting: Zero-copy user-space access</a></li><li><a href="ttps://netdevconf.info/2.1/papers/netdev.pdf">sendmsg copy avoidance with MSG_ZEROCOPY</a></li><li><a href="https://medium.com/@xunnan.xu/its-all-about-buffers-zero-copy-mmap-and-java-nio-50f2a1bfc05c">It’s all about buffers: zero-copy, mmap and Java NIO</a></li><li><a href="https://lwn.net/Articles/712467/">Linux Zero Copy</a></li><li><a href="https://lwn.net/Articles/386778/">Two new system calls: splice() and sync_file_range()</a></li><li><a href="https://www.joshbialkowski.com/posts/2018/linux_zero_copy/linux-zero-copy.html">Circular pipes</a></li><li><a href="https://lwn.net/Articles/178199/">The future of the page cache</a></li><li><a href="https://lwn.net/Articles/118750/">Provide a zero-copy method on KVM virtio-net</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/img/2021/0107/1.jpg&quot; alt=&quot;&amp;quot;linux-zero-copy&amp;quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;原文链接：&lt;a href=&quot;https://strikefreedom.top/linux-io-and-zero-copy&quot;&gt;https://strikefreedom.top/linux-io-and-zero-copy&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;早就想写篇关于介绍IO操作的技术总结，直到今天看到这篇文章介绍的如此逻辑清晰，也把自己之前掌握的知识串了起来，读完收获颇多。这里就做个搬运工，原文请参见上面链接。&lt;br&gt;如今的网络应用早已从 CPU 密集型转向了 I/O 密集型，网络服务器大多是基于 C-S 模型，也即 客户端 - 服务端 模型，客户端需要和服务端进行大量的网络通信，这也决定了现代网络应用的性能瓶颈：I/O。&lt;/p&gt;</summary>
    
    
    
    <category term="技术总结" scheme="http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="zero-copy" scheme="http://huangzhiyuan.github.io/tags/zero-copy/"/>
    
  </entry>
  
  <entry>
    <title>OpenMP 总结</title>
    <link href="http://huangzhiyuan.github.io/2020/08/26/guide-into-openmp/"/>
    <id>http://huangzhiyuan.github.io/2020/08/26/guide-into-openmp/</id>
    <published>2020-08-26T05:14:17.000Z</published>
    <updated>2020-08-26T07:13:54.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文档尝试快速介绍 OpenMP（如版本 4.5），这是一个简单的 C/C++/Fortran 编译器扩展 C++，它允许在现有源代码中添加并行性，而无需重写它。</p><span id="more"></span><h2 id="引言-多线程的重要性"><a href="#引言-多线程的重要性" class="headerlink" title="引言 多线程的重要性"></a>引言 多线程的重要性</h2><p>随着 CPU 速度不再像以前那样显著提高，多核系统正变得越来越流行。要利用这种功能，程序员在并行编程中知识化变得非常重要。本文档试图快速介绍 OpenMP，这是一个简单的 C/C++/Fortran 编译器扩展，允许将并行性添加到现有源代码中，而无需完全重写它。</p><h3 id="多个编译器支持"><a href="#多个编译器支持" class="headerlink" title="多个编译器支持"></a>多个编译器支持</h3><ul><li>GCC (GNU Compiler Collection)</li><li>Clang++</li><li>Solaris Studio</li><li>ICC (Intel C Compiler)</li><li>Microsoft Visual C++</li></ul><h2 id="C-中的OpenMP介绍"><a href="#C-中的OpenMP介绍" class="headerlink" title="C++中的OpenMP介绍"></a>C++中的OpenMP介绍</h2><p>OpenMP 由一组编译器<code>#pragmas</code>控制程序工作原理的编译器。<code>pragmas</code>的设计使即使编译器不支持它们，程序仍将产生正确的行为，但没有任何并行性。<br>下面是演示 OpenMP 的两个简单的示例程序。你可以像这样编译它们：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">g++ tmp.cpp -fopenmp</span><br></pre></td></tr></table></figure><p><strong>Example: Initializing a table in parallel (multiple threads)</strong><br>此代码将表初始化划分为多个线程，这些线程同时运行。每个线程初始化表的一部分</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;cmath&gt;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">  const int size = 256;</span><br><span class="line">  double sinTable[size];</span><br><span class="line"></span><br><span class="line">  #pragma omp parallel for</span><br><span class="line">  for(int n=0; n&lt;size; ++n)</span><br><span class="line">    sinTable[n] = std::sin(2 * M_PI * n / size);</span><br><span class="line"></span><br><span class="line">  // the table is now initialized</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Example: Initializing a table in parallel (single thread, SIMD)</strong><br>此版本需要至少对 OpenMP 4.0 的编译器支持，并使用并行浮点库，如 AMD ACML 或英特尔SVML(可以在GCC中使用通过添加‑mveclibabi=svml）。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;cmath&gt;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">  const int size = 256;</span><br><span class="line">  double sinTable[size];</span><br><span class="line"></span><br><span class="line">  #pragma omp simd</span><br><span class="line">  for(int n=0; n&lt;size; ++n)</span><br><span class="line">    sinTable[n] = std::sin(2 * M_PI * n / size);</span><br><span class="line"></span><br><span class="line">  // the table is now initialized</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Example: Initializing a table in parallel (multiple threads on another device)</strong><br>OpenMP 4.0 增加了对将代码卸载到不同设备（如 GPU）的支持。因此，单个程序中可以有三层并行性：单个线程处理多个数据;多个线程同时运行;和多个设备同时运行同一程序。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;cmath&gt;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">  const int size = 256;</span><br><span class="line">  double sinTable[size];</span><br><span class="line"></span><br><span class="line">  #pragma omp target teams distribute parallel for map(from:sinTable[0:256])</span><br><span class="line">  for(int n=0; n&lt;size; ++n)</span><br><span class="line">    sinTable[n] = std::sin(2 * M_PI * n / size);</span><br><span class="line"></span><br><span class="line">  // the table is now initialized</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>综上所述，程序中很少指示它并行运行。如果删除 #pragma 行，则结果仍然是运行并执行预期操作。#pragma的有效 C++ 程序。它确实同时计算 N 个值，其中 N 是线程数。在 GCC 中，libgomp 从处理器数目确定这个N的值。<br>在C 和 C++ 标准中，如果编译器遇到#pragma但它不支持，它将忽略它。因此，添加 OMP 语句可以安全地完成，而不会破坏与旧编译器的兼容性。<br>还有一个运行时库，可以通过 omp.h 访问，但它不太经常需要。如果需要，可以在不支持 OpenMP #define _OPENMP检查条件编译的格式。</p><h2 id="OpenMP语法解析"><a href="#OpenMP语法解析" class="headerlink" title="OpenMP语法解析"></a>OpenMP语法解析</h2><p>C 和 C++的所有 OpenMP 构造都用 #pragma，后跟参数，以新行结尾。#pragma通常仅适用于紧接着它的语句，但<code>barrier</code>和<code>flush</code>命令除外，它们没有关联的语句。</p><h3 id="The-parallel-construct"><a href="#The-parallel-construct" class="headerlink" title="The parallel construct"></a>The parallel construct</h3><p>parallel构造用parallel关键字启动并行块。它创建一组N个数目线程（其中 N 在运行时确定，通常来自 CPU 内核的数量，但可能受到一些因素的影响），所有这些线程都执行下一个语句（或下一个块，如果语句是 […] - 存储模块）。语句之后，线程将重新联接到一个线程中。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#pragma omp parallel</span><br><span class="line">&#123;</span><br><span class="line">  // Code inside this region runs in parallel.</span><br><span class="line">  printf(&quot;Hello!\n&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在内部，GCC 通过创建一个magic函数并移动关联的代码到该函数中实现此功能，使该块内声明的所有变量成为该函数的局部变量（从而成为每个线程的局部变量）。另一方面，ICC 使用类似于fork()的机制，并且不创建一个magic函数。当然，这两个实现都是有效和语义上相同的。<br>从上下文共享的变量以透明方式处理，有时通过传递引用，有时通过使用在并行块末尾刷新的寄存器变量（或每当执行<code>flush</code>时）。</p><p><strong>Parallelism conditionality clause: if</strong><br>通过在并行命令中包括 if 子句，可以使并行性成为条件，例如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">extern int parallelism_enabled;</span><br><span class="line">#pragma omp parallel for if(parallelism_enabled)</span><br><span class="line">for(int c=0; c&lt;n; ++c)</span><br><span class="line">  handle(c);</span><br></pre></td></tr></table></figure><p>在这种情况下，如果parallelism_enabled值为零，则for循环中开启的线程数将始终为一个。</p><h3 id="Loop-construct-for"><a href="#Loop-construct-for" class="headerlink" title="Loop construct: for"></a>Loop construct: for</h3><p><code>for</code>关键字把<code>for-loop</code>拆分，因此每个线程都可以单独处理在循环里面不同的部分。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#pragma omp for</span><br><span class="line">for(int n=0; n&lt;10; ++n)</span><br><span class="line">&#123;</span><br><span class="line">  printf(&quot; %d&quot;, n);</span><br><span class="line">&#125;</span><br><span class="line">printf(&quot;.\n&quot;);</span><br></pre></td></tr></table></figure><p>此循环将输出 0…9 一次。但是，它可以按任意顺序执行。它可以输出，例如</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0 5 6 7 1 8 2 3 4 9.</span><br></pre></td></tr></table></figure><p>事实上，上述代码和如下代码效果等同：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">int this_thread = omp_get_thread_num(), num_threads = omp_get_num_threads();</span><br><span class="line">int my_start = (this_thread  ) * 10 / num_threads;</span><br><span class="line">int my_end   = (this_thread+1) * 10 / num_threads;</span><br><span class="line">for(int n=my_start; n&lt;my_end; ++n)</span><br><span class="line">  printf(&quot; %d&quot;, n);</span><br></pre></td></tr></table></figure><p>因此每个线程都会得到不同的部分代码，并且它们并行的执行只属于自己的部分。</p><p>也可以显示的指定线程数目，通过使用关键字<code>num_threads</code>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#pragma omp parallel num_threads(3)</span><br><span class="line">&#123;</span><br><span class="line">  // This code will be executed by three threads.</span><br><span class="line"></span><br><span class="line">  // Chunks of this loop will be divided amongst</span><br><span class="line">  // the (three) threads of the current team.</span><br><span class="line">  #pragma omp for</span><br><span class="line">  for(int n=0; n&lt;10; ++n) printf(&quot; %d&quot;, n);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意OpenMP同样对C语言适用。在C里面，你需要显示的指定循环变量使用关键字<code>private</code>，因为C不允许在for循环体内声明变量。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">int n;</span><br><span class="line">#pragma omp for private(n)</span><br><span class="line">for(n=0; n&lt;10; ++n) printf(&quot; %d&quot;, n);</span><br><span class="line">printf(&quot;.\n&quot;);</span><br></pre></td></tr></table></figure><h3 id="parallel-for-and-a-team概念"><a href="#parallel-for-and-a-team概念" class="headerlink" title="parallel, for and a team概念"></a>parallel, for and a team概念</h3><ul><li>parallel for ：parallel for是两个命令的组合，parallel和for 。：parallel 创建一个新team，for为该team拆分以处理循环的不同部分。</li><li>for：用于在当前team的线程之间划分 for 循环的工作。它不创建线程，它只将工作划分到当前正在执行团队的线程之间。</li><li>team: 是当前执行所有线程的组合。在程序开始的时候，team里面只包含一个线程。parallel关键字把当前线程切分为多个线程team，直到执行完毕</li></ul><h2 id="调度策略"><a href="#调度策略" class="headerlink" title="调度策略"></a>调度策略</h2><p>for-loop 的调度算法可以显式控制。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#pragma omp for schedule(static)</span><br><span class="line">for(int n=0; n&lt;10; ++n) printf(&quot; %d&quot;, n);</span><br><span class="line">printf(&quot;.\n&quot;)</span><br></pre></td></tr></table></figure><p>共有5种调度算法：<code>static</code>, <code>dynamic</code>, <code>guided</code>, <code>auto</code>, <code>runtime</code> （OpenMP 4.0之后）。之外在OpenMP4.5之后，又添加了新的3种<code>monotonic</code>, <code>nonmonotonic</code>, <code>simd</code>。</p><p>使用如下demo验证：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;omp.h&gt;</span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line"></span><br><span class="line">#define COUNT 48</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">#pragma omp parallel for schedule(static)</span><br><span class="line">/* #pragma omp parallel for schedule(static, 2) */</span><br><span class="line">    for(int i = 0;i &lt; COUNT; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        printf(&quot;Thread: %d, Iteration: %d\n&quot;, omp_get_thread_num(), i);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Build and Run:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ g++ omp.cpp -fopenmp</span><br><span class="line">$ ./a.out</span><br></pre></td></tr></table></figure><p>本机配置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">(base) huang@mlp:~/taichi/omp$ lscpu</span><br><span class="line">Architecture:        x86_64</span><br><span class="line">CPU op-mode(s):      32-bit, 64-bit</span><br><span class="line">Byte Order:          Little Endian</span><br><span class="line">CPU(s):              12</span><br><span class="line">On-line CPU(s) list: 0-11</span><br><span class="line">Thread(s) per core:  2</span><br><span class="line">Core(s) per socket:  6</span><br><span class="line">Socket(s):           1</span><br><span class="line">NUMA node(s):        1</span><br><span class="line">Vendor ID:           GenuineIntel</span><br><span class="line">CPU family:          6</span><br><span class="line">Model:               158</span><br><span class="line">Model name:          Intel(R) Core(TM) i7-8700K CPU @ 3.70GHz</span><br><span class="line">Stepping:            10</span><br><span class="line">CPU MHz:             3907.882</span><br><span class="line">CPU max MHz:         4700.0000</span><br><span class="line">CPU min MHz:         800.0000</span><br><span class="line">BogoMIPS:            7399.70</span><br><span class="line">Virtualization:      VT-x</span><br><span class="line">L1d cache:           32K</span><br><span class="line">L1i cache:           32K</span><br><span class="line">L2 cache:            256K</span><br><span class="line">L3 cache:            12288K</span><br><span class="line">NUMA node0 CPU(s):   0-11</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="static"><a href="#static" class="headerlink" title="static"></a>static</h3><p><code>static</code>是默认的调度算法。在上面的所有例子中，每个线程独立的决定它们处理哪个loop片段。<br><code>schedule(static, chunk-size)</code>子句指定 for 循环具有静态调度类型。OpenMP 将迭代划分为大小块大小的块，并将区块按循环顺序分发到线程。当<code>chunk-size</code>没有指定的时候OpenMP将迭代划分为大小大致相等的区块，并且最多将一个区块分发到每个线程。<br>下面是关于<code>static</code>调度的例子。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">schedule(static):</span><br><span class="line">****************</span><br><span class="line">                ****************</span><br><span class="line">                                ****************</span><br><span class="line">                                                ****************</span><br></pre></td></tr></table></figure><p>实际效果如下：这个分配是静态的，“静态”体现在这个分配过程跟实际的运行是无关的，可以从逻辑上推断出哪几次迭代会在哪几个线程上运行。具体而言，对于一个N次迭代，使用M个线程，那么，[0,size-1]的size次的迭代是在第一个线程上运行，[size, size + size -1]是在第二个线程上运行，依次类推。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">(base) huang@mlp:~/omp$ ./a.out</span><br><span class="line">Thread: 11, Iteration: 44</span><br><span class="line">Thread: 11, Iteration: 45</span><br><span class="line">Thread: 11, Iteration: 46</span><br><span class="line">Thread: 5, Iteration: 20</span><br><span class="line">Thread: 5, Iteration: 21</span><br><span class="line">Thread: 5, Iteration: 22</span><br><span class="line">Thread: 5, Iteration: 23</span><br><span class="line">Thread: 1, Iteration: 4</span><br><span class="line">Thread: 1, Iteration: 5</span><br><span class="line">Thread: 1, Iteration: 6</span><br><span class="line">Thread: 1, Iteration: 7</span><br><span class="line">Thread: 10, Iteration: 40</span><br><span class="line">Thread: 10, Iteration: 41</span><br><span class="line">Thread: 10, Iteration: 42</span><br><span class="line">Thread: 10, Iteration: 43</span><br><span class="line">Thread: 7, Iteration: 28</span><br><span class="line">Thread: 7, Iteration: 29</span><br><span class="line">Thread: 7, Iteration: 30</span><br><span class="line">Thread: 7, Iteration: 31</span><br><span class="line">Thread: 8, Iteration: 32</span><br><span class="line">Thread: 8, Iteration: 33</span><br><span class="line">Thread: 8, Iteration: 34</span><br><span class="line">Thread: 2, Iteration: 8</span><br><span class="line">Thread: 11, Iteration: 47</span><br><span class="line">Thread: 6, Iteration: 24</span><br><span class="line">Thread: 6, Iteration: 25</span><br><span class="line">Thread: 6, Iteration: 26</span><br><span class="line">Thread: 6, Iteration: 27</span><br><span class="line">Thread: 4, Iteration: 16</span><br><span class="line">Thread: 4, Iteration: 17</span><br><span class="line">Thread: 4, Iteration: 18</span><br><span class="line">Thread: 4, Iteration: 19</span><br><span class="line">Thread: 2, Iteration: 9</span><br><span class="line">Thread: 2, Iteration: 10</span><br><span class="line">Thread: 2, Iteration: 11</span><br><span class="line">Thread: 9, Iteration: 36</span><br><span class="line">Thread: 9, Iteration: 37</span><br><span class="line">Thread: 9, Iteration: 38</span><br><span class="line">Thread: 9, Iteration: 39</span><br><span class="line">Thread: 8, Iteration: 35</span><br><span class="line">Thread: 0, Iteration: 0</span><br><span class="line">Thread: 0, Iteration: 1</span><br><span class="line">Thread: 0, Iteration: 2</span><br><span class="line">Thread: 0, Iteration: 3</span><br><span class="line">Thread: 3, Iteration: 12</span><br><span class="line">Thread: 3, Iteration: 13</span><br><span class="line">Thread: 3, Iteration: 14</span><br><span class="line">Thread: 3, Iteration: 15</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">schedule(static, 4):</span><br><span class="line">****            ****            ****            ****</span><br><span class="line">    ****            ****            ****            ****</span><br><span class="line">        ****            ****            ****            ****</span><br><span class="line">            ****            ****            ****            ****</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">schedule(static, 8):</span><br><span class="line">********                        ********</span><br><span class="line">        ********                        ********</span><br><span class="line">                ********                        ********</span><br><span class="line">                        ********                        ********</span><br></pre></td></tr></table></figure><p>让我解释一下这些例子。我们使用 64 次迭代并行化 for 循环，并且使用四个线程并行化 for 循环。示例中的每一行星数表示一个线程。每列表示迭代。当所有迭代具有相同的计算成本时，静态调度类型是适当的。</p><h3 id="dynamic"><a href="#dynamic" class="headerlink" title="dynamic"></a>dynamic</h3><p><code>schedule(dynamic, chunk-size)</code>子句指定 for 循环具有动态调度类型。OpenMP 将迭代划分为大小块大小的块。每个线程执行一个迭代区块，然后请求另一个区块，直到不再有可用的区块。没有将区块以特定顺序分布到线程。每次执行 for 循环时，顺序会更改。<br>如果没有指定<code>chunk-size</code>，默认为1。下面是一些动态调度的例子。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">schedule(dynamic):</span><br><span class="line">*   ** **  * * *  *      *  *    **   *  *  * *       *  *   *</span><br><span class="line">  *       *     *    * *     * *   *    *        * *   *    *</span><br><span class="line"> *       *    *     * *   *   *     *  *       *  *  *  *  *   *</span><br><span class="line">   *  *     *    * *    *  *    *    *    ** *  *   *     *   *</span><br></pre></td></tr></table></figure><p>对于dynamic，没有size参数的情况下，每个线程按先执行完先分配的方式执行1次循环。实际运行情况如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">Thread: 5, Iteration: 4</span><br><span class="line">Thread: 5, Iteration: 12</span><br><span class="line">Thread: 5, Iteration: 13</span><br><span class="line">Thread: 5, Iteration: 14</span><br><span class="line">Thread: 5, Iteration: 15</span><br><span class="line">Thread: 5, Iteration: 16</span><br><span class="line">Thread: 5, Iteration: 17</span><br><span class="line">Thread: 5, Iteration: 18</span><br><span class="line">Thread: 5, Iteration: 19</span><br><span class="line">Thread: 5, Iteration: 20</span><br><span class="line">Thread: 5, Iteration: 21</span><br><span class="line">Thread: 5, Iteration: 22</span><br><span class="line">Thread: 5, Iteration: 23</span><br><span class="line">Thread: 5, Iteration: 24</span><br><span class="line">Thread: 5, Iteration: 25</span><br><span class="line">Thread: 5, Iteration: 26</span><br><span class="line">Thread: 5, Iteration: 27</span><br><span class="line">Thread: 5, Iteration: 28</span><br><span class="line">Thread: 5, Iteration: 29</span><br><span class="line">Thread: 5, Iteration: 30</span><br><span class="line">Thread: 5, Iteration: 31</span><br><span class="line">Thread: 5, Iteration: 32</span><br><span class="line">Thread: 5, Iteration: 33</span><br><span class="line">Thread: 5, Iteration: 34</span><br><span class="line">Thread: 5, Iteration: 35</span><br><span class="line">Thread: 5, Iteration: 36</span><br><span class="line">Thread: 5, Iteration: 37</span><br><span class="line">Thread: 5, Iteration: 38</span><br><span class="line">Thread: 5, Iteration: 39</span><br><span class="line">Thread: 5, Iteration: 40</span><br><span class="line">Thread: 5, Iteration: 41</span><br><span class="line">Thread: 5, Iteration: 42</span><br><span class="line">Thread: 5, Iteration: 43</span><br><span class="line">Thread: 5, Iteration: 44</span><br><span class="line">Thread: 5, Iteration: 45</span><br><span class="line">Thread: 5, Iteration: 46</span><br><span class="line">Thread: 5, Iteration: 47</span><br><span class="line">Thread: 9, Iteration: 1</span><br><span class="line">Thread: 0, Iteration: 6</span><br><span class="line">Thread: 3, Iteration: 2</span><br><span class="line">Thread: 7, Iteration: 0</span><br><span class="line">Thread: 6, Iteration: 5</span><br><span class="line">Thread: 8, Iteration: 3</span><br><span class="line">Thread: 10, Iteration: 8</span><br><span class="line">Thread: 1, Iteration: 7</span><br><span class="line">Thread: 11, Iteration: 9</span><br><span class="line">Thread: 4, Iteration: 10</span><br><span class="line">Thread: 2, Iteration: 11</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">schedule(dynamic, 1):</span><br><span class="line">    *    *     *        *   *    * *  *  *         *  * *  * *</span><br><span class="line">*  *  *   * *     *  * * *    * *      *   ***  *   *         *</span><br><span class="line"> *   *  *  *  *    ** *    *      *  *  * *   *  *   *   *</span><br><span class="line">  *    *     *  **        *  * *    *          *  *    *  * *  *</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">schedule(dynamic, 4):</span><br><span class="line">            ****                    ****                    ****</span><br><span class="line">****            ****    ****            ****        ****</span><br><span class="line">    ****            ****    ****            ****        ****</span><br><span class="line">        ****                    ****            ****</span><br></pre></td></tr></table></figure><p>实际运行情况如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">(base) huang@mlp:~/taichi/omp$ ./a.out</span><br><span class="line">Thread: 0, Iteration: 12</span><br><span class="line">Thread: 0, Iteration: 13</span><br><span class="line">Thread: 0, Iteration: 14</span><br><span class="line">Thread: 0, Iteration: 15</span><br><span class="line">Thread: 3, Iteration: 28</span><br><span class="line">Thread: 3, Iteration: 29</span><br><span class="line">Thread: 3, Iteration: 30</span><br><span class="line">Thread: 3, Iteration: 31</span><br><span class="line">Thread: 8, Iteration: 8</span><br><span class="line">Thread: 8, Iteration: 9</span><br><span class="line">Thread: 8, Iteration: 10</span><br><span class="line">Thread: 8, Iteration: 11</span><br><span class="line">Thread: 4, Iteration: 4</span><br><span class="line">Thread: 4, Iteration: 5</span><br><span class="line">Thread: 4, Iteration: 6</span><br><span class="line">Thread: 4, Iteration: 7</span><br><span class="line">Thread: 11, Iteration: 36</span><br><span class="line">Thread: 11, Iteration: 37</span><br><span class="line">Thread: 11, Iteration: 38</span><br><span class="line">Thread: 11, Iteration: 39</span><br><span class="line">Thread: 5, Iteration: 24</span><br><span class="line">Thread: 5, Iteration: 25</span><br><span class="line">Thread: 7, Iteration: 32</span><br><span class="line">Thread: 2, Iteration: 44</span><br><span class="line">Thread: 9, Iteration: 16</span><br><span class="line">Thread: 10, Iteration: 0</span><br><span class="line">Thread: 10, Iteration: 1</span><br><span class="line">Thread: 10, Iteration: 2</span><br><span class="line">Thread: 10, Iteration: 3</span><br><span class="line">Thread: 7, Iteration: 33</span><br><span class="line">Thread: 7, Iteration: 34</span><br><span class="line">Thread: 7, Iteration: 35</span><br><span class="line">Thread: 9, Iteration: 17</span><br><span class="line">Thread: 9, Iteration: 18</span><br><span class="line">Thread: 9, Iteration: 19</span><br><span class="line">Thread: 6, Iteration: 20</span><br><span class="line">Thread: 6, Iteration: 21</span><br><span class="line">Thread: 6, Iteration: 22</span><br><span class="line">Thread: 6, Iteration: 23</span><br><span class="line">Thread: 5, Iteration: 26</span><br><span class="line">Thread: 5, Iteration: 27</span><br><span class="line">Thread: 2, Iteration: 45</span><br><span class="line">Thread: 2, Iteration: 46</span><br><span class="line">Thread: 2, Iteration: 47</span><br><span class="line">Thread: 1, Iteration: 40</span><br><span class="line">Thread: 1, Iteration: 41</span><br><span class="line">Thread: 1, Iteration: 42</span><br><span class="line">Thread: 1, Iteration: 43</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">schedule(dynamic, 8):</span><br><span class="line">                ********                                ********</span><br><span class="line">                        ********        ********</span><br><span class="line">********                        ********        ********</span><br><span class="line">        ********</span><br></pre></td></tr></table></figure><p>当迭代需要不同的计算成本时，动态调度类型是合适的。这意味着迭代之间的平衡很差。动态计划类型比静态计划类型具有更高的开销，因为它在运行时动态分布迭代。</p><h2 id="Guided"><a href="#Guided" class="headerlink" title="Guided"></a>Guided</h2><p><code>Guided</code>调度算法类似于动态计划类型。OpenMP 再次将迭代划分为块。每个线程执行一个迭代块，然后请求另一个区块，直到不再有可用的区块。与动态调度类型的区别在于<code>chunk-size</code>。区块的大小与未分配迭代数除以线程数成正比。因此，区块的大小减小。<br>区块的最小大小由<code>chunk-size</code>设置。我们在调度确定它：<code>schedule(guided, chunk-size)</code>。但是，包含上次迭代的区块的大小可能小于<code>chunk-size</code>。<br>如果我们不指定<code>chunk-size</code>，则默认为 1。对应示例如下。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">schedule(guided):</span><br><span class="line">                            *********                        *</span><br><span class="line">                ************                     *******  ***</span><br><span class="line">                                     *******                   *</span><br><span class="line">****************                            *****       **    *</span><br><span class="line"></span><br><span class="line">schedule(guided, 2):</span><br><span class="line">                ************                     ****     **</span><br><span class="line">                                     *******         ***    **</span><br><span class="line">                            *********</span><br><span class="line">****************                            *****       **    **</span><br><span class="line"></span><br><span class="line">schedule(guided, 4):</span><br><span class="line">                                     *******</span><br><span class="line">                ************                     ****    ****</span><br><span class="line">                            *********</span><br><span class="line">****************                            *****    ****    ***</span><br><span class="line"></span><br><span class="line">schedule(guided, 8):</span><br><span class="line">                ************                 ********        ***</span><br><span class="line">****************</span><br><span class="line">                                     ********</span><br><span class="line">                            *********                ********</span><br></pre></td></tr></table></figure><p>我们可以看出chunk的大小一直在递减。第一个chunk总是有16个iteration，这是因为我们有64个iteration共有4个线程去并行这个loop。<br>我们还可以看到，最小区块大小在计划子句中确定。唯一的例外是最后一个区块。其大小可能低于规定的最小大小。</p><p>当迭代之间不平衡时，引导调度类型是合适的。初始区块较大，因为它们减少了开销。较小的区块在计算结束时填充计划并改善负载平衡。当在计算接近尾声时发生负载平衡不佳的情况时，此调度类型尤其合适。</p><h3 id="Auto"><a href="#Auto" class="headerlink" title="Auto"></a>Auto</h3><p><code>Auto</code>调度算法将调度决策委托给编译器和runtime。在下面的示例中，编译器/系统确定静态调度。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">schedule(auto):</span><br><span class="line">****************</span><br><span class="line">                ****************</span><br><span class="line">                                ****************</span><br><span class="line">                                                ****************</span><br></pre></td></tr></table></figure><h3 id="Runtime"><a href="#Runtime" class="headerlink" title="Runtime"></a>Runtime</h3><p><code>Runtime</code>调度策略将有关计划的决定推迟到运行时。本例中，我们已经描述了指定计划类型的不同方法。一个选项是环境变量<code>OMP_SCHEDULE</code>，另一个选项是函数<code>omp_set_schedule</code>。</p><h2 id="Default"><a href="#Default" class="headerlink" title="Default"></a>Default</h2><p>如果我们不指定任何调度策略：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#pragma omp parallel for</span><br><span class="line">for (...)</span><br><span class="line">&#123; ... &#125;</span><br></pre></td></tr></table></figure><p><code>OpenMP</code>使用默认的调度策略，由内部的控制变量<code>def-sched-var</code>决定。在我的机器上面结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">Thread: 5, Iteration: 20</span><br><span class="line">Thread: 5, Iteration: 21</span><br><span class="line">Thread: 5, Iteration: 22</span><br><span class="line">Thread: 5, Iteration: 23</span><br><span class="line">Thread: 1, Iteration: 4</span><br><span class="line">Thread: 1, Iteration: 5</span><br><span class="line">Thread: 1, Iteration: 6</span><br><span class="line">Thread: 1, Iteration: 7</span><br><span class="line">Thread: 0, Iteration: 0</span><br><span class="line">Thread: 0, Iteration: 1</span><br><span class="line">Thread: 0, Iteration: 2</span><br><span class="line">Thread: 0, Iteration: 3</span><br><span class="line">Thread: 6, Iteration: 24</span><br><span class="line">Thread: 6, Iteration: 25</span><br><span class="line">Thread: 6, Iteration: 26</span><br><span class="line">Thread: 6, Iteration: 27</span><br><span class="line">Thread: 9, Iteration: 36</span><br><span class="line">Thread: 9, Iteration: 37</span><br><span class="line">Thread: 9, Iteration: 38</span><br><span class="line">Thread: 9, Iteration: 39</span><br><span class="line">Thread: 10, Iteration: 40</span><br><span class="line">Thread: 4, Iteration: 16</span><br><span class="line">Thread: 4, Iteration: 17</span><br><span class="line">Thread: 4, Iteration: 18</span><br><span class="line">Thread: 4, Iteration: 19</span><br><span class="line">Thread: 2, Iteration: 8</span><br><span class="line">Thread: 2, Iteration: 9</span><br><span class="line">Thread: 2, Iteration: 10</span><br><span class="line">Thread: 2, Iteration: 11</span><br><span class="line">Thread: 8, Iteration: 32</span><br><span class="line">Thread: 8, Iteration: 33</span><br><span class="line">Thread: 8, Iteration: 34</span><br><span class="line">Thread: 8, Iteration: 35</span><br><span class="line">Thread: 3, Iteration: 12</span><br><span class="line">Thread: 3, Iteration: 13</span><br><span class="line">Thread: 3, Iteration: 14</span><br><span class="line">Thread: 3, Iteration: 15</span><br><span class="line">Thread: 7, Iteration: 28</span><br><span class="line">Thread: 7, Iteration: 29</span><br><span class="line">Thread: 7, Iteration: 30</span><br><span class="line">Thread: 7, Iteration: 31</span><br><span class="line">Thread: 10, Iteration: 41</span><br><span class="line">Thread: 10, Iteration: 42</span><br><span class="line">Thread: 10, Iteration: 43</span><br><span class="line">Thread: 11, Iteration: 44</span><br><span class="line">Thread: 11, Iteration: 45</span><br><span class="line">Thread: 11, Iteration: 46</span><br><span class="line">Thread: 11, Iteration: 47</span><br></pre></td></tr></table></figure><p>参考：<br><a href="https://bisqwit.iki.fi/story/howto/openmp/">https://bisqwit.iki.fi/story/howto/openmp/</a><br><a href="https://www.openmp.org/wp-content/uploads/omp-hands-on-SC08.pdf">https://www.openmp.org/wp-content/uploads/omp-hands-on-SC08.pdf</a><br><a href="https://stackoverflow.com/questions/1448318/omp-parallel-vs-omp-parallel-for">https://stackoverflow.com/questions/1448318/omp-parallel-vs-omp-parallel-for</a><br><a href="http://jakascorner.com/blog/2016/06/omp-for-scheduling.html">http://jakascorner.com/blog/2016/06/omp-for-scheduling.html</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文档尝试快速介绍 OpenMP（如版本 4.5），这是一个简单的 C/C++/Fortran 编译器扩展 C++，它允许在现有源代码中添加并行性，而无需重写它。&lt;/p&gt;</summary>
    
    
    
    <category term="技术总结" scheme="http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="OpenMP" scheme="http://huangzhiyuan.github.io/tags/OpenMP/"/>
    
  </entry>
  
  <entry>
    <title>Hello World的魔幻打印</title>
    <link href="http://huangzhiyuan.github.io/2020/08/20/Obfuscating-hello-world/"/>
    <id>http://huangzhiyuan.github.io/2020/08/20/Obfuscating-hello-world/</id>
    <published>2020-08-20T12:57:53.000Z</published>
    <updated>2020-08-21T01:57:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天遇到了个很好玩的话题，怎么模（魔）糊（幻）打印Hello World。和平常第一次接触编程语言的Hello World不一样，平常一行就能完成的功能这里实现的能够异常复杂，并且生成了一堆看似乱码的字符，结果用python解析运行却能得到和Hello World一样的结果，原理我到现在还没完全搞明白，先做个记录后面慢慢理解吧。</p><span id="more"></span><p>先看要求：<br><strong>Task:</strong></p><p>Create an obfuscated program that prints Hello World! (exactly like that). Your program may not have any strings in it.</p><p><strong>Rules:</strong></p><p>You can use any programming language you like.<br>Make it as obfuscated as possible<br>This is a popularity-contest, so the answer with the most upvotes wins.</p><p>如下答案高票得选：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(lambda _, __, ___, ____, _____, ______, _______, ________: getattr(__import__(True.__class__.__name__[_] + [].__class__.__name__[__]), ().__class__.__eq__.__class__.__name__[:__] + ().__iter__().__class__.__name__[_____:________])(_, (lambda _, __, ___: _(_, __, ___))(lambda _, __, ___: chr(___ % __) + _(_, __, ___ // __) if ___ else (lambda: _).func_code.co_lnotab, _ &lt;&lt; ________, (((_____ &lt;&lt; ____) + _) &lt;&lt; ((___ &lt;&lt; _____) - ___)) + (((((___ &lt;&lt; __) - _) &lt;&lt; ___) + _) &lt;&lt; ((_____ &lt;&lt; ____) + (_ &lt;&lt; _))) + (((_______ &lt;&lt; __) - _) &lt;&lt; (((((_ &lt;&lt; ___) + _)) &lt;&lt; ___) + (_ &lt;&lt; _))) + (((_______ &lt;&lt; ___) + _) &lt;&lt; ((_ &lt;&lt; ______) + _)) + (((_______ &lt;&lt; ____) - _) &lt;&lt; ((_______ &lt;&lt; ___))) + (((_ &lt;&lt; ____) - _) &lt;&lt; ((((___ &lt;&lt; __) + _) &lt;&lt; __) - _)) - (_______ &lt;&lt; ((((___ &lt;&lt; __) - _) &lt;&lt; __) + _)) + (_______ &lt;&lt; (((((_ &lt;&lt; ___) + _)) &lt;&lt; __))) - ((((((_ &lt;&lt; ___) + _)) &lt;&lt; __) + _) &lt;&lt; ((((___ &lt;&lt; __) + _) &lt;&lt; _))) + (((_______ &lt;&lt; __) - _) &lt;&lt; (((((_ &lt;&lt; ___) + _)) &lt;&lt; _))) + (((___ &lt;&lt; ___) + _) &lt;&lt; ((_____ &lt;&lt; _))) + (_____ &lt;&lt; ______) + (_ &lt;&lt; ___))))(*(lambda _, __, ___: _(_, __, ___))((lambda _, __, ___: [__(___[(lambda: _).func_code.co_nlocals])] + _(_, __, ___[(lambda _: _).func_code.co_nlocals:]) if ___ else []), lambda _: _.func_code.co_argcount, (lambda _: _, lambda _, __: _, lambda _, __, ___: _, lambda _, __, ___, ____: _, lambda _, __, ___, ____, _____: _, lambda _, __, ___, ____, _____, ______: _, lambda _, __, ___, ____, _____, ______, _______: _, lambda _, __, ___, ____, _____, ______, _______, ________: _)))</span><br></pre></td></tr></table></figure><p>第一印象，什么鬼？？第二眼还是看不懂。<br>来看看作者怎么解释的。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Here is a more readable version: http://codepad.org/UzSmoxF2</span><br><span class="line"></span><br><span class="line">Notes:</span><br><span class="line">One line, single expression (i.e. no print statement).</span><br><span class="line">No strings, no ints; only functions, attribute access, lists, tuples, basic math, one True, and one star-args.</span><br><span class="line">Minimal builtin usage (__import__, getattr, and chr once each).</span><br><span class="line">The payload can be changed easily. Here is the program I wrote to generate it.</span><br><span class="line">Edit: I wrote a fairly substantial explanation of how this works on my blog.</span><br></pre></td></tr></table></figure><p>编写<code>generate.py</code>脚本通过对输入字符串加密：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"># How to use?</span><br><span class="line"># python generate.py &quot;the str needed to be encry&quot;</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">from math import ceil, log</span><br><span class="line">import sys</span><br><span class="line"></span><br><span class="line">def encode_string(s):</span><br><span class="line">    bytes = [ord(c) for c in s]</span><br><span class="line">    num = sum(bytes[i] * 256 ** i for i in range(len(bytes)))</span><br><span class="line">    return reduce(num, 0)</span><br><span class="line"></span><br><span class="line">def reduce(num, depth):</span><br><span class="line">    def _encode(num, depth):</span><br><span class="line">        if num == 0:</span><br><span class="line">            return &quot;_ - _&quot;</span><br><span class="line">        if num in range(9):</span><br><span class="line">            return &quot;_&quot; * num</span><br><span class="line">        return &quot;(&quot; + reduce(num, depth + 1) + &quot;)&quot;</span><br><span class="line">    result = &quot;&quot;</span><br><span class="line">    while num:</span><br><span class="line">        best_base = best_shift = 0</span><br><span class="line">        best = num</span><br><span class="line">        span = int(ceil(log(abs(num), 1.5))) + (16 &gt;&gt; depth)</span><br><span class="line">        for base in range(span):</span><br><span class="line">            for shift in range(span):</span><br><span class="line">                diff = abs(num) - (base &lt;&lt; shift)</span><br><span class="line">                if abs(diff) &lt; abs(best):</span><br><span class="line">                    best = diff</span><br><span class="line">                    best_base = base</span><br><span class="line">                    best_shift = shift</span><br><span class="line">        if result:</span><br><span class="line">            result += &quot; + &quot; if num &gt; 0 else &quot; - &quot;</span><br><span class="line">        elif num &lt; 0:</span><br><span class="line">            best_base = -best_base</span><br><span class="line">        if best_shift == 0:</span><br><span class="line">            result += _encode(best_base, depth)</span><br><span class="line">        else:</span><br><span class="line">            result += &quot;(%s &lt;&lt; %s)&quot; % (_encode(best_base, depth),</span><br><span class="line">                                      _encode(best_shift, depth))</span><br><span class="line">        num = best if num &gt; 0 else -best</span><br><span class="line">    return result</span><br><span class="line"></span><br><span class="line"># print(len(sys.argv))</span><br><span class="line"># print(&#x27; &#x27;.join(sys.argv[1:]))</span><br><span class="line">result = encode_string(str(&#x27; &#x27;.join(sys.argv[1:]))+&quot;\n&quot;)</span><br><span class="line">print(&quot;Encrypted Text: &quot;)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><p>Run tese case:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(base) huang@mlp:~/taichi/intel$ python generate.py huang zhiyuan</span><br><span class="line">Encrypted Text:</span><br><span class="line">(((_______ &lt;&lt; ___) - _) &lt;&lt; ((___ &lt;&lt; _____) + _)) + (((___ &lt;&lt; _____) + _) &lt;&lt; ((((___ &lt;&lt; __) - _) &lt;&lt; ___))) + (((((_ &lt;&lt; ____) - _) &lt;&lt; ___) - ___) &lt;&lt; ((_____ &lt;&lt; ____))) + (((((_ &lt;&lt; ____) - _) &lt;&lt; ___) + _) &lt;&lt; (((((_ &lt;&lt; ___) + _)) &lt;&lt; ___))) + (((((___ &lt;&lt; __) + _) &lt;&lt; ___) + _) &lt;&lt; ((_ &lt;&lt; ______))) + ((((___ &lt;&lt; __) + _)) &lt;&lt; ((((_ &lt;&lt; ____) - _) &lt;&lt; __) - _)) + (((((_ &lt;&lt; ____) - _) &lt;&lt; __) + _) &lt;&lt; ((___ &lt;&lt; ____) + _)) + (((_ &lt;&lt; ______) + _) &lt;&lt; ((_____ &lt;&lt; ___) - _)) - (((___ &lt;&lt; ____) + _) &lt;&lt; ((_ &lt;&lt; _____) - _)) - ((((((_ &lt;&lt; ___) + _)) &lt;&lt; __) - _) &lt;&lt; ((___ &lt;&lt; ___) - _)) - (((_ &lt;&lt; _____) - _) &lt;&lt; ((_ &lt;&lt; ____))) + (((_______ &lt;&lt; __) + _) &lt;&lt; ((_____ &lt;&lt; _))) + ((((___ &lt;&lt; __) - _)) &lt;&lt; _____) + (_ &lt;&lt; ___)</span><br></pre></td></tr></table></figure><p>利用上面生成的加密字符填充运行脚本<code>run.py</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">huang@mlp:~/taichi/intel$ cat run.py</span><br><span class="line"></span><br><span class="line">(lambda _, __, ___, ____, _____, ______, _______, ________:</span><br><span class="line">    getattr(</span><br><span class="line">        __import__(True.__class__.__name__[_] + [].__class__.__name__[__]),</span><br><span class="line">        ().__class__.__eq__.__class__.__name__[:__] +</span><br><span class="line">        ().__iter__().__class__.__name__[_____:________]</span><br><span class="line">    )(</span><br><span class="line">        _, (lambda _, __, ___: _(_, __, ___))(</span><br><span class="line">            lambda _, __, ___:</span><br><span class="line">                chr(___ % __) + _(_, __, ___ // __) if ___ else</span><br><span class="line">                (lambda: _).func_code.co_lnotab,</span><br><span class="line">            _ &lt;&lt; ________,</span><br><span class="line"></span><br><span class="line">            ## fill the encrypted text here</span><br><span class="line">            (((_____ &lt;&lt; _____) + _______) &lt;&lt; ((___ &lt;&lt; _____) + (_ &lt;&lt; __))) - ((((___ &lt;&lt; __) + _)) &lt;&lt; ((___ &lt;&lt; _____) - ___)) + (((___ &lt;&lt; _____) - ___) &lt;&lt; ((_____ &lt;&lt; ____) + (_ &lt;&lt; _))) + (((___ &lt;&lt; ____) - _) &lt;&lt; (((((_ &lt;&lt; ___) + _)) &lt;&lt; ___) + ___)) + (((((___ &lt;&lt; __) - _) &lt;&lt; __) + _) &lt;&lt; ((((_ &lt;&lt; ____) + _) &lt;&lt; __) - _)) + (((((___ &lt;&lt; __) - _) &lt;&lt; __) + _) &lt;&lt; ((((_ &lt;&lt; ____) - _) &lt;&lt; __) - _)) + (((((_ &lt;&lt; ____) - _) &lt;&lt; __) + _) &lt;&lt; ((___ &lt;&lt; ____) + _)) + (((_ &lt;&lt; ______) + _) &lt;&lt; ((_____ &lt;&lt; ___) - _)) - (((___ &lt;&lt; ____) + _) &lt;&lt; ((_ &lt;&lt; _____) - _)) - ((((((_ &lt;&lt; ___) + _)) &lt;&lt; __) - _) &lt;&lt; ((___ &lt;&lt; ___) - _)) - (((_ &lt;&lt; _____) - _) &lt;&lt; ((_ &lt;&lt; ____))) + (((_______ &lt;&lt; __) + _) &lt;&lt; ((_____ &lt;&lt; _))) + ((((___ &lt;&lt; __) - _)) &lt;&lt; _____) + (_ &lt;&lt; ___)</span><br><span class="line"></span><br><span class="line">        )</span><br><span class="line">    )</span><br><span class="line">)(</span><br><span class="line">    *(lambda _, __, ___: _(_, __, ___))(</span><br><span class="line">        (lambda _, __, ___:</span><br><span class="line">            [__(___[(lambda: _).func_code.co_nlocals])] +</span><br><span class="line">            _(_, __, ___[(lambda _: _).func_code.co_nlocals:]) if ___ else []</span><br><span class="line">        ),</span><br><span class="line">        lambda _: _.func_code.co_argcount,</span><br><span class="line">        (</span><br><span class="line">            lambda _: _,</span><br><span class="line">            lambda _, __: _,</span><br><span class="line">            lambda _, __, ___: _,</span><br><span class="line">            lambda _, __, ___, ____: _,</span><br><span class="line">            lambda _, __, ___, ____, _____: _,</span><br><span class="line">            lambda _, __, ___, ____, _____, ______: _,</span><br><span class="line">            lambda _, __, ___, ____, _____, ______, _______: _,</span><br><span class="line">            lambda _, __, ___, ____, _____, ______, _______, ________: _</span><br><span class="line">        )</span><br><span class="line">    )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>验（zhuang）证（bi）结（shi）果（ke）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) huang@mlp:~/taichi/intel$ python2 run.py</span><br><span class="line">huang zhiyuan</span><br></pre></td></tr></table></figure><p>版本目前来看只支持python2，否则会报error,从python2到python3有些built-in函数attribute命名规则发生了<a href="https://docs.python.org/3.0/whatsnew/3.0.html">改变</a>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The function attributes named func_X have been renamed to use the __X__ form, freeing up these names in the function attribute namespace for user-defined attributes. To wit, func_closure, func_code, func_defaults, func_dict, func_doc, func_globals, func_name were renamed to __closure__, __code__, __defaults__, __dict__, __doc__, __globals__, __name__, respectively.</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">(base) huang@mlp:~/taichi/intel$ python run.py</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;run.py&quot;, line 32, in &lt;module&gt;</span><br><span class="line">    lambda _, __, ___, ____, _____, ______, _______, ________: _</span><br><span class="line">  File &quot;run.py&quot;, line 18, in &lt;lambda&gt;</span><br><span class="line">    *(lambda _, __, ___: _(_, __, ___))(</span><br><span class="line">  File &quot;run.py&quot;, line 21, in &lt;lambda&gt;</span><br><span class="line">    _(_, __, ___[(lambda _: _).func_code.co_nlocals:]) if ___ else []</span><br><span class="line">AttributeError: &#x27;function&#x27; object has no attribute &#x27;func_code&#x27;</span><br></pre></td></tr></table></figure><p>参考链接：<br><a href="https://codegolf.stackexchange.com/questions/22533/weirdest-obfuscated-hello-world">https://codegolf.stackexchange.com/questions/22533/weirdest-obfuscated-hello-world</a><br><a href="https://benkurtovic.com/2014/06/01/obfuscating-hello-world.html">https://benkurtovic.com/2014/06/01/obfuscating-hello-world.html</a><br><a href="http://codepad.org/UzSmoxF2">http://codepad.org/UzSmoxF2</a><br><a href="http://codepad.org/oVuFVcB5">http://codepad.org/oVuFVcB5</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;今天遇到了个很好玩的话题，怎么模（魔）糊（幻）打印Hello World。和平常第一次接触编程语言的Hello World不一样，平常一行就能完成的功能这里实现的能够异常复杂，并且生成了一堆看似乱码的字符，结果用python解析运行却能得到和Hello World一样的结果，原理我到现在还没完全搞明白，先做个记录后面慢慢理解吧。&lt;/p&gt;</summary>
    
    
    
    <category term="技术总结" scheme="http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="加密" scheme="http://huangzhiyuan.github.io/tags/%E5%8A%A0%E5%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>分布式训练理论篇</title>
    <link href="http://huangzhiyuan.github.io/2020/08/12/distributed-training-theory/"/>
    <id>http://huangzhiyuan.github.io/2020/08/12/distributed-training-theory/</id>
    <published>2020-08-12T07:00:45.000Z</published>
    <updated>2020-08-12T08:32:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>我们为什么需要分布式，一方面是不得已而为之，例如</p><p>数据量太大，数据无法加载<br>模型太复杂，一个GPU放不下<br>或者我们也可以利用分布式提高我们的训练速度<br><img src="/img/2020/0812/distribution.png" alt="&quot;1&quot;"></p><span id="more"></span><h2 id="算法分类"><a href="#算法分类" class="headerlink" title="算法分类"></a>算法分类</h2><h3 id="Parameter-Server"><a href="#Parameter-Server" class="headerlink" title="Parameter Server"></a>Parameter Server</h3><p>机器分为Parameter Server 和 woker 两类，Parameter server 负责整合梯度，更新参数，woker负责计算，训练网络。</p><p><img src="/img/2020/0812/dsd.jpg" alt="&quot;1&quot;"></p><p>从上图可以很清楚的看到PS的流程，共分为4步。</p><ul><li>Task Scheduler：<br>加载数据，并将数据分发给不同的workers</li><li>Workers：</li></ul><p>1.计算梯度（1. compute）<br>加载训练数据; 从servers拉取最新的参数（4. Pull）<br>2.将梯度push到servers（2.Push）<br>从servers拉取最新的参数（4. Pull）</p><ul><li>Servers：</li></ul><p>1.聚合梯度<br>2.更新参数（3. Update）</p><p>通信成本为： O(d/b(2m-1)), 通信时间和GPU个数成正比。<br><strong>m:</strong> number of GPUs.<br><strong>d:</strong> number of parameters.<br><strong>b:</strong> network bandwidth.</p><h3 id="Ring-ALL-Reduce"><a href="#Ring-ALL-Reduce" class="headerlink" title="Ring ALL-Reduce"></a>Ring ALL-Reduce</h3><p>reduce 和 all-reduce的区别：</p><ul><li>reduce：server 获取reduce之后的结果（sum/avg/mean/count），其他节点并不知道reduce之后的结果。例如我们需要进行reduce_sum 的操作，那么所有的workers的值都会传给server，然后server将这些值进行相加。</li></ul><p><img src="/img/2020/0812/reduce.jpg" alt="&quot;1&quot;"></p><ul><li>all reduce：所有的节点都获取reduce之后的结果，不单单server有。all-reduce是reduce+broadcast。例如我们需要进行all-reduce_sum 的操作，那么所有的workers的值都会传给server，然后server将这些值进行相加，然后server会将相加的和broadcast给所有的workers。</li></ul><p><img src="/img/2020/0812/all-reduce.jpg" alt="&quot;1&quot;"></p><ul><li>all-to-all communication。或者说如果没有 server 机器，那么我们可以通过all-to-all 通信，让每个节点都获取其他节点的值，然后再在各自机器上进行reduce。</li></ul><p><img src="/img/2020/0812/all-to-all.jpg" alt="&quot;1&quot;"></p><p>首先我们有4台机器GPU 0:4, 我们的目标是将每个GPU计算出来的梯度 [公式] 进行相加，使得每个GPU都有所有GPU的梯度。</p><p><strong>Ring All-Reduce</strong><br><img src="/img/2020/0812/ring0.jpg" alt="&quot;1&quot;"></p><p><strong>Native Approach</strong><br>首先我们想到的方式是，我们将每个GPU的目前加和的梯度都向下传递，然后最后一个GPU可以得到最后的加和，然后它再将所有的结果传递给其他的没有获得结果的GPU。<br>整个过程如下：</p><ul><li>GPU0 将 g0 传给GPU1，GPU1得到 g0+g1 的结果<br><img src="/img/2020/0812/ring1.jpg" alt="&quot;1&quot;"></li><li>GPU1 将 g0+g1 传给 GPU2，GPU2得到 g0+g1+g2 的结果<br><img src="/img/2020/0812/ring2.jpg" alt="&quot;1&quot;"></li><li>GPU2 将 g0+g1+g2 传给 GPU3，GPU3得到 g0+g1+g2+g3 的结果<br><img src="/img/2020/0812/ring3.jpg" alt="&quot;1&quot;"></li><li>GPU3 将 g=g0+g1+g2+g3 传给GPU0， GPU0 得到 g</li><li>GPU0 将 g 传给GPU1， GPU1 得到 g</li><li>GPU1 将 g 传给GPU3， GPU1 得到 g<br><img src="/img/2020/0812/ring4.jpg" alt="&quot;1&quot;"><br><img src="/img/2020/0812/ring5.jpg" alt="&quot;1&quot;"></li></ul><p>最后每个worker自己做参数更新。</p><p>我们可以发现，这样子做的话，每次的数据传输都只用了一个通道，即只有两台GPU在进行传输，其他的3个通道都没有使用，造成了资源的浪费。<br>我们假设有 m 台机器，每台机器传输的参数量为 d, 传输带宽为 b， 我们可以得到传输的时间为 O(d*m/b)，传输的速度和GPU数目成正比，这极大地限制了集群的可扩展性。</p><p><strong>Ring All-Reduce (Efficient Approach)</strong><br>为了充分利用起其他的通道，我们可以采取 Ring All-reduce的方式进行传输。其主要的作用就是将传输的时间不再正比于GPU的数目。</p><p>首先我们将传输的梯度进行 m 等分split，每一次的传输传的参数量为 d/m。 如下图。</p><p><img src="/img/2020/0812/gpu.jpg" alt="&quot;1&quot;"></p><p>首先我们得到GPU_i 计算出来的梯度倍等切成后的 gi=[ai;bi;ci;di]（本例子为4等分），然后我们可以发现所有 ai 加起来是第一部分的加和，所有 bi 加起来是第2部分的加和，所有 ci 加起来是第3部分的加和，所有 di 加起来是第4部分的加和。并且 GPU0 可以传输给GPU1，GPU1 可以传输给GPU2，GPU2可以传输给GPU3，GPU3可以传输给GPU0，形成环状网络。</p><p>这个环状传输是同时进行的，其保证了每次传输的并行（纵向），且保证了每次的reduce计算都只在机器的一个部分，不会同一个机器多个部分同时进行（横向）。<br><img src="/img/2020/0812/gpu0.jpg" alt="&quot;1&quot;"></p><ul><li>首先进行第一次传输， GPU0 –a0–&gt; GPU1，GPU1 –b1–&gt; GPU2, GPU2 –c2–&gt; GPU3, GPU3 –d3–&gt; GPU0, 注意这边的初始传输值不是 a0, a1, a2,a3</li><li><img src="/img/2020/0812/gpu1.jpg" alt="&quot;1&quot;"></li><li>第二次传输，就是将目前得到的每个部门的reduce结果，传输给下个device。<br>GPU1 –a0+a1–&gt; GPU2，GPU2 –b1+b2–&gt; GPU3, GPU3 –c2+c3–&gt; GPU0, GPU0 –d0+d3–&gt; GPU1<br><img src="/img/2020/0812/gpu2.jpg" alt="&quot;1&quot;"></li><li>第三次传输，还是将上次传输的结果传给下一个device，注意这边传的是结果而不是中间过程，即例如传输 a0+a1+a2 是传输这个结果而不是三个a0,a1,a2参数,否则传输的参数量就增加了。<br>GPU2 –a0+a1+a2–&gt; GPU3，GPU3 –b1+b2+b3–&gt; GPU0, GPU0 –c0+c2+c3–&gt; GPU1, GPU1 –d0+d1+d3–&gt; GPU2<br><img src="/img/2020/0812/gpu3.jpg" alt="&quot;1&quot;"></li><li>第四次传输,还是将上次传输的结果传给下一个device，此时，每个部分都已经有一个机器获得了最终的结果。<br><img src="/img/2020/0812/gpu41.jpg" alt="&quot;1&quot;"></li><li>我们令a=a0+a1+a2+a3, b=b0+b1+b2+b3, c=c0+c1+c2+c3, d=d0+d1+d2+d3<br>GPU3 –a–&gt; GPU0，GPU0 –b–&gt; GPU1, GPU1 –d–&gt; GPU2, GPU2 –d–&gt; GPU3<br><img src="/img/2020/0812/gpu42.jpg" alt="&quot;1&quot;"></li><li>第五次传输，如下<br><img src="/img/2020/0812/gpu5.jpg" alt="&quot;1&quot;"></li><li>第六次传输<br><img src="/img/2020/0812/gpu6.jpg" alt="&quot;1&quot;"></li><li>得到最终的结果：<br><img src="/img/2020/0812/gpu7.jpg" alt="&quot;1&quot;"></li></ul><p>我们可以计算一下时间的开销：O(d/m *m/b) = O(d/m) ，传输的时间和GPU的个数无关！</p><h2 id="更新方式分类"><a href="#更新方式分类" class="headerlink" title="更新方式分类"></a>更新方式分类</h2><p>同步式和异步式的区别在于参数的更新，是否需要等其他的worker的梯度计算完成。同步式需要，而异步式不需要。</p><h3 id="同步式"><a href="#同步式" class="headerlink" title="同步式"></a>同步式</h3><p>我们以 MultiworkerMirroredStrategy 作为例子：<br>首先是数据并行，数据会被分发到不同的devices（下图的A/B/C）上，然后每个device 各自进行梯度计算，最后Parameter Device(s) 会等所有的梯度都计算完之后，再更新参数，最后将参数传给所有的devices。<br><img src="/img/2020/0812/sync.jpg" alt="&quot;1&quot;"><br>同步式的更新的优缺点：<br><strong>优点：</strong></p><ul><li>同步避免过多的通信（因为参数传输需要通信）</li><li>效果会比异步并行略好</li></ul><p><strong>缺点：</strong></p><ul><li>有的机器计算可能快，有的机器计算慢，这样就导致，个别机器会有计算资源的空耗，导致浪费。（单机多卡一般不会）<br><img src="/img/2020/0812/sync1.jpg" alt="&quot;1&quot;"></li></ul><h3 id="异步式"><a href="#异步式" class="headerlink" title="异步式"></a>异步式</h3><p>我们以 ParameterServerStrategy 作为例子：<br>首先我们的所有的数据还是进行并行化，分发到不同的devices（下图的A/B/C）上，每个 worker device 各自进行了梯度的计算后，传给parameter server，而参数服务器在收到了梯度之后，不用等其他机器的结果，直接去更新参数，得到新的参数值，传给各个worker。<br><img src="/img/2020/0812/async.jpg" alt="&quot;1&quot;"></p><p>异步式的更新的优缺点：<br><strong>优点：</strong></p><ul><li>参数更新不需要等其他机器，所有更高效，避免短板效应（多机多卡）</li><li>异步的计算会增加模型的泛化能力,因为异步不是严格正确的，所以模型更容忍错误</li></ul><p><strong>缺点：</strong></p><ul><li>异步式更新的次数多了，会导致更多的通信时间</li><li>每个GPU计算完梯度后就各自更新，速度快，但是最终模型的性能略低于同步数据并行方式。<br><img src="/img/2020/0812/async1.jpg" alt="&quot;1&quot;"></li></ul><h2 id="并行方式分类"><a href="#并行方式分类" class="headerlink" title="并行方式分类"></a>并行方式分类</h2><h3 id="数据并行"><a href="#数据并行" class="headerlink" title="数据并行"></a>数据并行</h3><p>数据同时分发给各个worker，并进行梯度计算。</p><h3 id="模型并行"><a href="#模型并行" class="headerlink" title="模型并行"></a>模型并行</h3><p>当我们的模型过大，不能放在一个GPU中，那么需要将模型分成多个部分，在各个worker 中。<br><img src="/img/2020/0812/model-para.jpg" alt="&quot;1&quot;"></p><h3 id="数据和模型并行"><a href="#数据和模型并行" class="headerlink" title="数据和模型并行"></a>数据和模型并行</h3><p>把数据和模型同时并行，进行训练。其中最出名的就是微软的<a href="https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/">ZeRO &amp; DeepSpeed: New system optimizations enable training models with over 100 billion parameters</a>算法。它的出现使得训练超级大模型成了可能。微软用这个算法，训练出了 <a href="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/">Turing-NLG: A 17-billion-parameter language model by Microsoft</a> 170亿的参数，直接刷新世界观（壕无人性）</p><p><img src="/img/2020/0812/pic.jpg" alt="&quot;1&quot;"></p><p>该ZeRO算法，主要如下图。</p><p><img src="/img/2020/0812/zero.jpg" alt="&quot;1&quot;"></p><p>数据被分为n+1份，并行处理。<br>然后整个模型结构被过大，不能但在单一的GPU中，它被存放在n+1个GPU中，每个GPU存放三个部分</p><ul><li>蓝色的部分是模型的参数</li><li>黄色的部分是模型的梯度</li><li>绿色的部分是模型梯度更新中间产生的数据，例如adam的中间产生数据。</li></ul><p>那么模型要如何进行训练呢？<br>首先我们知道模型在进行更新的过程是先将数据正向传播，计算loss和梯度，再反向传播进行更新。例如下图，我们的模型存放在两个GPU中，在正向传播过程中我们必须等到数据经过GPU1，才能到GPU2，在反向传播过程中，我们需要保证GPU2的参数都计算完成，才能传到GPU1，这就导致了每次都有一个机器是闲置的。</p><p><img src="/img/2020/0812/idle.jpg" alt="&quot;1&quot;"></p><p>这是因为我们的每个GPU都缺少了部分的模型参数，但是我们发现其实模型的参数并不大，大的是模型的 optimizer 的中间参数， 所以我们就是可以通过将所需要的其他部分的模型参数通过通信传输，获得，使得所有的部分都可以并行。</p><ul><li>在进行前向传播的时候，我们以 GPU0 做为例子，在data0 flow的时候，如果需要任何的参数，都可以通过通信的过程获得，比如说刚开始先forward flow完成GPU0的模型结构，之后需要GPU1的模型结构，GPU1 会将其的参数传给GPU0，之后GPU2会将其的参数传给GPU0… 以此类推，直至前行传播完成获得loss。计算出梯度。注意此时DATA1，…DATAn+1 都在并行进行前向传播。</li><li>再反向传播的过程中，我们以 GPU0 做为例子，首先会计算出n+1 部分模型的梯度Gn+1（黄色的部分）, GPU0 不会储存这个梯度，而是将它直接传输给GPU_{n+1}, 因为它负责更新n+1 部分模型（注意，此时所有的其他GPU计算出来的这部分的梯度都会传给它，最后在有它进行reduce），之后计算会计算出第n 部分模型的梯度Gn（黄色的部分），并传给 GPU_n，以此类推，算出得到第0部分模型的梯度G0（黄色的部分），然后并且收到了其他GPU关于此部分的梯度，并按照optimizer paramters进行第0部分的参数的更新。</li></ul><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p><a href="https://fyubang.com/2019/07/08/distributed-training/">https://fyubang.com/2019/07/08/distributed-training/</a><br><a href="https://zhuanlan.zhihu.com/p/129912419">https://zhuanlan.zhihu.com/p/129912419</a><br><a href="https://www.youtube.com/watch%3Fv%3DtC01FRB0M7w">https://www.youtube.com/watch%3Fv%3DtC01FRB0M7w</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;我们为什么需要分布式，一方面是不得已而为之，例如&lt;/p&gt;
&lt;p&gt;数据量太大，数据无法加载&lt;br&gt;模型太复杂，一个GPU放不下&lt;br&gt;或者我们也可以利用分布式提高我们的训练速度&lt;br&gt;&lt;img src=&quot;/img/2020/0812/distribution.png&quot; alt=&quot;&amp;quot;1&amp;quot;&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://huangzhiyuan.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="GPU" scheme="http://huangzhiyuan.github.io/tags/GPU/"/>
    
  </entry>
  
  <entry>
    <title>Copy issue in ATS GPU</title>
    <link href="http://huangzhiyuan.github.io/2020/08/11/copy-issue-in-ATS/"/>
    <id>http://huangzhiyuan.github.io/2020/08/11/copy-issue-in-ATS/</id>
    <published>2020-08-11T08:53:34.000Z</published>
    <updated>2020-08-11T09:13:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>Performance of copy using different data types.<br>Got a performance issue on ATS when copy a continuous buffer of u16 data type. It is almost 2x slower than copy a buffer of same byte-length but using u32 data type. Similarly, copy of u8 is even slower. Below is a test case of copy a same buffer using different data type of u32/u16/u8.</p><span id="more"></span><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><span class="line">// Build with DPC++ Compiler:</span><br><span class="line">// clang++ -std=c++14 -fsycl -O3 copy.cpp -o copy</span><br><span class="line">//</span><br><span class="line">// Run:</span><br><span class="line">// ./copy 65536 512</span><br><span class="line">//</span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;cmath&gt;</span><br><span class="line">#include &lt;chrono&gt;</span><br><span class="line">#include &quot;CL/sycl.hpp&quot;</span><br><span class="line">// Timer</span><br><span class="line">#define _(x) x</span><br><span class="line">#define __tstart(n) _(std::chrono::high_resolution_clock::time_point __s##n =  \</span><br><span class="line">                      std::chrono::high_resolution_clock::now());</span><br><span class="line">#define __tend(n)                                                              \</span><br><span class="line">  _(std::chrono::high_resolution_clock::time_point __e##n =                    \</span><br><span class="line">    std::chrono::high_resolution_clock::now());                                \</span><br><span class="line">  _(printf(&quot;time: %s, %.2f ms\n&quot;, #n,                                          \</span><br><span class="line">           std::chrono::duration&lt;float, std::milli&gt;(__e##n - __s##n).count()));</span><br><span class="line">namespace sycl = cl::sycl;</span><br><span class="line">int main(int argc, char * argv[])</span><br><span class="line">&#123;</span><br><span class="line">  if (argc &lt; 3) &#123;</span><br><span class="line">    std::cerr &lt;&lt; &quot;Usage: copy &lt;M length&gt; &lt;N length&gt;&quot; &lt;&lt; std::endl;</span><br><span class="line">    return argc;</span><br><span class="line">  &#125;</span><br><span class="line">  size_t M = std::atoi(argv[1]);</span><br><span class="line">  size_t N = std::atoi(argv[2]);</span><br><span class="line">  if (N % 4 != 0) &#123;</span><br><span class="line">    std::cerr &lt;&lt; &quot;Usage: N must be times of 4&quot; &lt;&lt; std::endl;</span><br><span class="line">  &#125;</span><br><span class="line">  sycl::queue q(sycl::default_selector&#123;&#125;);</span><br><span class="line">  auto ctx = q.get_context();</span><br><span class="line">  auto dev = q.get_device();</span><br><span class="line">  uint8_t *Y = static_cast&lt;uint8_t*&gt;(sycl::malloc_shared(M * N, dev, ctx));</span><br><span class="line">  uint8_t *Z = static_cast&lt;uint8_t*&gt;(sycl::malloc_shared(M * N, dev, ctx));</span><br><span class="line">  uint32_t *Z32 = (uint32_t*)Z;</span><br><span class="line">  uint32_t *Y32 = (uint32_t*)Y;</span><br><span class="line">  uint16_t *Z16 = (uint16_t*)Z;</span><br><span class="line">  uint16_t *Y16 = (uint16_t*)Y;</span><br><span class="line">  size_t N32 = N / 4;</span><br><span class="line">  size_t N16 = N / 2;</span><br><span class="line">  for (size_t i = 0; i &lt; M * N; i++) &#123;</span><br><span class="line">    Y[i] = i % 255;</span><br><span class="line">  &#125;</span><br><span class="line">  // warmup</span><br><span class="line">  &#123;</span><br><span class="line">    for (size_t j = 0; j &lt; 5; j++) &#123;</span><br><span class="line">      q.submit([&amp;](sycl::handler&amp; h) &#123;</span><br><span class="line">          h.parallel_for&lt;class u32_warmup&gt;( sycl::range&lt;2&gt;&#123;M, N32&#125;, [=] (sycl::id&lt;2&gt; it) &#123;</span><br><span class="line">              const int m = it[0];</span><br><span class="line">              const int n = it[1];</span><br><span class="line">              Z32[m * N32 + n] = Y32[m * N32 + n];</span><br><span class="line">          &#125;);</span><br><span class="line">      &#125;);</span><br><span class="line">      q.wait();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  __tstart(copy_as_u32);</span><br><span class="line">  &#123;</span><br><span class="line">    for (size_t j = 0; j &lt; 100; j++) &#123;</span><br><span class="line">      q.submit([&amp;](sycl::handler&amp; h) &#123;</span><br><span class="line">          h.parallel_for&lt;class u32&gt;( sycl::range&lt;2&gt;&#123;M, N32&#125;, [=] (sycl::id&lt;2&gt; it) &#123;</span><br><span class="line">              const int m = it[0];</span><br><span class="line">              const int n = it[1];</span><br><span class="line">              Z32[m * N32 + n] = Y32[m * N32 + n];</span><br><span class="line">          &#125;);</span><br><span class="line">      &#125;);</span><br><span class="line">      q.wait();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  __tend(copy_as_u32);</span><br><span class="line">  // warmup</span><br><span class="line">   &#123;</span><br><span class="line">    for (size_t j = 0; j &lt; 5; j++) &#123;</span><br><span class="line">      q.submit([&amp;](sycl::handler&amp; h) &#123;</span><br><span class="line">          h.parallel_for&lt;class u16_warmup&gt;( sycl::range&lt;2&gt;&#123;M, N16&#125;, [=] (sycl::id&lt;2&gt; it) &#123;</span><br><span class="line">              const int m = it[0];</span><br><span class="line">              const int n = it[1];</span><br><span class="line">              Z16[m * N16 + n] = Y16[m * N16 + n];</span><br><span class="line">          &#125;);</span><br><span class="line">      &#125;);</span><br><span class="line">      q.wait();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  __tstart(copy_as_u16);</span><br><span class="line">  &#123;</span><br><span class="line">    for (size_t j = 0; j &lt; 100; j++) &#123;</span><br><span class="line">      q.submit([&amp;](sycl::handler&amp; h) &#123;</span><br><span class="line">          h.parallel_for&lt;class u16&gt;( sycl::range&lt;2&gt;&#123;M, N16&#125;, [=] (sycl::id&lt;2&gt; it) &#123;</span><br><span class="line">              const int m = it[0];</span><br><span class="line">              const int n = it[1];</span><br><span class="line">              Z16[m * N16 + n] = Y16[m * N16 + n];</span><br><span class="line">          &#125;);</span><br><span class="line">      &#125;);</span><br><span class="line">      q.wait();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  __tend(copy_as_u16);</span><br><span class="line">  // warmup</span><br><span class="line">  &#123;</span><br><span class="line">    for (size_t j = 0; j &lt; 5; j++) &#123;</span><br><span class="line">      q.submit([&amp;](sycl::handler&amp; h) &#123;</span><br><span class="line">          h.parallel_for&lt;class u8_warmup&gt;( sycl::range&lt;2&gt;&#123;M, N&#125;, [=] (sycl::id&lt;2&gt; it) &#123;</span><br><span class="line">              const int m = it[0];</span><br><span class="line">              const int n = it[1];</span><br><span class="line">              Z[m * N + n] = Y[m * N + n];</span><br><span class="line">          &#125;);</span><br><span class="line">      &#125;);</span><br><span class="line">      q.wait();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  __tstart(copy_as_u8);</span><br><span class="line">  &#123;</span><br><span class="line">    for (size_t j = 0; j &lt; 100; j++) &#123;</span><br><span class="line">      q.submit([&amp;](sycl::handler&amp; h) &#123;</span><br><span class="line">          h.parallel_for&lt;class u8&gt;( sycl::range&lt;2&gt;&#123;M, N&#125;, [=] (sycl::id&lt;2&gt; it) &#123;</span><br><span class="line">              const int m = it[0];</span><br><span class="line">              const int n = it[1];</span><br><span class="line">              Z[m * N + n] = Y[m * N + n];</span><br><span class="line">          &#125;);</span><br><span class="line">      &#125;);</span><br><span class="line">      q.wait();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  __tend(copy_as_u8);</span><br><span class="line"></span><br><span class="line">  sycl::free(Y, ctx);</span><br><span class="line">  sycl::free(Z, ctx);</span><br><span class="line">  return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Build with DPC++:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ clang++ -std=c++14 -fsycl -O3 copy.cpp -o copy</span><br></pre></td></tr></table></figure><p>Run the bench on ATS 480EU:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ./copy 65536 512</span><br><span class="line">time: copy_as_u32, 37.74 ms</span><br><span class="line">time: copy_as_u16, 64.26 ms</span><br><span class="line">time: copy_as_u8, 114.77 ms</span><br></pre></td></tr></table></figure><p>It shows copy_as_u16 is 1.7x slower than copy_as_u32, copy_as_u8 is 3x slower than copy_as_u32 when running in a bench case of 100 iterations for each data type.</p><p>Checking with the generated code, copy_as_u32 is in a quite compact form:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Address     Source Line Assembly</span><br><span class="line">0x9a0 0     send.dc1 (16|M0) r45 r41 null 0x0 0x4205E01 &#123;$5&#125; [, msg-length:2, resp-length:2, header:no, func-control:5e01]</span><br><span class="line">0x9b0 0     sync.nop null &#123;Compacted, I@1&#125;</span><br><span class="line">0x9b8 0     send.dc1 (16|M16) r47 r43 null 0x0 0x4205E01 &#123;$6&#125; [, msg-length:2, resp-length:2, header:no, func-control:5e01]</span><br><span class="line">...</span><br><span class="line">0xde8 0     send.dc1 (16|M0) null r81 r45 0x80 0x4025E02 &#123;$5&#125; [, msg-length:2, resp-length:0, header:no, func-control:25e02]</span><br><span class="line">0xdf8 0     sync.nop null &#123;Compacted, I@1&#125;</span><br><span class="line">0xe00 0     send.dc1 (16|M16) null r83 r47 0x80 0x4025E02 &#123;$6&#125; [, msg-length:2, resp-length:0, header:no, func-control:25e02]</span><br></pre></td></tr></table></figure><p>However for copy_as_u16, the generated code is using only the low 16bit of the registers with the higher 16bit kept empty. And it issues 2x of send with additional movs.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Address     Source Line Assembly</span><br><span class="line">0x9a0 0     send.dc0 (16|M0) r45 r30 null 0x0 0x4210501 &#123;$5&#125; [, msg-length:2, resp-length:2, header:no, func-control:10501]</span><br><span class="line">0x9b0 0     sync.nop null &#123;Compacted, I@1&#125;</span><br><span class="line">0x9b8 0     send.dc0 (16|M16) r47 r32 null 0x0 0x4210501 &#123;$6&#125; [, msg-length:2, resp-length:2, header:no, func-control:10501]</span><br><span class="line">…</span><br><span class="line">0xdd0 0     mov (16|M0) r85.0&lt;1&gt;:ud r45.0&lt;2;1,0&gt;:uw &#123;$5.dst&#125;</span><br><span class="line">0xde0 0     mov (16|M16) r87.0&lt;1&gt;:ud r47.0&lt;2;1,0&gt;:uw &#123;$6.dst&#125;</span><br><span class="line">…</span><br><span class="line">0xe08 0     send.dc0 (16|M0) null r81 r85 0x80 0x4030502 &#123;$7&#125; [, msg-length:2, resp-length:0, header:no, func-control:30502]</span><br><span class="line">0xe18 0     sync.nop null &#123;Compacted, I@1&#125;</span><br><span class="line">0xe20 0     send.dc0 (16|M16) null r83 r87 0x80 0x4030502 &#123;$8&#125; [, msg-length:2, resp-length:0, header:no, func-control:30502]</span><br></pre></td></tr></table></figure><p>This seems to be a quite common performance penalty as in FP16 inference/training, most of math kernels uses the FP16 data type passed from the framework front-end. A lot of OPs (like embedding, concat, index and so on) have copy semantics. INT8 inference will have similar issue. We expect in case of contiguous copy, u8/u16 is same fast as u32.</p><p>the same issue can also be reproduced on Gen9:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$./copy 65536 512</span><br><span class="line">time: copy_as_u32, 379.55 ms</span><br><span class="line">time: copy_as_u16, 579.60 ms</span><br><span class="line">time: copy_as_u8, 764.89 ms</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;Performance of copy using different data types.&lt;br&gt;Got a performance issue on ATS when copy a continuous buffer of u16 data type. It is almost 2x slower than copy a buffer of same byte-length but using u32 data type. Similarly, copy of u8 is even slower. Below is a test case of copy a same buffer using different data type of u32/u16/u8.&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://huangzhiyuan.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="GPU" scheme="http://huangzhiyuan.github.io/tags/GPU/"/>
    
  </entry>
  
  <entry>
    <title>mlperf-v0.7</title>
    <link href="http://huangzhiyuan.github.io/2020/08/10/mlperf-v0-7/"/>
    <id>http://huangzhiyuan.github.io/2020/08/10/mlperf-v0-7/</id>
    <published>2020-08-10T01:45:19.000Z</published>
    <updated>2020-08-10T01:59:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>MLPerf是业内首套衡量机器学习软硬件性能的通用基准，由图灵奖得主David Patterson联合谷歌和几所著名高校于2018年发起。<br>MLPerf 是AI芯片的一个基准测试，主要包括：Training 和Inference两个方面的性能测试。Training是于测量系统将模型训练到目标质量指标的速度；Inference是用于测试系统使用训练有素的模型处理输入和产生结果的速度。<br>MLPerf基准联盟现有83家成员，包括谷歌、英伟达、微软、Facebook、阿里巴巴等73家企业和斯坦福、哈佛、多伦多大学等10所高校</p><span id="more"></span><p>随着AI技术的进步，今年的测试基准进一步加大了难度。</p><p>MLPerf训练测试基准包括图像分类、翻译、推荐系统和围棋等8个机器学习任务中，最终结果是这8项任务的训练时间，速度越快则性能越强。</p><p>具体的8项任务内容如下：</p><p><img src="/img/2020/0810/task.jpg" alt="&quot;1&quot;"></p><p>其中后三项是新加入或重新制定的标准：</p><p>1、BERT：用Wikipedia语料库训练BERT，这是首次将BERT引入MLPerf测试基准。</p><p>2、DLRM：用Criteo AI Lab的Terabyte点击率数据集训练的深度学习推荐模型（DLRM），广泛用于在线购物推荐、搜索结果和社交媒体内容排序。</p><p>3、Mini-Go：之前的MLPerf v0.5和v0.6也有训练围棋的强化学习任务，但却是迷你棋盘，此次v0.7将棋盘扩大为19×19全尺寸，这更能反映研究成果。</p><p>感谢<a href="https://github.com/zixuanweeei">zixuan</a>同学的整理，现将部分汇总结论整理如下：</p><h2 id="Submitter-–-Software-Relationship"><a href="#Submitter-–-Software-Relationship" class="headerlink" title="Submitter – Software Relationship"></a>Submitter – Software Relationship</h2><p><img src="/img/2020/0810/software.png" alt="&quot;softward&quot;"></p><ul><li>MxNet, Pytorch, Tensorflow are still the mainstream of deep learning framework.</li><li>Customized frameworks, i.e. Huawei MindSpore, Nvidia Merlin are also entering public view.</li></ul><h2 id="Submitter-–-Field-Relationship"><a href="#Submitter-–-Field-Relationship" class="headerlink" title="Submitter – Field Relationship"></a>Submitter – Field Relationship</h2><p><img src="/img/2020/0810/2.png" alt="&quot;2&quot;"></p><ul><li>Nvidia and Google are active in all the deep learning fields.<br>Image Classification benchmark is popular and is adopted by almost all the company.</li><li>RL, Recommendation, NLP received less attention.</li><li>Intel has no submission for NLP and Object detection.</li><li>China mainland companies, i.e. Alibaba, Inspur, Shenzhen*, needs more efforts to become remarkable</li></ul><h2 id="Software-–-Field-Relationship"><a href="#Software-–-Field-Relationship" class="headerlink" title="Software – Field Relationship"></a>Software – Field Relationship</h2><p><img src="/img/2020/0810/3.png" alt="&quot;3&quot;"></p><ul><li>Tensorflow is adopted in all the fields.</li><li>MxNet has the submissions for image classification and object detection (because gluon-cv toolkit is much more developed than others, like gluon-nlp?)</li><li>Combination of two frameworks (mxnet + pytorch) is also an option.</li></ul><p>参考链接：<br><a href="https://xueqiu.com/1097649362/155335766">https://xueqiu.com/1097649362/155335766</a><br><a href="https://mlperf.org/training-results-0-7">https://mlperf.org/training-results-0-7</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;MLPerf是业内首套衡量机器学习软硬件性能的通用基准，由图灵奖得主David Patterson联合谷歌和几所著名高校于2018年发起。&lt;br&gt;MLPerf 是AI芯片的一个基准测试，主要包括：Training 和Inference两个方面的性能测试。Training是于测量系统将模型训练到目标质量指标的速度；Inference是用于测试系统使用训练有素的模型处理输入和产生结果的速度。&lt;br&gt;MLPerf基准联盟现有83家成员，包括谷歌、英伟达、微软、Facebook、阿里巴巴等73家企业和斯坦福、哈佛、多伦多大学等10所高校&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://huangzhiyuan.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="MLPerf" scheme="http://huangzhiyuan.github.io/tags/MLPerf/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 模型参数可视化</title>
    <link href="http://huangzhiyuan.github.io/2020/07/02/pytorch-model-parameters-visualization/"/>
    <id>http://huangzhiyuan.github.io/2020/07/02/pytorch-model-parameters-visualization/</id>
    <published>2020-07-02T07:12:14.000Z</published>
    <updated>2020-07-02T09:12:44.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近在分析不同的数据类型在深度学习过程中的应用，看CUDA的doc发现有篇<a href="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html">文章</a>是关于<code>FP16</code>数据类型对模型训练，达到节省带宽和内存的目的。基于数据模型的精度损失问题，需要分析模型参数的数值分布规律，做到量化和缩放操作避免损失模型精度。此文用来使用jupyter notebook 和matplotlib 可视化模型参数的具体过程。</p><span id="more"></span><p>本文以Resnet50和Mobilenet 网络为例，这两个都是常用的CNN网络，一般用来作为benchmark的baseline，模型结果相对简单，参数不算太多，适合做量化分析，从整体上大致掌握数值的分布规律。</p><h2 id="加载Renet50-模型"><a href="#加载Renet50-模型" class="headerlink" title="加载Renet50 模型"></a>加载Renet50 模型</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import models as models</span><br><span class="line"></span><br><span class="line">model_arch = &quot;resnet50&quot;</span><br><span class="line"></span><br><span class="line">print(&quot;=&gt; using pre-trained model &#x27;&#123;&#125;&#x27;&quot;.format(model_arch))</span><br><span class="line">model = models.__dict__[model_arch](pretrained=True)</span><br><span class="line"></span><br><span class="line"># print model arch</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure><p>model结构如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">=&gt; using pre-trained model &#x27;resnet50&#x27;</span><br><span class="line">ResNet(</span><br><span class="line">  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)</span><br><span class="line">  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">  (relu): ReLU(inplace=True)</span><br><span class="line">  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)</span><br><span class="line">  (layer1): Sequential(</span><br><span class="line">    (0): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (2): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (layer2): Sequential(</span><br><span class="line">    (0): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)</span><br><span class="line">        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (2): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (3): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (layer3): Sequential(</span><br><span class="line">    (0): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)</span><br><span class="line">        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (2): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (3): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (4): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (5): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (layer4): Sequential(</span><br><span class="line">    (0): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)</span><br><span class="line">        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">    (2): Bottleneck(</span><br><span class="line">      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)</span><br><span class="line">  (fc): Linear(in_features=2048, out_features=1000, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="load-model-parameters"><a href="#load-model-parameters" class="headerlink" title="load model parameters"></a>load model parameters</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># print model parameters</span><br><span class="line">layer_list = []</span><br><span class="line">params_list = []</span><br><span class="line">for name, param in model.named_parameters():</span><br><span class="line">    if param.requires_grad:</span><br><span class="line">#         print(name, param.data.type())</span><br><span class="line">        layer_list.append(name)</span><br><span class="line">        params_list += param.data.view(-1, 1)</span><br><span class="line"></span><br><span class="line">params_list = [round(float(i.numpy().tolist()[0]), 3) for i in params_list]</span><br><span class="line">nonzero_norm_list=[i for i in params_list if i!=0]</span><br><span class="line">zero_num = 0</span><br><span class="line">for k in params_list:</span><br><span class="line">    if k == 0:</span><br><span class="line">        zero_num += 1</span><br></pre></td></tr></table></figure><h3 id="获取paramaters大致轮廓"><a href="#获取paramaters大致轮廓" class="headerlink" title="获取paramaters大致轮廓"></a>获取paramaters大致轮廓</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;all parameters: &quot;, len(params_list))</span><br><span class="line">print(&quot;all non zero parameters: &quot;, len(nonzero_norm_list))</span><br><span class="line">print(&quot;max value: &quot;, max(params_list))</span><br><span class="line">print(&quot;min value: &quot;, min(params_list))</span><br><span class="line">print(&quot;zero count: &quot;, zero_num)</span><br><span class="line"># print(&quot;top 10&quot;, params_list[:10])</span><br><span class="line"></span><br><span class="line">all parameters:  25557032</span><br><span class="line">all non zero parameters:  24695549</span><br><span class="line">max value:  1.32</span><br><span class="line">min value:  -0.782</span><br><span class="line">zero count:  861483</span><br></pre></td></tr></table></figure><h2 id="可视化参数"><a href="#可视化参数" class="headerlink" title="可视化参数"></a>可视化参数</h2><p>由于parameter大多在[0,1]之间，所以这里只取部分数据来说明分布：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from matplotlib import pyplot as plt</span><br><span class="line"></span><br><span class="line">plt.hist(params_list[:], bins=300)</span><br><span class="line">plt.title(&quot;Resnet50 parameter value range&quot;)</span><br><span class="line">plt.xlabel(&quot;Value Range&quot;)</span><br><span class="line">plt.ylabel(&quot;Counts&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/img/2020/0702/all.png" alt="&quot;range&quot;"><br>注意上图Y轴数量级（10e6），由于参数数量级较大，横轴值的分布较为集中， 大都分布在0左右, 极值范围在[-1, 1.5]。也可以只打印部分数值分析。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.hist(params_list[-8000:], bins=300)</span><br></pre></td></tr></table></figure><p><img src="/img/2020/0702/range.png" alt="&quot;range&quot;"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近在分析不同的数据类型在深度学习过程中的应用，看CUDA的doc发现有篇&lt;a href=&quot;https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html&quot;&gt;文章&lt;/a&gt;是关于&lt;code&gt;FP16&lt;/code&gt;数据类型对模型训练，达到节省带宽和内存的目的。基于数据模型的精度损失问题，需要分析模型参数的数值分布规律，做到量化和缩放操作避免损失模型精度。此文用来使用jupyter notebook 和matplotlib 可视化模型参数的具体过程。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://huangzhiyuan.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="pytorch" scheme="http://huangzhiyuan.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>机器翻译评价指标之BLEU解析</title>
    <link href="http://huangzhiyuan.github.io/2020/06/11/BLEU-in-NLP/"/>
    <id>http://huangzhiyuan.github.io/2020/06/11/BLEU-in-NLP/</id>
    <published>2020-06-11T05:52:54.000Z</published>
    <updated>2020-08-14T07:36:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>如何衡量翻译的好坏？ 机器翻译越接近专业的人工翻译，它就越好。 这是我们的提议背后的核心理念。 为了判断机器翻译的质量，可以根据一个数值指标来衡量其与一个或多个人工参考翻译的接近程度。 因此，我们的MT评估系统需要两个成分：</p><ol><li>一个“翻译接近度”数值指标</li><li>一个高质量的人工参考翻译语料库</li></ol><p>BLEU（Bilingual Evaluation Understudy），相信大家对这个评价指标的概念已经很熟悉，随便百度谷歌就有相关介绍。原论文为<a href="http://www.aclweb.org/anthology/P02-1040.pdf">BLEU: a Method for Automatic Evaluation of Machine Translation</a>，IBM出品。</p><p>本文通过一个例子详细介绍BLEU是如何计算以及NLTK <a href="http://www.nltk.org/_modules/nltk/align/bleu_score.html">nltk.align.bleu_score</a>模块的源码。</p><p>首先祭出公式：</p><p><img src="/img/2020/0611/bleu.png" alt="&quot;bleu&quot;"></p><p>其中，<br><img src="/img/2020/0611/bp.png" alt="&quot;bp&quot;"><br>注意这里的BLEU值是针对一条翻译（一个样本）来说的。</p><span id="more"></span><p>NLTKnltk.align.bleu_score模块实现了这里的公式，主要包括三个函数，两个私有函数分别计算P和BP，一个函数整合计算BLEU值。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 计算BLEU值</span><br><span class="line">def bleu(candidate, references, weights)</span><br><span class="line"></span><br><span class="line"># （1）私有函数，计算修正的n元精确率（Modified n-gram Precision）</span><br><span class="line">def _modified_precision(candidate, references, n)</span><br><span class="line"></span><br><span class="line"># （2）私有函数，计算BP惩罚因子</span><br><span class="line">def _brevity_penalty(candidate, references)</span><br></pre></td></tr></table></figure><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p>候选译文（Predicted）：<br>It is a guide to action which ensures that the military always obeys the commands of the party</p><p>参考译文（Gold Standard）<br>1：It is a guide to action that ensures that the military will forever heed Party commands<br>2：It is the guiding principle which guarantees the military forces always being under the command of the Party<br>3：It is the practical guide for the army always to heed the directions of the party</p><h2 id="Modified-n-gram-Precision计算（也即是Pn）"><a href="#Modified-n-gram-Precision计算（也即是Pn）" class="headerlink" title="Modified n-gram Precision计算（也即是Pn）"></a>Modified n-gram Precision计算（也即是Pn）</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def _modified_precision(candidate, references, n):</span><br><span class="line">    counts = Counter(ngrams(candidate, n))</span><br><span class="line"></span><br><span class="line">    if not counts:</span><br><span class="line">        return 0</span><br><span class="line"></span><br><span class="line">    max_counts = &#123;&#125;</span><br><span class="line">    for reference in references:</span><br><span class="line">        reference_counts = Counter(ngrams(reference, n))</span><br><span class="line">        for ngram in counts:</span><br><span class="line">            max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram])</span><br><span class="line"></span><br><span class="line">    clipped_counts = dict((ngram, min(count, max_counts[ngram])) for ngram, count in counts.items())</span><br><span class="line"></span><br><span class="line">    return sum(clipped_counts.values()) / sum(counts.values())</span><br></pre></td></tr></table></figure><p>我们这里n取值为4，也就是从1-gram计算到4-gram。</p><h3 id="Modified-1-gram-precision"><a href="#Modified-1-gram-precision" class="headerlink" title="Modified 1-gram precision"></a>Modified 1-gram precision</h3><p>首先统计候选译文里每个词出现的次数，然后统计每个词在参考译文中出现的次数，Max表示3个参考译文中的最大值，Min表示候选译文和Max两个的最小值。</p><p><img src="/img/2020/0611/1.png" alt="&quot;1&quot;"></p><h3 id="Modified-2-gram-precision"><a href="#Modified-2-gram-precision" class="headerlink" title="Modified 2-gram precision"></a>Modified 2-gram precision</h3><p><img src="/img/2020/0611/2.png" alt="&quot;2&quot;"></p><h3 id="Modified-3-gram-precision"><a href="#Modified-3-gram-precision" class="headerlink" title="Modified 3-gram precision"></a>Modified 3-gram precision</h3><p><img src="/img/2020/0611/3.png" alt="&quot;3&quot;"></p><h3 id="Modified-4-gram-precision"><a href="#Modified-4-gram-precision" class="headerlink" title="Modified 4-gram precision"></a>Modified 4-gram precision</h3><p><img src="/img/2020/0611/4.png" alt="&quot;4&quot;"></p><h2 id="Brevity-Penalty-计算"><a href="#Brevity-Penalty-计算" class="headerlink" title="Brevity Penalty 计算"></a>Brevity Penalty 计算</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def _brevity_penalty(candidate, references):</span><br><span class="line"></span><br><span class="line">    c = len(candidate)</span><br><span class="line">    ref_lens = (len(reference) for reference in references)</span><br><span class="line">    #这里有个知识点是Python中元组是可以比较的，如(0,1)&gt;(1,0)返回False，这里利用元组比较实现了选取参考翻译中长度最接近候选翻译的句子，当最接近的参考翻译有多个时，选取最短的。例如候选翻译长度是10，两个参考翻译长度分别为9和11，则r=9.</span><br><span class="line">    r = min(ref_lens, key=lambda ref_len: (abs(ref_len - c), ref_len))</span><br><span class="line">    print &#x27;r:&#x27;,r</span><br><span class="line"></span><br><span class="line">    if c &gt; r:</span><br><span class="line">        return 1</span><br><span class="line">    else:</span><br><span class="line">        return math.exp(1 - r / c)</span><br></pre></td></tr></table></figure><p>下面计算BP（Brevity Penalty），翻译过来就是“过短惩罚”。由BP的公式可知取值范围是(0,1]，候选句子越短，越接近0。</p><p>候选翻译句子长度为18，参考翻译分别为：16，18，16。<br>所以c = 18，r=18（参考翻译中选取长度最接近候选翻译的作为rr）</p><p>所以BP = e^0 =1</p><h2 id="整合"><a href="#整合" class="headerlink" title="整合"></a>整合</h2><p>最终<code>BLEU = 1 ⋅ exp(−0.684055269517) = 0.504566684006</code></p><p>BLEU的取值范围是[0,1]，0最差，1最好。</p><p>通过计算过程，我们可以看到，BLEU值其实也就是“改进版的n-gram”加上“过短惩罚因子”。</p><p>参考：<br><a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/yiyibooks/BLEU_a_Method_for_Automatic_Evaluation_of_Machine_Translation/index.html">https://link.zhihu.com/?target=https%3A//www.yiyibooks.cn/yiyibooks/BLEU_a_Method_for_Automatic_Evaluation_of_Machine_Translation/index.html</a><br><a href="https://blog.csdn.net/guolindonggld/java/article/details/56966200">https://blog.csdn.net/guolindonggld/java/article/details/56966200</a><br><a href="https://www.zhihu.com/question/304798594/answer/567383628">https://www.zhihu.com/question/304798594/answer/567383628</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;如何衡量翻译的好坏？ 机器翻译越接近专业的人工翻译，它就越好。 这是我们的提议背后的核心理念。 为了判断机器翻译的质量，可以根据一个数值指标来衡量其与一个或多个人工参考翻译的接近程度。 因此，我们的MT评估系统需要两个成分：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;一个“翻译接近度”数值指标&lt;/li&gt;
&lt;li&gt;一个高质量的人工参考翻译语料库&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;BLEU（Bilingual Evaluation Understudy），相信大家对这个评价指标的概念已经很熟悉，随便百度谷歌就有相关介绍。原论文为&lt;a href=&quot;http://www.aclweb.org/anthology/P02-1040.pdf&quot;&gt;BLEU: a Method for Automatic Evaluation of Machine Translation&lt;/a&gt;，IBM出品。&lt;/p&gt;
&lt;p&gt;本文通过一个例子详细介绍BLEU是如何计算以及NLTK &lt;a href=&quot;http://www.nltk.org/_modules/nltk/align/bleu_score.html&quot;&gt;nltk.align.bleu_score&lt;/a&gt;模块的源码。&lt;/p&gt;
&lt;p&gt;首先祭出公式：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/2020/0611/bleu.png&quot; alt=&quot;&amp;quot;bleu&amp;quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;其中，&lt;br&gt;&lt;img src=&quot;/img/2020/0611/bp.png&quot; alt=&quot;&amp;quot;bp&amp;quot;&quot;&gt;&lt;br&gt;注意这里的BLEU值是针对一条翻译（一个样本）来说的。&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="http://huangzhiyuan.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="mAP" scheme="http://huangzhiyuan.github.io/tags/mAP/"/>
    
  </entry>
  
  <entry>
    <title>使用Vtune工具反汇编SYCL代码样例</title>
    <link href="http://huangzhiyuan.github.io/2020/05/22/sycl-assemble-vtune/"/>
    <id>http://huangzhiyuan.github.io/2020/05/22/sycl-assemble-vtune/</id>
    <published>2020-05-22T01:44:26.000Z</published>
    <updated>2020-05-22T02:28:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近需要验证一个<code>bit_cast</code>转换函数在GPU kernel里面的底层实现形式，之前一直猜想是<code>Mov</code>指令完成的，CPU端的代码往往通过gdb反汇编很容易看到每行代码对应个汇编语言，但是对于GPU的kernel，我却一直没有什么经验。刚好可以利用Intel最近准备release的<a href="https://software.intel.com/content/www/us/en/develop/tools/oneapi.html?cid=sem&source=sa360&campid=2020_q2_iags_us_iagsoapi_iagsoapiee_awa_text-link_brand_exact_cd_dpd-oneapi-home_O-20WWS_google_div_oos_non-pbm&ad_group=brand_oneapi-home_awa&intel_term=intel+oneapi&sa360id=43700053523136066&gclid=Cj0KCQjwzZj2BRDVARIsABs3l9IXlGeC-Ez1z9BMTgrMlEOh3zXpuGhO4AenRk9X8H_oThOAUi1je2IaAiIaEALw_wcB">OneAPI</a>开发工具包，借助里面的gdb-oneapi和VTune工具来实现。本文就是记录这次踩坑过程。<br> <img src="/img/2020/0522/oneapi.png" alt="&quot;oneapi&quot;"></p><span id="more"></span><p> <img src="/img/2020/0522/tool.png" alt="&quot;tool&quot;"><br>这里使用到的就是Intel vtune profiler工具。该工具官方解释：<strong>Locate performance bottlenecks fast. Advanced sampling and profiling techniques quickly analyze your code, isolate issues, and deliver insights for optimizing performance on modern processors.</strong></p><h2 id="Source-Code"><a href="#Source-Code" class="headerlink" title="Source Code"></a>Source Code</h2><p>先上测试代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">#pragma OPENCL EXTENSION cl_khr_fp16 : enable</span><br><span class="line"></span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;CL/sycl.hpp&gt;</span><br><span class="line"></span><br><span class="line">//class kernel1;</span><br><span class="line">class kernel2;</span><br><span class="line">namespace sycl = cl::sycl;</span><br><span class="line"></span><br><span class="line">template &lt;typename To, typename From&gt; To bit_cast(const From &amp;from) &#123;</span><br><span class="line">#if __cpp_lib_bit_cast</span><br><span class="line">  return std::bit_cast&lt;To&gt;(from);</span><br><span class="line">#else</span><br><span class="line"></span><br><span class="line">#if __has_builtin(__builtin_bit_cast)</span><br><span class="line">  return __builtin_bit_cast(To, from);  // clang path</span><br><span class="line">#else</span><br><span class="line">  To to;</span><br><span class="line">  detail::memcpy(&amp;to, &amp;from, sizeof(To));</span><br><span class="line">  return to;</span><br><span class="line">#endif // __has_builtin(__builtin_bit_cast)</span><br><span class="line">#endif // __cpp_lib_bit_cast</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void add() &#123;</span><br><span class="line">  sycl::float4 a = &#123;1.0, 2.0, 3.0, 4.0&#125;;</span><br><span class="line">  sycl::float4 b = &#123;4.0, 3.0, 2.0, 3.0&#125;;</span><br><span class="line">  sycl::float4 c = &#123;.0, 0.0, 0.0, 0.0&#125;;</span><br><span class="line"></span><br><span class="line">  sycl::default_selector device_selector;</span><br><span class="line"></span><br><span class="line">  sycl::queue queue(device_selector);</span><br><span class="line">  std::cout &lt;&lt; &quot;Running on &quot;</span><br><span class="line">            &lt;&lt; queue.get_device().get_info&lt;sycl::info::device::name&gt;()</span><br><span class="line">            &lt;&lt; &quot;\n&quot;;</span><br><span class="line">  &#123;</span><br><span class="line">  sycl::buffer&lt;sycl::float4, 1&gt; a_sycl(&amp;a, sycl::range&lt;1&gt;(1));</span><br><span class="line">  sycl::buffer&lt;sycl::float4, 1&gt; b_sycl(&amp;b, sycl::range&lt;1&gt;(1));</span><br><span class="line">  sycl::buffer&lt;sycl::float4, 1&gt; c_sycl(&amp;c, sycl::range&lt;1&gt;(1));</span><br><span class="line"></span><br><span class="line">  queue.submit([&amp;] (sycl::handler&amp; cgh) &#123;</span><br><span class="line">  auto a_acc = a_sycl.get_access&lt;sycl::access::mode::read&gt;(cgh);</span><br><span class="line">  auto b_acc = b_sycl.get_access&lt;sycl::access::mode::read&gt;(cgh);</span><br><span class="line">  auto c_acc = c_sycl.get_access&lt;sycl::access::mode::discard_write&gt;(cgh);</span><br><span class="line"></span><br><span class="line">  cgh.single_task&lt;class vector_addition&gt;([=] () &#123;</span><br><span class="line">    c_acc[0] = a_acc[0] + b_acc[0];</span><br><span class="line">    &#125;);</span><br><span class="line">  &#125;);</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">  std::cout &lt;&lt; &quot;  A &#123; &quot; &lt;&lt; a.x() &lt;&lt; &quot;, &quot; &lt;&lt; a.y() &lt;&lt; &quot;, &quot; &lt;&lt; a.z() &lt;&lt; &quot;, &quot; &lt;&lt; a.w() &lt;&lt; &quot; &#125;\n&quot;</span><br><span class="line">        &lt;&lt; &quot;+ B &#123; &quot; &lt;&lt; b.x() &lt;&lt; &quot;, &quot; &lt;&lt; b.y() &lt;&lt; &quot;, &quot; &lt;&lt; b.z() &lt;&lt; &quot;, &quot; &lt;&lt; b.w() &lt;&lt; &quot; &#125;\n&quot;</span><br><span class="line">        &lt;&lt; &quot;------------------\n&quot;</span><br><span class="line">        &lt;&lt; &quot;= C &#123; &quot; &lt;&lt; c.x() &lt;&lt; &quot;, &quot; &lt;&lt; c.y() &lt;&lt; &quot;, &quot; &lt;&lt; c.z() &lt;&lt; &quot;, &quot; &lt;&lt; c.w() &lt;&lt; &quot; &#125;&quot;</span><br><span class="line">        &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void bitCast() &#123;</span><br><span class="line">  // sycl::gpu_selector device_selector;</span><br><span class="line">  // sycl::queue queue(device_selector);</span><br><span class="line">  cl::sycl::queue queue;</span><br><span class="line">  std::cout &lt;&lt; &quot;Running on &quot; &lt;&lt; queue.get_device().get_info&lt;cl::sycl::info::device::name&gt;() &lt;&lt; &quot;\n&quot;;</span><br><span class="line"></span><br><span class="line">  constexpr size_t LENGTH = 64;</span><br><span class="line">  unsigned short res[64] = &#123;0&#125;;</span><br><span class="line">  cl::sycl::range&lt;1&gt; data_range &#123;LENGTH&#125;;</span><br><span class="line">  &#123;</span><br><span class="line">    cl::sycl::buffer&lt;unsigned short, 1&gt; buf_res(res, 1);</span><br><span class="line">    cl::sycl::half tmp(0.0f);</span><br><span class="line">    queue.submit([&amp;] (sycl::handler&amp; cgh) &#123;</span><br><span class="line">      auto a_acc = buf_res.get_access&lt;sycl::access::mode::discard_write&gt;(cgh);</span><br><span class="line">      cgh.parallel_for&lt;class kernel2&gt;(data_range, [=](cl::sycl::id&lt;1&gt; index) &#123;</span><br><span class="line">        a_acc[index] = bit_cast&lt;unsigned short, cl::sycl::half&gt;(tmp);</span><br><span class="line">        // a_acc[index] = 0x42;</span><br><span class="line">        //a_acc[0] = __builtin_bit_cast(unsigned short, tmp);</span><br><span class="line">      &#125;);</span><br><span class="line">    &#125;);</span><br><span class="line">    queue.wait_and_throw();</span><br><span class="line">  &#125;</span><br><span class="line">  std::cout &lt;&lt; &quot;bit_cast &quot; &lt;&lt; res[0] &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main() &#123;</span><br><span class="line">  add();</span><br><span class="line">  bitCast();</span><br><span class="line">  return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这段code比较简单，借助SYCL环境，有两个functions，<code>add()``用来验证简单的</code>scalar<code> 加法，</code>bitCast()<code>用来验证`__builtin_bit_cast()</code> function（需要clang++编译器支持）。<br>这里直接安装整个Intel OneAPI工具包即可。<br>这里的kernel block块是下面，需要验证<code>a_acc[index] = bit_cast&lt;unsigned short, cl::sycl::half&gt;(tmp);</code>对应的汇编代码实现。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cl::sycl::range&lt;1&gt; data_range &#123;LENGTH&#125;;</span><br><span class="line">&#123;</span><br><span class="line">  cl::sycl::buffer&lt;unsigned short, 1&gt; buf_res(res, 1);</span><br><span class="line">  cl::sycl::half tmp(0.0f);</span><br><span class="line">  queue.submit([&amp;] (sycl::handler&amp; cgh) &#123;</span><br><span class="line">    auto a_acc = buf_res.get_access&lt;sycl::access::mode::discard_write&gt;(cgh);</span><br><span class="line">    cgh.parallel_for&lt;class kernel2&gt;(data_range, [=](cl::sycl::id&lt;1&gt; index) &#123;</span><br><span class="line">      a_acc[index] = bit_cast&lt;unsigned short, cl::sycl::half&gt;(tmp);</span><br><span class="line">      // a_acc[index] = 0x42;</span><br><span class="line">      //a_acc[0] = __builtin_bit_cast(unsigned short, tmp);</span><br><span class="line">    &#125;);</span><br><span class="line">  &#125;);</span><br><span class="line">  queue.wait_and_throw();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Build"><a href="#Build" class="headerlink" title="Build"></a>Build</h2><p><code>build</code>步采用makefile来编译。<br>安装并配置oneapie环境变量。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /home/zhiyuanh/intel/inteloneapi/setvars.sh</span><br></pre></td></tr></table></figure><p>makefile文件夹内容如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">ONE_API_ROOT := /home/zhiyuanh/intel/inteloneapi</span><br><span class="line"></span><br><span class="line">SYCL_ROOT := $&#123;ONE_API_ROOT&#125;/compiler/latest/linux</span><br><span class="line"></span><br><span class="line">SYCLCXX := $(SYCL_ROOT)/bin/clang++</span><br><span class="line"></span><br><span class="line">CXXFLAGS := -I $(SYCL_ROOT)/lib/clang/10.0.0/include -fsycl -g -O0</span><br><span class="line"></span><br><span class="line">LDFLAGS := -L $(SYCL_ROOT)/lib -lOpenCL -fsycl -g</span><br><span class="line"></span><br><span class="line">first.o: first.cpp</span><br><span class="line">        $(SYCLCXX) -std=c++11 $(CXXFLAGS) -c first.cpp</span><br><span class="line"></span><br><span class="line">clean:</span><br><span class="line">        rm -f first.exe first.o</span><br></pre></td></tr></table></figure><p>之后<code>make</code>生成<code>first</code>目标文件。</p><h2 id="Vtune采集"><a href="#Vtune采集" class="headerlink" title="Vtune采集"></a>Vtune采集</h2><p>导入vtune并选择<code>GPU offload（preview）</code>模式。<br> <img src="/img/2020/0522/vtune.png" alt="&quot;vtune&quot;"></p><p> 查看对应的source code和生成的汇编code。<br>  <img src="/img/2020/0522/assemble.png" alt="&quot;assemble&quot;"></p><p>这里猜想77行内的源代码对应的是右边标红的<code>mov</code>指令，因为<code>cl::sycl::half</code>共有16个bit，对应的汇编符号是<code>UW</code>。该<code>mov</code>指令将一个<code>UW</code>写到<code>r9</code>寄存器，然后再最后send发射出去。中间没有任何一个环节有修改<code>r9</code>的指令。</p><p>  <img src="/img/2020/0522/inst.png" alt="&quot;inst&quot;"></p><p>这里做一小测试，将77行注释，换成一行<code>a_acc[index] = 0x42</code>常量赋值，如果真是对应那条<code>mov</code>指令，那么现在的新的指令里面会有<code>0x42</code>这个立即数。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cl::sycl::range&lt;1&gt; data_range &#123;LENGTH&#125;;</span><br><span class="line">&#123;</span><br><span class="line">  cl::sycl::buffer&lt;unsigned short, 1&gt; buf_res(res, 1);</span><br><span class="line">  cl::sycl::half tmp(0.0f);</span><br><span class="line">  queue.submit([&amp;] (sycl::handler&amp; cgh) &#123;</span><br><span class="line">    auto a_acc = buf_res.get_access&lt;sycl::access::mode::discard_write&gt;(cgh);</span><br><span class="line">    cgh.parallel_for&lt;class kernel2&gt;(data_range, [=](cl::sycl::id&lt;1&gt; index) &#123;</span><br><span class="line">      //a_acc[index] = bit_cast&lt;unsigned short, cl::sycl::half&gt;(tmp);</span><br><span class="line">      a_acc[index] = 0x42;</span><br><span class="line">      //a_acc[0] = __builtin_bit_cast(unsigned short, tmp);</span><br><span class="line">    &#125;);</span><br><span class="line">  &#125;);</span><br><span class="line">  queue.wait_and_throw();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="/img/2020/0522/check.png" alt="&quot;check&quot;"><br>果然和我们猜想一样。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p><code>bit_cast</code>转换函数在GPU kernel里面的底层实现形式，之前一直猜想是<code>Mov</code>指令完成的，但也有一些其他额外的指令比如<code>mul</code>, <code>or</code>, <code>shl</code>这些可能是算地址偏移量相关。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近需要验证一个&lt;code&gt;bit_cast&lt;/code&gt;转换函数在GPU kernel里面的底层实现形式，之前一直猜想是&lt;code&gt;Mov&lt;/code&gt;指令完成的，CPU端的代码往往通过gdb反汇编很容易看到每行代码对应个汇编语言，但是对于GPU的kernel，我却一直没有什么经验。刚好可以利用Intel最近准备release的&lt;a href=&quot;https://software.intel.com/content/www/us/en/develop/tools/oneapi.html?cid=sem&amp;source=sa360&amp;campid=2020_q2_iags_us_iagsoapi_iagsoapiee_awa_text-link_brand_exact_cd_dpd-oneapi-home_O-20WWS_google_div_oos_non-pbm&amp;ad_group=brand_oneapi-home_awa&amp;intel_term=intel+oneapi&amp;sa360id=43700053523136066&amp;gclid=Cj0KCQjwzZj2BRDVARIsABs3l9IXlGeC-Ez1z9BMTgrMlEOh3zXpuGhO4AenRk9X8H_oThOAUi1je2IaAiIaEALw_wcB&quot;&gt;OneAPI&lt;/a&gt;开发工具包，借助里面的gdb-oneapi和VTune工具来实现。本文就是记录这次踩坑过程。&lt;br&gt; &lt;img src=&quot;/img/2020/0522/oneapi.png&quot; alt=&quot;&amp;quot;oneapi&amp;quot;&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="GPU" scheme="http://huangzhiyuan.github.io/categories/GPU/"/>
    
    
    <category term="SYCL" scheme="http://huangzhiyuan.github.io/tags/SYCL/"/>
    
    <category term="Vtune" scheme="http://huangzhiyuan.github.io/tags/Vtune/"/>
    
  </entry>
  
  <entry>
    <title>Atomic Operations in CUDA</title>
    <link href="http://huangzhiyuan.github.io/2020/05/18/atomic-operations/"/>
    <id>http://huangzhiyuan.github.io/2020/05/18/atomic-operations/</id>
    <published>2020-05-18T06:06:21.000Z</published>
    <updated>2020-05-18T06:16:54.000Z</updated>
    
    <content type="html"><![CDATA[<p>This tutorial will discuss how to perform atomic operations in CUDA, which are often essential for many algorithms. Atomic operations are easy to use, and extremely useful in many applications. Atomic operations help avoid race conditions and can be used to make code simpler to write.</p><span id="more"></span><h2 id="What-are-atomic-operations"><a href="#What-are-atomic-operations" class="headerlink" title="What are atomic operations?"></a>What are atomic operations?</h2><p>Atomic operations are operations which are performed without interference from any other threads. Atomic operations are often used to prevent race conditions which are common problems in mulithreaded applications. For example, suppose you have two threads named A and B. Now suppose each thread wants to increase the value of memory location 0x1234 by one. Suppose the value at memory location 0x1234 is 5. If A and B both want to increase the value at location 0x1234 at the same time, each thread will first have to read the value. Depending on when the reads occur, it is possible that both A and B will read a value of 5. After adding a value of 1, both A and B will want to write 6 into the memory location, which is not correct! The value, 5, should have been increased twice (once by each thread), but instead, the value was only increased once! This is called a race condition, and can happen in any multi-threaded program if the programmer is not careful.</p><h2 id="How-to-avoid-race-conditions"><a href="#How-to-avoid-race-conditions" class="headerlink" title="How to avoid race conditions"></a>How to avoid race conditions</h2><p>Fortunately, race conditions are easy to avoid in CUDA. An atomic operation is capable of reading, modifying, and writing a value back to memory without the interference of any other threads, which guarentees that a race condition won’t occur. Atomic operations in CUDA generally work for both shared memory and global memory. Atomic operations in shared memory are generally used to prevent race conditions between different threads within the same thread block. Atomic operations in global memory are used to prevent race conditions between two different threads regaurdless of which thread block they are in. Please note that shared memory is generally much faster than global memory.</p><p>Example:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int atomicAdd(int* address, int val);</span><br></pre></td></tr></table></figure><p>This atomicAdd function can be called within a kernel. When a thread executes this operation, a memory address is read, has the value of ‘val’ added to it, and the result is written back to memory. The original value of the memory at location ‘address’ is returned to the thread. Many algorithms which require atomic operations will not need to use the original value at the memory location. For a full list of available atomic functions, please read a CUDA programming guide version 1.1 or later.</p><h2 id="Performance-notes"><a href="#Performance-notes" class="headerlink" title="Performance notes"></a>Performance notes</h2><p>There are a couple things to beware of when using atomic operations. As mentioned before, shared memory is much faster than global memory, so atomic operations in shared memory tend to complete faster than atomic operations in global memory. While atomic operations are often necessary in some algorithms, it is important to minimize their usage when possible, especially with global memory accesses.</p><p>Also beware of serialization. If two threads perform an atomic operation at the same memory address at the same time, those operations will be serialized. The order in which the operations complete is undefined, which is fine, but the serialization can be quite costly.</p><p>Example of SLOW code:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">__shared__ totalSum;</span><br><span class="line">if (threadIdx.x == 0) totalSum = 0;</span><br><span class="line">__syncthreads();</span><br><span class="line"></span><br><span class="line">int localVal = pValues[blockIdx.x * blockDim.x + threadIdx.x];</span><br><span class="line">atomicAdd(&amp;totalSum, 1);</span><br><span class="line">__syncthreads();</span><br></pre></td></tr></table></figure><p>The code you see above is very simple. Each thread reads a value from memory sequentially, which is quite fast. If the sum of all those numbers is needed, you might think it would be okay to simply use an atomicAdd operations. This would effectively calculate the final sum in one line of code, which might seem great. Unfortunately, all of those operations will be serialized, which is extremely slow. If you have 512 threads per thread block, each block would have to do 512 sequential additions. However, using a reduction method discussed in a previous tutorial would be able to accomplish the same task with just 511 additions. The key here is that these additions can be done in parallel. Generally, 16 or 32 additions can be done completely parallel, making it much faster than using an atomicAdd for a reduction problem such as this. So whenever you use atomic operations, be sure to program such that there won’t need to be too many sequential operations. Failure to do so will result in a dramatic loss of parallelism, and thus a dramatic loss in performance. However, when atomic operations are used correctly, they are extremely useful.</p><h2 id="Compatibility-notes"><a href="#Compatibility-notes" class="headerlink" title="Compatibility notes"></a>Compatibility notes</h2><p>When nVidia released their first CUDA capable cards, the original 8800GTX with 768MB memory and the 8800GTS with 640 MB of memory, CUDA was a new technology. These original CUDA capable cards are the only ones which do not support atomic operations. Every nVidia GPU that is a core 84 or higher supports CUDA 1.1 or higher, and thus supports atomic operations.</p><p>Even after the introduction of atomic operations with CUDA 1.1, there are still a couple atomic operations which were added later, such as 64-bit atomic operations, etc. Because there are a <em>lot</em> of CUDA 1.1 cards in consumer hands right now, I would recommend only using atomic operations with 32-bit integers and 32-bit unsigned integers. This will ensure that your application will work on the largest number of graphics cards available in the market today.</p><p>For full compatibility with all CUDA devices including those with compute capability 1.0, you may wish to you ifdefs in your code. This way, when your program executes on a device which supports atomic operations, they will be used, but your program will still be able to execute alternate, less efficient code if the device only has compute capability 1.0. When you compile to support atomic operations, the constant, CUDA_NO_SM_11_ATOMIC_INTRINSICS will be defined.</p><h2 id="Compiler-issues"><a href="#Compiler-issues" class="headerlink" title="Compiler issues"></a>Compiler issues</h2><p>With Visual Studio, you shouldn’t have any trouble compiling programs with atomic intrinsics. You may want to use the histogram64 program as a template for starting your own. However, if you are working on Linux or Mac OS, you may need to add “-arch=sm_11” as a compiler flag for nvcc. You can tell if it’s working by placing the following code inside the main function of your program:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#ifndef CUDA_NO_SM_11_ATOMIC_INTRINSICS</span><br><span class="line">printf(&quot;WARNING! Not using atomics!\n&quot;);</span><br><span class="line">#endif</span><br></pre></td></tr></table></figure><p>Refer to this link: <a href="http://supercomputingblog.com/cuda/cuda-tutorial-4-atomic-operations/">http://supercomputingblog.com/cuda/cuda-tutorial-4-atomic-operations/</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;This tutorial will discuss how to perform atomic operations in CUDA, which are often essential for many algorithms. Atomic operations are easy to use, and extremely useful in many applications. Atomic operations help avoid race conditions and can be used to make code simpler to write.&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="http://huangzhiyuan.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="atomic" scheme="http://huangzhiyuan.github.io/tags/atomic/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch Hook用法解析</title>
    <link href="http://huangzhiyuan.github.io/2020/05/12/hook-in-pytorch/"/>
    <id>http://huangzhiyuan.github.io/2020/05/12/hook-in-pytorch/</id>
    <published>2020-05-12T08:43:39.000Z</published>
    <updated>2020-05-12T09:12:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>hook在维基百科中定义：钩子编程（hooking），也称作“挂钩”，是计算机程序设计术语，指通过拦截软件模块间的函数调用、消息传递、事件传递来修改或扩展操作系统、应用程序或其他软件组件的行为的各种技术。处理被拦截的函数调用、事件、消息的代码，被称为钩子（hook）。</p><p>Hook 是 PyTorch 中一个十分有用的特性。利用它，我们可以<strong>不必改变网络输入输出的结构，方便地获取、改变网络中间层变量的值和梯度</strong>。这个功能被广泛用于可视化神经网络中间层的 feature、gradient，从而诊断神经网络中可能出现的问题，分析网络有效性。本文将结合代码，由浅入深地介绍 pytorch 中 hook 的用法。本文分为三部分：</p><ul><li>Hook for Tensors ：针对 Tensor 的 hook</li><li>Hook for Modules：针对例如 nn.Conv2dnn.Linear等网络模块的 hook</li><li>Guided Backpropagation：利用 Hook 实现的一段神经网络可视化代码</li></ul><span id="more"></span><h2 id="Hook-for-Tensors"><a href="#Hook-for-Tensors" class="headerlink" title="Hook for Tensors"></a>Hook for Tensors</h2><p>在pytorch docs搜索hook，可以发现有四个hook相关的函数，分别为register_hook，register_backward_hook，register_forward_hook，register_forward_pre_hook。其中register_hook属于tensor类，而后面三个属于moudule类。</p><p>在 PyTorch 的计算图（computation graph）中，只有叶子结点（leaf nodes）的变量会保留梯度。而所有中间变量的梯度只被用于反向传播，一旦完成反向传播，中间变量的梯度就将自动释放，从而节约内存。如下面这段代码所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">X----------             W ------</span><br><span class="line">          -                    -</span><br><span class="line">          -----&gt; + ---&gt; Z ---&gt; * ---&gt; O</span><br><span class="line">          -</span><br><span class="line">Y----------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">x = torch.Tensor([0, 1, 2, 3]).requires_grad_()</span><br><span class="line">y = torch.Tensor([4, 5, 6, 7]).requires_grad_()</span><br><span class="line">w = torch.Tensor([1, 2, 3, 4]).requires_grad_()</span><br><span class="line">z = x+y</span><br><span class="line"># z.retain_grad()</span><br><span class="line"></span><br><span class="line">o = w.matmul(z)</span><br><span class="line">o.backward()</span><br><span class="line"># o.retain_grad()</span><br><span class="line"></span><br><span class="line">print(&#x27;x.requires_grad:&#x27;, x.requires_grad) # True</span><br><span class="line">print(&#x27;y.requires_grad:&#x27;, y.requires_grad) # True</span><br><span class="line">print(&#x27;z.requires_grad:&#x27;, z.requires_grad) # True</span><br><span class="line">print(&#x27;w.requires_grad:&#x27;, w.requires_grad) # True</span><br><span class="line">print(&#x27;o.requires_grad:&#x27;, o.requires_grad) # True</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(&#x27;x.grad:&#x27;, x.grad) # tensor([1., 2., 3., 4.])</span><br><span class="line">print(&#x27;y.grad:&#x27;, y.grad) # tensor([1., 2., 3., 4.])</span><br><span class="line">print(&#x27;w.grad:&#x27;, w.grad) # tensor([ 4.,  6.,  8., 10.])</span><br><span class="line">print(&#x27;z.grad:&#x27;, z.grad) # None</span><br><span class="line">print(&#x27;o.grad:&#x27;, o.grad) # None</span><br></pre></td></tr></table></figure><p>由于 z 和 o 为中间变量（并非直接指定数值的变量，而是由别的变量计算得到的变量），它们虽然 requires_grad 的参数都是 True，但是反向传播后，它们的梯度并没有保存下来，而是直接删除了，因此是 None。如果想在反向传播之后保留它们的梯度，则需要特殊指定：把上面代码中的z.retain_grad() 和 o.retain_grad的注释去掉，可以得到它们对应的梯度，运行结果如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x.requires_grad: True</span><br><span class="line">y.requires_grad: True</span><br><span class="line">z.requires_grad: True</span><br><span class="line">w.requires_grad: True</span><br><span class="line">o.requires_grad: True</span><br><span class="line">x.grad: tensor([1., 2., 3., 4.])</span><br><span class="line">y.grad: tensor([1., 2., 3., 4.])</span><br><span class="line">w.grad: tensor([ 4.,  6.,  8., 10.])</span><br><span class="line">z.grad: tensor([1., 2., 3., 4.])</span><br><span class="line">o.grad: tensor(1.)</span><br></pre></td></tr></table></figure><p>但是，这种加 <code>retain_grad()</code> 的方案会增加内存占用，并不是个好办法，对此的一种替代方案，就是用 hook 保存中间变量的梯度。</p><p>对于中间变量z，hook 的使用方式为：<code>z.register_hook(hook_fn)</code>，其中 <code>hook_fn</code> 为一个用户自定义的函数，其签名为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hook_fn(grad) -&gt; Tensor or None</span><br></pre></td></tr></table></figure><p>它的输入为变量 <code>z</code> 的梯度，输出为一个 Tensor 或者是 None （None 一般用于直接打印梯度）。反向传播时，梯度传播到变量 z，再继续向前传播之前，将会传入 hook_fn。如果 hook_fn的返回值是 None，那么梯度将不改变，继续向前传播，如果 hook_fn的返回值是 Tensor 类型，则该 Tensor 将取代 z 原有的梯度，向前传播。</p><p>下面的示例代码中 hook_fn 不改变梯度值，仅仅是打印梯度：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">x = torch.Tensor([0, 1, 2, 3]).requires_grad_()</span><br><span class="line">y = torch.Tensor([4, 5, 6, 7]).requires_grad_()</span><br><span class="line">w = torch.Tensor([1, 2, 3, 4]).requires_grad_()</span><br><span class="line">z = x+y</span><br><span class="line"></span><br><span class="line"># ===================</span><br><span class="line">def hook_fn(grad):</span><br><span class="line">    print(grad)</span><br><span class="line"></span><br><span class="line">z.register_hook(hook_fn)</span><br><span class="line"># ===================</span><br><span class="line"></span><br><span class="line">o = w.matmul(z)</span><br><span class="line"></span><br><span class="line">print(&#x27;=====Start backprop=====&#x27;)</span><br><span class="line">o.backward()</span><br><span class="line">print(&#x27;=====End backprop=====&#x27;)</span><br><span class="line"></span><br><span class="line">print(&#x27;x.grad:&#x27;, x.grad)</span><br><span class="line">print(&#x27;y.grad:&#x27;, y.grad)</span><br><span class="line">print(&#x27;w.grad:&#x27;, w.grad)</span><br><span class="line">print(&#x27;z.grad:&#x27;, z.grad)</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">=====Start backprop=====</span><br><span class="line">tensor([1., 2., 3., 4.])</span><br><span class="line">=====End backprop=====</span><br><span class="line">x.grad: tensor([1., 2., 3., 4.])</span><br><span class="line">y.grad: tensor([1., 2., 3., 4.])</span><br><span class="line">w.grad: tensor([ 4.,  6.,  8., 10.])</span><br><span class="line">z.grad: None</span><br></pre></td></tr></table></figure><p>我们发现，z 绑定了hook_fn后，梯度反向传播时将会打印出 o 对 z 的偏导，和上文中 z.retain_grad()方法得到的 z 的偏导一致。</p><p>接下来可以试一下，在 hook_fn 中改变梯度值，看看会有什么结果。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">x = torch.Tensor([0, 1, 2, 3]).requires_grad_()</span><br><span class="line">y = torch.Tensor([4, 5, 6, 7]).requires_grad_()</span><br><span class="line">w = torch.Tensor([1, 2, 3, 4]).requires_grad_()</span><br><span class="line">z = x + y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># ===================</span><br><span class="line">def hook_fn(grad):</span><br><span class="line">    g = 2 * grad</span><br><span class="line">    print(g)</span><br><span class="line">    return g</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">z.register_hook(hook_fn)</span><br><span class="line"># ===================</span><br><span class="line"></span><br><span class="line">o = w.matmul(z)</span><br><span class="line"></span><br><span class="line">print(&#x27;=====Start backprop=====&#x27;)</span><br><span class="line">o.backward()</span><br><span class="line">print(&#x27;=====End backprop=====&#x27;)</span><br><span class="line"></span><br><span class="line">print(&#x27;x.grad:&#x27;, x.grad)</span><br><span class="line">print(&#x27;y.grad:&#x27;, y.grad)</span><br><span class="line">print(&#x27;w.grad:&#x27;, w.grad)</span><br><span class="line">print(&#x27;z.grad:&#x27;, z.grad)</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">=====Start backprop=====</span><br><span class="line">tensor([2., 4., 6., 8.])</span><br><span class="line">=====End backprop=====</span><br><span class="line">x.grad: tensor([2., 4., 6., 8.])</span><br><span class="line">y.grad: tensor([2., 4., 6., 8.])</span><br><span class="line">w.grad: tensor([ 4.,  6.,  8., 10.])</span><br><span class="line">z.grad: None</span><br></pre></td></tr></table></figure><p>发现 z 的梯度变为两倍后，受其影响，x和y的梯度也都变成了原来的两倍。在实际代码中，为了方便，也可以用 lambda 表达式来代替函数，简写为如下形式：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">x = torch.Tensor([0, 1, 2, 3]).requires_grad_()</span><br><span class="line">y = torch.Tensor([4, 5, 6, 7]).requires_grad_()</span><br><span class="line">w = torch.Tensor([1, 2, 3, 4]).requires_grad_()</span><br><span class="line">z = x + y</span><br><span class="line"></span><br><span class="line"># ===================</span><br><span class="line">z.register_hook(lambda x: 2*x)</span><br><span class="line">z.register_hook(lambda x: print(x))</span><br><span class="line"># ===================</span><br><span class="line"></span><br><span class="line">o = w.matmul(z)</span><br><span class="line"></span><br><span class="line">print(&#x27;=====Start backprop=====&#x27;)</span><br><span class="line">o.backward()</span><br><span class="line">print(&#x27;=====End backprop=====&#x27;)</span><br><span class="line"></span><br><span class="line">print(&#x27;x.grad:&#x27;, x.grad)</span><br><span class="line">print(&#x27;y.grad:&#x27;, y.grad)</span><br><span class="line">print(&#x27;w.grad:&#x27;, w.grad)</span><br><span class="line">print(&#x27;z.grad:&#x27;, z.grad)</span><br></pre></td></tr></table></figure><p>运行结果和上面的代码相同，我们发现一个变量可以绑定多个 hook_fn，反向传播时，它们按绑定顺序依次执行。例如上面的代码中，第一个绑定的 hook_fn把 z的梯度乘以2，第二个绑定的 hook_fn打印z的梯度。因此反向传播时，也是按照这个顺序执行的，打印出来的 z的梯度值，是其原本梯度值的两倍。</p><p>特征图打印应用：直接利用pytorch已有的resnet18进行特征图打印，只打印卷积层的特征图，</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torchvision.models import resnet18</span><br><span class="line">import torch.nn as nn</span><br><span class="line">from torchvision import transforms</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">def viz(module, input):</span><br><span class="line">    x = input[0][0]</span><br><span class="line">    #最多显示4张图</span><br><span class="line">    min_num = np.minimum(4, x.size()[0])</span><br><span class="line">    for i in range(min_num):</span><br><span class="line">        plt.subplot(1, 4, i+1)</span><br><span class="line">        plt.imshow(x[i].cpu())</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import cv2</span><br><span class="line">import numpy as np</span><br><span class="line">def main():</span><br><span class="line">    t = transforms.Compose([transforms.ToPILImage(),</span><br><span class="line">                            transforms.Resize((224, 224)),</span><br><span class="line">                            transforms.ToTensor(),</span><br><span class="line">                            transforms.Normalize(mean=[0.485, 0.456, 0.406],</span><br><span class="line">                                                 std=[0.229, 0.224, 0.225])</span><br><span class="line">                            ])</span><br><span class="line"></span><br><span class="line">    device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br><span class="line"></span><br><span class="line">    model = resnet18(pretrained=True).to(device)</span><br><span class="line">    for name, m in model.named_modules():</span><br><span class="line">        # if not isinstance(m, torch.nn.ModuleList) and \</span><br><span class="line">        #         not isinstance(m, torch.nn.Sequential) and \</span><br><span class="line">        #         type(m) in torch.nn.__dict__.values():</span><br><span class="line">        # 这里只对卷积层的feature map进行显示</span><br><span class="line">        if isinstance(m, torch.nn.Conv2d):</span><br><span class="line">            m.register_forward_pre_hook(viz)</span><br><span class="line">    img = cv2.imread(&#x27;./cat.jpeg&#x27;)</span><br><span class="line">    img = t(img).unsqueeze(0).to(device)</span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        model(img)</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p> 第一层卷积层输入<br> <img src="/img/2020/0512/conv1.jpg" alt="&quot;conv1&quot;"></p><p>第四层卷积层的输入<br>  <img src="/img/2020/0512/conv2.jpg" alt="&quot;conv2&quot;"></p><p>模型大小，算力计算, 同样的用法，可以直接参考<a href="https://github.com/sksq96/pytorch-summary">pytorch-summary</a>这个项目。</p><h2 id="Hook-for-Modules"><a href="#Hook-for-Modules" class="headerlink" title="Hook for Modules"></a>Hook for Modules</h2><p>网络模块 module 不像上一节中的 Tensor，拥有显式的变量名可以直接访问，而是被封装在神经网络中间。我们通常只能获得网络整体的输入和输出，对于夹在网络中间的模块，我们不但很难得知它输入/输出的梯度，甚至连它输入输出的数值都无法获得。除非设计网络时，在 forward 函数的返回值中包含中间 module 的输出，或者用很麻烦的办法，把网络按照 module 的名称拆分再组合，让中间层提取的 feature 暴露出来。</p><p>为了解决这个麻烦，PyTorch 设计了两种 hook：register_forward_hook 和 register_backward_hook，分别用来获取正/反向传播时，中间层模块输入和输出的 feature/gradient，大大降低了获取模型内部信息流的难度。具体用法参考<a href="https://zhuanlan.zhihu.com/p/75054200">半小时学会 PyTorch Hook</a>。</p><p>参考：<br><a href="https://zhuanlan.zhihu.com/p/73868323">https://zhuanlan.zhihu.com/p/73868323</a><br><a href="https://github.com/sksq96/pytorch-summary">https://github.com/sksq96/pytorch-summary</a><br><a href="https://oldpan.me/archives/pytorch-autograd-hook">https://oldpan.me/archives/pytorch-autograd-hook</a><br><a href="https://pytorch.org/docs/stable/search.html?q=hook&amp;check_keywords=yes&amp;area=default">https://pytorch.org/docs/stable/search.html?q=hook&amp;check_keywords=yes&amp;area=default</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;hook在维基百科中定义：钩子编程（hooking），也称作“挂钩”，是计算机程序设计术语，指通过拦截软件模块间的函数调用、消息传递、事件传递来修改或扩展操作系统、应用程序或其他软件组件的行为的各种技术。处理被拦截的函数调用、事件、消息的代码，被称为钩子（hook）。&lt;/p&gt;
&lt;p&gt;Hook 是 PyTorch 中一个十分有用的特性。利用它，我们可以&lt;strong&gt;不必改变网络输入输出的结构，方便地获取、改变网络中间层变量的值和梯度&lt;/strong&gt;。这个功能被广泛用于可视化神经网络中间层的 feature、gradient，从而诊断神经网络中可能出现的问题，分析网络有效性。本文将结合代码，由浅入深地介绍 pytorch 中 hook 的用法。本文分为三部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hook for Tensors ：针对 Tensor 的 hook&lt;/li&gt;
&lt;li&gt;Hook for Modules：针对例如 nn.Conv2dnn.Linear等网络模块的 hook&lt;/li&gt;
&lt;li&gt;Guided Backpropagation：利用 Hook 实现的一段神经网络可视化代码&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://huangzhiyuan.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="hook" scheme="http://huangzhiyuan.github.io/tags/hook/"/>
    
  </entry>
  
  <entry>
    <title>CPU threads affinity hyperthreading</title>
    <link href="http://huangzhiyuan.github.io/2020/05/07/cpu-threads-affinity-hyperthreadign/"/>
    <id>http://huangzhiyuan.github.io/2020/05/07/cpu-threads-affinity-hyperthreadign/</id>
    <published>2020-05-07T06:37:52.000Z</published>
    <updated>2020-05-07T07:23:28.000Z</updated>
    
    <content type="html"><![CDATA[<p>This post is not a tutorial on C++11 threads, but it uses them as the main threading mechanism to demonstrate its points. It starts with a basic example but then quickly veers off into the specialized area of thread affinities, hardware topologies and performance implications of hyperthreading. It does as much as feasible in portable C++, clearly marking the deviations into platform-specific calls for the really specialized stuff.</p><span id="more"></span><p><strong>CPU Socket</strong>: refers to a physical connector on a motherboard that accepts a single physical chip. It is commonplace for modern CPUs to provide multiple physical cores which are exposed to the operating system as logical CPUs that can perform parallel execution streams. This document refers to a socket and physical CPU synonymously. See Also: <a href="https://en.wikipedia.org/wiki/CPU_socket">CPU Socket</a><br><strong>NUMA</strong>: Non-niform Memory Access, refers to the commonplace architecture in which machines with multiple CPU sockets divide the memory banks of RAM into nodes on a per-socket basis. Access to memory on a socket’s “local” memory node is faster than accessing memory on a remote node tied to a different socket. See Also: <a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access">Numa</a><br><strong>CPU Core</strong>: Contemporary CPUs are likely to run multiple cores which are exposed to the underlying OS as a CPU. See Also: <a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access">Multi-core processing</a><br><strong>Hyper-threading</strong>: Intel technology to make a single core appear logically as multiple cores on the same chip to improve the performance<br><strong>Logical CPU</strong>: What the operating system sees as a CPU. The number of CPUs available to the OS is <code>num sockets</code> * <code>cores per socket</code> * <code>hyper threads per core</code>.<br><strong>Processor Affinity</strong>: Refers to the act of restricting the set of logical CPUs on which a particular program thread can execute.</p><p><strong>Benefits of thread affinitization</strong><br>Pinning a thread to a particular CPU ensures that the OS won’t reschedule the thread to another core and incur a context switch that would force the thread to reload its working state from main memory which results in jitter. When all critical threads in the processing pipeline are pinned to their own CPU and busy spinning, the OS scheduler is less likely to schedule another thread onto that core, keeping the threads’ processor caches hot.</p><h2 id="Logical-CPUs-cores-and-threads"><a href="#Logical-CPUs-cores-and-threads" class="headerlink" title="Logical CPUs, cores and threads"></a>Logical CPUs, cores and threads</h2><p>Most modern machines are multi-CPU. Whether these CPUs are divided into sockets and hardware cores depends on the machine, of course, but the OS sees a number of “logical” CPUs that can execute tasks concurrently.</p><p>The easiest way to get this information on Linux is to cat /proc/cpuinfo, which lists the system’s CPUs in order, providing some infromation about each (such as current frequency, cache size, etc). On my (8-CPU) machine:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/cpuinfo</span><br><span class="line">processor   : 0</span><br><span class="line">vendor_id   : GenuineIntel</span><br><span class="line">cpu family  : 6</span><br><span class="line">model               : 60</span><br><span class="line">model name  : Intel(R) Core(TM) i7-4771 CPU @ 3.50GHz</span><br><span class="line">[...]</span><br><span class="line">stepping    : 3</span><br><span class="line">microcode   : 0x7</span><br><span class="line">cpu MHz             : 3501.000</span><br><span class="line">cache size  : 8192 KB</span><br><span class="line">physical id : 0</span><br><span class="line">siblings    : 8</span><br><span class="line">core id             : 0</span><br><span class="line">cpu cores   : 4</span><br><span class="line">apicid              : 0</span><br><span class="line">[...]</span><br><span class="line"></span><br><span class="line">processor   : 1</span><br><span class="line">vendor_id   : GenuineIntel</span><br><span class="line">cpu family  : 6</span><br><span class="line">[...]</span><br><span class="line"></span><br><span class="line">[...]</span><br><span class="line">processor   : 7</span><br><span class="line">vendor_id   : GenuineIntel</span><br><span class="line">cpu family  : 6</span><br></pre></td></tr></table></figure><p>A summary output can be obtained from lscpu:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ lscpu</span><br><span class="line">Architecture:          x86_64</span><br><span class="line">CPU op-mode(s):        32-bit, 64-bit</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                8</span><br><span class="line">On-line CPU(s) list:   0-7</span><br><span class="line">Thread(s) per core:    2</span><br><span class="line">Core(s) per socket:    4</span><br><span class="line">Socket(s):             1</span><br><span class="line">NUMA node(s):          1</span><br><span class="line">Vendor ID:             GenuineIntel</span><br><span class="line">CPU family:            6</span><br><span class="line">Model:                 60</span><br><span class="line">Stepping:              3</span><br><span class="line">CPU MHz:               3501.000</span><br><span class="line">BogoMIPS:              6984.09</span><br><span class="line">Virtualization:        VT-x</span><br><span class="line">L1d cache:             32K</span><br><span class="line">L1i cache:             32K</span><br><span class="line">L2 cache:              256K</span><br><span class="line">L3 cache:              8192K</span><br><span class="line">NUMA node0 CPU(s):     0-7</span><br></pre></td></tr></table></figure><p>Here it’s also very easy to see that the machine has 4 cores, each having two HW threads (see hyperthreading). And yet the OS sees them as 8 “CPUs” numbered 0-7.</p><h2 id="Launching-a-thread-per-CPU"><a href="#Launching-a-thread-per-CPU" class="headerlink" title="Launching a thread per CPU"></a>Launching a thread per CPU</h2><p>The C++11 threading library gracefully made available a utility function that we can use to find out how many CPUs the machine has, so that we could plan our parallelism strategy. The function is called hardware_concurrency, and here is a complete example that uses it to launch an appropriate number of threads. The following is just a code snippet; full code samples for this post, along with a Makefile for Linux can be found in this <a href="https://github.com/eliben/code-for-blog/tree/master/2016/threads-affinity">repository</a>.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">int main(int argc, const char** argv) &#123;</span><br><span class="line">  unsigned num_cpus = std::thread::hardware_concurrency();</span><br><span class="line">  std::cout &lt;&lt; &quot;Launching &quot; &lt;&lt; num_cpus &lt;&lt; &quot; threads\n&quot;;</span><br><span class="line"></span><br><span class="line">  // A mutex ensures orderly access to std::cout from multiple threads.</span><br><span class="line">  std::mutex iomutex;</span><br><span class="line">  std::vector&lt;std::thread&gt; threads(num_cpus);</span><br><span class="line">  for (unsigned i = 0; i &lt; num_cpus; ++i) &#123;</span><br><span class="line">    threads[i] = std::thread([&amp;iomutex, i] &#123;</span><br><span class="line">      &#123;</span><br><span class="line">        // Use a lexical scope and lock_guard to safely lock the mutex only for</span><br><span class="line">        // the duration of std::cout usage.</span><br><span class="line">        std::lock_guard&lt;std::mutex&gt; iolock(iomutex);</span><br><span class="line">        std::cout &lt;&lt; &quot;Thread #&quot; &lt;&lt; i &lt;&lt; &quot; is running\n&quot;;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      // Simulate important work done by the tread by sleeping for a bit...</span><br><span class="line">      std::this_thread::sleep_for(std::chrono::milliseconds(200));</span><br><span class="line"></span><br><span class="line">    &#125;);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  for (auto&amp; t : threads) &#123;</span><br><span class="line">    t.join();</span><br><span class="line">  &#125;</span><br><span class="line">  return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>A std::thread is a thin wrapper around a platform-specific thread object; this is something we’ll use to our advantage shortly. So when we launch a std::thread, and actual OS thread is launched. This is fairly low-level thread control, but in this article I won’t detour into higher-level constructs like task-based parallelism, leaving this to some future post.</p><h2 id="Thread-affinity"><a href="#Thread-affinity" class="headerlink" title="Thread affinity"></a>Thread affinity</h2><p>So we know how to query the system for the number of CPUs it has, and how to launch any number of threads. Now let’s do something a bit more advanced.</p><p>All modern OSes support setting CPU affinity per thread. Affinity means that instead of being free to run the thread on any CPU it feels like, the OS scheduler is asked to only schedule a given thread to a single CPU or a pre-defined set of CPUs. By default, the affinity covers all logical CPUs in the system, so the OS can pick any of them for any thread, based on its scheduling considerations. In addition, the OS will sometimes migrate threads between CPUs if it makes sense to the scheduler (though it should try to miminize migrations because of the loss of warm caches on the core from which the thread was migrated). Let’s observe this in action with another code sample:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">int main(int argc, const char** argv) &#123;</span><br><span class="line">  constexpr unsigned num_threads = 4;</span><br><span class="line">  // A mutex ensures orderly access to std::cout from multiple threads.</span><br><span class="line">  std::mutex iomutex;</span><br><span class="line">  std::vector&lt;std::thread&gt; threads(num_threads);</span><br><span class="line">  for (unsigned i = 0; i &lt; num_threads; ++i) &#123;</span><br><span class="line">    threads[i] = std::thread([&amp;iomutex, i] &#123;</span><br><span class="line">      while (1) &#123;</span><br><span class="line">        &#123;</span><br><span class="line">          // Use a lexical scope and lock_guard to safely lock the mutex only</span><br><span class="line">          // for the duration of std::cout usage.</span><br><span class="line">          std::lock_guard&lt;std::mutex&gt; iolock(iomutex);</span><br><span class="line">          std::cout &lt;&lt; &quot;Thread #&quot; &lt;&lt; i &lt;&lt; &quot;: on CPU &quot; &lt;&lt; sched_getcpu() &lt;&lt; &quot;\n&quot;;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // Simulate important work done by the tread by sleeping for a bit...</span><br><span class="line">        std::this_thread::sleep_for(std::chrono::milliseconds(900));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  for (auto&amp; t : threads) &#123;</span><br><span class="line">    t.join();</span><br><span class="line">  &#125;</span><br><span class="line">  return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>This sample launches four threads that loop infinitely, sleeping and reporting which CPU they run on. The reporting is done via the <code>sched_getcpu</code> function (glibc specific - other platforms will have other APIs with similar functionality). Here’s a sample run:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ ./launch-threads-report-cpu</span><br><span class="line">Thread #0: on CPU 5</span><br><span class="line">Thread #1: on CPU 5</span><br><span class="line">Thread #2: on CPU 2</span><br><span class="line">Thread #3: on CPU 5</span><br><span class="line">Thread #0: on CPU 2</span><br><span class="line">Thread #1: on CPU 5</span><br><span class="line">Thread #2: on CPU 3</span><br><span class="line">Thread #3: on CPU 5</span><br><span class="line">Thread #0: on CPU 3</span><br><span class="line">Thread #2: on CPU 7</span><br><span class="line">Thread #1: on CPU 5</span><br><span class="line">Thread #3: on CPU 0</span><br><span class="line">Thread #0: on CPU 3</span><br><span class="line">Thread #2: on CPU 7</span><br><span class="line">Thread #1: on CPU 5</span><br><span class="line">Thread #3: on CPU 0</span><br><span class="line">Thread #0: on CPU 3</span><br><span class="line">Thread #2: on CPU 7</span><br><span class="line">Thread #1: on CPU 5</span><br><span class="line">Thread #3: on CPU 0</span><br><span class="line">^C</span><br></pre></td></tr></table></figure><p>Some observations: the threads are sometimes scheduled onto the same CPU, and sometimes onto different CPUs. Also, there’s quite a bit of migration going on. Eventually, the scheduler managed to place each thread onto a different CPU, and keep it there. Different constraints (such as system load) could result in a different scheduling, of course.</p><p>Now let’s rerun the same sample, but this time using <code>taskset</code> to restrict the affinity of the process to only two CPUs - 5 and 6:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ taskset -c 5,6 ./launch-threads-report-cpu</span><br><span class="line">Thread #0: on CPU 5</span><br><span class="line">Thread #2: on CPU 6</span><br><span class="line">Thread #1: on CPU 5</span><br><span class="line">Thread #3: on CPU 6</span><br><span class="line">Thread #0: on CPU 5</span><br><span class="line">Thread #2: on CPU 6</span><br><span class="line">Thread #1: on CPU 5</span><br><span class="line">Thread #3: on CPU 6</span><br><span class="line">Thread #0: on CPU 5</span><br><span class="line">Thread #1: on CPU 5</span><br><span class="line">Thread #2: on CPU 6</span><br><span class="line">Thread #3: on CPU 6</span><br><span class="line">Thread #0: on CPU 5</span><br><span class="line">Thread #1: on CPU 6</span><br><span class="line">Thread #2: on CPU 6</span><br><span class="line">Thread #3: on CPU 6</span><br><span class="line">^C</span><br></pre></td></tr></table></figure><p>As expected, though there’s some migration happening here, all threads remain faithfully locked to CPUs 5 and 6, as instructed.</p><h2 id="Setting-CPU-affinity-programatically"><a href="#Setting-CPU-affinity-programatically" class="headerlink" title="Setting CPU affinity programatically"></a>Setting CPU affinity programatically</h2><p>As we’ve seen earlier, command-line tools like taskset let us control the CPU affinity of a whole process. Sometimes, however, we’d like to do something more fine-grained and set the affinities of specific threads from within the program. How do we do that?</p><p>On Linux, we can use the pthread-specific <a href="http://man7.org/linux/man-pages/man3/pthread_setaffinity_np.3.html">pthread_setaffinity_np function</a>. Here’s an example that reproduces what we did before, but this time from inside the program. In fact, let’s go a bit more fancy and pin each thread to a single known CPU by setting its affinity:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">int main(int argc, const char** argv) &#123;</span><br><span class="line">  constexpr unsigned num_threads = 4;</span><br><span class="line">  // A mutex ensures orderly access to std::cout from multiple threads.</span><br><span class="line">  std::mutex iomutex;</span><br><span class="line">  std::vector&lt;std::thread&gt; threads(num_threads);</span><br><span class="line">  for (unsigned i = 0; i &lt; num_threads; ++i) &#123;</span><br><span class="line">    threads[i] = std::thread([&amp;iomutex, i] &#123;</span><br><span class="line">      std::this_thread::sleep_for(std::chrono::milliseconds(20));</span><br><span class="line">      while (1) &#123;</span><br><span class="line">        &#123;</span><br><span class="line">          // Use a lexical scope and lock_guard to safely lock the mutex only</span><br><span class="line">          // for the duration of std::cout usage.</span><br><span class="line">          std::lock_guard&lt;std::mutex&gt; iolock(iomutex);</span><br><span class="line">          std::cout &lt;&lt; &quot;Thread #&quot; &lt;&lt; i &lt;&lt; &quot;: on CPU &quot; &lt;&lt; sched_getcpu() &lt;&lt; &quot;\n&quot;;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // Simulate important work done by the tread by sleeping for a bit...</span><br><span class="line">        std::this_thread::sleep_for(std::chrono::milliseconds(900));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    // Create a cpu_set_t object representing a set of CPUs. Clear it and mark</span><br><span class="line">    // only CPU i as set.</span><br><span class="line">    cpu_set_t cpuset;</span><br><span class="line">    CPU_ZERO(&amp;cpuset);</span><br><span class="line">    CPU_SET(i, &amp;cpuset);</span><br><span class="line">    int rc = pthread_setaffinity_np(threads[i].native_handle(),</span><br><span class="line">                                    sizeof(cpu_set_t), &amp;cpuset);</span><br><span class="line">    if (rc != 0) &#123;</span><br><span class="line">      std::cerr &lt;&lt; &quot;Error calling pthread_setaffinity_np: &quot; &lt;&lt; rc &lt;&lt; &quot;\n&quot;;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  for (auto&amp; t : threads) &#123;</span><br><span class="line">    t.join();</span><br><span class="line">  &#125;</span><br><span class="line">  return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Note how we use the native_handle method discussed earlier in order to pass the underlying native handle to the pthread call (it takes a pthread_t ID as its first argument). The output of this program on my machine is:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ ./set-affinity</span><br><span class="line">Thread #0: on CPU 0</span><br><span class="line">Thread #1: on CPU 1</span><br><span class="line">Thread #2: on CPU 2</span><br><span class="line">Thread #3: on CPU 3</span><br><span class="line">Thread #0: on CPU 0</span><br><span class="line">Thread #1: on CPU 1</span><br><span class="line">Thread #2: on CPU 2</span><br><span class="line">Thread #3: on CPU 3</span><br><span class="line">Thread #0: on CPU 0</span><br><span class="line">Thread #1: on CPU 1</span><br><span class="line">Thread #2: on CPU 2</span><br><span class="line">Thread #3: on CPU 3</span><br><span class="line">^C</span><br></pre></td></tr></table></figure><p>The threads get pinned to single CPUs exactly as requested.</p><p>refers:<br><a href="https://pages.tacc.utexas.edu/~eijkhout/pcse/html/omp-affinity.html#Threadbinding">https://pages.tacc.utexas.edu/~eijkhout/pcse/html/omp-affinity.html#Threadbinding</a><br><a href="https://docs.neeveresearch.com/display/TALONDOC/Tuning+Thread+Affinitization+and+NUMA">https://docs.neeveresearch.com/display/TALONDOC/Tuning+Thread+Affinitization+and+NUMA</a><br><a href="https://eli.thegreenplace.net/2016/c11-threads-affinity-and-hyperthreading/">https://eli.thegreenplace.net/2016/c11-threads-affinity-and-hyperthreading/</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;This post is not a tutorial on C++11 threads, but it uses them as the main threading mechanism to demonstrate its points. It starts with a basic example but then quickly veers off into the specialized area of thread affinities, hardware topologies and performance implications of hyperthreading. It does as much as feasible in portable C++, clearly marking the deviations into platform-specific calls for the really specialized stuff.&lt;/p&gt;</summary>
    
    
    
    <category term="技术总结" scheme="http://huangzhiyuan.github.io/categories/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="affinity" scheme="http://huangzhiyuan.github.io/tags/affinity/"/>
    
    <category term="hyperthreading" scheme="http://huangzhiyuan.github.io/tags/hyperthreading/"/>
    
    <category term="NUMA" scheme="http://huangzhiyuan.github.io/tags/NUMA/"/>
    
  </entry>
  
</feed>
